I0223 20:38:37.032350      23 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-277121970
I0223 20:38:37.032374      23 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0223 20:38:37.032610      23 e2e.go:109] Starting e2e run "a24d4a15-af2e-4544-8538-91d24c5862e9" on Ginkgo node 1
{"msg":"Test Suite starting","total":280,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1582490316 - Will randomize all specs
Will run 280 of 4843 specs

Feb 23 20:38:37.040: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 20:38:37.042: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 23 20:38:37.051: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 23 20:38:37.076: INFO: 16 / 16 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 23 20:38:37.076: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Feb 23 20:38:37.076: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 23 20:38:37.082: INFO: e2e test version: v1.17.3
Feb 23 20:38:37.083: INFO: kube-apiserver version: v1.17.3
Feb 23 20:38:37.083: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 20:38:37.087: INFO: Cluster IP family: ipv4
SSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:38:37.087: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename crd-webhook
Feb 23 20:38:37.117: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 23 20:38:37.569: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 23 20:38:39.585: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718087118, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718087118, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718087118, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718087118, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 20:38:41.589: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718087118, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718087118, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718087118, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718087118, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 20:38:43.589: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718087118, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718087118, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718087118, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718087118, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 20:38:46.609: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:38:46.616: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:38:52.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6709" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

â€¢ [SLOW TEST:15.721 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":280,"completed":1,"skipped":5,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:38:52.808: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Feb 23 20:38:52.896: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Feb 23 20:38:52.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 create -f - --namespace=kubectl-6567'
Feb 23 20:38:53.764: INFO: stderr: ""
Feb 23 20:38:53.764: INFO: stdout: "service/agnhost-slave created\n"
Feb 23 20:38:53.764: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Feb 23 20:38:53.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 create -f - --namespace=kubectl-6567'
Feb 23 20:38:54.017: INFO: stderr: ""
Feb 23 20:38:54.017: INFO: stdout: "service/agnhost-master created\n"
Feb 23 20:38:54.018: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 23 20:38:54.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 create -f - --namespace=kubectl-6567'
Feb 23 20:38:54.199: INFO: stderr: ""
Feb 23 20:38:54.199: INFO: stdout: "service/frontend created\n"
Feb 23 20:38:54.199: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 23 20:38:54.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 create -f - --namespace=kubectl-6567'
Feb 23 20:38:54.360: INFO: stderr: ""
Feb 23 20:38:54.360: INFO: stdout: "deployment.apps/frontend created\n"
Feb 23 20:38:54.361: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 23 20:38:54.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 create -f - --namespace=kubectl-6567'
Feb 23 20:38:54.540: INFO: stderr: ""
Feb 23 20:38:54.540: INFO: stdout: "deployment.apps/agnhost-master created\n"
Feb 23 20:38:54.540: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 23 20:38:54.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 create -f - --namespace=kubectl-6567'
Feb 23 20:38:54.678: INFO: stderr: ""
Feb 23 20:38:54.678: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Feb 23 20:38:54.678: INFO: Waiting for all frontend pods to be Running.
Feb 23 20:39:04.731: INFO: Waiting for frontend to serve content.
Feb 23 20:39:04.738: INFO: Trying to add a new entry to the guestbook.
Feb 23 20:39:04.747: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 23 20:39:09.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete --grace-period=0 --force -f - --namespace=kubectl-6567'
Feb 23 20:39:09.844: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 20:39:09.844: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 23 20:39:09.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete --grace-period=0 --force -f - --namespace=kubectl-6567'
Feb 23 20:39:09.938: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 20:39:09.938: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 23 20:39:09.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete --grace-period=0 --force -f - --namespace=kubectl-6567'
Feb 23 20:39:10.019: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 20:39:10.019: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 23 20:39:10.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete --grace-period=0 --force -f - --namespace=kubectl-6567'
Feb 23 20:39:10.102: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 20:39:10.102: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 23 20:39:10.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete --grace-period=0 --force -f - --namespace=kubectl-6567'
Feb 23 20:39:10.221: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 20:39:10.221: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 23 20:39:10.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete --grace-period=0 --force -f - --namespace=kubectl-6567'
Feb 23 20:39:10.310: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 20:39:10.310: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:39:10.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6567" for this suite.

â€¢ [SLOW TEST:17.525 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:386
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":280,"completed":2,"skipped":13,"failed":0}
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:39:10.333: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 23 20:39:10.412: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9526 /api/v1/namespaces/watch-9526/configmaps/e2e-watch-test-configmap-a bb454e92-b9f5-4846-b29d-6901ba7cb6a8 3778 0 2020-02-23 20:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 23 20:39:10.412: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9526 /api/v1/namespaces/watch-9526/configmaps/e2e-watch-test-configmap-a bb454e92-b9f5-4846-b29d-6901ba7cb6a8 3778 0 2020-02-23 20:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 23 20:39:20.423: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9526 /api/v1/namespaces/watch-9526/configmaps/e2e-watch-test-configmap-a bb454e92-b9f5-4846-b29d-6901ba7cb6a8 3889 0 2020-02-23 20:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 23 20:39:20.423: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9526 /api/v1/namespaces/watch-9526/configmaps/e2e-watch-test-configmap-a bb454e92-b9f5-4846-b29d-6901ba7cb6a8 3889 0 2020-02-23 20:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 23 20:39:30.433: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9526 /api/v1/namespaces/watch-9526/configmaps/e2e-watch-test-configmap-a bb454e92-b9f5-4846-b29d-6901ba7cb6a8 3924 0 2020-02-23 20:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 23 20:39:30.433: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9526 /api/v1/namespaces/watch-9526/configmaps/e2e-watch-test-configmap-a bb454e92-b9f5-4846-b29d-6901ba7cb6a8 3924 0 2020-02-23 20:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 23 20:39:40.447: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9526 /api/v1/namespaces/watch-9526/configmaps/e2e-watch-test-configmap-a bb454e92-b9f5-4846-b29d-6901ba7cb6a8 3955 0 2020-02-23 20:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 23 20:39:40.447: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9526 /api/v1/namespaces/watch-9526/configmaps/e2e-watch-test-configmap-a bb454e92-b9f5-4846-b29d-6901ba7cb6a8 3955 0 2020-02-23 20:39:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 23 20:39:50.457: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9526 /api/v1/namespaces/watch-9526/configmaps/e2e-watch-test-configmap-b 8376bc6e-7fd6-41b3-aaca-c6d3818ba705 3986 0 2020-02-23 20:39:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 23 20:39:50.457: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9526 /api/v1/namespaces/watch-9526/configmaps/e2e-watch-test-configmap-b 8376bc6e-7fd6-41b3-aaca-c6d3818ba705 3986 0 2020-02-23 20:39:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 23 20:40:00.470: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9526 /api/v1/namespaces/watch-9526/configmaps/e2e-watch-test-configmap-b 8376bc6e-7fd6-41b3-aaca-c6d3818ba705 4018 0 2020-02-23 20:39:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 23 20:40:00.470: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9526 /api/v1/namespaces/watch-9526/configmaps/e2e-watch-test-configmap-b 8376bc6e-7fd6-41b3-aaca-c6d3818ba705 4018 0 2020-02-23 20:39:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:40:10.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9526" for this suite.

â€¢ [SLOW TEST:60.153 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":280,"completed":3,"skipped":13,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:40:10.487: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:40:10.520: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:40:16.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1218" for this suite.

â€¢ [SLOW TEST:6.081 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":280,"completed":4,"skipped":42,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:40:16.568: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1464
STEP: creating an pod
Feb 23 20:40:16.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-6512 -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 23 20:40:16.670: INFO: stderr: ""
Feb 23 20:40:16.670: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Feb 23 20:40:16.670: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 23 20:40:16.670: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6512" to be "running and ready, or succeeded"
Feb 23 20:40:16.673: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.404602ms
Feb 23 20:40:18.680: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.0098851s
Feb 23 20:40:18.680: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 23 20:40:18.680: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Feb 23 20:40:18.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 logs logs-generator logs-generator --namespace=kubectl-6512'
Feb 23 20:40:18.751: INFO: stderr: ""
Feb 23 20:40:18.751: INFO: stdout: "I0223 20:40:17.499510       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/zfls 339\nI0223 20:40:17.699934       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/fdbw 240\nI0223 20:40:17.900869       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/ntr 340\nI0223 20:40:18.100533       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/n5sf 296\nI0223 20:40:18.311807       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/w5w 571\nI0223 20:40:18.500076       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/g8fg 387\nI0223 20:40:18.699739       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/b66k 348\n"
Feb 23 20:40:20.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 logs logs-generator logs-generator --namespace=kubectl-6512'
Feb 23 20:40:20.829: INFO: stderr: ""
Feb 23 20:40:20.829: INFO: stdout: "I0223 20:40:17.499510       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/zfls 339\nI0223 20:40:17.699934       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/fdbw 240\nI0223 20:40:17.900869       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/ntr 340\nI0223 20:40:18.100533       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/n5sf 296\nI0223 20:40:18.311807       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/w5w 571\nI0223 20:40:18.500076       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/g8fg 387\nI0223 20:40:18.699739       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/b66k 348\nI0223 20:40:18.899918       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/kqwg 557\nI0223 20:40:19.123808       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/hgn 263\nI0223 20:40:19.299768       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/djs 569\nI0223 20:40:19.499699       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/kbhx 337\nI0223 20:40:19.699704       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/bw6r 451\nI0223 20:40:19.899780       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/ctb 354\nI0223 20:40:20.103034       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/bl8 576\nI0223 20:40:20.299647       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/rb6n 513\nI0223 20:40:20.499681       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/cc7 479\nI0223 20:40:20.700802       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/mfv 341\n"
STEP: limiting log lines
Feb 23 20:40:20.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 logs logs-generator logs-generator --namespace=kubectl-6512 --tail=1'
Feb 23 20:40:20.897: INFO: stderr: ""
Feb 23 20:40:20.897: INFO: stdout: "I0223 20:40:20.700802       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/mfv 341\n"
Feb 23 20:40:20.897: INFO: got output "I0223 20:40:20.700802       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/mfv 341\n"
STEP: limiting log bytes
Feb 23 20:40:20.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 logs logs-generator logs-generator --namespace=kubectl-6512 --limit-bytes=1'
Feb 23 20:40:20.957: INFO: stderr: ""
Feb 23 20:40:20.957: INFO: stdout: "I"
Feb 23 20:40:20.957: INFO: got output "I"
STEP: exposing timestamps
Feb 23 20:40:20.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 logs logs-generator logs-generator --namespace=kubectl-6512 --tail=1 --timestamps'
Feb 23 20:40:21.024: INFO: stderr: ""
Feb 23 20:40:21.024: INFO: stdout: "2020-02-23T20:40:20.899821082Z I0223 20:40:20.899727       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/p9bb 294\n"
Feb 23 20:40:21.024: INFO: got output "2020-02-23T20:40:20.899821082Z I0223 20:40:20.899727       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/p9bb 294\n"
STEP: restricting to a time range
Feb 23 20:40:23.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 logs logs-generator logs-generator --namespace=kubectl-6512 --since=1s'
Feb 23 20:40:23.587: INFO: stderr: ""
Feb 23 20:40:23.587: INFO: stdout: "I0223 20:40:22.699672       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/kxp 279\nI0223 20:40:22.902636       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/d76z 394\nI0223 20:40:23.100050       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/tck 374\nI0223 20:40:23.303001       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/default/pods/bc7t 391\nI0223 20:40:23.500362       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/s7n 312\n"
Feb 23 20:40:23.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 logs logs-generator logs-generator --namespace=kubectl-6512 --since=24h'
Feb 23 20:40:23.652: INFO: stderr: ""
Feb 23 20:40:23.652: INFO: stdout: "I0223 20:40:17.499510       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/zfls 339\nI0223 20:40:17.699934       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/fdbw 240\nI0223 20:40:17.900869       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/ntr 340\nI0223 20:40:18.100533       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/n5sf 296\nI0223 20:40:18.311807       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/w5w 571\nI0223 20:40:18.500076       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/g8fg 387\nI0223 20:40:18.699739       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/b66k 348\nI0223 20:40:18.899918       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/kqwg 557\nI0223 20:40:19.123808       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/hgn 263\nI0223 20:40:19.299768       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/djs 569\nI0223 20:40:19.499699       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/kbhx 337\nI0223 20:40:19.699704       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/bw6r 451\nI0223 20:40:19.899780       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/ctb 354\nI0223 20:40:20.103034       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/bl8 576\nI0223 20:40:20.299647       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/rb6n 513\nI0223 20:40:20.499681       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/cc7 479\nI0223 20:40:20.700802       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/mfv 341\nI0223 20:40:20.899727       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/p9bb 294\nI0223 20:40:21.099961       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/28s 244\nI0223 20:40:21.300356       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/pvqp 392\nI0223 20:40:21.499785       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/d8t 536\nI0223 20:40:21.699887       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/lxcr 348\nI0223 20:40:21.900073       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/r88 597\nI0223 20:40:22.099707       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/mkz 298\nI0223 20:40:22.301214       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/vdqc 251\nI0223 20:40:22.500455       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/vsnf 245\nI0223 20:40:22.699672       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/kxp 279\nI0223 20:40:22.902636       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/d76z 394\nI0223 20:40:23.100050       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/tck 374\nI0223 20:40:23.303001       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/default/pods/bc7t 391\nI0223 20:40:23.500362       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/s7n 312\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1470
Feb 23 20:40:23.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete pod logs-generator --namespace=kubectl-6512'
Feb 23 20:40:27.717: INFO: stderr: ""
Feb 23 20:40:27.717: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:40:27.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6512" for this suite.

â€¢ [SLOW TEST:11.157 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":280,"completed":5,"skipped":48,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:40:27.725: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:40:27.752: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:40:33.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7325" for this suite.

â€¢ [SLOW TEST:5.585 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":280,"completed":6,"skipped":74,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:40:33.310: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Feb 23 20:40:33.351: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 20:40:40.746: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:40:57.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3546" for this suite.

â€¢ [SLOW TEST:24.347 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":280,"completed":7,"skipped":110,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:40:57.658: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 23 20:40:57.705: INFO: Waiting up to 5m0s for pod "pod-62997dfa-41ec-4983-abbe-9c483359c6fb" in namespace "emptydir-9184" to be "success or failure"
Feb 23 20:40:57.710: INFO: Pod "pod-62997dfa-41ec-4983-abbe-9c483359c6fb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.01998ms
Feb 23 20:40:59.715: INFO: Pod "pod-62997dfa-41ec-4983-abbe-9c483359c6fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009783892s
Feb 23 20:41:01.719: INFO: Pod "pod-62997dfa-41ec-4983-abbe-9c483359c6fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014016884s
STEP: Saw pod success
Feb 23 20:41:01.719: INFO: Pod "pod-62997dfa-41ec-4983-abbe-9c483359c6fb" satisfied condition "success or failure"
Feb 23 20:41:01.723: INFO: Trying to get logs from node worker01 pod pod-62997dfa-41ec-4983-abbe-9c483359c6fb container test-container: <nil>
STEP: delete the pod
Feb 23 20:41:01.746: INFO: Waiting for pod pod-62997dfa-41ec-4983-abbe-9c483359c6fb to disappear
Feb 23 20:41:01.748: INFO: Pod pod-62997dfa-41ec-4983-abbe-9c483359c6fb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:41:01.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9184" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":8,"skipped":114,"failed":0}
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:41:01.758: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 23 20:41:01.788: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:41:05.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5476" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":280,"completed":9,"skipped":117,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:41:05.057: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:41:05.085: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 23 20:41:06.119: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:41:06.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4701" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":280,"completed":10,"skipped":127,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:41:06.141: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:41:12.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6056" for this suite.

â€¢ [SLOW TEST:6.071 seconds]
[sig-apps] Job
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":280,"completed":11,"skipped":164,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:41:12.211: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-e73f07bd-ff9b-438e-b0b6-649307f3ba0a
STEP: Creating a pod to test consume secrets
Feb 23 20:41:12.293: INFO: Waiting up to 5m0s for pod "pod-secrets-20709891-e5ce-4754-8186-4c12ab79f73c" in namespace "secrets-4838" to be "success or failure"
Feb 23 20:41:12.301: INFO: Pod "pod-secrets-20709891-e5ce-4754-8186-4c12ab79f73c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.56636ms
Feb 23 20:41:14.317: INFO: Pod "pod-secrets-20709891-e5ce-4754-8186-4c12ab79f73c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024408314s
Feb 23 20:41:16.321: INFO: Pod "pod-secrets-20709891-e5ce-4754-8186-4c12ab79f73c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028078468s
Feb 23 20:41:18.324: INFO: Pod "pod-secrets-20709891-e5ce-4754-8186-4c12ab79f73c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031396745s
Feb 23 20:41:20.328: INFO: Pod "pod-secrets-20709891-e5ce-4754-8186-4c12ab79f73c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.034968384s
Feb 23 20:41:22.332: INFO: Pod "pod-secrets-20709891-e5ce-4754-8186-4c12ab79f73c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.039170293s
Feb 23 20:41:24.337: INFO: Pod "pod-secrets-20709891-e5ce-4754-8186-4c12ab79f73c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.044289603s
STEP: Saw pod success
Feb 23 20:41:24.337: INFO: Pod "pod-secrets-20709891-e5ce-4754-8186-4c12ab79f73c" satisfied condition "success or failure"
Feb 23 20:41:24.340: INFO: Trying to get logs from node worker01 pod pod-secrets-20709891-e5ce-4754-8186-4c12ab79f73c container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 20:41:24.356: INFO: Waiting for pod pod-secrets-20709891-e5ce-4754-8186-4c12ab79f73c to disappear
Feb 23 20:41:24.358: INFO: Pod pod-secrets-20709891-e5ce-4754-8186-4c12ab79f73c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:41:24.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4838" for this suite.

â€¢ [SLOW TEST:12.159 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":12,"skipped":196,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:41:24.371: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-6e1c1d06-a16c-4c52-95e1-ebc6f6ed4b7a
STEP: Creating secret with name secret-projected-all-test-volume-8593205e-5f91-4870-bd57-33babea1db8e
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 23 20:41:24.426: INFO: Waiting up to 5m0s for pod "projected-volume-c5814221-8153-4ef6-9b85-1882902f5622" in namespace "projected-1908" to be "success or failure"
Feb 23 20:41:24.432: INFO: Pod "projected-volume-c5814221-8153-4ef6-9b85-1882902f5622": Phase="Pending", Reason="", readiness=false. Elapsed: 5.436472ms
Feb 23 20:41:26.435: INFO: Pod "projected-volume-c5814221-8153-4ef6-9b85-1882902f5622": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008819605s
STEP: Saw pod success
Feb 23 20:41:26.435: INFO: Pod "projected-volume-c5814221-8153-4ef6-9b85-1882902f5622" satisfied condition "success or failure"
Feb 23 20:41:26.438: INFO: Trying to get logs from node worker01 pod projected-volume-c5814221-8153-4ef6-9b85-1882902f5622 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 23 20:41:26.463: INFO: Waiting for pod projected-volume-c5814221-8153-4ef6-9b85-1882902f5622 to disappear
Feb 23 20:41:26.465: INFO: Pod projected-volume-c5814221-8153-4ef6-9b85-1882902f5622 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:41:26.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1908" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":280,"completed":13,"skipped":209,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:41:26.473: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 23 20:41:26.763: INFO: Pod name wrapped-volume-race-b21ae240-13c1-4799-914e-cea063c936e1: Found 3 pods out of 5
Feb 23 20:41:31.768: INFO: Pod name wrapped-volume-race-b21ae240-13c1-4799-914e-cea063c936e1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b21ae240-13c1-4799-914e-cea063c936e1 in namespace emptydir-wrapper-3192, will wait for the garbage collector to delete the pods
Feb 23 20:41:41.864: INFO: Deleting ReplicationController wrapped-volume-race-b21ae240-13c1-4799-914e-cea063c936e1 took: 13.140257ms
Feb 23 20:41:41.964: INFO: Terminating ReplicationController wrapped-volume-race-b21ae240-13c1-4799-914e-cea063c936e1 pods took: 100.492058ms
STEP: Creating RC which spawns configmap-volume pods
Feb 23 20:41:47.879: INFO: Pod name wrapped-volume-race-d001cb7d-df27-4296-b486-5b28ad9126e9: Found 0 pods out of 5
Feb 23 20:41:52.887: INFO: Pod name wrapped-volume-race-d001cb7d-df27-4296-b486-5b28ad9126e9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d001cb7d-df27-4296-b486-5b28ad9126e9 in namespace emptydir-wrapper-3192, will wait for the garbage collector to delete the pods
Feb 23 20:42:02.970: INFO: Deleting ReplicationController wrapped-volume-race-d001cb7d-df27-4296-b486-5b28ad9126e9 took: 11.87944ms
Feb 23 20:42:03.870: INFO: Terminating ReplicationController wrapped-volume-race-d001cb7d-df27-4296-b486-5b28ad9126e9 pods took: 900.370924ms
STEP: Creating RC which spawns configmap-volume pods
Feb 23 20:42:11.189: INFO: Pod name wrapped-volume-race-bfcd54b2-0415-4dbd-b2ea-0b0653bf0c13: Found 0 pods out of 5
Feb 23 20:42:16.195: INFO: Pod name wrapped-volume-race-bfcd54b2-0415-4dbd-b2ea-0b0653bf0c13: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-bfcd54b2-0415-4dbd-b2ea-0b0653bf0c13 in namespace emptydir-wrapper-3192, will wait for the garbage collector to delete the pods
Feb 23 20:42:28.294: INFO: Deleting ReplicationController wrapped-volume-race-bfcd54b2-0415-4dbd-b2ea-0b0653bf0c13 took: 11.511715ms
Feb 23 20:42:29.196: INFO: Terminating ReplicationController wrapped-volume-race-bfcd54b2-0415-4dbd-b2ea-0b0653bf0c13 pods took: 901.817491ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:42:40.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3192" for this suite.

â€¢ [SLOW TEST:74.359 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":280,"completed":14,"skipped":221,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:42:40.837: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:42:40.865: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:43:42.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9223" for this suite.

â€¢ [SLOW TEST:61.352 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":280,"completed":15,"skipped":241,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:43:42.192: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 23 20:43:42.305: INFO: Waiting up to 5m0s for pod "pod-53c8a80b-a7af-48e2-b8e0-55622fe06e05" in namespace "emptydir-4172" to be "success or failure"
Feb 23 20:43:42.309: INFO: Pod "pod-53c8a80b-a7af-48e2-b8e0-55622fe06e05": Phase="Pending", Reason="", readiness=false. Elapsed: 4.360549ms
Feb 23 20:43:44.312: INFO: Pod "pod-53c8a80b-a7af-48e2-b8e0-55622fe06e05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007101704s
STEP: Saw pod success
Feb 23 20:43:44.313: INFO: Pod "pod-53c8a80b-a7af-48e2-b8e0-55622fe06e05" satisfied condition "success or failure"
Feb 23 20:43:44.317: INFO: Trying to get logs from node worker01 pod pod-53c8a80b-a7af-48e2-b8e0-55622fe06e05 container test-container: <nil>
STEP: delete the pod
Feb 23 20:43:44.339: INFO: Waiting for pod pod-53c8a80b-a7af-48e2-b8e0-55622fe06e05 to disappear
Feb 23 20:43:44.344: INFO: Pod pod-53c8a80b-a7af-48e2-b8e0-55622fe06e05 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:43:44.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4172" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":16,"skipped":258,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:43:44.361: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 23 20:43:44.411: INFO: Waiting up to 5m0s for pod "pod-d968257b-49bc-49de-b1ee-264e59c62c7b" in namespace "emptydir-9140" to be "success or failure"
Feb 23 20:43:44.415: INFO: Pod "pod-d968257b-49bc-49de-b1ee-264e59c62c7b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007308ms
Feb 23 20:43:46.421: INFO: Pod "pod-d968257b-49bc-49de-b1ee-264e59c62c7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010507157s
STEP: Saw pod success
Feb 23 20:43:46.421: INFO: Pod "pod-d968257b-49bc-49de-b1ee-264e59c62c7b" satisfied condition "success or failure"
Feb 23 20:43:46.425: INFO: Trying to get logs from node worker01 pod pod-d968257b-49bc-49de-b1ee-264e59c62c7b container test-container: <nil>
STEP: delete the pod
Feb 23 20:43:46.440: INFO: Waiting for pod pod-d968257b-49bc-49de-b1ee-264e59c62c7b to disappear
Feb 23 20:43:46.445: INFO: Pod pod-d968257b-49bc-49de-b1ee-264e59c62c7b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:43:46.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9140" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":17,"skipped":273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:43:46.459: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 23 20:43:46.494: INFO: Waiting up to 5m0s for pod "pod-4888e95a-67c2-44b4-8c55-48ba46341b27" in namespace "emptydir-6932" to be "success or failure"
Feb 23 20:43:46.502: INFO: Pod "pod-4888e95a-67c2-44b4-8c55-48ba46341b27": Phase="Pending", Reason="", readiness=false. Elapsed: 7.459341ms
Feb 23 20:43:48.508: INFO: Pod "pod-4888e95a-67c2-44b4-8c55-48ba46341b27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013327293s
STEP: Saw pod success
Feb 23 20:43:48.508: INFO: Pod "pod-4888e95a-67c2-44b4-8c55-48ba46341b27" satisfied condition "success or failure"
Feb 23 20:43:48.512: INFO: Trying to get logs from node worker01 pod pod-4888e95a-67c2-44b4-8c55-48ba46341b27 container test-container: <nil>
STEP: delete the pod
Feb 23 20:43:48.530: INFO: Waiting for pod pod-4888e95a-67c2-44b4-8c55-48ba46341b27 to disappear
Feb 23 20:43:48.534: INFO: Pod pod-4888e95a-67c2-44b4-8c55-48ba46341b27 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:43:48.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6932" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":18,"skipped":321,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:43:48.542: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3946.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3946.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 20:44:12.632: INFO: DNS probes using dns-3946/dns-test-71e242de-67f9-4a0b-aed9-1edef62da17f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:44:12.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3946" for this suite.

â€¢ [SLOW TEST:24.123 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":280,"completed":19,"skipped":329,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:44:12.664: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-3ff0fe32-c788-44d1-a3a1-46433e2f2edb
STEP: Creating a pod to test consume secrets
Feb 23 20:44:12.714: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-aa3c0c55-bd15-4739-8878-85739eea7633" in namespace "projected-7688" to be "success or failure"
Feb 23 20:44:12.716: INFO: Pod "pod-projected-secrets-aa3c0c55-bd15-4739-8878-85739eea7633": Phase="Pending", Reason="", readiness=false. Elapsed: 2.60839ms
Feb 23 20:44:14.721: INFO: Pod "pod-projected-secrets-aa3c0c55-bd15-4739-8878-85739eea7633": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007240911s
STEP: Saw pod success
Feb 23 20:44:14.721: INFO: Pod "pod-projected-secrets-aa3c0c55-bd15-4739-8878-85739eea7633" satisfied condition "success or failure"
Feb 23 20:44:14.725: INFO: Trying to get logs from node worker01 pod pod-projected-secrets-aa3c0c55-bd15-4739-8878-85739eea7633 container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 20:44:14.749: INFO: Waiting for pod pod-projected-secrets-aa3c0c55-bd15-4739-8878-85739eea7633 to disappear
Feb 23 20:44:14.753: INFO: Pod pod-projected-secrets-aa3c0c55-bd15-4739-8878-85739eea7633 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:44:14.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7688" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":20,"skipped":336,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:44:14.765: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 23 20:44:23.356: INFO: Successfully updated pod "pod-update-1e212de9-df0a-4047-84e9-9d44dd1488a0"
STEP: verifying the updated pod is in kubernetes
Feb 23 20:44:23.366: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:44:23.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-204" for this suite.

â€¢ [SLOW TEST:8.610 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":280,"completed":21,"skipped":364,"failed":0}
S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:44:23.376: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:44:23.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2122" for this suite.
â€¢{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":280,"completed":22,"skipped":365,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:44:23.447: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:44:23.490: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 23 20:44:23.504: INFO: Number of nodes with available pods: 0
Feb 23 20:44:23.504: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 23 20:44:23.519: INFO: Number of nodes with available pods: 0
Feb 23 20:44:23.519: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:24.522: INFO: Number of nodes with available pods: 0
Feb 23 20:44:24.523: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:25.525: INFO: Number of nodes with available pods: 1
Feb 23 20:44:25.525: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 23 20:44:25.547: INFO: Number of nodes with available pods: 1
Feb 23 20:44:25.547: INFO: Number of running nodes: 0, number of available pods: 1
Feb 23 20:44:26.552: INFO: Number of nodes with available pods: 0
Feb 23 20:44:26.552: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 23 20:44:26.567: INFO: Number of nodes with available pods: 0
Feb 23 20:44:26.567: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:27.574: INFO: Number of nodes with available pods: 0
Feb 23 20:44:27.574: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:28.579: INFO: Number of nodes with available pods: 0
Feb 23 20:44:28.579: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:29.574: INFO: Number of nodes with available pods: 0
Feb 23 20:44:29.574: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:30.576: INFO: Number of nodes with available pods: 0
Feb 23 20:44:30.577: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:31.573: INFO: Number of nodes with available pods: 0
Feb 23 20:44:31.573: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:32.576: INFO: Number of nodes with available pods: 0
Feb 23 20:44:32.577: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:33.582: INFO: Number of nodes with available pods: 0
Feb 23 20:44:33.582: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:34.570: INFO: Number of nodes with available pods: 0
Feb 23 20:44:34.570: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:35.572: INFO: Number of nodes with available pods: 0
Feb 23 20:44:35.572: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:36.573: INFO: Number of nodes with available pods: 0
Feb 23 20:44:36.573: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:37.572: INFO: Number of nodes with available pods: 0
Feb 23 20:44:37.572: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:38.570: INFO: Number of nodes with available pods: 0
Feb 23 20:44:38.570: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:39.570: INFO: Number of nodes with available pods: 0
Feb 23 20:44:39.570: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:44:40.573: INFO: Number of nodes with available pods: 1
Feb 23 20:44:40.573: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4918, will wait for the garbage collector to delete the pods
Feb 23 20:44:40.646: INFO: Deleting DaemonSet.extensions daemon-set took: 11.575922ms
Feb 23 20:44:41.547: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.321004ms
Feb 23 20:44:43.849: INFO: Number of nodes with available pods: 0
Feb 23 20:44:43.849: INFO: Number of running nodes: 0, number of available pods: 0
Feb 23 20:44:43.852: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4918/daemonsets","resourceVersion":"6670"},"items":null}

Feb 23 20:44:43.855: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4918/pods","resourceVersion":"6670"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:44:43.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4918" for this suite.

â€¢ [SLOW TEST:20.432 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":280,"completed":23,"skipped":368,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:44:43.879: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-883c54a7-b8ef-4019-801a-55c6d6bbf67a
STEP: Creating a pod to test consume secrets
Feb 23 20:44:43.919: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-68e9b56d-5165-4d8a-a0c9-28f15a8a8918" in namespace "projected-7407" to be "success or failure"
Feb 23 20:44:43.933: INFO: Pod "pod-projected-secrets-68e9b56d-5165-4d8a-a0c9-28f15a8a8918": Phase="Pending", Reason="", readiness=false. Elapsed: 13.649099ms
Feb 23 20:44:45.936: INFO: Pod "pod-projected-secrets-68e9b56d-5165-4d8a-a0c9-28f15a8a8918": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016451455s
STEP: Saw pod success
Feb 23 20:44:45.936: INFO: Pod "pod-projected-secrets-68e9b56d-5165-4d8a-a0c9-28f15a8a8918" satisfied condition "success or failure"
Feb 23 20:44:45.940: INFO: Trying to get logs from node worker01 pod pod-projected-secrets-68e9b56d-5165-4d8a-a0c9-28f15a8a8918 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 23 20:44:45.959: INFO: Waiting for pod pod-projected-secrets-68e9b56d-5165-4d8a-a0c9-28f15a8a8918 to disappear
Feb 23 20:44:45.962: INFO: Pod pod-projected-secrets-68e9b56d-5165-4d8a-a0c9-28f15a8a8918 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:44:45.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7407" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":24,"skipped":368,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:44:45.971: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-943.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-943.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-943.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-943.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 20:44:48.044: INFO: DNS probes using dns-test-4385e50b-7982-4bba-8705-e60d966554f2 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-943.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-943.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-943.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-943.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 20:44:50.109: INFO: DNS probes using dns-test-bd8c9e83-8839-4625-b464-b6b45e514630 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-943.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-943.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-943.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-943.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 20:44:52.210: INFO: DNS probes using dns-test-095b2546-e3fd-41cb-9e43-705c882071cc succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:44:52.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-943" for this suite.

â€¢ [SLOW TEST:6.316 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":280,"completed":25,"skipped":378,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:44:52.288: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-46a2e8da-2ef7-4c3e-8662-5bbce4b30fb5
STEP: Creating a pod to test consume secrets
Feb 23 20:44:52.345: INFO: Waiting up to 5m0s for pod "pod-secrets-760b8b61-9a2b-43b6-93df-94609249a2de" in namespace "secrets-9229" to be "success or failure"
Feb 23 20:44:52.349: INFO: Pod "pod-secrets-760b8b61-9a2b-43b6-93df-94609249a2de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.420262ms
Feb 23 20:44:54.352: INFO: Pod "pod-secrets-760b8b61-9a2b-43b6-93df-94609249a2de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007209396s
STEP: Saw pod success
Feb 23 20:44:54.352: INFO: Pod "pod-secrets-760b8b61-9a2b-43b6-93df-94609249a2de" satisfied condition "success or failure"
Feb 23 20:44:54.357: INFO: Trying to get logs from node worker01 pod pod-secrets-760b8b61-9a2b-43b6-93df-94609249a2de container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 20:44:54.391: INFO: Waiting for pod pod-secrets-760b8b61-9a2b-43b6-93df-94609249a2de to disappear
Feb 23 20:44:54.393: INFO: Pod pod-secrets-760b8b61-9a2b-43b6-93df-94609249a2de no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:44:54.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9229" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":26,"skipped":393,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:44:54.401: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 23 20:44:56.975: INFO: Successfully updated pod "annotationupdatebbc02010-c90a-4449-b46c-599e0d177196"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:45:01.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2758" for this suite.

â€¢ [SLOW TEST:6.616 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":27,"skipped":454,"failed":0}
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:45:01.017: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 23 20:45:05.098: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 20:45:05.105: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 20:45:07.106: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 20:45:07.109: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 20:45:09.105: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 20:45:09.109: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 23 20:45:11.107: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 23 20:45:11.115: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:45:11.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4744" for this suite.

â€¢ [SLOW TEST:10.110 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":280,"completed":28,"skipped":457,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:45:11.127: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-fa64ad9e-99bc-47e2-928e-465e37430365
STEP: Creating a pod to test consume configMaps
Feb 23 20:45:11.171: INFO: Waiting up to 5m0s for pod "pod-configmaps-7121ff0d-6ac0-41fd-ad60-c673ed9c4292" in namespace "configmap-2583" to be "success or failure"
Feb 23 20:45:11.174: INFO: Pod "pod-configmaps-7121ff0d-6ac0-41fd-ad60-c673ed9c4292": Phase="Pending", Reason="", readiness=false. Elapsed: 2.917941ms
Feb 23 20:45:13.180: INFO: Pod "pod-configmaps-7121ff0d-6ac0-41fd-ad60-c673ed9c4292": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008751693s
STEP: Saw pod success
Feb 23 20:45:13.180: INFO: Pod "pod-configmaps-7121ff0d-6ac0-41fd-ad60-c673ed9c4292" satisfied condition "success or failure"
Feb 23 20:45:13.183: INFO: Trying to get logs from node worker01 pod pod-configmaps-7121ff0d-6ac0-41fd-ad60-c673ed9c4292 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 20:45:13.208: INFO: Waiting for pod pod-configmaps-7121ff0d-6ac0-41fd-ad60-c673ed9c4292 to disappear
Feb 23 20:45:13.210: INFO: Pod pod-configmaps-7121ff0d-6ac0-41fd-ad60-c673ed9c4292 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:45:13.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2583" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":29,"skipped":470,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:45:13.219: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 20:45:13.259: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4b47c385-b499-4478-824e-071a66ee5a0d" in namespace "projected-5552" to be "success or failure"
Feb 23 20:45:13.263: INFO: Pod "downwardapi-volume-4b47c385-b499-4478-824e-071a66ee5a0d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.256307ms
Feb 23 20:45:15.271: INFO: Pod "downwardapi-volume-4b47c385-b499-4478-824e-071a66ee5a0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01172586s
STEP: Saw pod success
Feb 23 20:45:15.271: INFO: Pod "downwardapi-volume-4b47c385-b499-4478-824e-071a66ee5a0d" satisfied condition "success or failure"
Feb 23 20:45:15.273: INFO: Trying to get logs from node worker01 pod downwardapi-volume-4b47c385-b499-4478-824e-071a66ee5a0d container client-container: <nil>
STEP: delete the pod
Feb 23 20:45:15.291: INFO: Waiting for pod downwardapi-volume-4b47c385-b499-4478-824e-071a66ee5a0d to disappear
Feb 23 20:45:15.293: INFO: Pod downwardapi-volume-4b47c385-b499-4478-824e-071a66ee5a0d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:45:15.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5552" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":30,"skipped":485,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:45:15.304: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:45:18.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4881" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":280,"completed":31,"skipped":526,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:45:18.377: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:45:34.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7118" for this suite.

â€¢ [SLOW TEST:16.182 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":280,"completed":32,"skipped":537,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:45:34.561: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 23 20:45:36.617: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:45:36.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3775" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":280,"completed":33,"skipped":547,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:45:36.656: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:45:36.708: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-908481f3-f5b4-4573-a63d-82530d2136f2" in namespace "security-context-test-8624" to be "success or failure"
Feb 23 20:45:36.712: INFO: Pod "busybox-readonly-false-908481f3-f5b4-4573-a63d-82530d2136f2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.861211ms
Feb 23 20:45:38.717: INFO: Pod "busybox-readonly-false-908481f3-f5b4-4573-a63d-82530d2136f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008849875s
Feb 23 20:45:38.717: INFO: Pod "busybox-readonly-false-908481f3-f5b4-4573-a63d-82530d2136f2" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:45:38.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8624" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":280,"completed":34,"skipped":558,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:45:38.729: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-da1322f1-f54a-4c90-b844-773ea9f1d042
STEP: Creating a pod to test consume configMaps
Feb 23 20:45:38.773: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e1a457d7-97c3-40fe-ac0e-f380cf4bca32" in namespace "projected-2250" to be "success or failure"
Feb 23 20:45:38.782: INFO: Pod "pod-projected-configmaps-e1a457d7-97c3-40fe-ac0e-f380cf4bca32": Phase="Pending", Reason="", readiness=false. Elapsed: 9.639063ms
Feb 23 20:45:40.788: INFO: Pod "pod-projected-configmaps-e1a457d7-97c3-40fe-ac0e-f380cf4bca32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014998789s
STEP: Saw pod success
Feb 23 20:45:40.788: INFO: Pod "pod-projected-configmaps-e1a457d7-97c3-40fe-ac0e-f380cf4bca32" satisfied condition "success or failure"
Feb 23 20:45:40.794: INFO: Trying to get logs from node worker01 pod pod-projected-configmaps-e1a457d7-97c3-40fe-ac0e-f380cf4bca32 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 20:45:40.812: INFO: Waiting for pod pod-projected-configmaps-e1a457d7-97c3-40fe-ac0e-f380cf4bca32 to disappear
Feb 23 20:45:40.818: INFO: Pod pod-projected-configmaps-e1a457d7-97c3-40fe-ac0e-f380cf4bca32 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:45:40.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2250" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":35,"skipped":611,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:45:40.829: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-6517
STEP: creating replication controller nodeport-test in namespace services-6517
I0223 20:45:40.887437      23 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-6517, replica count: 2
Feb 23 20:45:43.937: INFO: Creating new exec pod
I0223 20:45:43.937761      23 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 23 20:45:46.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=services-6517 execpod65fnb -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Feb 23 20:45:47.085: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 23 20:45:47.085: INFO: stdout: ""
Feb 23 20:45:47.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=services-6517 execpod65fnb -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.176 80'
Feb 23 20:45:47.213: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.176 80\nConnection to 10.32.0.176 80 port [tcp/http] succeeded!\n"
Feb 23 20:45:47.213: INFO: stdout: ""
Feb 23 20:45:47.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=services-6517 execpod65fnb -- /bin/sh -x -c nc -zv -t -w 2 192.168.180.100 32113'
Feb 23 20:45:47.338: INFO: stderr: "+ nc -zv -t -w 2 192.168.180.100 32113\nConnection to 192.168.180.100 32113 port [tcp/32113] succeeded!\n"
Feb 23 20:45:47.338: INFO: stdout: ""
Feb 23 20:45:47.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=services-6517 execpod65fnb -- /bin/sh -x -c nc -zv -t -w 2 192.168.180.101 32113'
Feb 23 20:45:47.467: INFO: stderr: "+ nc -zv -t -w 2 192.168.180.101 32113\nConnection to 192.168.180.101 32113 port [tcp/32113] succeeded!\n"
Feb 23 20:45:47.467: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:45:47.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6517" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:6.648 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":280,"completed":36,"skipped":616,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:45:47.476: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6691
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6691
I0223 20:45:47.537194      23 runners.go:189] Created replication controller with name: externalname-service, namespace: services-6691, replica count: 2
Feb 23 20:45:50.588: INFO: Creating new exec pod
I0223 20:45:50.588597      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 23 20:45:53.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=services-6691 execpodpf54b -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 23 20:45:53.725: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 23 20:45:53.725: INFO: stdout: ""
Feb 23 20:45:53.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=services-6691 execpodpf54b -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.165 80'
Feb 23 20:45:53.847: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.165 80\nConnection to 10.32.0.165 80 port [tcp/http] succeeded!\n"
Feb 23 20:45:53.847: INFO: stdout: ""
Feb 23 20:45:53.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=services-6691 execpodpf54b -- /bin/sh -x -c nc -zv -t -w 2 192.168.180.100 30984'
Feb 23 20:45:53.976: INFO: stderr: "+ nc -zv -t -w 2 192.168.180.100 30984\nConnection to 192.168.180.100 30984 port [tcp/30984] succeeded!\n"
Feb 23 20:45:53.976: INFO: stdout: ""
Feb 23 20:45:53.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=services-6691 execpodpf54b -- /bin/sh -x -c nc -zv -t -w 2 192.168.180.101 30984'
Feb 23 20:45:54.099: INFO: stderr: "+ nc -zv -t -w 2 192.168.180.101 30984\nConnection to 192.168.180.101 30984 port [tcp/30984] succeeded!\n"
Feb 23 20:45:54.099: INFO: stdout: ""
Feb 23 20:45:54.099: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:45:54.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6691" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:6.682 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":280,"completed":37,"skipped":626,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:45:54.159: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:45:54.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2436" for this suite.
â€¢{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":280,"completed":38,"skipped":639,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:45:54.250: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 20:45:54.297: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84d1ce02-e007-400e-811b-23760ab4a1be" in namespace "downward-api-8906" to be "success or failure"
Feb 23 20:45:54.306: INFO: Pod "downwardapi-volume-84d1ce02-e007-400e-811b-23760ab4a1be": Phase="Pending", Reason="", readiness=false. Elapsed: 8.410748ms
Feb 23 20:45:56.312: INFO: Pod "downwardapi-volume-84d1ce02-e007-400e-811b-23760ab4a1be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015101925s
STEP: Saw pod success
Feb 23 20:45:56.312: INFO: Pod "downwardapi-volume-84d1ce02-e007-400e-811b-23760ab4a1be" satisfied condition "success or failure"
Feb 23 20:45:56.315: INFO: Trying to get logs from node worker01 pod downwardapi-volume-84d1ce02-e007-400e-811b-23760ab4a1be container client-container: <nil>
STEP: delete the pod
Feb 23 20:45:56.331: INFO: Waiting for pod downwardapi-volume-84d1ce02-e007-400e-811b-23760ab4a1be to disappear
Feb 23 20:45:56.336: INFO: Pod downwardapi-volume-84d1ce02-e007-400e-811b-23760ab4a1be no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:45:56.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8906" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":39,"skipped":660,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:45:56.345: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:45:56.379: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Feb 23 20:46:03.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-8698 create -f -'
Feb 23 20:46:04.472: INFO: stderr: ""
Feb 23 20:46:04.472: INFO: stdout: "e2e-test-crd-publish-openapi-338-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 23 20:46:04.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-8698 delete e2e-test-crd-publish-openapi-338-crds test-foo'
Feb 23 20:46:04.534: INFO: stderr: ""
Feb 23 20:46:04.534: INFO: stdout: "e2e-test-crd-publish-openapi-338-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 23 20:46:04.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-8698 apply -f -'
Feb 23 20:46:04.697: INFO: stderr: ""
Feb 23 20:46:04.697: INFO: stdout: "e2e-test-crd-publish-openapi-338-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 23 20:46:04.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-8698 delete e2e-test-crd-publish-openapi-338-crds test-foo'
Feb 23 20:46:04.758: INFO: stderr: ""
Feb 23 20:46:04.758: INFO: stdout: "e2e-test-crd-publish-openapi-338-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Feb 23 20:46:04.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-8698 create -f -'
Feb 23 20:46:04.830: INFO: rc: 1
Feb 23 20:46:04.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-8698 apply -f -'
Feb 23 20:46:04.921: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Feb 23 20:46:04.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-8698 create -f -'
Feb 23 20:46:04.994: INFO: rc: 1
Feb 23 20:46:04.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-8698 apply -f -'
Feb 23 20:46:05.124: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Feb 23 20:46:05.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 explain e2e-test-crd-publish-openapi-338-crds'
Feb 23 20:46:05.304: INFO: stderr: ""
Feb 23 20:46:05.304: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-338-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Feb 23 20:46:05.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 explain e2e-test-crd-publish-openapi-338-crds.metadata'
Feb 23 20:46:05.504: INFO: stderr: ""
Feb 23 20:46:05.504: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-338-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 23 20:46:05.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 explain e2e-test-crd-publish-openapi-338-crds.spec'
Feb 23 20:46:05.597: INFO: stderr: ""
Feb 23 20:46:05.597: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-338-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 23 20:46:05.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 explain e2e-test-crd-publish-openapi-338-crds.spec.bars'
Feb 23 20:46:05.689: INFO: stderr: ""
Feb 23 20:46:05.689: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-338-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Feb 23 20:46:05.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 explain e2e-test-crd-publish-openapi-338-crds.spec.bars2'
Feb 23 20:46:05.765: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:46:08.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8698" for this suite.

â€¢ [SLOW TEST:12.306 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":280,"completed":40,"skipped":670,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:46:08.652: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-0654b961-65c1-4f19-8086-8b64c8972f56
STEP: Creating a pod to test consume secrets
Feb 23 20:46:08.782: INFO: Waiting up to 5m0s for pod "pod-secrets-c0632ebe-3fbb-4ae0-a9a6-523aff6c4f50" in namespace "secrets-2712" to be "success or failure"
Feb 23 20:46:08.785: INFO: Pod "pod-secrets-c0632ebe-3fbb-4ae0-a9a6-523aff6c4f50": Phase="Pending", Reason="", readiness=false. Elapsed: 3.302942ms
Feb 23 20:46:10.793: INFO: Pod "pod-secrets-c0632ebe-3fbb-4ae0-a9a6-523aff6c4f50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010962005s
STEP: Saw pod success
Feb 23 20:46:10.793: INFO: Pod "pod-secrets-c0632ebe-3fbb-4ae0-a9a6-523aff6c4f50" satisfied condition "success or failure"
Feb 23 20:46:10.797: INFO: Trying to get logs from node worker01 pod pod-secrets-c0632ebe-3fbb-4ae0-a9a6-523aff6c4f50 container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 20:46:10.818: INFO: Waiting for pod pod-secrets-c0632ebe-3fbb-4ae0-a9a6-523aff6c4f50 to disappear
Feb 23 20:46:10.821: INFO: Pod pod-secrets-c0632ebe-3fbb-4ae0-a9a6-523aff6c4f50 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:46:10.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2712" for this suite.
STEP: Destroying namespace "secret-namespace-3330" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":280,"completed":41,"skipped":685,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:46:10.839: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 20:46:11.386: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 20:46:14.414: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:46:14.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2210" for this suite.
STEP: Destroying namespace "webhook-2210-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":280,"completed":42,"skipped":706,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:46:14.549: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 23 20:46:17.121: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-638 pod-service-account-1e8fae12-336e-47f8-b9ff-d74588ed0f1b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 23 20:46:17.257: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-638 pod-service-account-1e8fae12-336e-47f8-b9ff-d74588ed0f1b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 23 20:46:17.373: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-638 pod-service-account-1e8fae12-336e-47f8-b9ff-d74588ed0f1b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:46:17.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-638" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":280,"completed":43,"skipped":708,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:46:17.508: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:46:17.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7663" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":280,"completed":44,"skipped":712,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:46:17.560: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-9ae88dea-d141-4eb5-81fb-be7b853282fa
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-9ae88dea-d141-4eb5-81fb-be7b853282fa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:46:21.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6974" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":45,"skipped":715,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:46:21.647: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1897
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 23 20:46:21.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-9005'
Feb 23 20:46:21.756: INFO: stderr: ""
Feb 23 20:46:21.756: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Feb 23 20:46:26.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pod e2e-test-httpd-pod --namespace=kubectl-9005 -o json'
Feb 23 20:46:26.890: INFO: stderr: ""
Feb 23 20:46:26.890: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.200.5.3/32\"\n        },\n        \"creationTimestamp\": \"2020-02-23T20:46:22Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9005\",\n        \"resourceVersion\": \"7985\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9005/pods/e2e-test-httpd-pod\",\n        \"uid\": \"718c8c63-4ce2-4ede-9496-57a3a61dee3a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-bmk8n\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker01\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-bmk8n\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-bmk8n\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-23T20:46:21Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-23T20:46:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-23T20:46:23Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-23T20:46:21Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://cbe2adeaa4faac15f67c923cd02356ea3677133f09cb5a110cadc73b82f3c79c\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-02-23T20:46:22Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.180.101\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.200.5.3\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.200.5.3\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-02-23T20:46:21Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 23 20:46:26.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 replace -f - --namespace=kubectl-9005'
Feb 23 20:46:27.155: INFO: stderr: ""
Feb 23 20:46:27.155: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1902
Feb 23 20:46:27.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete pods e2e-test-httpd-pod --namespace=kubectl-9005'
Feb 23 20:46:37.716: INFO: stderr: ""
Feb 23 20:46:37.716: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:46:37.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9005" for this suite.

â€¢ [SLOW TEST:16.076 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1893
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":280,"completed":46,"skipped":721,"failed":0}
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:46:37.723: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Feb 23 20:46:37.761: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-5419" to be "success or failure"
Feb 23 20:46:37.764: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.228914ms
Feb 23 20:46:39.766: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005067072s
STEP: Saw pod success
Feb 23 20:46:39.766: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 23 20:46:39.773: INFO: Trying to get logs from node worker01 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 23 20:46:39.794: INFO: Waiting for pod pod-host-path-test to disappear
Feb 23 20:46:39.797: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:46:39.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-5419" for this suite.
â€¢{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":47,"skipped":721,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:46:39.808: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9288.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9288.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9288.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9288.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9288.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9288.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9288.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9288.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9288.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9288.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9288.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9288.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9288.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 53.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.53_udp@PTR;check="$$(dig +tcp +noall +answer +search 53.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.53_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9288.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9288.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9288.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9288.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9288.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9288.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9288.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9288.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9288.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9288.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9288.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9288.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9288.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 53.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.53_udp@PTR;check="$$(dig +tcp +noall +answer +search 53.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.53_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 20:46:41.903: INFO: Unable to read wheezy_udp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:41.906: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:41.908: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:41.912: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:41.933: INFO: Unable to read jessie_udp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:41.939: INFO: Unable to read jessie_tcp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:41.942: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:41.947: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:41.968: INFO: Lookups using dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1 failed for: [wheezy_udp@dns-test-service.dns-9288.svc.cluster.local wheezy_tcp@dns-test-service.dns-9288.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9288.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9288.svc.cluster.local jessie_udp@dns-test-service.dns-9288.svc.cluster.local jessie_tcp@dns-test-service.dns-9288.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9288.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9288.svc.cluster.local]

Feb 23 20:46:46.972: INFO: Unable to read wheezy_udp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:46.975: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:47.006: INFO: Unable to read jessie_udp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:47.009: INFO: Unable to read jessie_tcp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:47.031: INFO: Lookups using dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1 failed for: [wheezy_udp@dns-test-service.dns-9288.svc.cluster.local wheezy_tcp@dns-test-service.dns-9288.svc.cluster.local jessie_udp@dns-test-service.dns-9288.svc.cluster.local jessie_tcp@dns-test-service.dns-9288.svc.cluster.local]

Feb 23 20:46:51.973: INFO: Unable to read wheezy_udp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:51.976: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:51.998: INFO: Unable to read jessie_udp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:52.001: INFO: Unable to read jessie_tcp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:52.030: INFO: Lookups using dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1 failed for: [wheezy_udp@dns-test-service.dns-9288.svc.cluster.local wheezy_tcp@dns-test-service.dns-9288.svc.cluster.local jessie_udp@dns-test-service.dns-9288.svc.cluster.local jessie_tcp@dns-test-service.dns-9288.svc.cluster.local]

Feb 23 20:46:56.971: INFO: Unable to read wheezy_udp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:56.974: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:56.998: INFO: Unable to read jessie_udp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:57.002: INFO: Unable to read jessie_tcp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:46:57.038: INFO: Lookups using dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1 failed for: [wheezy_udp@dns-test-service.dns-9288.svc.cluster.local wheezy_tcp@dns-test-service.dns-9288.svc.cluster.local jessie_udp@dns-test-service.dns-9288.svc.cluster.local jessie_tcp@dns-test-service.dns-9288.svc.cluster.local]

Feb 23 20:47:01.974: INFO: Unable to read wheezy_udp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:47:01.977: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:47:02.000: INFO: Unable to read jessie_udp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:47:02.003: INFO: Unable to read jessie_tcp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:47:02.030: INFO: Lookups using dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1 failed for: [wheezy_udp@dns-test-service.dns-9288.svc.cluster.local wheezy_tcp@dns-test-service.dns-9288.svc.cluster.local jessie_udp@dns-test-service.dns-9288.svc.cluster.local jessie_tcp@dns-test-service.dns-9288.svc.cluster.local]

Feb 23 20:47:06.977: INFO: Unable to read wheezy_udp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:47:06.981: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:47:07.008: INFO: Unable to read jessie_udp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:47:07.010: INFO: Unable to read jessie_tcp@dns-test-service.dns-9288.svc.cluster.local from pod dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1: the server could not find the requested resource (get pods dns-test-fc3abe43-6825-4af4-acea-9af299679ca1)
Feb 23 20:47:07.036: INFO: Lookups using dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1 failed for: [wheezy_udp@dns-test-service.dns-9288.svc.cluster.local wheezy_tcp@dns-test-service.dns-9288.svc.cluster.local jessie_udp@dns-test-service.dns-9288.svc.cluster.local jessie_tcp@dns-test-service.dns-9288.svc.cluster.local]

Feb 23 20:47:12.038: INFO: DNS probes using dns-9288/dns-test-fc3abe43-6825-4af4-acea-9af299679ca1 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:47:12.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9288" for this suite.

â€¢ [SLOW TEST:32.348 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":280,"completed":48,"skipped":731,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:47:12.156: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 23 20:47:15.243: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:47:16.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8053" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":280,"completed":49,"skipped":746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:47:16.280: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 20:47:16.315: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f7f6f66-366f-4e25-add0-88ee67bf8bd6" in namespace "downward-api-2419" to be "success or failure"
Feb 23 20:47:16.319: INFO: Pod "downwardapi-volume-4f7f6f66-366f-4e25-add0-88ee67bf8bd6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.381832ms
Feb 23 20:47:18.323: INFO: Pod "downwardapi-volume-4f7f6f66-366f-4e25-add0-88ee67bf8bd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007882162s
STEP: Saw pod success
Feb 23 20:47:18.323: INFO: Pod "downwardapi-volume-4f7f6f66-366f-4e25-add0-88ee67bf8bd6" satisfied condition "success or failure"
Feb 23 20:47:18.328: INFO: Trying to get logs from node worker01 pod downwardapi-volume-4f7f6f66-366f-4e25-add0-88ee67bf8bd6 container client-container: <nil>
STEP: delete the pod
Feb 23 20:47:18.345: INFO: Waiting for pod downwardapi-volume-4f7f6f66-366f-4e25-add0-88ee67bf8bd6 to disappear
Feb 23 20:47:18.348: INFO: Pod downwardapi-volume-4f7f6f66-366f-4e25-add0-88ee67bf8bd6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:47:18.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2419" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":50,"skipped":773,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:47:18.358: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 23 20:47:18.397: INFO: Waiting up to 5m0s for pod "pod-5baa84df-cf77-4806-80ad-24b2a2ef7c88" in namespace "emptydir-46" to be "success or failure"
Feb 23 20:47:18.401: INFO: Pod "pod-5baa84df-cf77-4806-80ad-24b2a2ef7c88": Phase="Pending", Reason="", readiness=false. Elapsed: 3.240217ms
Feb 23 20:47:20.409: INFO: Pod "pod-5baa84df-cf77-4806-80ad-24b2a2ef7c88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010987597s
STEP: Saw pod success
Feb 23 20:47:20.409: INFO: Pod "pod-5baa84df-cf77-4806-80ad-24b2a2ef7c88" satisfied condition "success or failure"
Feb 23 20:47:20.413: INFO: Trying to get logs from node worker01 pod pod-5baa84df-cf77-4806-80ad-24b2a2ef7c88 container test-container: <nil>
STEP: delete the pod
Feb 23 20:47:20.431: INFO: Waiting for pod pod-5baa84df-cf77-4806-80ad-24b2a2ef7c88 to disappear
Feb 23 20:47:20.435: INFO: Pod pod-5baa84df-cf77-4806-80ad-24b2a2ef7c88 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:47:20.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-46" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":51,"skipped":852,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:47:20.444: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-54c84b1c-541e-4576-8610-a720b7d44041
STEP: Creating a pod to test consume secrets
Feb 23 20:47:20.490: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6194fe68-85ca-41ad-af7b-618ca377096c" in namespace "projected-5724" to be "success or failure"
Feb 23 20:47:20.493: INFO: Pod "pod-projected-secrets-6194fe68-85ca-41ad-af7b-618ca377096c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.771116ms
Feb 23 20:47:22.498: INFO: Pod "pod-projected-secrets-6194fe68-85ca-41ad-af7b-618ca377096c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00701602s
STEP: Saw pod success
Feb 23 20:47:22.498: INFO: Pod "pod-projected-secrets-6194fe68-85ca-41ad-af7b-618ca377096c" satisfied condition "success or failure"
Feb 23 20:47:22.502: INFO: Trying to get logs from node worker01 pod pod-projected-secrets-6194fe68-85ca-41ad-af7b-618ca377096c container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 23 20:47:22.520: INFO: Waiting for pod pod-projected-secrets-6194fe68-85ca-41ad-af7b-618ca377096c to disappear
Feb 23 20:47:22.524: INFO: Pod pod-projected-secrets-6194fe68-85ca-41ad-af7b-618ca377096c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:47:22.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5724" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":52,"skipped":875,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:47:22.536: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 23 20:47:22.593: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8740 /api/v1/namespaces/watch-8740/configmaps/e2e-watch-test-resource-version e054913b-5504-4c74-91a7-d5a539acba1a 8450 0 2020-02-23 20:47:23 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 23 20:47:22.593: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8740 /api/v1/namespaces/watch-8740/configmaps/e2e-watch-test-resource-version e054913b-5504-4c74-91a7-d5a539acba1a 8451 0 2020-02-23 20:47:23 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:47:22.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8740" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":280,"completed":53,"skipped":891,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:47:22.607: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 20:47:22.648: INFO: Waiting up to 5m0s for pod "downwardapi-volume-949ffd23-eac5-4bd5-a470-14d1650683d5" in namespace "downward-api-5452" to be "success or failure"
Feb 23 20:47:22.661: INFO: Pod "downwardapi-volume-949ffd23-eac5-4bd5-a470-14d1650683d5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.585469ms
Feb 23 20:47:24.665: INFO: Pod "downwardapi-volume-949ffd23-eac5-4bd5-a470-14d1650683d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017597708s
STEP: Saw pod success
Feb 23 20:47:24.665: INFO: Pod "downwardapi-volume-949ffd23-eac5-4bd5-a470-14d1650683d5" satisfied condition "success or failure"
Feb 23 20:47:24.668: INFO: Trying to get logs from node worker01 pod downwardapi-volume-949ffd23-eac5-4bd5-a470-14d1650683d5 container client-container: <nil>
STEP: delete the pod
Feb 23 20:47:24.686: INFO: Waiting for pod downwardapi-volume-949ffd23-eac5-4bd5-a470-14d1650683d5 to disappear
Feb 23 20:47:24.690: INFO: Pod downwardapi-volume-949ffd23-eac5-4bd5-a470-14d1650683d5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:47:24.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5452" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":54,"skipped":909,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:47:24.702: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-h9d2
STEP: Creating a pod to test atomic-volume-subpath
Feb 23 20:47:24.746: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-h9d2" in namespace "subpath-6430" to be "success or failure"
Feb 23 20:47:24.755: INFO: Pod "pod-subpath-test-downwardapi-h9d2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.050601ms
Feb 23 20:47:26.762: INFO: Pod "pod-subpath-test-downwardapi-h9d2": Phase="Running", Reason="", readiness=true. Elapsed: 2.016282857s
Feb 23 20:47:28.768: INFO: Pod "pod-subpath-test-downwardapi-h9d2": Phase="Running", Reason="", readiness=true. Elapsed: 4.021890409s
Feb 23 20:47:30.774: INFO: Pod "pod-subpath-test-downwardapi-h9d2": Phase="Running", Reason="", readiness=true. Elapsed: 6.02850956s
Feb 23 20:47:32.780: INFO: Pod "pod-subpath-test-downwardapi-h9d2": Phase="Running", Reason="", readiness=true. Elapsed: 8.034138637s
Feb 23 20:47:34.787: INFO: Pod "pod-subpath-test-downwardapi-h9d2": Phase="Running", Reason="", readiness=true. Elapsed: 10.041547791s
Feb 23 20:47:36.791: INFO: Pod "pod-subpath-test-downwardapi-h9d2": Phase="Running", Reason="", readiness=true. Elapsed: 12.045586335s
Feb 23 20:47:38.798: INFO: Pod "pod-subpath-test-downwardapi-h9d2": Phase="Running", Reason="", readiness=true. Elapsed: 14.052602368s
Feb 23 20:47:40.807: INFO: Pod "pod-subpath-test-downwardapi-h9d2": Phase="Running", Reason="", readiness=true. Elapsed: 16.061039899s
Feb 23 20:47:42.814: INFO: Pod "pod-subpath-test-downwardapi-h9d2": Phase="Running", Reason="", readiness=true. Elapsed: 18.068424874s
Feb 23 20:47:44.816: INFO: Pod "pod-subpath-test-downwardapi-h9d2": Phase="Running", Reason="", readiness=true. Elapsed: 20.070730036s
Feb 23 20:47:46.823: INFO: Pod "pod-subpath-test-downwardapi-h9d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.07742172s
STEP: Saw pod success
Feb 23 20:47:46.823: INFO: Pod "pod-subpath-test-downwardapi-h9d2" satisfied condition "success or failure"
Feb 23 20:47:46.827: INFO: Trying to get logs from node worker01 pod pod-subpath-test-downwardapi-h9d2 container test-container-subpath-downwardapi-h9d2: <nil>
STEP: delete the pod
Feb 23 20:47:46.848: INFO: Waiting for pod pod-subpath-test-downwardapi-h9d2 to disappear
Feb 23 20:47:46.852: INFO: Pod pod-subpath-test-downwardapi-h9d2 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-h9d2
Feb 23 20:47:46.852: INFO: Deleting pod "pod-subpath-test-downwardapi-h9d2" in namespace "subpath-6430"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:47:46.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6430" for this suite.

â€¢ [SLOW TEST:22.163 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":280,"completed":55,"skipped":952,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:47:46.866: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:47:46.898: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:47:52.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7018" for this suite.

â€¢ [SLOW TEST:6.075 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":280,"completed":56,"skipped":956,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:47:52.940: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-b293ada3-59c9-4e17-9d0f-f77b952e72f2 in namespace container-probe-3217
Feb 23 20:47:54.995: INFO: Started pod liveness-b293ada3-59c9-4e17-9d0f-f77b952e72f2 in namespace container-probe-3217
STEP: checking the pod's current state and verifying that restartCount is present
Feb 23 20:47:55.000: INFO: Initial restart count of pod liveness-b293ada3-59c9-4e17-9d0f-f77b952e72f2 is 0
Feb 23 20:48:09.052: INFO: Restart count of pod container-probe-3217/liveness-b293ada3-59c9-4e17-9d0f-f77b952e72f2 is now 1 (14.052592645s elapsed)
Feb 23 20:48:29.113: INFO: Restart count of pod container-probe-3217/liveness-b293ada3-59c9-4e17-9d0f-f77b952e72f2 is now 2 (34.113129493s elapsed)
Feb 23 20:48:49.171: INFO: Restart count of pod container-probe-3217/liveness-b293ada3-59c9-4e17-9d0f-f77b952e72f2 is now 3 (54.171752333s elapsed)
Feb 23 20:49:09.423: INFO: Restart count of pod container-probe-3217/liveness-b293ada3-59c9-4e17-9d0f-f77b952e72f2 is now 4 (1m14.423092232s elapsed)
Feb 23 20:50:11.619: INFO: Restart count of pod container-probe-3217/liveness-b293ada3-59c9-4e17-9d0f-f77b952e72f2 is now 5 (2m16.618960339s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:50:11.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3217" for this suite.

â€¢ [SLOW TEST:138.712 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":280,"completed":57,"skipped":957,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:50:11.652: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 23 20:50:11.683: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 23 20:50:11.691: INFO: Waiting for terminating namespaces to be deleted...
Feb 23 20:50:11.695: INFO: 
Logging pods the kubelet thinks is on node worker00 before test
Feb 23 20:50:11.712: INFO: etcd-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container etcd ready: true, restart count 0
Feb 23 20:50:11.712: INFO: ceph-osd-worker00-556546b495-tmvn5 from storage started at 2020-02-23 20:28:14 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 20:50:11.712: INFO: sonobuoy-systemd-logs-daemon-set-975a2eaddfc64d32-ct4lk from sonobuoy started at 2020-02-23 20:38:34 +0000 UTC (2 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 20:50:11.712: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 20:50:11.712: INFO: csi-rbdplugin-provisioner-7494f65674-2h6xd from storage started at 2020-02-23 20:28:35 +0000 UTC (5 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 20:50:11.712: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 20:50:11.712: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 20:50:11.712: INFO: 	Container csi-snapshotter ready: true, restart count 2
Feb 23 20:50:11.712: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 20:50:11.712: INFO: kube-scheduler-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container kube-scheduler ready: true, restart count 2
Feb 23 20:50:11.712: INFO: calico-node-st65h from networking started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 20:50:11.712: INFO: ceph-mgr-94b9dd996-ggbsc from storage started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container ceph-mgr ready: true, restart count 0
Feb 23 20:50:11.712: INFO: csi-rbdplugin-provisioner-7494f65674-7tn5d from storage started at 2020-02-23 20:28:35 +0000 UTC (5 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 20:50:11.712: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 20:50:11.712: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 20:50:11.712: INFO: 	Container csi-snapshotter ready: true, restart count 2
Feb 23 20:50:11.712: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 20:50:11.712: INFO: csi-rbdplugin-z2bdm from storage started at 2020-02-23 20:28:35 +0000 UTC (3 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 20:50:11.712: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 20:50:11.712: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 20:50:11.712: INFO: csi-cephfsplugin-q68vh from storage started at 2020-02-23 20:28:35 +0000 UTC (3 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 20:50:11.712: INFO: 	Container driver-registrar ready: true, restart count 1
Feb 23 20:50:11.712: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 20:50:11.712: INFO: ceph-mds-worker00-6f479b4486-bdmhh from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container ceph-mds ready: true, restart count 3
Feb 23 20:50:11.712: INFO: ceph-mon-worker00-5cf654d469-bcbdt from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 20:50:11.712: INFO: coredns-676544c7b9-j9z7b from kube-system started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container coredns ready: true, restart count 0
Feb 23 20:50:11.712: INFO: calico-kube-controllers-7cd585bcd-vmw47 from networking started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container calico-kube-controllers ready: true, restart count 2
Feb 23 20:50:11.712: INFO: csi-cephfsplugin-provisioner-6cd7596f75-tw7n4 from storage started at 2020-02-23 20:28:35 +0000 UTC (4 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 20:50:11.712: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 20:50:11.712: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 20:50:11.712: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 20:50:11.712: INFO: csi-rbdplugin-provisioner-7494f65674-ddxmx from storage started at 2020-02-23 20:28:35 +0000 UTC (5 container statuses recorded)
Feb 23 20:50:11.712: INFO: 	Container csi-attacher ready: true, restart count 1
Feb 23 20:50:11.714: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 20:50:11.714: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 20:50:11.714: INFO: 	Container csi-snapshotter ready: true, restart count 2
Feb 23 20:50:11.714: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 20:50:11.714: INFO: csi-cephfsplugin-provisioner-6cd7596f75-9hfw2 from storage started at 2020-02-23 20:28:35 +0000 UTC (4 container statuses recorded)
Feb 23 20:50:11.714: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 20:50:11.714: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 20:50:11.714: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 20:50:11.714: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 20:50:11.714: INFO: ceph-setup-6j8xr from storage started at 2020-02-23 20:28:36 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.714: INFO: 	Container ceph ready: false, restart count 2
Feb 23 20:50:11.714: INFO: kube-apiserver-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.714: INFO: 	Container kube-apiserver ready: true, restart count 1
Feb 23 20:50:11.714: INFO: csi-cephfsplugin-provisioner-6cd7596f75-492rn from storage started at 2020-02-23 20:28:35 +0000 UTC (4 container statuses recorded)
Feb 23 20:50:11.714: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 20:50:11.714: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 20:50:11.714: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 20:50:11.715: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 20:50:11.715: INFO: metallb-controller-b96bfbbf8-p9lnh from networking started at 2020-02-23 20:28:36 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.715: INFO: 	Container controller ready: true, restart count 0
Feb 23 20:50:11.715: INFO: metallb-speaker-74sbt from networking started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.715: INFO: 	Container speaker ready: true, restart count 0
Feb 23 20:50:11.715: INFO: gobetween-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.715: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 20:50:11.715: INFO: kube-proxy-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.715: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 20:50:11.715: INFO: coredns-676544c7b9-q29b9 from kube-system started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.715: INFO: 	Container coredns ready: true, restart count 0
Feb 23 20:50:11.715: INFO: ceph-rgw-57cd48f74c-lf65j from storage started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.715: INFO: 	Container ceph-rgw ready: true, restart count 0
Feb 23 20:50:11.715: INFO: kube-controller-manager-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.715: INFO: 	Container kube-controller-manager ready: true, restart count 2
Feb 23 20:50:11.715: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
Feb 23 20:50:11.731: INFO: csi-cephfsplugin-7cvst from storage started at 2020-02-23 20:31:01 +0000 UTC (3 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 20:50:11.731: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 20:50:11.731: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 20:50:11.731: INFO: kubernetes-dashboard-f957cddcb-xf79k from kube-system started at 2020-02-23 20:33:06 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 23 20:50:11.731: INFO: gobetween-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 20:50:11.731: INFO: kube-proxy-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 20:50:11.731: INFO: ceph-mon-worker01-bdb694876-bwgrw from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 20:50:11.731: INFO: csi-rbdplugin-c6629 from storage started at 2020-02-23 20:31:01 +0000 UTC (3 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 20:50:11.731: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 20:50:11.731: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 20:50:11.731: INFO: sonobuoy from sonobuoy started at 2020-02-23 20:38:32 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 23 20:50:11.731: INFO: etcd-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container etcd ready: true, restart count 1
Feb 23 20:50:11.731: INFO: kube-apiserver-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 23 20:50:11.731: INFO: ceph-mds-worker01-7f5fdb58c6-vz8jk from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container ceph-mds ready: true, restart count 2
Feb 23 20:50:11.731: INFO: kube-scheduler-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container kube-scheduler ready: true, restart count 2
Feb 23 20:50:11.731: INFO: kube-controller-manager-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container kube-controller-manager ready: true, restart count 2
Feb 23 20:50:11.731: INFO: dashboard-metrics-scraper-58475bc987-qhvsn from kube-system started at 2020-02-23 20:33:06 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 23 20:50:11.731: INFO: sonobuoy-e2e-job-5674935bca8a45cb from sonobuoy started at 2020-02-23 20:38:33 +0000 UTC (2 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container e2e ready: true, restart count 0
Feb 23 20:50:11.731: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 20:50:11.731: INFO: sonobuoy-systemd-logs-daemon-set-975a2eaddfc64d32-jk65g from sonobuoy started at 2020-02-23 20:38:33 +0000 UTC (2 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 20:50:11.731: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 20:50:11.731: INFO: calico-node-wlnml from networking started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 20:50:11.731: INFO: ceph-osd-worker01-67947c799-h78d6 from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 20:50:11.731: INFO: metallb-speaker-nx7ns from networking started at 2020-02-23 20:31:01 +0000 UTC (1 container statuses recorded)
Feb 23 20:50:11.731: INFO: 	Container speaker ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-fe5c91f0-9a88-4be6-9e4c-35c0ec83e899 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-fe5c91f0-9a88-4be6-9e4c-35c0ec83e899 off the node worker01
STEP: verifying the node doesn't have the label kubernetes.io/e2e-fe5c91f0-9a88-4be6-9e4c-35c0ec83e899
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:50:19.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3091" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

â€¢ [SLOW TEST:8.219 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":280,"completed":58,"skipped":961,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:50:19.871: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-tt6g
STEP: Creating a pod to test atomic-volume-subpath
Feb 23 20:50:19.914: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tt6g" in namespace "subpath-4548" to be "success or failure"
Feb 23 20:50:19.917: INFO: Pod "pod-subpath-test-configmap-tt6g": Phase="Pending", Reason="", readiness=false. Elapsed: 3.673417ms
Feb 23 20:50:21.927: INFO: Pod "pod-subpath-test-configmap-tt6g": Phase="Running", Reason="", readiness=true. Elapsed: 2.013258067s
Feb 23 20:50:23.933: INFO: Pod "pod-subpath-test-configmap-tt6g": Phase="Running", Reason="", readiness=true. Elapsed: 4.019686226s
Feb 23 20:50:25.937: INFO: Pod "pod-subpath-test-configmap-tt6g": Phase="Running", Reason="", readiness=true. Elapsed: 6.023605098s
Feb 23 20:50:27.941: INFO: Pod "pod-subpath-test-configmap-tt6g": Phase="Running", Reason="", readiness=true. Elapsed: 8.027611223s
Feb 23 20:50:29.948: INFO: Pod "pod-subpath-test-configmap-tt6g": Phase="Running", Reason="", readiness=true. Elapsed: 10.034609518s
Feb 23 20:50:31.956: INFO: Pod "pod-subpath-test-configmap-tt6g": Phase="Running", Reason="", readiness=true. Elapsed: 12.041923903s
Feb 23 20:50:33.962: INFO: Pod "pod-subpath-test-configmap-tt6g": Phase="Running", Reason="", readiness=true. Elapsed: 14.048505245s
Feb 23 20:50:35.968: INFO: Pod "pod-subpath-test-configmap-tt6g": Phase="Running", Reason="", readiness=true. Elapsed: 16.054506369s
Feb 23 20:50:37.979: INFO: Pod "pod-subpath-test-configmap-tt6g": Phase="Running", Reason="", readiness=true. Elapsed: 18.065124923s
Feb 23 20:50:39.983: INFO: Pod "pod-subpath-test-configmap-tt6g": Phase="Running", Reason="", readiness=true. Elapsed: 20.069512046s
Feb 23 20:50:41.990: INFO: Pod "pod-subpath-test-configmap-tt6g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.075843685s
STEP: Saw pod success
Feb 23 20:50:41.990: INFO: Pod "pod-subpath-test-configmap-tt6g" satisfied condition "success or failure"
Feb 23 20:50:41.993: INFO: Trying to get logs from node worker01 pod pod-subpath-test-configmap-tt6g container test-container-subpath-configmap-tt6g: <nil>
STEP: delete the pod
Feb 23 20:50:42.012: INFO: Waiting for pod pod-subpath-test-configmap-tt6g to disappear
Feb 23 20:50:42.019: INFO: Pod pod-subpath-test-configmap-tt6g no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tt6g
Feb 23 20:50:42.019: INFO: Deleting pod "pod-subpath-test-configmap-tt6g" in namespace "subpath-4548"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:50:42.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4548" for this suite.

â€¢ [SLOW TEST:22.158 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":280,"completed":59,"skipped":962,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:50:42.031: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Feb 23 20:50:42.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 cluster-info'
Feb 23 20:50:42.128: INFO: stderr: ""
Feb 23 20:50:42.128: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.32.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.32.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:50:42.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-938" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":280,"completed":60,"skipped":1009,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:50:42.137: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-7068
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7068
STEP: Creating statefulset with conflicting port in namespace statefulset-7068
STEP: Waiting until pod test-pod will start running in namespace statefulset-7068
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7068
Feb 23 20:50:46.200: INFO: Observed stateful pod in namespace: statefulset-7068, name: ss-0, uid: 230dda7b-4e07-4ddd-8e87-34b3528025e3, status phase: Pending. Waiting for statefulset controller to delete.
Feb 23 20:50:46.597: INFO: Observed stateful pod in namespace: statefulset-7068, name: ss-0, uid: 230dda7b-4e07-4ddd-8e87-34b3528025e3, status phase: Failed. Waiting for statefulset controller to delete.
Feb 23 20:50:46.604: INFO: Observed stateful pod in namespace: statefulset-7068, name: ss-0, uid: 230dda7b-4e07-4ddd-8e87-34b3528025e3, status phase: Failed. Waiting for statefulset controller to delete.
Feb 23 20:50:46.609: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7068
STEP: Removing pod with conflicting port in namespace statefulset-7068
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7068 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 23 20:50:50.644: INFO: Deleting all statefulset in ns statefulset-7068
Feb 23 20:50:50.648: INFO: Scaling statefulset ss to 0
Feb 23 20:51:00.665: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 20:51:00.671: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:51:00.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7068" for this suite.

â€¢ [SLOW TEST:18.557 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":280,"completed":61,"skipped":1010,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:51:00.695: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:51:00.733: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-132c85e8-500b-4ae6-9a18-70514695cc9e" in namespace "security-context-test-8813" to be "success or failure"
Feb 23 20:51:00.737: INFO: Pod "alpine-nnp-false-132c85e8-500b-4ae6-9a18-70514695cc9e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.756123ms
Feb 23 20:51:02.743: INFO: Pod "alpine-nnp-false-132c85e8-500b-4ae6-9a18-70514695cc9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010373584s
Feb 23 20:51:04.751: INFO: Pod "alpine-nnp-false-132c85e8-500b-4ae6-9a18-70514695cc9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017578103s
Feb 23 20:51:06.759: INFO: Pod "alpine-nnp-false-132c85e8-500b-4ae6-9a18-70514695cc9e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.026287038s
Feb 23 20:51:08.770: INFO: Pod "alpine-nnp-false-132c85e8-500b-4ae6-9a18-70514695cc9e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.037000408s
Feb 23 20:51:10.779: INFO: Pod "alpine-nnp-false-132c85e8-500b-4ae6-9a18-70514695cc9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.046501855s
Feb 23 20:51:10.780: INFO: Pod "alpine-nnp-false-132c85e8-500b-4ae6-9a18-70514695cc9e" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:51:10.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8813" for this suite.

â€¢ [SLOW TEST:10.102 seconds]
[k8s.io] Security Context
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:289
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":62,"skipped":1023,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:51:10.797: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:51:10.838: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-25f2a2a9-ce52-406e-9e85-5eb2a2e92203" in namespace "security-context-test-5530" to be "success or failure"
Feb 23 20:51:10.846: INFO: Pod "busybox-privileged-false-25f2a2a9-ce52-406e-9e85-5eb2a2e92203": Phase="Pending", Reason="", readiness=false. Elapsed: 8.080739ms
Feb 23 20:51:12.851: INFO: Pod "busybox-privileged-false-25f2a2a9-ce52-406e-9e85-5eb2a2e92203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013068804s
Feb 23 20:51:12.851: INFO: Pod "busybox-privileged-false-25f2a2a9-ce52-406e-9e85-5eb2a2e92203" satisfied condition "success or failure"
Feb 23 20:51:12.858: INFO: Got logs for pod "busybox-privileged-false-25f2a2a9-ce52-406e-9e85-5eb2a2e92203": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:51:12.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5530" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":63,"skipped":1031,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:51:12.887: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0223 20:51:43.556504      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 23 20:51:43.556: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:51:43.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1654" for this suite.

â€¢ [SLOW TEST:30.678 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":280,"completed":64,"skipped":1042,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:51:43.567: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-206bd12a-1a44-45ff-9c40-ae897d09b173
STEP: Creating a pod to test consume configMaps
Feb 23 20:51:43.616: INFO: Waiting up to 5m0s for pod "pod-configmaps-1aff1b0b-15e2-4e48-b973-713f1f002a8c" in namespace "configmap-7978" to be "success or failure"
Feb 23 20:51:43.619: INFO: Pod "pod-configmaps-1aff1b0b-15e2-4e48-b973-713f1f002a8c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.018325ms
Feb 23 20:51:45.625: INFO: Pod "pod-configmaps-1aff1b0b-15e2-4e48-b973-713f1f002a8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00876139s
STEP: Saw pod success
Feb 23 20:51:45.625: INFO: Pod "pod-configmaps-1aff1b0b-15e2-4e48-b973-713f1f002a8c" satisfied condition "success or failure"
Feb 23 20:51:45.628: INFO: Trying to get logs from node worker01 pod pod-configmaps-1aff1b0b-15e2-4e48-b973-713f1f002a8c container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 20:51:45.651: INFO: Waiting for pod pod-configmaps-1aff1b0b-15e2-4e48-b973-713f1f002a8c to disappear
Feb 23 20:51:45.654: INFO: Pod pod-configmaps-1aff1b0b-15e2-4e48-b973-713f1f002a8c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:51:45.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7978" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":65,"skipped":1089,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:51:45.662: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 20:51:46.248: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 20:51:49.275: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:51:49.280: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8713-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:51:54.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5930" for this suite.
STEP: Destroying namespace "webhook-5930-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:9.341 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":280,"completed":66,"skipped":1104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:51:55.019: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:51:55.068: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Creating first CR 
Feb 23 20:52:00.642: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-23T20:52:01Z generation:1 name:name1 resourceVersion:10050 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:62f6ecad-e0ec-47bd-bea6-c1717624dd62] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Feb 23 20:52:10.653: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-23T20:52:11Z generation:1 name:name2 resourceVersion:10081 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:eeeee4ad-f53d-4933-ba5f-e310d946b152] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Feb 23 20:52:20.668: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-23T20:52:01Z generation:2 name:name1 resourceVersion:10112 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:62f6ecad-e0ec-47bd-bea6-c1717624dd62] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Feb 23 20:52:30.675: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-23T20:52:11Z generation:2 name:name2 resourceVersion:10143 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:eeeee4ad-f53d-4933-ba5f-e310d946b152] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Feb 23 20:52:40.688: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-23T20:52:01Z generation:2 name:name1 resourceVersion:10174 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:62f6ecad-e0ec-47bd-bea6-c1717624dd62] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Feb 23 20:52:50.697: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-23T20:52:11Z generation:2 name:name2 resourceVersion:10205 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:eeeee4ad-f53d-4933-ba5f-e310d946b152] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:53:01.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9156" for this suite.

â€¢ [SLOW TEST:66.209 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":280,"completed":67,"skipped":1137,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:53:01.228: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-828ce6d3-9ce4-4f95-8fd2-6d2d4c2d7029 in namespace container-probe-8101
Feb 23 20:53:03.278: INFO: Started pod busybox-828ce6d3-9ce4-4f95-8fd2-6d2d4c2d7029 in namespace container-probe-8101
STEP: checking the pod's current state and verifying that restartCount is present
Feb 23 20:53:03.281: INFO: Initial restart count of pod busybox-828ce6d3-9ce4-4f95-8fd2-6d2d4c2d7029 is 0
Feb 23 20:53:51.453: INFO: Restart count of pod container-probe-8101/busybox-828ce6d3-9ce4-4f95-8fd2-6d2d4c2d7029 is now 1 (48.172524536s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:53:51.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8101" for this suite.

â€¢ [SLOW TEST:50.262 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":68,"skipped":1166,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:53:51.490: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-084b80c5-1519-473d-bb93-d17033b98575
STEP: Creating a pod to test consume secrets
Feb 23 20:53:51.531: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4fd9d8ab-b2ab-430b-9106-d1deb1ce910d" in namespace "projected-2026" to be "success or failure"
Feb 23 20:53:51.534: INFO: Pod "pod-projected-secrets-4fd9d8ab-b2ab-430b-9106-d1deb1ce910d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.785076ms
Feb 23 20:53:53.538: INFO: Pod "pod-projected-secrets-4fd9d8ab-b2ab-430b-9106-d1deb1ce910d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007215105s
STEP: Saw pod success
Feb 23 20:53:53.538: INFO: Pod "pod-projected-secrets-4fd9d8ab-b2ab-430b-9106-d1deb1ce910d" satisfied condition "success or failure"
Feb 23 20:53:53.542: INFO: Trying to get logs from node worker01 pod pod-projected-secrets-4fd9d8ab-b2ab-430b-9106-d1deb1ce910d container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 23 20:53:53.565: INFO: Waiting for pod pod-projected-secrets-4fd9d8ab-b2ab-430b-9106-d1deb1ce910d to disappear
Feb 23 20:53:53.567: INFO: Pod pod-projected-secrets-4fd9d8ab-b2ab-430b-9106-d1deb1ce910d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:53:53.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2026" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":69,"skipped":1169,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:53:53.579: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-d22e91c3-0df5-4ed0-87a7-e55d6d4cab39
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:53:55.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4237" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":70,"skipped":1175,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:53:55.662: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:53:55.712: INFO: Create a RollingUpdate DaemonSet
Feb 23 20:53:55.734: INFO: Check that daemon pods launch on every node of the cluster
Feb 23 20:53:55.741: INFO: Number of nodes with available pods: 0
Feb 23 20:53:55.741: INFO: Node worker00 is running more than one daemon pod
Feb 23 20:53:56.755: INFO: Number of nodes with available pods: 1
Feb 23 20:53:56.755: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:53:57.750: INFO: Number of nodes with available pods: 2
Feb 23 20:53:57.750: INFO: Number of running nodes: 2, number of available pods: 2
Feb 23 20:53:57.751: INFO: Update the DaemonSet to trigger a rollout
Feb 23 20:53:57.760: INFO: Updating DaemonSet daemon-set
Feb 23 20:54:01.777: INFO: Roll back the DaemonSet before rollout is complete
Feb 23 20:54:01.785: INFO: Updating DaemonSet daemon-set
Feb 23 20:54:01.785: INFO: Make sure DaemonSet rollback is complete
Feb 23 20:54:01.793: INFO: Wrong image for pod: daemon-set-bnbwf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 23 20:54:01.793: INFO: Pod daemon-set-bnbwf is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4116, will wait for the garbage collector to delete the pods
Feb 23 20:54:03.881: INFO: Deleting DaemonSet.extensions daemon-set took: 12.707118ms
Feb 23 20:54:04.782: INFO: Terminating DaemonSet.extensions daemon-set pods took: 901.163797ms
Feb 23 20:54:17.788: INFO: Number of nodes with available pods: 0
Feb 23 20:54:17.788: INFO: Number of running nodes: 0, number of available pods: 0
Feb 23 20:54:17.792: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4116/daemonsets","resourceVersion":"10695"},"items":null}

Feb 23 20:54:17.796: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4116/pods","resourceVersion":"10695"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:54:17.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4116" for this suite.

â€¢ [SLOW TEST:22.148 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":280,"completed":71,"skipped":1207,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:54:17.809: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 20:54:17.850: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a88485e-9f0a-4b32-888b-40fdc8378197" in namespace "downward-api-1445" to be "success or failure"
Feb 23 20:54:17.863: INFO: Pod "downwardapi-volume-2a88485e-9f0a-4b32-888b-40fdc8378197": Phase="Pending", Reason="", readiness=false. Elapsed: 12.777936ms
Feb 23 20:54:19.873: INFO: Pod "downwardapi-volume-2a88485e-9f0a-4b32-888b-40fdc8378197": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023087564s
Feb 23 20:54:21.880: INFO: Pod "downwardapi-volume-2a88485e-9f0a-4b32-888b-40fdc8378197": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029794777s
STEP: Saw pod success
Feb 23 20:54:21.880: INFO: Pod "downwardapi-volume-2a88485e-9f0a-4b32-888b-40fdc8378197" satisfied condition "success or failure"
Feb 23 20:54:21.883: INFO: Trying to get logs from node worker01 pod downwardapi-volume-2a88485e-9f0a-4b32-888b-40fdc8378197 container client-container: <nil>
STEP: delete the pod
Feb 23 20:54:21.901: INFO: Waiting for pod downwardapi-volume-2a88485e-9f0a-4b32-888b-40fdc8378197 to disappear
Feb 23 20:54:21.904: INFO: Pod downwardapi-volume-2a88485e-9f0a-4b32-888b-40fdc8378197 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:54:21.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1445" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":72,"skipped":1209,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:54:21.912: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 20:54:22.371: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 20:54:25.390: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:54:37.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3713" for this suite.
STEP: Destroying namespace "webhook-3713-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:15.669 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":280,"completed":73,"skipped":1211,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:54:37.582: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 23 20:54:37.633: INFO: Waiting up to 5m0s for pod "pod-17b3fa27-e6e2-4341-b919-b8d6adaa87f3" in namespace "emptydir-5919" to be "success or failure"
Feb 23 20:54:37.635: INFO: Pod "pod-17b3fa27-e6e2-4341-b919-b8d6adaa87f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.501798ms
Feb 23 20:54:39.641: INFO: Pod "pod-17b3fa27-e6e2-4341-b919-b8d6adaa87f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00839693s
STEP: Saw pod success
Feb 23 20:54:39.641: INFO: Pod "pod-17b3fa27-e6e2-4341-b919-b8d6adaa87f3" satisfied condition "success or failure"
Feb 23 20:54:39.645: INFO: Trying to get logs from node worker01 pod pod-17b3fa27-e6e2-4341-b919-b8d6adaa87f3 container test-container: <nil>
STEP: delete the pod
Feb 23 20:54:39.662: INFO: Waiting for pod pod-17b3fa27-e6e2-4341-b919-b8d6adaa87f3 to disappear
Feb 23 20:54:39.665: INFO: Pod pod-17b3fa27-e6e2-4341-b919-b8d6adaa87f3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:54:39.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5919" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":74,"skipped":1254,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:54:39.675: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Feb 23 20:54:39.704: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Feb 23 20:54:55.948: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 20:55:03.812: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:55:19.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2453" for this suite.

â€¢ [SLOW TEST:39.452 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":280,"completed":75,"skipped":1299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:55:19.127: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Feb 23 20:55:25.191: INFO: 0 pods remaining
Feb 23 20:55:25.191: INFO: 0 pods has nil DeletionTimestamp
Feb 23 20:55:25.191: INFO: 
STEP: Gathering metrics
Feb 23 20:55:26.188: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0223 20:55:26.188578      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 23 20:55:26.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-395" for this suite.

â€¢ [SLOW TEST:7.074 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":280,"completed":76,"skipped":1356,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:55:26.202: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Feb 23 20:55:26.237: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:55:48.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6737" for this suite.

â€¢ [SLOW TEST:22.440 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":280,"completed":77,"skipped":1402,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:55:48.642: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-t2qq
STEP: Creating a pod to test atomic-volume-subpath
Feb 23 20:55:48.689: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-t2qq" in namespace "subpath-3378" to be "success or failure"
Feb 23 20:55:48.694: INFO: Pod "pod-subpath-test-secret-t2qq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.822348ms
Feb 23 20:55:50.699: INFO: Pod "pod-subpath-test-secret-t2qq": Phase="Running", Reason="", readiness=true. Elapsed: 2.010720712s
Feb 23 20:55:52.712: INFO: Pod "pod-subpath-test-secret-t2qq": Phase="Running", Reason="", readiness=true. Elapsed: 4.022925904s
Feb 23 20:55:54.717: INFO: Pod "pod-subpath-test-secret-t2qq": Phase="Running", Reason="", readiness=true. Elapsed: 6.028570443s
Feb 23 20:55:56.721: INFO: Pod "pod-subpath-test-secret-t2qq": Phase="Running", Reason="", readiness=true. Elapsed: 8.032153455s
Feb 23 20:55:58.725: INFO: Pod "pod-subpath-test-secret-t2qq": Phase="Running", Reason="", readiness=true. Elapsed: 10.03643553s
Feb 23 20:56:00.731: INFO: Pod "pod-subpath-test-secret-t2qq": Phase="Running", Reason="", readiness=true. Elapsed: 12.042537681s
Feb 23 20:56:02.736: INFO: Pod "pod-subpath-test-secret-t2qq": Phase="Running", Reason="", readiness=true. Elapsed: 14.047716812s
Feb 23 20:56:04.744: INFO: Pod "pod-subpath-test-secret-t2qq": Phase="Running", Reason="", readiness=true. Elapsed: 16.055235896s
Feb 23 20:56:06.750: INFO: Pod "pod-subpath-test-secret-t2qq": Phase="Running", Reason="", readiness=true. Elapsed: 18.061388866s
Feb 23 20:56:08.754: INFO: Pod "pod-subpath-test-secret-t2qq": Phase="Running", Reason="", readiness=true. Elapsed: 20.064967052s
Feb 23 20:56:10.764: INFO: Pod "pod-subpath-test-secret-t2qq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.075417238s
STEP: Saw pod success
Feb 23 20:56:10.764: INFO: Pod "pod-subpath-test-secret-t2qq" satisfied condition "success or failure"
Feb 23 20:56:10.767: INFO: Trying to get logs from node worker01 pod pod-subpath-test-secret-t2qq container test-container-subpath-secret-t2qq: <nil>
STEP: delete the pod
Feb 23 20:56:10.794: INFO: Waiting for pod pod-subpath-test-secret-t2qq to disappear
Feb 23 20:56:10.797: INFO: Pod pod-subpath-test-secret-t2qq no longer exists
STEP: Deleting pod pod-subpath-test-secret-t2qq
Feb 23 20:56:10.797: INFO: Deleting pod "pod-subpath-test-secret-t2qq" in namespace "subpath-3378"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:56:10.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3378" for this suite.

â€¢ [SLOW TEST:22.167 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":280,"completed":78,"skipped":1406,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:56:10.809: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2462
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2462
STEP: creating replication controller externalsvc in namespace services-2462
I0223 20:56:10.879515      23 runners.go:189] Created replication controller with name: externalsvc, namespace: services-2462, replica count: 2
I0223 20:56:13.931038      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Feb 23 20:56:13.955: INFO: Creating new exec pod
Feb 23 20:56:15.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=services-2462 execpodfbhrj -- /bin/sh -x -c nslookup nodeport-service'
Feb 23 20:56:16.819: INFO: stderr: "+ nslookup nodeport-service\n"
Feb 23 20:56:16.819: INFO: stdout: "Server:\t\t10.32.0.10\nAddress:\t10.32.0.10#53\n\nnodeport-service.services-2462.svc.cluster.local\tcanonical name = externalsvc.services-2462.svc.cluster.local.\nName:\texternalsvc.services-2462.svc.cluster.local\nAddress: 10.32.0.75\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2462, will wait for the garbage collector to delete the pods
Feb 23 20:56:16.888: INFO: Deleting ReplicationController externalsvc took: 13.721196ms
Feb 23 20:56:17.790: INFO: Terminating ReplicationController externalsvc pods took: 902.078603ms
Feb 23 20:56:30.318: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:56:30.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2462" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:19.545 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":280,"completed":79,"skipped":1445,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:56:30.362: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Feb 23 20:56:30.406: INFO: Waiting up to 5m0s for pod "client-containers-bde79438-012c-40e5-aaf3-c36e83f5b730" in namespace "containers-4406" to be "success or failure"
Feb 23 20:56:30.409: INFO: Pod "client-containers-bde79438-012c-40e5-aaf3-c36e83f5b730": Phase="Pending", Reason="", readiness=false. Elapsed: 3.237727ms
Feb 23 20:56:32.412: INFO: Pod "client-containers-bde79438-012c-40e5-aaf3-c36e83f5b730": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005955794s
STEP: Saw pod success
Feb 23 20:56:32.412: INFO: Pod "client-containers-bde79438-012c-40e5-aaf3-c36e83f5b730" satisfied condition "success or failure"
Feb 23 20:56:32.415: INFO: Trying to get logs from node worker01 pod client-containers-bde79438-012c-40e5-aaf3-c36e83f5b730 container test-container: <nil>
STEP: delete the pod
Feb 23 20:56:32.435: INFO: Waiting for pod client-containers-bde79438-012c-40e5-aaf3-c36e83f5b730 to disappear
Feb 23 20:56:32.438: INFO: Pod client-containers-bde79438-012c-40e5-aaf3-c36e83f5b730 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:56:32.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4406" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":280,"completed":80,"skipped":1513,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:56:32.448: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:56:32.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 create -f - --namespace=kubectl-4167'
Feb 23 20:56:32.745: INFO: stderr: ""
Feb 23 20:56:32.745: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Feb 23 20:56:32.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 create -f - --namespace=kubectl-4167'
Feb 23 20:56:32.861: INFO: stderr: ""
Feb 23 20:56:32.861: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Feb 23 20:56:33.870: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 20:56:33.870: INFO: Found 0 / 1
Feb 23 20:56:34.891: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 20:56:34.891: INFO: Found 1 / 1
Feb 23 20:56:34.891: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 23 20:56:34.895: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 20:56:34.895: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 23 20:56:34.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 describe pod agnhost-master-d7rkx --namespace=kubectl-4167'
Feb 23 20:56:34.968: INFO: stderr: ""
Feb 23 20:56:34.968: INFO: stdout: "Name:         agnhost-master-d7rkx\nNamespace:    kubectl-4167\nPriority:     0\nNode:         worker01/192.168.180.101\nStart Time:   Sun, 23 Feb 2020 20:56:32 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 10.200.5.44/32\nStatus:       Running\nIP:           10.200.5.44\nIPs:\n  IP:           10.200.5.44\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   containerd://671335391b04787473508d9e7127735eae2000b6db3d0ab601a03dc687a16a3e\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 23 Feb 2020 20:56:33 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lk4fc (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-lk4fc:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-lk4fc\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From               Message\n  ----    ------     ----       ----               -------\n  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned kubectl-4167/agnhost-master-d7rkx to worker01\n  Normal  Pulled     1s         kubelet, worker01  Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    1s         kubelet, worker01  Created container agnhost-master\n  Normal  Started    1s         kubelet, worker01  Started container agnhost-master\n"
Feb 23 20:56:34.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 describe rc agnhost-master --namespace=kubectl-4167'
Feb 23 20:56:35.042: INFO: stderr: ""
Feb 23 20:56:35.042: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-4167\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-master-d7rkx\n"
Feb 23 20:56:35.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 describe service agnhost-master --namespace=kubectl-4167'
Feb 23 20:56:35.104: INFO: stderr: ""
Feb 23 20:56:35.104: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-4167\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.32.0.53\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.200.5.44:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 23 20:56:35.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 describe node worker00'
Feb 23 20:56:35.191: INFO: stderr: ""
Feb 23 20:56:35.191: INFO: stdout: "Name:               worker00\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=worker00\n                    kubernetes.io/os=linux\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"cephfs.csi.ceph.com\":\"worker00\",\"rbd.csi.ceph.com\":\"worker00\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.180.100/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.200.131.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 23 Feb 2020 20:27:54 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  worker00\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 23 Feb 2020 20:56:27 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sun, 23 Feb 2020 20:30:38 +0000   Sun, 23 Feb 2020 20:30:38 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sun, 23 Feb 2020 20:51:55 +0000   Sun, 23 Feb 2020 20:27:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sun, 23 Feb 2020 20:51:55 +0000   Sun, 23 Feb 2020 20:27:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sun, 23 Feb 2020 20:51:55 +0000   Sun, 23 Feb 2020 20:27:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sun, 23 Feb 2020 20:51:55 +0000   Sun, 23 Feb 2020 20:28:34 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.180.100\n  Hostname:    worker00\nCapacity:\n  cpu:                4\n  ephemeral-storage:  64800356Ki\n  hugepages-2Mi:      0\n  memory:             4038884Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  59720007991\n  hugepages-2Mi:      0\n  memory:             3936484Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 53171cfbc876460b9e3577350cf904c5\n  System UUID:                E8CF3553-B891-184D-9CFD-C8C603BAF644\n  Boot ID:                    71f8228f-bbe3-412a-a786-40438b86340e\n  Kernel Version:             4.15.0-76-generic\n  OS Image:                   Ubuntu 18.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.3.3\n  Kubelet Version:            v1.17.3\n  Kube-Proxy Version:         v1.17.3\nPodCIDR:                      10.200.0.0/24\nPodCIDRs:                     10.200.0.0/24\nNon-terminated Pods:          (26 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-676544c7b9-j9z7b                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 coredns-676544c7b9-q29b9                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 etcd-worker00                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 gobetween-worker00                                         100m (2%)     0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 kube-apiserver-worker00                                    250m (6%)     0 (0%)      0 (0%)           0 (0%)         27m\n  kube-system                 kube-controller-manager-worker00                           200m (5%)     0 (0%)      0 (0%)           0 (0%)         27m\n  kube-system                 kube-proxy-worker00                                        200m (5%)     0 (0%)      0 (0%)           0 (0%)         27m\n  kube-system                 kube-scheduler-worker00                                    100m (2%)     0 (0%)      0 (0%)           0 (0%)         27m\n  networking                  calico-kube-controllers-7cd585bcd-vmw47                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  networking                  calico-node-st65h                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         28m\n  networking                  metallb-controller-b96bfbbf8-p9lnh                         100m (2%)     100m (2%)   100Mi (2%)       100Mi (2%)     28m\n  networking                  metallb-speaker-74sbt                                      100m (2%)     100m (2%)   100Mi (2%)       100Mi (2%)     28m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-975a2eaddfc64d32-ct4lk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m\n  storage                     ceph-mds-worker00-6f479b4486-bdmhh                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  storage                     ceph-mgr-94b9dd996-ggbsc                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  storage                     ceph-mon-worker00-5cf654d469-bcbdt                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  storage                     ceph-osd-worker00-556546b495-tmvn5                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  storage                     ceph-rgw-57cd48f74c-lf65j                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  storage                     csi-cephfsplugin-provisioner-6cd7596f75-492rn              0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  storage                     csi-cephfsplugin-provisioner-6cd7596f75-9hfw2              0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  storage                     csi-cephfsplugin-provisioner-6cd7596f75-tw7n4              0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  storage                     csi-cephfsplugin-q68vh                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  storage                     csi-rbdplugin-provisioner-7494f65674-2h6xd                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  storage                     csi-rbdplugin-provisioner-7494f65674-7tn5d                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  storage                     csi-rbdplugin-provisioner-7494f65674-ddxmx                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  storage                     csi-rbdplugin-z2bdm                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1300m (32%)  200m (5%)\n  memory             200Mi (5%)   200Mi (5%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:\n  Type    Reason                   Age                From                  Message\n  ----    ------                   ----               ----                  -------\n  Normal  NodeHasSufficientMemory  29m (x8 over 29m)  kubelet, worker00     Node worker00 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    29m (x7 over 29m)  kubelet, worker00     Node worker00 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     29m (x7 over 29m)  kubelet, worker00     Node worker00 status is now: NodeHasSufficientPID\n  Normal  Starting                 28m                kube-proxy, worker00  Starting kube-proxy.\n"
Feb 23 20:56:35.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 describe namespace kubectl-4167'
Feb 23 20:56:35.264: INFO: stderr: ""
Feb 23 20:56:35.264: INFO: stdout: "Name:         kubectl-4167\nLabels:       e2e-framework=kubectl\n              e2e-run=a24d4a15-af2e-4544-8538-91d24c5862e9\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:56:35.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4167" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":280,"completed":81,"skipped":1578,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:56:35.277: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb 23 20:56:35.324: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 23 20:56:42.368: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:56:42.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5383" for this suite.

â€¢ [SLOW TEST:7.114 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":280,"completed":82,"skipped":1599,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:56:42.391: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Feb 23 20:56:42.425: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-277121970 proxy --unix-socket=/tmp/kubectl-proxy-unix053079033/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:56:42.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1251" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":280,"completed":83,"skipped":1609,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:56:42.476: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Feb 23 20:56:43.034: INFO: created pod pod-service-account-defaultsa
Feb 23 20:56:43.034: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 23 20:56:43.041: INFO: created pod pod-service-account-mountsa
Feb 23 20:56:43.041: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 23 20:56:43.049: INFO: created pod pod-service-account-nomountsa
Feb 23 20:56:43.049: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 23 20:56:43.058: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 23 20:56:43.058: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 23 20:56:43.079: INFO: created pod pod-service-account-mountsa-mountspec
Feb 23 20:56:43.079: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 23 20:56:43.088: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 23 20:56:43.088: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 23 20:56:43.104: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 23 20:56:43.104: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 23 20:56:43.118: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 23 20:56:43.118: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 23 20:56:43.130: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 23 20:56:43.130: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:56:43.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7308" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":280,"completed":84,"skipped":1611,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:56:43.142: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 20:56:43.662: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 23 20:56:45.678: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088204, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088204, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088204, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088204, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 20:56:47.682: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088204, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088204, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088204, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088204, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 20:56:49.685: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088204, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088204, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088204, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088204, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 20:56:52.697: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:56:52.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6439" for this suite.
STEP: Destroying namespace "webhook-6439-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:9.703 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":280,"completed":85,"skipped":1627,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:56:52.850: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 23 20:56:54.916: INFO: &Pod{ObjectMeta:{send-events-085530e9-a5d5-4e3a-92f7-818b143d2283  events-3584 /api/v1/namespaces/events-3584/pods/send-events-085530e9-a5d5-4e3a-92f7-818b143d2283 3d6cc494-7acb-4d5a-9af8-ccd77d26a609 12188 0 2020-02-23 20:56:53 +0000 UTC <nil> <nil> map[name:foo time:892652713] map[cni.projectcalico.org/podIP:10.200.5.55/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m2mns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m2mns,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m2mns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:56:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:56:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:56:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:56:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:10.200.5.55,StartTime:2020-02-23 20:56:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 20:56:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://0cad1d5f028701e1a4fada50fabbfcc6ef48c42eedb233f25d5ce9ff7880691a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Feb 23 20:56:56.922: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 23 20:56:58.927: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:56:58.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3584" for this suite.

â€¢ [SLOW TEST:6.106 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":280,"completed":86,"skipped":1635,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:56:58.956: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 23 20:56:58.992: INFO: Waiting up to 5m0s for pod "downward-api-32779ff3-c4bf-4bef-965e-bea040f56d66" in namespace "downward-api-3705" to be "success or failure"
Feb 23 20:56:58.996: INFO: Pod "downward-api-32779ff3-c4bf-4bef-965e-bea040f56d66": Phase="Pending", Reason="", readiness=false. Elapsed: 3.461825ms
Feb 23 20:57:00.999: INFO: Pod "downward-api-32779ff3-c4bf-4bef-965e-bea040f56d66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006522032s
STEP: Saw pod success
Feb 23 20:57:00.999: INFO: Pod "downward-api-32779ff3-c4bf-4bef-965e-bea040f56d66" satisfied condition "success or failure"
Feb 23 20:57:01.002: INFO: Trying to get logs from node worker01 pod downward-api-32779ff3-c4bf-4bef-965e-bea040f56d66 container dapi-container: <nil>
STEP: delete the pod
Feb 23 20:57:01.020: INFO: Waiting for pod downward-api-32779ff3-c4bf-4bef-965e-bea040f56d66 to disappear
Feb 23 20:57:01.028: INFO: Pod downward-api-32779ff3-c4bf-4bef-965e-bea040f56d66 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:57:01.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3705" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":280,"completed":87,"skipped":1637,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:57:01.037: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 23 20:57:01.074: INFO: Waiting up to 5m0s for pod "pod-d8d70506-e832-4cb4-bdbc-37291319e3c1" in namespace "emptydir-9011" to be "success or failure"
Feb 23 20:57:01.083: INFO: Pod "pod-d8d70506-e832-4cb4-bdbc-37291319e3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.045439ms
Feb 23 20:57:03.096: INFO: Pod "pod-d8d70506-e832-4cb4-bdbc-37291319e3c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021313115s
STEP: Saw pod success
Feb 23 20:57:03.096: INFO: Pod "pod-d8d70506-e832-4cb4-bdbc-37291319e3c1" satisfied condition "success or failure"
Feb 23 20:57:03.099: INFO: Trying to get logs from node worker01 pod pod-d8d70506-e832-4cb4-bdbc-37291319e3c1 container test-container: <nil>
STEP: delete the pod
Feb 23 20:57:03.120: INFO: Waiting for pod pod-d8d70506-e832-4cb4-bdbc-37291319e3c1 to disappear
Feb 23 20:57:03.123: INFO: Pod pod-d8d70506-e832-4cb4-bdbc-37291319e3c1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:57:03.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9011" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":88,"skipped":1645,"failed":0}
SSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:57:03.132: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-fbaed062-73ad-4d1d-9c53-bf156887b780
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:57:03.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3761" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":280,"completed":89,"skipped":1648,"failed":0}
SSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:57:03.177: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-2998
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2998
STEP: Deleting pre-stop pod
Feb 23 20:57:12.262: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:57:12.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2998" for this suite.

â€¢ [SLOW TEST:9.117 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":280,"completed":90,"skipped":1653,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:57:12.298: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-1203
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1203 to expose endpoints map[]
Feb 23 20:57:12.353: INFO: Get endpoints failed (7.648964ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Feb 23 20:57:13.359: INFO: successfully validated that service endpoint-test2 in namespace services-1203 exposes endpoints map[] (1.013292473s elapsed)
STEP: Creating pod pod1 in namespace services-1203
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1203 to expose endpoints map[pod1:[80]]
Feb 23 20:57:15.397: INFO: successfully validated that service endpoint-test2 in namespace services-1203 exposes endpoints map[pod1:[80]] (2.02998804s elapsed)
STEP: Creating pod pod2 in namespace services-1203
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1203 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 23 20:57:17.451: INFO: successfully validated that service endpoint-test2 in namespace services-1203 exposes endpoints map[pod1:[80] pod2:[80]] (2.04836933s elapsed)
STEP: Deleting pod pod1 in namespace services-1203
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1203 to expose endpoints map[pod2:[80]]
Feb 23 20:57:17.477: INFO: successfully validated that service endpoint-test2 in namespace services-1203 exposes endpoints map[pod2:[80]] (11.11251ms elapsed)
STEP: Deleting pod pod2 in namespace services-1203
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1203 to expose endpoints map[]
Feb 23 20:57:18.506: INFO: successfully validated that service endpoint-test2 in namespace services-1203 exposes endpoints map[] (1.015430605s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:57:18.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1203" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:6.268 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":280,"completed":91,"skipped":1657,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:57:18.567: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-d13f686a-2fcb-4220-869d-db7701e8ecbb
STEP: Creating a pod to test consume configMaps
Feb 23 20:57:18.644: INFO: Waiting up to 5m0s for pod "pod-configmaps-2042c407-b461-41a8-bffa-b3fa3f28f5e3" in namespace "configmap-1693" to be "success or failure"
Feb 23 20:57:18.650: INFO: Pod "pod-configmaps-2042c407-b461-41a8-bffa-b3fa3f28f5e3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.248021ms
Feb 23 20:57:20.658: INFO: Pod "pod-configmaps-2042c407-b461-41a8-bffa-b3fa3f28f5e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014472082s
STEP: Saw pod success
Feb 23 20:57:20.659: INFO: Pod "pod-configmaps-2042c407-b461-41a8-bffa-b3fa3f28f5e3" satisfied condition "success or failure"
Feb 23 20:57:20.663: INFO: Trying to get logs from node worker01 pod pod-configmaps-2042c407-b461-41a8-bffa-b3fa3f28f5e3 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 20:57:20.681: INFO: Waiting for pod pod-configmaps-2042c407-b461-41a8-bffa-b3fa3f28f5e3 to disappear
Feb 23 20:57:20.684: INFO: Pod pod-configmaps-2042c407-b461-41a8-bffa-b3fa3f28f5e3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:57:20.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1693" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":92,"skipped":1658,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:57:20.691: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 23 20:57:20.759: INFO: Number of nodes with available pods: 0
Feb 23 20:57:20.760: INFO: Node worker00 is running more than one daemon pod
Feb 23 20:57:21.768: INFO: Number of nodes with available pods: 0
Feb 23 20:57:21.768: INFO: Node worker00 is running more than one daemon pod
Feb 23 20:57:22.767: INFO: Number of nodes with available pods: 2
Feb 23 20:57:22.767: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 23 20:57:22.799: INFO: Number of nodes with available pods: 1
Feb 23 20:57:22.799: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:57:23.805: INFO: Number of nodes with available pods: 1
Feb 23 20:57:23.805: INFO: Node worker01 is running more than one daemon pod
Feb 23 20:57:24.808: INFO: Number of nodes with available pods: 2
Feb 23 20:57:24.808: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1815, will wait for the garbage collector to delete the pods
Feb 23 20:57:24.878: INFO: Deleting DaemonSet.extensions daemon-set took: 11.157978ms
Feb 23 20:57:25.779: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.674053ms
Feb 23 20:57:37.782: INFO: Number of nodes with available pods: 0
Feb 23 20:57:37.782: INFO: Number of running nodes: 0, number of available pods: 0
Feb 23 20:57:37.785: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1815/daemonsets","resourceVersion":"12660"},"items":null}

Feb 23 20:57:37.789: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1815/pods","resourceVersion":"12660"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:57:37.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1815" for this suite.

â€¢ [SLOW TEST:17.115 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":280,"completed":93,"skipped":1672,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:57:37.808: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-e5d93f30-0f4a-4d4c-aced-98ee06a795dc
STEP: Creating configMap with name cm-test-opt-upd-84de430d-e165-4073-80d1-48057389d898
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e5d93f30-0f4a-4d4c-aced-98ee06a795dc
STEP: Updating configmap cm-test-opt-upd-84de430d-e165-4073-80d1-48057389d898
STEP: Creating configMap with name cm-test-opt-create-aaa222a8-e042-4026-a8b7-69e3caf321b7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:57:43.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6787" for this suite.

â€¢ [SLOW TEST:6.148 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":94,"skipped":1731,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:57:43.959: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:58:00.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2771" for this suite.

â€¢ [SLOW TEST:16.177 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":280,"completed":95,"skipped":1751,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:58:00.136: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-3177/configmap-test-473bf7bf-23d4-42e2-8925-c8bdc7ad144e
STEP: Creating a pod to test consume configMaps
Feb 23 20:58:00.179: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d1b0a78-2f6f-4e96-9a76-df09b8fedbbc" in namespace "configmap-3177" to be "success or failure"
Feb 23 20:58:00.183: INFO: Pod "pod-configmaps-9d1b0a78-2f6f-4e96-9a76-df09b8fedbbc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.52397ms
Feb 23 20:58:02.188: INFO: Pod "pod-configmaps-9d1b0a78-2f6f-4e96-9a76-df09b8fedbbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008784694s
STEP: Saw pod success
Feb 23 20:58:02.188: INFO: Pod "pod-configmaps-9d1b0a78-2f6f-4e96-9a76-df09b8fedbbc" satisfied condition "success or failure"
Feb 23 20:58:02.192: INFO: Trying to get logs from node worker01 pod pod-configmaps-9d1b0a78-2f6f-4e96-9a76-df09b8fedbbc container env-test: <nil>
STEP: delete the pod
Feb 23 20:58:02.216: INFO: Waiting for pod pod-configmaps-9d1b0a78-2f6f-4e96-9a76-df09b8fedbbc to disappear
Feb 23 20:58:02.220: INFO: Pod pod-configmaps-9d1b0a78-2f6f-4e96-9a76-df09b8fedbbc no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:58:02.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3177" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":280,"completed":96,"skipped":1758,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:58:02.229: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-pvsr
STEP: Creating a pod to test atomic-volume-subpath
Feb 23 20:58:02.279: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pvsr" in namespace "subpath-70" to be "success or failure"
Feb 23 20:58:02.282: INFO: Pod "pod-subpath-test-projected-pvsr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.862745ms
Feb 23 20:58:04.286: INFO: Pod "pod-subpath-test-projected-pvsr": Phase="Running", Reason="", readiness=true. Elapsed: 2.006578015s
Feb 23 20:58:06.291: INFO: Pod "pod-subpath-test-projected-pvsr": Phase="Running", Reason="", readiness=true. Elapsed: 4.012282191s
Feb 23 20:58:08.295: INFO: Pod "pod-subpath-test-projected-pvsr": Phase="Running", Reason="", readiness=true. Elapsed: 6.016309912s
Feb 23 20:58:10.307: INFO: Pod "pod-subpath-test-projected-pvsr": Phase="Running", Reason="", readiness=true. Elapsed: 8.027522045s
Feb 23 20:58:12.311: INFO: Pod "pod-subpath-test-projected-pvsr": Phase="Running", Reason="", readiness=true. Elapsed: 10.031567699s
Feb 23 20:58:14.315: INFO: Pod "pod-subpath-test-projected-pvsr": Phase="Running", Reason="", readiness=true. Elapsed: 12.036001017s
Feb 23 20:58:16.318: INFO: Pod "pod-subpath-test-projected-pvsr": Phase="Running", Reason="", readiness=true. Elapsed: 14.039363669s
Feb 23 20:58:18.327: INFO: Pod "pod-subpath-test-projected-pvsr": Phase="Running", Reason="", readiness=true. Elapsed: 16.047560189s
Feb 23 20:58:20.331: INFO: Pod "pod-subpath-test-projected-pvsr": Phase="Running", Reason="", readiness=true. Elapsed: 18.052076763s
Feb 23 20:58:22.336: INFO: Pod "pod-subpath-test-projected-pvsr": Phase="Running", Reason="", readiness=true. Elapsed: 20.056985531s
Feb 23 20:58:24.343: INFO: Pod "pod-subpath-test-projected-pvsr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.063658727s
STEP: Saw pod success
Feb 23 20:58:24.344: INFO: Pod "pod-subpath-test-projected-pvsr" satisfied condition "success or failure"
Feb 23 20:58:24.350: INFO: Trying to get logs from node worker01 pod pod-subpath-test-projected-pvsr container test-container-subpath-projected-pvsr: <nil>
STEP: delete the pod
Feb 23 20:58:24.368: INFO: Waiting for pod pod-subpath-test-projected-pvsr to disappear
Feb 23 20:58:24.371: INFO: Pod pod-subpath-test-projected-pvsr no longer exists
STEP: Deleting pod pod-subpath-test-projected-pvsr
Feb 23 20:58:24.371: INFO: Deleting pod "pod-subpath-test-projected-pvsr" in namespace "subpath-70"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:58:24.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-70" for this suite.

â€¢ [SLOW TEST:22.154 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":280,"completed":97,"skipped":1767,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:58:24.384: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:58:24.414: INFO: Creating deployment "test-recreate-deployment"
Feb 23 20:58:24.420: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 23 20:58:24.428: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 23 20:58:26.438: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 23 20:58:26.441: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 23 20:58:26.447: INFO: Updating deployment test-recreate-deployment
Feb 23 20:58:26.447: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 23 20:58:26.532: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1533 /apis/apps/v1/namespaces/deployment-1533/deployments/test-recreate-deployment 53f974d1-38c8-4f78-b1fa-f206d3025809 13065 2 2020-02-23 20:58:24 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00580aee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-02-23 20:58:26 +0000 UTC,LastTransitionTime:2020-02-23 20:58:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-02-23 20:58:26 +0000 UTC,LastTransitionTime:2020-02-23 20:58:24 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 23 20:58:26.534: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-1533 /apis/apps/v1/namespaces/deployment-1533/replicasets/test-recreate-deployment-5f94c574ff 6e4a3e6f-b566-4b8e-a12d-3125a28d0a42 13062 1 2020-02-23 20:58:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 53f974d1-38c8-4f78-b1fa-f206d3025809 0xc00580b297 0xc00580b298}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00580b2f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 23 20:58:26.534: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 23 20:58:26.534: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-1533 /apis/apps/v1/namespaces/deployment-1533/replicasets/test-recreate-deployment-799c574856 88ed238f-d614-4f60-bd43-119aa445b923 13054 2 2020-02-23 20:58:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 53f974d1-38c8-4f78-b1fa-f206d3025809 0xc00580b367 0xc00580b368}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00580b3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 23 20:58:26.537: INFO: Pod "test-recreate-deployment-5f94c574ff-bx5wh" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-bx5wh test-recreate-deployment-5f94c574ff- deployment-1533 /api/v1/namespaces/deployment-1533/pods/test-recreate-deployment-5f94c574ff-bx5wh 07c9b043-0158-4e44-859d-f046331bdf89 13066 0 2020-02-23 20:58:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 6e4a3e6f-b566-4b8e-a12d-3125a28d0a42 0xc00580b867 0xc00580b868}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mmpjb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mmpjb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mmpjb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 20:58:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:58:26.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1533" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":98,"skipped":1776,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:58:26.554: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:58:26.581: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 23 20:58:34.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-4022 create -f -'
Feb 23 20:58:35.470: INFO: stderr: ""
Feb 23 20:58:35.470: INFO: stdout: "e2e-test-crd-publish-openapi-549-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 23 20:58:35.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-4022 delete e2e-test-crd-publish-openapi-549-crds test-cr'
Feb 23 20:58:35.532: INFO: stderr: ""
Feb 23 20:58:35.532: INFO: stdout: "e2e-test-crd-publish-openapi-549-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 23 20:58:35.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-4022 apply -f -'
Feb 23 20:58:36.788: INFO: stderr: ""
Feb 23 20:58:36.788: INFO: stdout: "e2e-test-crd-publish-openapi-549-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 23 20:58:36.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-4022 delete e2e-test-crd-publish-openapi-549-crds test-cr'
Feb 23 20:58:36.854: INFO: stderr: ""
Feb 23 20:58:36.854: INFO: stdout: "e2e-test-crd-publish-openapi-549-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 23 20:58:36.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 explain e2e-test-crd-publish-openapi-549-crds'
Feb 23 20:58:38.089: INFO: stderr: ""
Feb 23 20:58:38.089: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-549-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:58:40.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4022" for this suite.

â€¢ [SLOW TEST:14.454 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":280,"completed":99,"skipped":1785,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:58:41.009: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 23 20:58:41.048: INFO: Waiting up to 5m0s for pod "pod-6da1a57e-6fc3-4bea-9279-94d2ed372892" in namespace "emptydir-9668" to be "success or failure"
Feb 23 20:58:41.052: INFO: Pod "pod-6da1a57e-6fc3-4bea-9279-94d2ed372892": Phase="Pending", Reason="", readiness=false. Elapsed: 3.78726ms
Feb 23 20:58:43.056: INFO: Pod "pod-6da1a57e-6fc3-4bea-9279-94d2ed372892": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007972341s
STEP: Saw pod success
Feb 23 20:58:43.056: INFO: Pod "pod-6da1a57e-6fc3-4bea-9279-94d2ed372892" satisfied condition "success or failure"
Feb 23 20:58:43.058: INFO: Trying to get logs from node worker01 pod pod-6da1a57e-6fc3-4bea-9279-94d2ed372892 container test-container: <nil>
STEP: delete the pod
Feb 23 20:58:43.078: INFO: Waiting for pod pod-6da1a57e-6fc3-4bea-9279-94d2ed372892 to disappear
Feb 23 20:58:43.080: INFO: Pod pod-6da1a57e-6fc3-4bea-9279-94d2ed372892 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:58:43.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9668" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":100,"skipped":1787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:58:43.093: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-c7f77afd-ac54-4df8-b5ea-662f94e6dc9a
STEP: Creating secret with name s-test-opt-upd-d1c4a903-e5c6-4ce2-8b44-b4f67e593d00
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c7f77afd-ac54-4df8-b5ea-662f94e6dc9a
STEP: Updating secret s-test-opt-upd-d1c4a903-e5c6-4ce2-8b44-b4f67e593d00
STEP: Creating secret with name s-test-opt-create-d9a418df-bfbf-4ac6-82fb-68d8791772e2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:58:47.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1481" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":101,"skipped":1850,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:58:47.238: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-5224/secret-test-d08115a5-82b1-4d9e-9568-6eeac66d859e
STEP: Creating a pod to test consume secrets
Feb 23 20:58:47.277: INFO: Waiting up to 5m0s for pod "pod-configmaps-2c3eb247-9448-4757-a73d-ea46f35285cb" in namespace "secrets-5224" to be "success or failure"
Feb 23 20:58:47.281: INFO: Pod "pod-configmaps-2c3eb247-9448-4757-a73d-ea46f35285cb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.9909ms
Feb 23 20:58:49.287: INFO: Pod "pod-configmaps-2c3eb247-9448-4757-a73d-ea46f35285cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009913411s
STEP: Saw pod success
Feb 23 20:58:49.287: INFO: Pod "pod-configmaps-2c3eb247-9448-4757-a73d-ea46f35285cb" satisfied condition "success or failure"
Feb 23 20:58:49.292: INFO: Trying to get logs from node worker01 pod pod-configmaps-2c3eb247-9448-4757-a73d-ea46f35285cb container env-test: <nil>
STEP: delete the pod
Feb 23 20:58:49.320: INFO: Waiting for pod pod-configmaps-2c3eb247-9448-4757-a73d-ea46f35285cb to disappear
Feb 23 20:58:49.323: INFO: Pod pod-configmaps-2c3eb247-9448-4757-a73d-ea46f35285cb no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:58:49.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5224" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":102,"skipped":1869,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:58:49.331: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-0808eb4d-f82f-4ded-bb5f-ce6c6a10f191
STEP: Creating a pod to test consume configMaps
Feb 23 20:58:49.373: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-87eeec2c-4d8c-49fb-b3e6-c3c86b34b2bc" in namespace "projected-2009" to be "success or failure"
Feb 23 20:58:49.383: INFO: Pod "pod-projected-configmaps-87eeec2c-4d8c-49fb-b3e6-c3c86b34b2bc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.443063ms
Feb 23 20:58:51.391: INFO: Pod "pod-projected-configmaps-87eeec2c-4d8c-49fb-b3e6-c3c86b34b2bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01680784s
STEP: Saw pod success
Feb 23 20:58:51.391: INFO: Pod "pod-projected-configmaps-87eeec2c-4d8c-49fb-b3e6-c3c86b34b2bc" satisfied condition "success or failure"
Feb 23 20:58:51.394: INFO: Trying to get logs from node worker01 pod pod-projected-configmaps-87eeec2c-4d8c-49fb-b3e6-c3c86b34b2bc container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 20:58:51.418: INFO: Waiting for pod pod-projected-configmaps-87eeec2c-4d8c-49fb-b3e6-c3c86b34b2bc to disappear
Feb 23 20:58:51.421: INFO: Pod pod-projected-configmaps-87eeec2c-4d8c-49fb-b3e6-c3c86b34b2bc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:58:51.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2009" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":103,"skipped":1895,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:58:51.429: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:58:51.456: INFO: Creating deployment "webserver-deployment"
Feb 23 20:58:51.464: INFO: Waiting for observed generation 1
Feb 23 20:58:53.475: INFO: Waiting for all required pods to come up
Feb 23 20:58:53.480: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 23 20:58:55.494: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 23 20:58:55.500: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 23 20:58:55.508: INFO: Updating deployment webserver-deployment
Feb 23 20:58:55.508: INFO: Waiting for observed generation 2
Feb 23 20:58:57.517: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 23 20:58:57.520: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 23 20:58:57.523: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 23 20:58:57.541: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 23 20:58:57.541: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 23 20:58:57.544: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 23 20:58:57.548: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 23 20:58:57.548: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 23 20:58:57.555: INFO: Updating deployment webserver-deployment
Feb 23 20:58:57.555: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 23 20:58:57.564: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 23 20:58:57.573: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 23 20:58:57.621: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-185 /apis/apps/v1/namespaces/deployment-185/deployments/webserver-deployment 8cf3d6fa-e83d-416d-a692-17cb50629a22 13623 3 2020-02-23 20:58:51 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0070ba348 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-02-23 20:58:56 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-02-23 20:58:57 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 23 20:58:57.665: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-185 /apis/apps/v1/namespaces/deployment-185/replicasets/webserver-deployment-c7997dcc8 f1bfc0cf-7463-45cc-a9eb-ec7e930576ae 13619 3 2020-02-23 20:58:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8cf3d6fa-e83d-416d-a692-17cb50629a22 0xc007185f17 0xc007185f18}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007185fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 23 20:58:57.665: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 23 20:58:57.665: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-185 /apis/apps/v1/namespaces/deployment-185/replicasets/webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 13618 3 2020-02-23 20:58:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8cf3d6fa-e83d-416d-a692-17cb50629a22 0xc007185e57 0xc007185e58}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007185eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 23 20:58:57.722: INFO: Pod "webserver-deployment-595b5b9587-5b78f" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5b78f webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-5b78f c9bf1bbd-d957-4306-961e-0b198df87888 13676 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704c587 0xc00704c588}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.723: INFO: Pod "webserver-deployment-595b5b9587-5gck6" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5gck6 webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-5gck6 5bc39374-a293-487d-81dc-737359adb31b 13633 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704c6b0 0xc00704c6b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.723: INFO: Pod "webserver-deployment-595b5b9587-65nj9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-65nj9 webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-65nj9 57666988-cd79-498d-b5b9-cc03660f8c96 13655 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704c7c0 0xc00704c7c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.727: INFO: Pod "webserver-deployment-595b5b9587-67rbn" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-67rbn webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-67rbn 8a2f657e-d459-4841-bd60-99cd9e3d1a90 13459 0 2020-02-23 20:58:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.131.172/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704c910 0xc00704c911}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:10.200.131.172,StartTime:2020-02-23 20:58:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 20:58:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://fecdfaf44082ee771df05dc0e33a0225be445346f4829941a36bd38232d459be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.727: INFO: Pod "webserver-deployment-595b5b9587-6qt6w" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6qt6w webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-6qt6w 1a10f6d8-ccd0-4573-977a-ed46adbe64b9 13649 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704cad7 0xc00704cad8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 20:58:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.727: INFO: Pod "webserver-deployment-595b5b9587-7mh27" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7mh27 webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-7mh27 6b9bb0ee-7299-4731-8e72-565928e632c0 13484 0 2020-02-23 20:58:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.5.18/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704cc67 0xc00704cc68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:10.200.5.18,StartTime:2020-02-23 20:58:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 20:58:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://70abd635841e0278b036582adf48e57768c52c89b35479a9edc1b96c1e37dd3f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.727: INFO: Pod "webserver-deployment-595b5b9587-9z7wh" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9z7wh webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-9z7wh 45c7600e-a26b-49dd-9bd3-848cac721b37 13660 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704cde0 0xc00704cde1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.728: INFO: Pod "webserver-deployment-595b5b9587-fgfrb" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fgfrb webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-fgfrb bdb254dd-de0d-4a7d-87a2-39a265f67df7 13501 0 2020-02-23 20:58:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.131.173/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704cec7 0xc00704cec8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:10.200.131.173,StartTime:2020-02-23 20:58:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 20:58:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://cb575e9b45c6e25f4e8e45abb7ee861ca6f923ff696086a6e3fb7cefab6f3cf0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.728: INFO: Pod "webserver-deployment-595b5b9587-gh2xx" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-gh2xx webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-gh2xx b412ac68-56b9-4951-99c2-445aae2d9ba8 13463 0 2020-02-23 20:58:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.5.19/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704d057 0xc00704d058}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:10.200.5.19,StartTime:2020-02-23 20:58:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 20:58:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://b74f277ec10b10f18a76f97adbfd0b548042004856c3ac66d5a3ee5af3aab420,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.728: INFO: Pod "webserver-deployment-595b5b9587-hg5lg" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hg5lg webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-hg5lg 5fc5f086-679e-48a1-b36e-aedb09f6f3ce 13635 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704d1d0 0xc00704d1d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.728: INFO: Pod "webserver-deployment-595b5b9587-jzwh6" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jzwh6 webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-jzwh6 c1c1fe83-df3b-4d7a-b791-020c521be6dc 13495 0 2020-02-23 20:58:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.131.174/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704d2e0 0xc00704d2e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:10.200.131.174,StartTime:2020-02-23 20:58:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 20:58:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://c244ca2c2ef9e0225426255b40712936db49b73c31ac35171db6f54188694163,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.728: INFO: Pod "webserver-deployment-595b5b9587-kdwjf" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-kdwjf webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-kdwjf f4939d2a-f800-4931-aa4b-69ed3bf4ee8d 13497 0 2020-02-23 20:58:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.131.171/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704d4a7 0xc00704d4a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:10.200.131.171,StartTime:2020-02-23 20:58:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 20:58:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://766b8d714f8a50fb736abe12bed394dca1894c737aaf8ed236b6af1b654436a2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.729: INFO: Pod "webserver-deployment-595b5b9587-kljl9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-kljl9 webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-kljl9 454b0f7f-a179-425b-87a1-a37a2d1f985b 13679 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704d637 0xc00704d638}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.729: INFO: Pod "webserver-deployment-595b5b9587-lbbv7" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lbbv7 webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-lbbv7 4cf0d46f-86e8-4392-8f97-59e14876dcee 13666 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704d7b0 0xc00704d7b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.729: INFO: Pod "webserver-deployment-595b5b9587-mfhzw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mfhzw webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-mfhzw 332401d9-2a62-4fe1-82c4-bd5f9f47eeeb 13661 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704d8f0 0xc00704d8f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.729: INFO: Pod "webserver-deployment-595b5b9587-mgtxc" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mgtxc webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-mgtxc b100b4f7-ed0c-4c95-9940-7f88b159defd 13651 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704da30 0xc00704da31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.729: INFO: Pod "webserver-deployment-595b5b9587-qh44s" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qh44s webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-qh44s 56939d59-27cf-4337-9bc8-241744d56e3b 13462 0 2020-02-23 20:58:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.131.170/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704db80 0xc00704db81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:10.200.131.170,StartTime:2020-02-23 20:58:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 20:58:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://ff46d2dc49ec6daa65982f1e80258af46fc696c7c1db0b01588bb376c097843c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.131.170,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.729: INFO: Pod "webserver-deployment-595b5b9587-qwrtn" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qwrtn webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-qwrtn 2d2cd5c3-7df7-4afb-a6f5-0ad895f7152b 13668 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704dd47 0xc00704dd48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.729: INFO: Pod "webserver-deployment-595b5b9587-tf6ds" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tf6ds webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-tf6ds 28eb52c7-c271-4ec3-92e3-9992e36df5e3 13467 0 2020-02-23 20:58:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.200.5.15/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc00704de77 0xc00704de78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:10.200.5.15,StartTime:2020-02-23 20:58:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 20:58:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:containerd://8d2ac4b1bff050a13df7011fc3070be1efa581219fe9bebe8de1e5dcd3490ba4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.729: INFO: Pod "webserver-deployment-595b5b9587-xd9t6" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xd9t6 webserver-deployment-595b5b9587- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-595b5b9587-xd9t6 c87bef22-11ba-426d-a229-d9e601582434 13677 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2ba696a9-aca5-4782-a290-989546e51e43 0xc005ebc040 0xc005ebc041}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.730: INFO: Pod "webserver-deployment-c7997dcc8-4d57m" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4d57m webserver-deployment-c7997dcc8- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-c7997dcc8-4d57m 1a91caae-980f-46ab-97d8-5e21f2ba89c4 13670 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f1bfc0cf-7463-45cc-a9eb-ec7e930576ae 0xc005ebc150 0xc005ebc151}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.730: INFO: Pod "webserver-deployment-c7997dcc8-dghnd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-dghnd webserver-deployment-c7997dcc8- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-c7997dcc8-dghnd cec2c3e4-32dc-4e61-a3d9-bb3d8bda385d 13656 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f1bfc0cf-7463-45cc-a9eb-ec7e930576ae 0xc005ebc247 0xc005ebc248}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.730: INFO: Pod "webserver-deployment-c7997dcc8-kljq7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-kljq7 webserver-deployment-c7997dcc8- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-c7997dcc8-kljq7 4ab3c9ea-f2b8-4d2d-915a-97e69ed5f640 13665 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f1bfc0cf-7463-45cc-a9eb-ec7e930576ae 0xc005ebc3f0 0xc005ebc3f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.730: INFO: Pod "webserver-deployment-c7997dcc8-lsp4l" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lsp4l webserver-deployment-c7997dcc8- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-c7997dcc8-lsp4l ad485d14-4e43-4b17-b48f-337359850222 13588 0 2020-02-23 20:58:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.5.24/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f1bfc0cf-7463-45cc-a9eb-ec7e930576ae 0xc005ebc550 0xc005ebc551}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 20:58:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.730: INFO: Pod "webserver-deployment-c7997dcc8-lxbwl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lxbwl webserver-deployment-c7997dcc8- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-c7997dcc8-lxbwl 9f5f009f-93fd-4dca-93d2-1e79c279b1d2 13674 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f1bfc0cf-7463-45cc-a9eb-ec7e930576ae 0xc005ebc717 0xc005ebc718}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.730: INFO: Pod "webserver-deployment-c7997dcc8-qdd2v" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qdd2v webserver-deployment-c7997dcc8- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-c7997dcc8-qdd2v 47024e5d-cbf3-41e8-90f1-8cc64ac400fb 13640 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f1bfc0cf-7463-45cc-a9eb-ec7e930576ae 0xc005ebc870 0xc005ebc871}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:,StartTime:2020-02-23 20:58:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.730: INFO: Pod "webserver-deployment-c7997dcc8-t7lwc" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-t7lwc webserver-deployment-c7997dcc8- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-c7997dcc8-t7lwc 78dcf7bb-b762-4ed4-9810-63a707e49eae 13607 0 2020-02-23 20:58:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.5.23/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f1bfc0cf-7463-45cc-a9eb-ec7e930576ae 0xc005ebca17 0xc005ebca18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 20:58:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.730: INFO: Pod "webserver-deployment-c7997dcc8-tm2gs" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-tm2gs webserver-deployment-c7997dcc8- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-c7997dcc8-tm2gs 75791e8c-e71b-4c66-a979-0f4f9f8fbba6 13585 0 2020-02-23 20:58:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.5.20/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f1bfc0cf-7463-45cc-a9eb-ec7e930576ae 0xc005ebcba7 0xc005ebcba8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:,StartTime:2020-02-23 20:58:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.731: INFO: Pod "webserver-deployment-c7997dcc8-w4gmc" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-w4gmc webserver-deployment-c7997dcc8- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-c7997dcc8-w4gmc 465c087f-93ba-4c68-b9a2-8fc1b08d49dc 13645 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f1bfc0cf-7463-45cc-a9eb-ec7e930576ae 0xc005ebcd27 0xc005ebcd28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.731: INFO: Pod "webserver-deployment-c7997dcc8-x8nph" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-x8nph webserver-deployment-c7997dcc8- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-c7997dcc8-x8nph a5b7613d-88dd-42e0-8043-16721beffcb3 13583 0 2020-02-23 20:58:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.131.175/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f1bfc0cf-7463-45cc-a9eb-ec7e930576ae 0xc005ebcf00 0xc005ebcf01}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:,StartTime:2020-02-23 20:58:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.731: INFO: Pod "webserver-deployment-c7997dcc8-xkgrm" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xkgrm webserver-deployment-c7997dcc8- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-c7997dcc8-xkgrm ac360f8b-dc42-4478-b5dd-79b5b712429f 13662 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f1bfc0cf-7463-45cc-a9eb-ec7e930576ae 0xc005ebd107 0xc005ebd108}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.732: INFO: Pod "webserver-deployment-c7997dcc8-xxnhm" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xxnhm webserver-deployment-c7997dcc8- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-c7997dcc8-xxnhm e3cf939d-e76e-429e-9f1c-d1081e7a32c2 13597 0 2020-02-23 20:58:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.200.131.176/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f1bfc0cf-7463-45cc-a9eb-ec7e930576ae 0xc005ebd290 0xc005ebd291}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker00,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.100,PodIP:,StartTime:2020-02-23 20:58:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 23 20:58:57.732: INFO: Pod "webserver-deployment-c7997dcc8-z9swb" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-z9swb webserver-deployment-c7997dcc8- deployment-185 /api/v1/namespaces/deployment-185/pods/webserver-deployment-c7997dcc8-z9swb 6483dae9-485d-49a7-ae5b-e64009fc6c68 13643 0 2020-02-23 20:58:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 f1bfc0cf-7463-45cc-a9eb-ec7e930576ae 0xc005ebd467 0xc005ebd468}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2r8vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2r8vw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2r8vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 20:58:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:58:57.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-185" for this suite.

â€¢ [SLOW TEST:6.347 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":280,"completed":104,"skipped":1908,"failed":0}
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:58:57.776: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1632
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 23 20:58:57.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-5987'
Feb 23 20:58:57.942: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 23 20:58:57.942: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Feb 23 20:58:57.987: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-2qv47]
Feb 23 20:58:57.987: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-2qv47" in namespace "kubectl-5987" to be "running and ready"
Feb 23 20:58:58.033: INFO: Pod "e2e-test-httpd-rc-2qv47": Phase="Pending", Reason="", readiness=false. Elapsed: 45.937685ms
Feb 23 20:59:00.048: INFO: Pod "e2e-test-httpd-rc-2qv47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061793316s
Feb 23 20:59:02.055: INFO: Pod "e2e-test-httpd-rc-2qv47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068040959s
Feb 23 20:59:04.074: INFO: Pod "e2e-test-httpd-rc-2qv47": Phase="Pending", Reason="", readiness=false. Elapsed: 6.087021568s
Feb 23 20:59:06.077: INFO: Pod "e2e-test-httpd-rc-2qv47": Phase="Running", Reason="", readiness=true. Elapsed: 8.090335765s
Feb 23 20:59:06.077: INFO: Pod "e2e-test-httpd-rc-2qv47" satisfied condition "running and ready"
Feb 23 20:59:06.077: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-2qv47]
Feb 23 20:59:06.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 logs rc/e2e-test-httpd-rc --namespace=kubectl-5987'
Feb 23 20:59:06.161: INFO: stderr: ""
Feb 23 20:59:06.161: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.200.5.33. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.200.5.33. Set the 'ServerName' directive globally to suppress this message\n[Sun Feb 23 20:59:00.253572 2020] [mpm_event:notice] [pid 1:tid 139999379561320] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Sun Feb 23 20:59:00.253602 2020] [core:notice] [pid 1:tid 139999379561320] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1637
Feb 23 20:59:06.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete rc e2e-test-httpd-rc --namespace=kubectl-5987'
Feb 23 20:59:06.271: INFO: stderr: ""
Feb 23 20:59:06.271: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:59:06.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5987" for this suite.

â€¢ [SLOW TEST:8.508 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1628
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":280,"completed":105,"skipped":1908,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:59:06.284: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Feb 23 20:59:06.351: INFO: Waiting up to 5m0s for pod "var-expansion-bcd7c872-6b9c-4d06-9550-e182a76a3b17" in namespace "var-expansion-1509" to be "success or failure"
Feb 23 20:59:06.355: INFO: Pod "var-expansion-bcd7c872-6b9c-4d06-9550-e182a76a3b17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.261485ms
Feb 23 20:59:08.358: INFO: Pod "var-expansion-bcd7c872-6b9c-4d06-9550-e182a76a3b17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007161218s
Feb 23 20:59:10.363: INFO: Pod "var-expansion-bcd7c872-6b9c-4d06-9550-e182a76a3b17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012075156s
Feb 23 20:59:12.369: INFO: Pod "var-expansion-bcd7c872-6b9c-4d06-9550-e182a76a3b17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018168037s
STEP: Saw pod success
Feb 23 20:59:12.369: INFO: Pod "var-expansion-bcd7c872-6b9c-4d06-9550-e182a76a3b17" satisfied condition "success or failure"
Feb 23 20:59:12.374: INFO: Trying to get logs from node worker01 pod var-expansion-bcd7c872-6b9c-4d06-9550-e182a76a3b17 container dapi-container: <nil>
STEP: delete the pod
Feb 23 20:59:12.393: INFO: Waiting for pod var-expansion-bcd7c872-6b9c-4d06-9550-e182a76a3b17 to disappear
Feb 23 20:59:12.396: INFO: Pod var-expansion-bcd7c872-6b9c-4d06-9550-e182a76a3b17 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:59:12.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1509" for this suite.

â€¢ [SLOW TEST:6.120 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":280,"completed":106,"skipped":1932,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:59:12.404: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 20:59:12.840: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 20:59:15.864: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 20:59:15.870: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1880-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:59:21.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7549" for this suite.
STEP: Destroying namespace "webhook-7549-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:9.627 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":280,"completed":107,"skipped":1939,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:59:22.053: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 23 20:59:24.110: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 20:59:24.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3782" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":108,"skipped":1952,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 20:59:24.135: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-c97296f6-5b36-4df7-a5ca-9ee68f82afcf
STEP: Creating secret with name s-test-opt-upd-e1da02d8-8a83-4c73-a85c-d48a6273da35
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c97296f6-5b36-4df7-a5ca-9ee68f82afcf
STEP: Updating secret s-test-opt-upd-e1da02d8-8a83-4c73-a85c-d48a6273da35
STEP: Creating secret with name s-test-opt-create-05b8ccb6-da30-4b92-9cb1-107356724681
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:01:00.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8616" for this suite.

â€¢ [SLOW TEST:96.587 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":109,"skipped":1970,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:01:00.723: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:01:00.752: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:01:02.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-510" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":280,"completed":110,"skipped":1985,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:01:02.862: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1523
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-1523
I0223 21:01:02.917863      23 runners.go:189] Created replication controller with name: externalname-service, namespace: services-1523, replica count: 2
Feb 23 21:01:05.969: INFO: Creating new exec pod
I0223 21:01:05.969083      23 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 23 21:01:08.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=services-1523 execpodskplv -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 23 21:01:09.122: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 23 21:01:09.122: INFO: stdout: ""
Feb 23 21:01:09.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=services-1523 execpodskplv -- /bin/sh -x -c nc -zv -t -w 2 10.32.0.133 80'
Feb 23 21:01:09.237: INFO: stderr: "+ nc -zv -t -w 2 10.32.0.133 80\nConnection to 10.32.0.133 80 port [tcp/http] succeeded!\n"
Feb 23 21:01:09.237: INFO: stdout: ""
Feb 23 21:01:09.237: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:01:09.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1523" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:6.414 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":280,"completed":111,"skipped":2014,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:01:09.276: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:01:20.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2864" for this suite.

â€¢ [SLOW TEST:11.123 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":280,"completed":112,"skipped":2046,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:01:20.399: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:01:22.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5977" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":113,"skipped":2063,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:01:22.480: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 23 21:01:26.563: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 21:01:26.569: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 21:01:28.573: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 21:01:28.581: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 21:01:30.572: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 21:01:30.576: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 21:01:32.571: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 21:01:32.576: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 21:01:34.570: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 21:01:34.576: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 21:01:36.572: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 21:01:36.576: INFO: Pod pod-with-poststart-http-hook still exists
Feb 23 21:01:38.571: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 23 21:01:38.575: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:01:38.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2887" for this suite.

â€¢ [SLOW TEST:16.107 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":280,"completed":114,"skipped":2064,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:01:38.587: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-3fb8cb80-fdfd-454c-a4ee-2d826f3be7ed
STEP: Creating a pod to test consume configMaps
Feb 23 21:01:38.631: INFO: Waiting up to 5m0s for pod "pod-configmaps-46d03394-a374-4fb6-b366-091647323aee" in namespace "configmap-3837" to be "success or failure"
Feb 23 21:01:38.644: INFO: Pod "pod-configmaps-46d03394-a374-4fb6-b366-091647323aee": Phase="Pending", Reason="", readiness=false. Elapsed: 12.780185ms
Feb 23 21:01:40.648: INFO: Pod "pod-configmaps-46d03394-a374-4fb6-b366-091647323aee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016905964s
STEP: Saw pod success
Feb 23 21:01:40.649: INFO: Pod "pod-configmaps-46d03394-a374-4fb6-b366-091647323aee" satisfied condition "success or failure"
Feb 23 21:01:40.651: INFO: Trying to get logs from node worker01 pod pod-configmaps-46d03394-a374-4fb6-b366-091647323aee container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 21:01:40.666: INFO: Waiting for pod pod-configmaps-46d03394-a374-4fb6-b366-091647323aee to disappear
Feb 23 21:01:40.672: INFO: Pod pod-configmaps-46d03394-a374-4fb6-b366-091647323aee no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:01:40.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3837" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":115,"skipped":2075,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:01:40.683: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 21:01:40.728: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e9477d3b-0a8d-4784-9dcb-b7b70985c81e" in namespace "projected-2370" to be "success or failure"
Feb 23 21:01:40.733: INFO: Pod "downwardapi-volume-e9477d3b-0a8d-4784-9dcb-b7b70985c81e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.550116ms
Feb 23 21:01:42.736: INFO: Pod "downwardapi-volume-e9477d3b-0a8d-4784-9dcb-b7b70985c81e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007778209s
STEP: Saw pod success
Feb 23 21:01:42.736: INFO: Pod "downwardapi-volume-e9477d3b-0a8d-4784-9dcb-b7b70985c81e" satisfied condition "success or failure"
Feb 23 21:01:42.746: INFO: Trying to get logs from node worker01 pod downwardapi-volume-e9477d3b-0a8d-4784-9dcb-b7b70985c81e container client-container: <nil>
STEP: delete the pod
Feb 23 21:01:42.764: INFO: Waiting for pod downwardapi-volume-e9477d3b-0a8d-4784-9dcb-b7b70985c81e to disappear
Feb 23 21:01:42.767: INFO: Pod downwardapi-volume-e9477d3b-0a8d-4784-9dcb-b7b70985c81e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:01:42.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2370" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":116,"skipped":2078,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:01:42.775: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-5769
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5769
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5769
Feb 23 21:01:42.832: INFO: Found 0 stateful pods, waiting for 1
Feb 23 21:01:52.837: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 23 21:01:52.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-5769 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 21:01:52.990: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 21:01:52.990: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 21:01:52.990: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 21:01:52.994: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 23 21:02:02.998: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 21:02:02.998: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 21:02:03.014: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999669s
Feb 23 21:02:04.018: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994793555s
Feb 23 21:02:05.022: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990189002s
Feb 23 21:02:06.029: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.986327909s
Feb 23 21:02:07.034: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979790658s
Feb 23 21:02:08.038: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.973970793s
Feb 23 21:02:09.044: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.969989664s
Feb 23 21:02:10.051: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.963932596s
Feb 23 21:02:11.063: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.954992515s
Feb 23 21:02:12.070: INFO: Verifying statefulset ss doesn't scale past 1 for another 945.196877ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5769
Feb 23 21:02:13.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-5769 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:02:13.184: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 23 21:02:13.184: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 21:02:13.184: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 21:02:13.187: INFO: Found 1 stateful pods, waiting for 3
Feb 23 21:02:23.198: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 21:02:23.198: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 21:02:23.198: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 23 21:02:23.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-5769 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 21:02:23.327: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 21:02:23.327: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 21:02:23.327: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 21:02:23.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-5769 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 21:02:23.464: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 21:02:23.464: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 21:02:23.464: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 21:02:23.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-5769 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 21:02:23.596: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 21:02:23.596: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 21:02:23.596: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 21:02:23.596: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 21:02:23.599: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 23 21:02:33.605: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 21:02:33.605: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 21:02:33.605: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 21:02:33.621: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999611s
Feb 23 21:02:34.624: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991708498s
Feb 23 21:02:35.628: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988383187s
Feb 23 21:02:36.633: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983690525s
Feb 23 21:02:37.641: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979545062s
Feb 23 21:02:38.650: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971579378s
Feb 23 21:02:39.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.962219245s
Feb 23 21:02:40.658: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957747466s
Feb 23 21:02:41.669: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.953167308s
Feb 23 21:02:42.677: INFO: Verifying statefulset ss doesn't scale past 3 for another 942.759362ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5769
Feb 23 21:02:43.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-5769 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:02:43.800: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 23 21:02:43.800: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 21:02:43.800: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 21:02:43.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-5769 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:02:43.963: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 23 21:02:43.963: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 21:02:43.963: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 21:02:43.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-5769 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:02:44.083: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 23 21:02:44.083: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 21:02:44.083: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 21:02:44.083: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 23 21:02:54.099: INFO: Deleting all statefulset in ns statefulset-5769
Feb 23 21:02:54.103: INFO: Scaling statefulset ss to 0
Feb 23 21:02:54.118: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 21:02:54.121: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:02:54.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5769" for this suite.

â€¢ [SLOW TEST:71.371 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":280,"completed":117,"skipped":2115,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:02:54.147: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:03:01.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6453" for this suite.

â€¢ [SLOW TEST:7.060 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":280,"completed":118,"skipped":2115,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:03:01.206: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:03:01.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 version'
Feb 23 21:03:01.281: INFO: stderr: ""
Feb 23 21:03:01.281: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.3\", GitCommit:\"06ad960bfd03b39c8310aaf92d1e7c12ce618213\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T18:14:22Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.3\", GitCommit:\"06ad960bfd03b39c8310aaf92d1e7c12ce618213\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T18:07:13Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:03:01.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5935" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":280,"completed":119,"skipped":2123,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:03:01.292: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:03:01.356: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"3fd7e4ab-9e93-4aa1-b55f-b0ff2ffd9f80", Controller:(*bool)(0xc0070bb7a6), BlockOwnerDeletion:(*bool)(0xc0070bb7a7)}}
Feb 23 21:03:01.364: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"e41225ae-57cc-4ab1-af07-abc7f1df5934", Controller:(*bool)(0xc0055cc4f6), BlockOwnerDeletion:(*bool)(0xc0055cc4f7)}}
Feb 23 21:03:01.383: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6903d886-d789-44d8-b49f-dddd3a0515ef", Controller:(*bool)(0xc0070bb986), BlockOwnerDeletion:(*bool)(0xc0070bb987)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:03:06.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-255" for this suite.

â€¢ [SLOW TEST:5.122 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":280,"completed":120,"skipped":2140,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:03:06.414: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 21:03:06.464: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8421128b-8862-42f6-904e-e2aece92764f" in namespace "downward-api-8861" to be "success or failure"
Feb 23 21:03:06.472: INFO: Pod "downwardapi-volume-8421128b-8862-42f6-904e-e2aece92764f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.620018ms
Feb 23 21:03:08.476: INFO: Pod "downwardapi-volume-8421128b-8862-42f6-904e-e2aece92764f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011489707s
STEP: Saw pod success
Feb 23 21:03:08.476: INFO: Pod "downwardapi-volume-8421128b-8862-42f6-904e-e2aece92764f" satisfied condition "success or failure"
Feb 23 21:03:08.480: INFO: Trying to get logs from node worker01 pod downwardapi-volume-8421128b-8862-42f6-904e-e2aece92764f container client-container: <nil>
STEP: delete the pod
Feb 23 21:03:08.499: INFO: Waiting for pod downwardapi-volume-8421128b-8862-42f6-904e-e2aece92764f to disappear
Feb 23 21:03:08.505: INFO: Pod downwardapi-volume-8421128b-8862-42f6-904e-e2aece92764f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:03:08.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8861" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":121,"skipped":2148,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:03:08.515: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Feb 23 21:03:08.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 create -f - --namespace=kubectl-6509'
Feb 23 21:03:09.572: INFO: stderr: ""
Feb 23 21:03:09.572: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Feb 23 21:03:10.577: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 21:03:10.577: INFO: Found 0 / 1
Feb 23 21:03:11.576: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 21:03:11.576: INFO: Found 1 / 1
Feb 23 21:03:11.576: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 23 21:03:11.580: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 21:03:11.580: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 23 21:03:11.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 patch pod agnhost-master-948qz --namespace=kubectl-6509 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 23 21:03:11.668: INFO: stderr: ""
Feb 23 21:03:11.668: INFO: stdout: "pod/agnhost-master-948qz patched\n"
STEP: checking annotations
Feb 23 21:03:11.673: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 21:03:11.673: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:03:11.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6509" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":280,"completed":122,"skipped":2156,"failed":0}

------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:03:11.684: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:03:11.711: INFO: Creating ReplicaSet my-hostname-basic-9dda7b14-783b-4f34-98f2-e767bf9380c5
Feb 23 21:03:11.720: INFO: Pod name my-hostname-basic-9dda7b14-783b-4f34-98f2-e767bf9380c5: Found 0 pods out of 1
Feb 23 21:03:16.724: INFO: Pod name my-hostname-basic-9dda7b14-783b-4f34-98f2-e767bf9380c5: Found 1 pods out of 1
Feb 23 21:03:16.724: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9dda7b14-783b-4f34-98f2-e767bf9380c5" is running
Feb 23 21:03:16.727: INFO: Pod "my-hostname-basic-9dda7b14-783b-4f34-98f2-e767bf9380c5-hrnss" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 21:03:11 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 21:03:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 21:03:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 21:03:11 +0000 UTC Reason: Message:}])
Feb 23 21:03:16.727: INFO: Trying to dial the pod
Feb 23 21:03:21.744: INFO: Controller my-hostname-basic-9dda7b14-783b-4f34-98f2-e767bf9380c5: Got expected result from replica 1 [my-hostname-basic-9dda7b14-783b-4f34-98f2-e767bf9380c5-hrnss]: "my-hostname-basic-9dda7b14-783b-4f34-98f2-e767bf9380c5-hrnss", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:03:21.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1376" for this suite.

â€¢ [SLOW TEST:10.068 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":123,"skipped":2156,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:03:21.752: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Feb 23 21:03:21.793: INFO: Waiting up to 5m0s for pod "client-containers-91df125c-8687-4970-a394-57c52ccb678a" in namespace "containers-8810" to be "success or failure"
Feb 23 21:03:21.799: INFO: Pod "client-containers-91df125c-8687-4970-a394-57c52ccb678a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.75217ms
Feb 23 21:03:23.805: INFO: Pod "client-containers-91df125c-8687-4970-a394-57c52ccb678a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012075647s
STEP: Saw pod success
Feb 23 21:03:23.805: INFO: Pod "client-containers-91df125c-8687-4970-a394-57c52ccb678a" satisfied condition "success or failure"
Feb 23 21:03:23.810: INFO: Trying to get logs from node worker01 pod client-containers-91df125c-8687-4970-a394-57c52ccb678a container test-container: <nil>
STEP: delete the pod
Feb 23 21:03:23.827: INFO: Waiting for pod client-containers-91df125c-8687-4970-a394-57c52ccb678a to disappear
Feb 23 21:03:23.830: INFO: Pod client-containers-91df125c-8687-4970-a394-57c52ccb678a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:03:23.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8810" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":280,"completed":124,"skipped":2169,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:03:23.844: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:03:23.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9119" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":280,"completed":125,"skipped":2209,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:03:23.911: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 21:03:24.553: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 21:03:27.605: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:03:27.612: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8288-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:03:33.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6103" for this suite.
STEP: Destroying namespace "webhook-6103-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:10.013 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":280,"completed":126,"skipped":2212,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:03:33.924: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-3612bc2e-b981-404e-85da-a26eafc530ae
STEP: Creating a pod to test consume configMaps
Feb 23 21:03:34.001: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-07b9396f-bc31-494c-a2f1-52bd0a8a549a" in namespace "projected-7329" to be "success or failure"
Feb 23 21:03:34.004: INFO: Pod "pod-projected-configmaps-07b9396f-bc31-494c-a2f1-52bd0a8a549a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.300275ms
Feb 23 21:03:36.007: INFO: Pod "pod-projected-configmaps-07b9396f-bc31-494c-a2f1-52bd0a8a549a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005855668s
Feb 23 21:03:38.012: INFO: Pod "pod-projected-configmaps-07b9396f-bc31-494c-a2f1-52bd0a8a549a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010608491s
STEP: Saw pod success
Feb 23 21:03:38.012: INFO: Pod "pod-projected-configmaps-07b9396f-bc31-494c-a2f1-52bd0a8a549a" satisfied condition "success or failure"
Feb 23 21:03:38.015: INFO: Trying to get logs from node worker01 pod pod-projected-configmaps-07b9396f-bc31-494c-a2f1-52bd0a8a549a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 21:03:38.037: INFO: Waiting for pod pod-projected-configmaps-07b9396f-bc31-494c-a2f1-52bd0a8a549a to disappear
Feb 23 21:03:38.041: INFO: Pod pod-projected-configmaps-07b9396f-bc31-494c-a2f1-52bd0a8a549a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:03:38.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7329" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":280,"completed":127,"skipped":2216,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:03:38.055: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 21:03:38.520: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 21:03:41.546: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Feb 23 21:03:43.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 attach --namespace=webhook-4809 to-be-attached-pod -i -c=container1'
Feb 23 21:03:43.683: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:03:43.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4809" for this suite.
STEP: Destroying namespace "webhook-4809-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.717 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":280,"completed":128,"skipped":2219,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:03:43.772: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 21:03:44.269: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 21:03:47.290: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:03:47.297: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:03:53.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6243" for this suite.
STEP: Destroying namespace "webhook-6243-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:9.735 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":280,"completed":129,"skipped":2225,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:03:53.507: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:03:55.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9278" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":280,"completed":130,"skipped":2235,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:03:55.591: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-4418
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 23 21:03:55.623: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 23 21:04:17.703: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.200.131.188:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4418 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:04:17.703: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:04:17.800: INFO: Found all expected endpoints: [netserver-0]
Feb 23 21:04:17.802: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.200.5.62:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4418 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:04:17.802: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:04:17.876: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:04:17.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4418" for this suite.

â€¢ [SLOW TEST:22.294 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":131,"skipped":2235,"failed":0}
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:04:17.887: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:04:19.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6483" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":132,"skipped":2235,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:04:19.957: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:04:19.992: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 23 21:04:25.000: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 23 21:04:25.000: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 23 21:04:27.041: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1922 /apis/apps/v1/namespaces/deployment-1922/deployments/test-cleanup-deployment 2e1e02c2-9b4b-4b22-963b-86616d9049b3 16642 1 2020-02-23 21:04:25 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003230278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-23 21:04:25 +0000 UTC,LastTransitionTime:2020-02-23 21:04:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-55ffc6b7b6" has successfully progressed.,LastUpdateTime:2020-02-23 21:04:26 +0000 UTC,LastTransitionTime:2020-02-23 21:04:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 23 21:04:27.044: INFO: New ReplicaSet "test-cleanup-deployment-55ffc6b7b6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6  deployment-1922 /apis/apps/v1/namespaces/deployment-1922/replicasets/test-cleanup-deployment-55ffc6b7b6 b6cabe17-b933-4a07-bde6-ff005294b3b5 16631 1 2020-02-23 21:04:25 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 2e1e02c2-9b4b-4b22-963b-86616d9049b3 0xc007231da7 0xc007231da8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55ffc6b7b6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007231e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 23 21:04:27.047: INFO: Pod "test-cleanup-deployment-55ffc6b7b6-zhfkh" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6-zhfkh test-cleanup-deployment-55ffc6b7b6- deployment-1922 /api/v1/namespaces/deployment-1922/pods/test-cleanup-deployment-55ffc6b7b6-zhfkh 1f32f852-376d-4a34-8cde-ad0518ab8c3b 16630 0 2020-02-23 21:04:25 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[cni.projectcalico.org/podIP:10.200.5.4/32] [{apps/v1 ReplicaSet test-cleanup-deployment-55ffc6b7b6 b6cabe17-b933-4a07-bde6-ff005294b3b5 0xc003a001d7 0xc003a001d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-d95gr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-d95gr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-d95gr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 21:04:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 21:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 21:04:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 21:04:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:10.200.5.4,StartTime:2020-02-23 21:04:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 21:04:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://85aedae5ac80b364754d0214bfed365f6714ec1ee3cafcee2f9bafa392bd646b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:04:27.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1922" for this suite.

â€¢ [SLOW TEST:7.099 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":280,"completed":133,"skipped":2238,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:04:27.056: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 21:04:27.673: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 23 21:04:29.687: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088667, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088667, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088667, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088667, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 21:04:32.699: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:04:42.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5625" for this suite.
STEP: Destroying namespace "webhook-5625-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:15.947 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":280,"completed":134,"skipped":2284,"failed":0}
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:04:43.003: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-5566
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 23 21:04:43.068: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 23 21:05:01.164: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.5.9:8080/dial?request=hostname&protocol=udp&host=10.200.131.189&port=8081&tries=1'] Namespace:pod-network-test-5566 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:05:01.164: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:05:01.225: INFO: Waiting for responses: map[]
Feb 23 21:05:01.228: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.5.9:8080/dial?request=hostname&protocol=udp&host=10.200.5.7&port=8081&tries=1'] Namespace:pod-network-test-5566 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:05:01.228: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:05:01.297: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:05:01.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5566" for this suite.

â€¢ [SLOW TEST:18.302 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":135,"skipped":2284,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:05:01.306: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 21:05:01.851: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 23 21:05:03.862: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088702, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088702, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088702, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718088702, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 21:05:06.873: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:05:06.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9107" for this suite.
STEP: Destroying namespace "webhook-9107-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.719 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":280,"completed":136,"skipped":2291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:05:07.026: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-4dab2450-c321-41fc-a101-5d0278b91ef9
STEP: Creating a pod to test consume configMaps
Feb 23 21:05:07.082: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d3b8ae7-4710-4227-a687-f0f19a8d8b96" in namespace "configmap-8620" to be "success or failure"
Feb 23 21:05:07.097: INFO: Pod "pod-configmaps-9d3b8ae7-4710-4227-a687-f0f19a8d8b96": Phase="Pending", Reason="", readiness=false. Elapsed: 15.195469ms
Feb 23 21:05:09.104: INFO: Pod "pod-configmaps-9d3b8ae7-4710-4227-a687-f0f19a8d8b96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022058562s
STEP: Saw pod success
Feb 23 21:05:09.104: INFO: Pod "pod-configmaps-9d3b8ae7-4710-4227-a687-f0f19a8d8b96" satisfied condition "success or failure"
Feb 23 21:05:09.109: INFO: Trying to get logs from node worker01 pod pod-configmaps-9d3b8ae7-4710-4227-a687-f0f19a8d8b96 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 21:05:09.128: INFO: Waiting for pod pod-configmaps-9d3b8ae7-4710-4227-a687-f0f19a8d8b96 to disappear
Feb 23 21:05:09.132: INFO: Pod pod-configmaps-9d3b8ae7-4710-4227-a687-f0f19a8d8b96 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:05:09.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8620" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":137,"skipped":2319,"failed":0}

------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:05:09.141: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Feb 23 21:05:09.177: INFO: Created pod &Pod{ObjectMeta:{dns-2062  dns-2062 /api/v1/namespaces/dns-2062/pods/dns-2062 01689da0-0e85-4fbd-9e13-850cab84ba9a 17068 0 2020-02-23 21:05:09 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9xh2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9xh2m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9xh2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Feb 23 21:05:11.188: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2062 PodName:dns-2062 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:05:11.188: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Verifying customized DNS server is configured on pod...
Feb 23 21:05:11.260: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2062 PodName:dns-2062 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:05:11.260: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:05:11.328: INFO: Deleting pod dns-2062...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:05:11.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2062" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":280,"completed":138,"skipped":2319,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:05:11.362: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 21:05:11.400: INFO: Waiting up to 5m0s for pod "downwardapi-volume-16f35006-4b3b-4e3b-98fa-247cb9aed6b0" in namespace "projected-9249" to be "success or failure"
Feb 23 21:05:11.403: INFO: Pod "downwardapi-volume-16f35006-4b3b-4e3b-98fa-247cb9aed6b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.664963ms
Feb 23 21:05:13.408: INFO: Pod "downwardapi-volume-16f35006-4b3b-4e3b-98fa-247cb9aed6b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008173652s
STEP: Saw pod success
Feb 23 21:05:13.410: INFO: Pod "downwardapi-volume-16f35006-4b3b-4e3b-98fa-247cb9aed6b0" satisfied condition "success or failure"
Feb 23 21:05:13.414: INFO: Trying to get logs from node worker01 pod downwardapi-volume-16f35006-4b3b-4e3b-98fa-247cb9aed6b0 container client-container: <nil>
STEP: delete the pod
Feb 23 21:05:13.442: INFO: Waiting for pod downwardapi-volume-16f35006-4b3b-4e3b-98fa-247cb9aed6b0 to disappear
Feb 23 21:05:13.444: INFO: Pod downwardapi-volume-16f35006-4b3b-4e3b-98fa-247cb9aed6b0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:05:13.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9249" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":139,"skipped":2324,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:05:13.455: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:05:13.483: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 23 21:05:21.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-1183 create -f -'
Feb 23 21:05:22.280: INFO: stderr: ""
Feb 23 21:05:22.280: INFO: stdout: "e2e-test-crd-publish-openapi-6273-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 23 21:05:22.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-1183 delete e2e-test-crd-publish-openapi-6273-crds test-cr'
Feb 23 21:05:22.339: INFO: stderr: ""
Feb 23 21:05:22.339: INFO: stdout: "e2e-test-crd-publish-openapi-6273-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 23 21:05:22.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-1183 apply -f -'
Feb 23 21:05:22.597: INFO: stderr: ""
Feb 23 21:05:22.597: INFO: stdout: "e2e-test-crd-publish-openapi-6273-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 23 21:05:22.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-1183 delete e2e-test-crd-publish-openapi-6273-crds test-cr'
Feb 23 21:05:22.659: INFO: stderr: ""
Feb 23 21:05:22.659: INFO: stdout: "e2e-test-crd-publish-openapi-6273-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Feb 23 21:05:22.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 explain e2e-test-crd-publish-openapi-6273-crds'
Feb 23 21:05:23.066: INFO: stderr: ""
Feb 23 21:05:23.066: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6273-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:05:24.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1183" for this suite.

â€¢ [SLOW TEST:11.519 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":280,"completed":140,"skipped":2341,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:05:24.973: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Feb 23 21:05:27.033: INFO: Pod pod-hostip-55e08f80-969e-49ee-9330-825e52508b75 has hostIP: 192.168.180.101
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:05:27.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8361" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":280,"completed":141,"skipped":2343,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:05:27.043: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Feb 23 21:05:27.081: INFO: Waiting up to 5m0s for pod "var-expansion-a4501a69-1503-42b0-9e60-5b285a5b664c" in namespace "var-expansion-1723" to be "success or failure"
Feb 23 21:05:27.085: INFO: Pod "var-expansion-a4501a69-1503-42b0-9e60-5b285a5b664c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.919222ms
Feb 23 21:05:29.089: INFO: Pod "var-expansion-a4501a69-1503-42b0-9e60-5b285a5b664c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008355905s
STEP: Saw pod success
Feb 23 21:05:29.089: INFO: Pod "var-expansion-a4501a69-1503-42b0-9e60-5b285a5b664c" satisfied condition "success or failure"
Feb 23 21:05:29.094: INFO: Trying to get logs from node worker01 pod var-expansion-a4501a69-1503-42b0-9e60-5b285a5b664c container dapi-container: <nil>
STEP: delete the pod
Feb 23 21:05:29.119: INFO: Waiting for pod var-expansion-a4501a69-1503-42b0-9e60-5b285a5b664c to disappear
Feb 23 21:05:29.125: INFO: Pod var-expansion-a4501a69-1503-42b0-9e60-5b285a5b664c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:05:29.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1723" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":280,"completed":142,"skipped":2346,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:05:29.135: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-6131
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Feb 23 21:05:29.182: INFO: Found 0 stateful pods, waiting for 3
Feb 23 21:05:39.189: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 21:05:39.189: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 21:05:39.189: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 21:05:39.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-6131 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 21:05:39.327: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 21:05:39.327: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 21:05:39.327: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 23 21:05:49.360: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 23 21:05:59.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-6131 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:05:59.509: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 23 21:05:59.509: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 21:05:59.509: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 21:06:09.532: INFO: Waiting for StatefulSet statefulset-6131/ss2 to complete update
Feb 23 21:06:09.532: INFO: Waiting for Pod statefulset-6131/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 23 21:06:09.532: INFO: Waiting for Pod statefulset-6131/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 23 21:06:19.538: INFO: Waiting for StatefulSet statefulset-6131/ss2 to complete update
Feb 23 21:06:19.538: INFO: Waiting for Pod statefulset-6131/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 23 21:06:19.538: INFO: Waiting for Pod statefulset-6131/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 23 21:06:29.763: INFO: Waiting for StatefulSet statefulset-6131/ss2 to complete update
Feb 23 21:06:29.763: INFO: Waiting for Pod statefulset-6131/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Feb 23 21:06:39.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-6131 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 21:06:39.691: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 21:06:39.691: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 21:06:39.691: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 21:06:49.723: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 23 21:06:59.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-6131 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:06:59.903: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 23 21:06:59.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 21:06:59.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 23 21:07:19.920: INFO: Deleting all statefulset in ns statefulset-6131
Feb 23 21:07:19.923: INFO: Scaling statefulset ss2 to 0
Feb 23 21:07:39.946: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 21:07:39.949: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:07:39.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6131" for this suite.

â€¢ [SLOW TEST:130.839 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":280,"completed":143,"skipped":2397,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:07:39.977: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 23 21:07:40.006: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 23 21:07:40.013: INFO: Waiting for terminating namespaces to be deleted...
Feb 23 21:07:40.016: INFO: 
Logging pods the kubelet thinks is on node worker00 before test
Feb 23 21:07:40.032: INFO: etcd-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.032: INFO: 	Container etcd ready: true, restart count 0
Feb 23 21:07:40.032: INFO: ceph-osd-worker00-556546b495-tmvn5 from storage started at 2020-02-23 20:28:14 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.032: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 21:07:40.032: INFO: sonobuoy-systemd-logs-daemon-set-975a2eaddfc64d32-ct4lk from sonobuoy started at 2020-02-23 20:38:34 +0000 UTC (2 container statuses recorded)
Feb 23 21:07:40.032: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 21:07:40.032: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 21:07:40.032: INFO: csi-rbdplugin-provisioner-7494f65674-7tn5d from storage started at 2020-02-23 20:28:35 +0000 UTC (5 container statuses recorded)
Feb 23 21:07:40.032: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 21:07:40.032: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:07:40.032: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:07:40.032: INFO: 	Container csi-snapshotter ready: true, restart count 2
Feb 23 21:07:40.033: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:07:40.033: INFO: csi-rbdplugin-provisioner-7494f65674-2h6xd from storage started at 2020-02-23 20:28:35 +0000 UTC (5 container statuses recorded)
Feb 23 21:07:40.033: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 21:07:40.033: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 21:07:40.033: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:07:40.033: INFO: 	Container csi-snapshotter ready: true, restart count 2
Feb 23 21:07:40.033: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:07:40.033: INFO: kube-scheduler-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.033: INFO: 	Container kube-scheduler ready: true, restart count 2
Feb 23 21:07:40.033: INFO: calico-node-st65h from networking started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.033: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 21:07:40.033: INFO: ceph-mgr-94b9dd996-ggbsc from storage started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.033: INFO: 	Container ceph-mgr ready: true, restart count 0
Feb 23 21:07:40.033: INFO: calico-kube-controllers-7cd585bcd-vmw47 from networking started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.033: INFO: 	Container calico-kube-controllers ready: true, restart count 2
Feb 23 21:07:40.033: INFO: csi-rbdplugin-z2bdm from storage started at 2020-02-23 20:28:35 +0000 UTC (3 container statuses recorded)
Feb 23 21:07:40.033: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:07:40.033: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 21:07:40.033: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:07:40.033: INFO: csi-cephfsplugin-q68vh from storage started at 2020-02-23 20:28:35 +0000 UTC (3 container statuses recorded)
Feb 23 21:07:40.033: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:07:40.033: INFO: 	Container driver-registrar ready: true, restart count 1
Feb 23 21:07:40.033: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:07:40.033: INFO: ceph-mds-worker00-6f479b4486-bdmhh from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.033: INFO: 	Container ceph-mds ready: true, restart count 3
Feb 23 21:07:40.034: INFO: ceph-mon-worker00-5cf654d469-bcbdt from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.034: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 21:07:40.034: INFO: coredns-676544c7b9-j9z7b from kube-system started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.034: INFO: 	Container coredns ready: true, restart count 0
Feb 23 21:07:40.034: INFO: ceph-setup-6j8xr from storage started at 2020-02-23 20:28:36 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.034: INFO: 	Container ceph ready: false, restart count 2
Feb 23 21:07:40.034: INFO: csi-cephfsplugin-provisioner-6cd7596f75-tw7n4 from storage started at 2020-02-23 20:28:35 +0000 UTC (4 container statuses recorded)
Feb 23 21:07:40.034: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:07:40.034: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 21:07:40.034: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 21:07:40.034: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:07:40.034: INFO: csi-rbdplugin-provisioner-7494f65674-ddxmx from storage started at 2020-02-23 20:28:35 +0000 UTC (5 container statuses recorded)
Feb 23 21:07:40.034: INFO: 	Container csi-attacher ready: true, restart count 1
Feb 23 21:07:40.034: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:07:40.034: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:07:40.034: INFO: 	Container csi-snapshotter ready: true, restart count 2
Feb 23 21:07:40.035: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:07:40.035: INFO: csi-cephfsplugin-provisioner-6cd7596f75-9hfw2 from storage started at 2020-02-23 20:28:35 +0000 UTC (4 container statuses recorded)
Feb 23 21:07:40.035: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:07:40.035: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 21:07:40.035: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:07:40.035: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:07:40.035: INFO: kube-apiserver-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.035: INFO: 	Container kube-apiserver ready: true, restart count 1
Feb 23 21:07:40.035: INFO: csi-cephfsplugin-provisioner-6cd7596f75-492rn from storage started at 2020-02-23 20:28:35 +0000 UTC (4 container statuses recorded)
Feb 23 21:07:40.035: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:07:40.035: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 21:07:40.035: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:07:40.035: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:07:40.035: INFO: metallb-controller-b96bfbbf8-p9lnh from networking started at 2020-02-23 20:28:36 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.036: INFO: 	Container controller ready: true, restart count 0
Feb 23 21:07:40.036: INFO: ceph-rgw-57cd48f74c-lf65j from storage started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.036: INFO: 	Container ceph-rgw ready: true, restart count 0
Feb 23 21:07:40.036: INFO: metallb-speaker-74sbt from networking started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.036: INFO: 	Container speaker ready: true, restart count 0
Feb 23 21:07:40.036: INFO: gobetween-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.036: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 21:07:40.036: INFO: kube-proxy-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.036: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 21:07:40.036: INFO: coredns-676544c7b9-q29b9 from kube-system started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.036: INFO: 	Container coredns ready: true, restart count 0
Feb 23 21:07:40.036: INFO: kube-controller-manager-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.036: INFO: 	Container kube-controller-manager ready: true, restart count 2
Feb 23 21:07:40.036: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
Feb 23 21:07:40.051: INFO: kube-scheduler-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container kube-scheduler ready: true, restart count 2
Feb 23 21:07:40.051: INFO: kube-controller-manager-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container kube-controller-manager ready: true, restart count 2
Feb 23 21:07:40.051: INFO: calico-node-wlnml from networking started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 21:07:40.051: INFO: metallb-speaker-nx7ns from networking started at 2020-02-23 20:31:01 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container speaker ready: true, restart count 0
Feb 23 21:07:40.051: INFO: sonobuoy-e2e-job-5674935bca8a45cb from sonobuoy started at 2020-02-23 20:38:33 +0000 UTC (2 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container e2e ready: true, restart count 0
Feb 23 21:07:40.051: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 21:07:40.051: INFO: sonobuoy-systemd-logs-daemon-set-975a2eaddfc64d32-jk65g from sonobuoy started at 2020-02-23 20:38:33 +0000 UTC (2 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 21:07:40.051: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 21:07:40.051: INFO: gobetween-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 21:07:40.051: INFO: kube-proxy-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 21:07:40.051: INFO: ceph-mon-worker01-bdb694876-bwgrw from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 21:07:40.051: INFO: csi-cephfsplugin-7cvst from storage started at 2020-02-23 20:31:01 +0000 UTC (3 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:07:40.051: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 21:07:40.051: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:07:40.051: INFO: etcd-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container etcd ready: true, restart count 1
Feb 23 21:07:40.051: INFO: ceph-mds-worker01-7f5fdb58c6-vz8jk from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container ceph-mds ready: true, restart count 2
Feb 23 21:07:40.051: INFO: ceph-osd-worker01-67947c799-h78d6 from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 21:07:40.051: INFO: dashboard-metrics-scraper-58475bc987-qhvsn from kube-system started at 2020-02-23 20:33:06 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 23 21:07:40.051: INFO: kubernetes-dashboard-f957cddcb-xf79k from kube-system started at 2020-02-23 20:33:06 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 23 21:07:40.051: INFO: kube-apiserver-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 23 21:07:40.051: INFO: csi-rbdplugin-c6629 from storage started at 2020-02-23 20:31:01 +0000 UTC (3 container statuses recorded)
Feb 23 21:07:40.051: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:07:40.051: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 21:07:40.051: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:07:40.052: INFO: sonobuoy from sonobuoy started at 2020-02-23 20:38:32 +0000 UTC (1 container statuses recorded)
Feb 23 21:07:40.052: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node worker00
STEP: verifying the node has the label node worker01
Feb 23 21:07:40.089: INFO: Pod coredns-676544c7b9-j9z7b requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod coredns-676544c7b9-q29b9 requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod dashboard-metrics-scraper-58475bc987-qhvsn requesting resource cpu=0m on Node worker01
Feb 23 21:07:40.089: INFO: Pod etcd-worker00 requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod etcd-worker01 requesting resource cpu=0m on Node worker01
Feb 23 21:07:40.089: INFO: Pod gobetween-worker00 requesting resource cpu=100m on Node worker00
Feb 23 21:07:40.089: INFO: Pod gobetween-worker01 requesting resource cpu=100m on Node worker01
Feb 23 21:07:40.089: INFO: Pod kube-apiserver-worker00 requesting resource cpu=250m on Node worker00
Feb 23 21:07:40.089: INFO: Pod kube-apiserver-worker01 requesting resource cpu=250m on Node worker01
Feb 23 21:07:40.089: INFO: Pod kube-controller-manager-worker00 requesting resource cpu=200m on Node worker00
Feb 23 21:07:40.089: INFO: Pod kube-controller-manager-worker01 requesting resource cpu=200m on Node worker01
Feb 23 21:07:40.089: INFO: Pod kube-proxy-worker00 requesting resource cpu=200m on Node worker00
Feb 23 21:07:40.089: INFO: Pod kube-proxy-worker01 requesting resource cpu=200m on Node worker01
Feb 23 21:07:40.089: INFO: Pod kube-scheduler-worker00 requesting resource cpu=100m on Node worker00
Feb 23 21:07:40.089: INFO: Pod kube-scheduler-worker01 requesting resource cpu=100m on Node worker01
Feb 23 21:07:40.089: INFO: Pod kubernetes-dashboard-f957cddcb-xf79k requesting resource cpu=100m on Node worker01
Feb 23 21:07:40.089: INFO: Pod calico-kube-controllers-7cd585bcd-vmw47 requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod calico-node-st65h requesting resource cpu=250m on Node worker00
Feb 23 21:07:40.089: INFO: Pod calico-node-wlnml requesting resource cpu=250m on Node worker01
Feb 23 21:07:40.089: INFO: Pod metallb-controller-b96bfbbf8-p9lnh requesting resource cpu=100m on Node worker00
Feb 23 21:07:40.089: INFO: Pod metallb-speaker-74sbt requesting resource cpu=100m on Node worker00
Feb 23 21:07:40.089: INFO: Pod metallb-speaker-nx7ns requesting resource cpu=100m on Node worker01
Feb 23 21:07:40.089: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker01
Feb 23 21:07:40.089: INFO: Pod sonobuoy-e2e-job-5674935bca8a45cb requesting resource cpu=0m on Node worker01
Feb 23 21:07:40.089: INFO: Pod sonobuoy-systemd-logs-daemon-set-975a2eaddfc64d32-ct4lk requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod sonobuoy-systemd-logs-daemon-set-975a2eaddfc64d32-jk65g requesting resource cpu=0m on Node worker01
Feb 23 21:07:40.089: INFO: Pod ceph-mds-worker00-6f479b4486-bdmhh requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod ceph-mds-worker01-7f5fdb58c6-vz8jk requesting resource cpu=0m on Node worker01
Feb 23 21:07:40.089: INFO: Pod ceph-mgr-94b9dd996-ggbsc requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod ceph-mon-worker00-5cf654d469-bcbdt requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod ceph-mon-worker01-bdb694876-bwgrw requesting resource cpu=0m on Node worker01
Feb 23 21:07:40.089: INFO: Pod ceph-osd-worker00-556546b495-tmvn5 requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod ceph-osd-worker01-67947c799-h78d6 requesting resource cpu=0m on Node worker01
Feb 23 21:07:40.089: INFO: Pod ceph-rgw-57cd48f74c-lf65j requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod csi-cephfsplugin-7cvst requesting resource cpu=0m on Node worker01
Feb 23 21:07:40.089: INFO: Pod csi-cephfsplugin-provisioner-6cd7596f75-492rn requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod csi-cephfsplugin-provisioner-6cd7596f75-9hfw2 requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod csi-cephfsplugin-provisioner-6cd7596f75-tw7n4 requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod csi-cephfsplugin-q68vh requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod csi-rbdplugin-c6629 requesting resource cpu=0m on Node worker01
Feb 23 21:07:40.089: INFO: Pod csi-rbdplugin-provisioner-7494f65674-2h6xd requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod csi-rbdplugin-provisioner-7494f65674-7tn5d requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod csi-rbdplugin-provisioner-7494f65674-ddxmx requesting resource cpu=0m on Node worker00
Feb 23 21:07:40.089: INFO: Pod csi-rbdplugin-z2bdm requesting resource cpu=0m on Node worker00
STEP: Starting Pods to consume most of the cluster CPU.
Feb 23 21:07:40.089: INFO: Creating a pod which consumes cpu=1890m on Node worker00
Feb 23 21:07:40.098: INFO: Creating a pod which consumes cpu=1890m on Node worker01
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9428c538-7aaf-4afd-a3cc-edfdd7d6e1f8.15f62424f8343361], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3885/filler-pod-9428c538-7aaf-4afd-a3cc-edfdd7d6e1f8 to worker00]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9428c538-7aaf-4afd-a3cc-edfdd7d6e1f8.15f624251ed54518], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9428c538-7aaf-4afd-a3cc-edfdd7d6e1f8.15f624252309a20f], Reason = [Created], Message = [Created container filler-pod-9428c538-7aaf-4afd-a3cc-edfdd7d6e1f8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9428c538-7aaf-4afd-a3cc-edfdd7d6e1f8.15f624252c77b19d], Reason = [Started], Message = [Started container filler-pod-9428c538-7aaf-4afd-a3cc-edfdd7d6e1f8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cbfb6905-4dea-4bf1-8cfb-e25d9026f44f.15f62424f8963eee], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3885/filler-pod-cbfb6905-4dea-4bf1-8cfb-e25d9026f44f to worker01]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cbfb6905-4dea-4bf1-8cfb-e25d9026f44f.15f624251609bfc2], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cbfb6905-4dea-4bf1-8cfb-e25d9026f44f.15f624251a7d6587], Reason = [Created], Message = [Created container filler-pod-cbfb6905-4dea-4bf1-8cfb-e25d9026f44f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cbfb6905-4dea-4bf1-8cfb-e25d9026f44f.15f6242523f4095d], Reason = [Started], Message = [Started container filler-pod-cbfb6905-4dea-4bf1-8cfb-e25d9026f44f]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f62425711edcaf], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node worker01
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker00
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:07:43.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3885" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":280,"completed":144,"skipped":2455,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:07:43.188: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 21:07:43.221: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd956561-1a41-4dc7-b9f6-234a1d060641" in namespace "downward-api-9686" to be "success or failure"
Feb 23 21:07:43.224: INFO: Pod "downwardapi-volume-fd956561-1a41-4dc7-b9f6-234a1d060641": Phase="Pending", Reason="", readiness=false. Elapsed: 2.738224ms
Feb 23 21:07:45.228: INFO: Pod "downwardapi-volume-fd956561-1a41-4dc7-b9f6-234a1d060641": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006504205s
STEP: Saw pod success
Feb 23 21:07:45.228: INFO: Pod "downwardapi-volume-fd956561-1a41-4dc7-b9f6-234a1d060641" satisfied condition "success or failure"
Feb 23 21:07:45.232: INFO: Trying to get logs from node worker01 pod downwardapi-volume-fd956561-1a41-4dc7-b9f6-234a1d060641 container client-container: <nil>
STEP: delete the pod
Feb 23 21:07:45.260: INFO: Waiting for pod downwardapi-volume-fd956561-1a41-4dc7-b9f6-234a1d060641 to disappear
Feb 23 21:07:45.264: INFO: Pod downwardapi-volume-fd956561-1a41-4dc7-b9f6-234a1d060641 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:07:45.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9686" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":145,"skipped":2464,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:07:45.280: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-d2394fae-dfa7-4d38-b872-1b7138c5b737 in namespace container-probe-6966
Feb 23 21:07:47.356: INFO: Started pod busybox-d2394fae-dfa7-4d38-b872-1b7138c5b737 in namespace container-probe-6966
STEP: checking the pod's current state and verifying that restartCount is present
Feb 23 21:07:47.361: INFO: Initial restart count of pod busybox-d2394fae-dfa7-4d38-b872-1b7138c5b737 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:11:48.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6966" for this suite.

â€¢ [SLOW TEST:242.900 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":146,"skipped":2470,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:11:48.180: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-232a9ca2-2648-46ce-ac56-728cd4a3a7bf
STEP: Creating a pod to test consume secrets
Feb 23 21:11:48.219: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-68f428b2-d013-4c8d-b624-048de82b1e0a" in namespace "projected-8069" to be "success or failure"
Feb 23 21:11:48.222: INFO: Pod "pod-projected-secrets-68f428b2-d013-4c8d-b624-048de82b1e0a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119795ms
Feb 23 21:11:50.226: INFO: Pod "pod-projected-secrets-68f428b2-d013-4c8d-b624-048de82b1e0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006616872s
STEP: Saw pod success
Feb 23 21:11:50.226: INFO: Pod "pod-projected-secrets-68f428b2-d013-4c8d-b624-048de82b1e0a" satisfied condition "success or failure"
Feb 23 21:11:50.229: INFO: Trying to get logs from node worker01 pod pod-projected-secrets-68f428b2-d013-4c8d-b624-048de82b1e0a container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 23 21:11:50.257: INFO: Waiting for pod pod-projected-secrets-68f428b2-d013-4c8d-b624-048de82b1e0a to disappear
Feb 23 21:11:50.260: INFO: Pod pod-projected-secrets-68f428b2-d013-4c8d-b624-048de82b1e0a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:11:50.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8069" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":147,"skipped":2477,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:11:50.270: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 23 21:11:50.317: INFO: Waiting up to 5m0s for pod "pod-6081b146-ad11-403b-8f11-34d71ed7bc99" in namespace "emptydir-3425" to be "success or failure"
Feb 23 21:11:50.320: INFO: Pod "pod-6081b146-ad11-403b-8f11-34d71ed7bc99": Phase="Pending", Reason="", readiness=false. Elapsed: 3.074583ms
Feb 23 21:11:52.324: INFO: Pod "pod-6081b146-ad11-403b-8f11-34d71ed7bc99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006982591s
STEP: Saw pod success
Feb 23 21:11:52.324: INFO: Pod "pod-6081b146-ad11-403b-8f11-34d71ed7bc99" satisfied condition "success or failure"
Feb 23 21:11:52.328: INFO: Trying to get logs from node worker01 pod pod-6081b146-ad11-403b-8f11-34d71ed7bc99 container test-container: <nil>
STEP: delete the pod
Feb 23 21:11:52.347: INFO: Waiting for pod pod-6081b146-ad11-403b-8f11-34d71ed7bc99 to disappear
Feb 23 21:11:52.350: INFO: Pod pod-6081b146-ad11-403b-8f11-34d71ed7bc99 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:11:52.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3425" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":148,"skipped":2489,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:11:52.357: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-866c18a4-96e4-4d33-8fa0-6d06c8201ae5
STEP: Creating a pod to test consume secrets
Feb 23 21:11:52.404: INFO: Waiting up to 5m0s for pod "pod-secrets-8bf3a5e6-88b4-44a6-bbbb-8ca1fadee2f7" in namespace "secrets-9889" to be "success or failure"
Feb 23 21:11:52.408: INFO: Pod "pod-secrets-8bf3a5e6-88b4-44a6-bbbb-8ca1fadee2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.712901ms
Feb 23 21:11:54.411: INFO: Pod "pod-secrets-8bf3a5e6-88b4-44a6-bbbb-8ca1fadee2f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007167264s
STEP: Saw pod success
Feb 23 21:11:54.411: INFO: Pod "pod-secrets-8bf3a5e6-88b4-44a6-bbbb-8ca1fadee2f7" satisfied condition "success or failure"
Feb 23 21:11:54.416: INFO: Trying to get logs from node worker01 pod pod-secrets-8bf3a5e6-88b4-44a6-bbbb-8ca1fadee2f7 container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 21:11:54.440: INFO: Waiting for pod pod-secrets-8bf3a5e6-88b4-44a6-bbbb-8ca1fadee2f7 to disappear
Feb 23 21:11:54.442: INFO: Pod pod-secrets-8bf3a5e6-88b4-44a6-bbbb-8ca1fadee2f7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:11:54.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9889" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":149,"skipped":2491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:11:54.458: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-00dd1560-b71b-4594-9fd8-eac1ce5f7bc6
STEP: Creating a pod to test consume secrets
Feb 23 21:11:54.505: INFO: Waiting up to 5m0s for pod "pod-secrets-6b1226cc-d09f-418d-b370-206455f40465" in namespace "secrets-8892" to be "success or failure"
Feb 23 21:11:54.509: INFO: Pod "pod-secrets-6b1226cc-d09f-418d-b370-206455f40465": Phase="Pending", Reason="", readiness=false. Elapsed: 3.422302ms
Feb 23 21:11:56.515: INFO: Pod "pod-secrets-6b1226cc-d09f-418d-b370-206455f40465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009491075s
STEP: Saw pod success
Feb 23 21:11:56.516: INFO: Pod "pod-secrets-6b1226cc-d09f-418d-b370-206455f40465" satisfied condition "success or failure"
Feb 23 21:11:56.520: INFO: Trying to get logs from node worker01 pod pod-secrets-6b1226cc-d09f-418d-b370-206455f40465 container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 21:11:56.539: INFO: Waiting for pod pod-secrets-6b1226cc-d09f-418d-b370-206455f40465 to disappear
Feb 23 21:11:56.543: INFO: Pod pod-secrets-6b1226cc-d09f-418d-b370-206455f40465 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:11:56.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8892" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":150,"skipped":2524,"failed":0}
SSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:11:56.555: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:11:56.595: INFO: Waiting up to 5m0s for pod "busybox-user-65534-b227ae93-f18f-4cb8-a72e-728234ffbbe1" in namespace "security-context-test-672" to be "success or failure"
Feb 23 21:11:56.601: INFO: Pod "busybox-user-65534-b227ae93-f18f-4cb8-a72e-728234ffbbe1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.096876ms
Feb 23 21:11:58.608: INFO: Pod "busybox-user-65534-b227ae93-f18f-4cb8-a72e-728234ffbbe1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013667112s
Feb 23 21:11:58.608: INFO: Pod "busybox-user-65534-b227ae93-f18f-4cb8-a72e-728234ffbbe1" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:11:58.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-672" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":151,"skipped":2528,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:11:58.623: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 23 21:12:01.211: INFO: Successfully updated pod "labelsupdate911107cc-ae45-43ca-897f-e40e427ff55d"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:12:05.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7176" for this suite.

â€¢ [SLOW TEST:6.620 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":152,"skipped":2528,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:12:05.243: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 23 21:12:07.815: INFO: Successfully updated pod "labelsupdateb42a7e8b-7181-4b8b-b5b7-935c8d18fb90"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:12:09.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5817" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":153,"skipped":2531,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:12:09.850: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 23 21:12:09.884: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 23 21:12:09.893: INFO: Waiting for terminating namespaces to be deleted...
Feb 23 21:12:09.896: INFO: 
Logging pods the kubelet thinks is on node worker00 before test
Feb 23 21:12:09.909: INFO: kube-apiserver-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.909: INFO: 	Container kube-apiserver ready: true, restart count 1
Feb 23 21:12:09.909: INFO: csi-cephfsplugin-provisioner-6cd7596f75-492rn from storage started at 2020-02-23 20:28:35 +0000 UTC (4 container statuses recorded)
Feb 23 21:12:09.909: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:12:09.909: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 21:12:09.909: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:12:09.909: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:12:09.909: INFO: metallb-controller-b96bfbbf8-p9lnh from networking started at 2020-02-23 20:28:36 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.909: INFO: 	Container controller ready: true, restart count 0
Feb 23 21:12:09.909: INFO: gobetween-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.909: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 21:12:09.909: INFO: kube-proxy-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.909: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 21:12:09.909: INFO: coredns-676544c7b9-q29b9 from kube-system started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.909: INFO: 	Container coredns ready: true, restart count 0
Feb 23 21:12:09.909: INFO: ceph-rgw-57cd48f74c-lf65j from storage started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.909: INFO: 	Container ceph-rgw ready: true, restart count 0
Feb 23 21:12:09.909: INFO: metallb-speaker-74sbt from networking started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.909: INFO: 	Container speaker ready: true, restart count 0
Feb 23 21:12:09.909: INFO: kube-controller-manager-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.909: INFO: 	Container kube-controller-manager ready: true, restart count 2
Feb 23 21:12:09.909: INFO: etcd-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.909: INFO: 	Container etcd ready: true, restart count 0
Feb 23 21:12:09.909: INFO: ceph-osd-worker00-556546b495-tmvn5 from storage started at 2020-02-23 20:28:14 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.909: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 21:12:09.910: INFO: sonobuoy-systemd-logs-daemon-set-975a2eaddfc64d32-ct4lk from sonobuoy started at 2020-02-23 20:38:34 +0000 UTC (2 container statuses recorded)
Feb 23 21:12:09.910: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 21:12:09.910: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 21:12:09.910: INFO: kube-scheduler-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.910: INFO: 	Container kube-scheduler ready: true, restart count 2
Feb 23 21:12:09.910: INFO: calico-node-st65h from networking started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.910: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 21:12:09.910: INFO: ceph-mgr-94b9dd996-ggbsc from storage started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.910: INFO: 	Container ceph-mgr ready: true, restart count 0
Feb 23 21:12:09.910: INFO: csi-rbdplugin-provisioner-7494f65674-7tn5d from storage started at 2020-02-23 20:28:35 +0000 UTC (5 container statuses recorded)
Feb 23 21:12:09.910: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 21:12:09.910: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:12:09.910: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:12:09.910: INFO: 	Container csi-snapshotter ready: true, restart count 2
Feb 23 21:12:09.910: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:12:09.910: INFO: csi-rbdplugin-provisioner-7494f65674-2h6xd from storage started at 2020-02-23 20:28:35 +0000 UTC (5 container statuses recorded)
Feb 23 21:12:09.910: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 21:12:09.910: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 21:12:09.910: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:12:09.910: INFO: 	Container csi-snapshotter ready: true, restart count 2
Feb 23 21:12:09.910: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:12:09.910: INFO: ceph-mds-worker00-6f479b4486-bdmhh from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.910: INFO: 	Container ceph-mds ready: true, restart count 3
Feb 23 21:12:09.910: INFO: ceph-mon-worker00-5cf654d469-bcbdt from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.910: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 21:12:09.910: INFO: coredns-676544c7b9-j9z7b from kube-system started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.910: INFO: 	Container coredns ready: true, restart count 0
Feb 23 21:12:09.910: INFO: calico-kube-controllers-7cd585bcd-vmw47 from networking started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.910: INFO: 	Container calico-kube-controllers ready: true, restart count 2
Feb 23 21:12:09.910: INFO: csi-rbdplugin-z2bdm from storage started at 2020-02-23 20:28:35 +0000 UTC (3 container statuses recorded)
Feb 23 21:12:09.910: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:12:09.910: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 21:12:09.910: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:12:09.910: INFO: csi-cephfsplugin-q68vh from storage started at 2020-02-23 20:28:35 +0000 UTC (3 container statuses recorded)
Feb 23 21:12:09.911: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:12:09.911: INFO: 	Container driver-registrar ready: true, restart count 1
Feb 23 21:12:09.911: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:12:09.911: INFO: csi-cephfsplugin-provisioner-6cd7596f75-tw7n4 from storage started at 2020-02-23 20:28:35 +0000 UTC (4 container statuses recorded)
Feb 23 21:12:09.911: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:12:09.911: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 21:12:09.911: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 21:12:09.911: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:12:09.911: INFO: csi-rbdplugin-provisioner-7494f65674-ddxmx from storage started at 2020-02-23 20:28:35 +0000 UTC (5 container statuses recorded)
Feb 23 21:12:09.911: INFO: 	Container csi-attacher ready: true, restart count 1
Feb 23 21:12:09.911: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:12:09.911: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:12:09.911: INFO: 	Container csi-snapshotter ready: true, restart count 2
Feb 23 21:12:09.911: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:12:09.911: INFO: csi-cephfsplugin-provisioner-6cd7596f75-9hfw2 from storage started at 2020-02-23 20:28:35 +0000 UTC (4 container statuses recorded)
Feb 23 21:12:09.911: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:12:09.911: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 21:12:09.911: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:12:09.911: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:12:09.911: INFO: ceph-setup-6j8xr from storage started at 2020-02-23 20:28:36 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.911: INFO: 	Container ceph ready: false, restart count 2
Feb 23 21:12:09.911: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
Feb 23 21:12:09.920: INFO: ceph-osd-worker01-67947c799-h78d6 from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 21:12:09.920: INFO: dashboard-metrics-scraper-58475bc987-qhvsn from kube-system started at 2020-02-23 20:33:06 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 23 21:12:09.920: INFO: kubernetes-dashboard-f957cddcb-xf79k from kube-system started at 2020-02-23 20:33:06 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 23 21:12:09.920: INFO: kube-apiserver-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 23 21:12:09.920: INFO: csi-rbdplugin-c6629 from storage started at 2020-02-23 20:31:01 +0000 UTC (3 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:12:09.920: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 21:12:09.920: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:12:09.920: INFO: sonobuoy from sonobuoy started at 2020-02-23 20:38:32 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 23 21:12:09.920: INFO: kube-scheduler-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container kube-scheduler ready: true, restart count 2
Feb 23 21:12:09.920: INFO: kube-controller-manager-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container kube-controller-manager ready: true, restart count 2
Feb 23 21:12:09.920: INFO: calico-node-wlnml from networking started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 21:12:09.920: INFO: metallb-speaker-nx7ns from networking started at 2020-02-23 20:31:01 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container speaker ready: true, restart count 0
Feb 23 21:12:09.920: INFO: sonobuoy-e2e-job-5674935bca8a45cb from sonobuoy started at 2020-02-23 20:38:33 +0000 UTC (2 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container e2e ready: true, restart count 0
Feb 23 21:12:09.920: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 21:12:09.920: INFO: sonobuoy-systemd-logs-daemon-set-975a2eaddfc64d32-jk65g from sonobuoy started at 2020-02-23 20:38:33 +0000 UTC (2 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 21:12:09.920: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 21:12:09.920: INFO: labelsupdateb42a7e8b-7181-4b8b-b5b7-935c8d18fb90 from downward-api-5817 started at 2020-02-23 21:12:05 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container client-container ready: true, restart count 0
Feb 23 21:12:09.920: INFO: gobetween-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 21:12:09.920: INFO: kube-proxy-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 21:12:09.920: INFO: ceph-mon-worker01-bdb694876-bwgrw from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 21:12:09.920: INFO: csi-cephfsplugin-7cvst from storage started at 2020-02-23 20:31:01 +0000 UTC (3 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:12:09.920: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 21:12:09.920: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:12:09.920: INFO: labelsupdate911107cc-ae45-43ca-897f-e40e427ff55d from projected-7176 started at 2020-02-23 21:11:58 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container client-container ready: true, restart count 0
Feb 23 21:12:09.920: INFO: etcd-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container etcd ready: true, restart count 1
Feb 23 21:12:09.920: INFO: ceph-mds-worker01-7f5fdb58c6-vz8jk from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:12:09.920: INFO: 	Container ceph-mds ready: true, restart count 2
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-680c96d1-8a81-40f7-9fc1-cdc945fb2ec8 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-680c96d1-8a81-40f7-9fc1-cdc945fb2ec8 off the node worker01
STEP: verifying the node doesn't have the label kubernetes.io/e2e-680c96d1-8a81-40f7-9fc1-cdc945fb2ec8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:12:14.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6422" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":280,"completed":154,"skipped":2535,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:12:14.027: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 21:12:14.391: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 21:12:17.413: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:12:17.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8287" for this suite.
STEP: Destroying namespace "webhook-8287-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":280,"completed":155,"skipped":2556,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:12:17.573: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 23 21:12:17.614: INFO: Waiting up to 5m0s for pod "pod-8bd24d78-24ca-476a-bb38-f039a49292c7" in namespace "emptydir-6086" to be "success or failure"
Feb 23 21:12:17.619: INFO: Pod "pod-8bd24d78-24ca-476a-bb38-f039a49292c7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.769053ms
Feb 23 21:12:19.622: INFO: Pod "pod-8bd24d78-24ca-476a-bb38-f039a49292c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007342245s
STEP: Saw pod success
Feb 23 21:12:19.622: INFO: Pod "pod-8bd24d78-24ca-476a-bb38-f039a49292c7" satisfied condition "success or failure"
Feb 23 21:12:19.627: INFO: Trying to get logs from node worker01 pod pod-8bd24d78-24ca-476a-bb38-f039a49292c7 container test-container: <nil>
STEP: delete the pod
Feb 23 21:12:19.650: INFO: Waiting for pod pod-8bd24d78-24ca-476a-bb38-f039a49292c7 to disappear
Feb 23 21:12:19.653: INFO: Pod pod-8bd24d78-24ca-476a-bb38-f039a49292c7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:12:19.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6086" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":156,"skipped":2570,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:12:19.662: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 23 21:12:19.718: INFO: Waiting up to 5m0s for pod "pod-b19ea070-4aec-4939-850b-44ebe3d8a222" in namespace "emptydir-1173" to be "success or failure"
Feb 23 21:12:19.722: INFO: Pod "pod-b19ea070-4aec-4939-850b-44ebe3d8a222": Phase="Pending", Reason="", readiness=false. Elapsed: 3.417386ms
Feb 23 21:12:21.725: INFO: Pod "pod-b19ea070-4aec-4939-850b-44ebe3d8a222": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006416822s
STEP: Saw pod success
Feb 23 21:12:21.725: INFO: Pod "pod-b19ea070-4aec-4939-850b-44ebe3d8a222" satisfied condition "success or failure"
Feb 23 21:12:21.729: INFO: Trying to get logs from node worker01 pod pod-b19ea070-4aec-4939-850b-44ebe3d8a222 container test-container: <nil>
STEP: delete the pod
Feb 23 21:12:21.753: INFO: Waiting for pod pod-b19ea070-4aec-4939-850b-44ebe3d8a222 to disappear
Feb 23 21:12:21.757: INFO: Pod pod-b19ea070-4aec-4939-850b-44ebe3d8a222 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:12:21.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1173" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":157,"skipped":2576,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:12:21.767: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Feb 23 21:12:24.321: INFO: Successfully updated pod "adopt-release-2lkcp"
STEP: Checking that the Job readopts the Pod
Feb 23 21:12:24.321: INFO: Waiting up to 15m0s for pod "adopt-release-2lkcp" in namespace "job-3194" to be "adopted"
Feb 23 21:12:24.324: INFO: Pod "adopt-release-2lkcp": Phase="Running", Reason="", readiness=true. Elapsed: 3.77067ms
Feb 23 21:12:26.328: INFO: Pod "adopt-release-2lkcp": Phase="Running", Reason="", readiness=true. Elapsed: 2.007809253s
Feb 23 21:12:26.328: INFO: Pod "adopt-release-2lkcp" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Feb 23 21:12:26.838: INFO: Successfully updated pod "adopt-release-2lkcp"
STEP: Checking that the Job releases the Pod
Feb 23 21:12:26.838: INFO: Waiting up to 15m0s for pod "adopt-release-2lkcp" in namespace "job-3194" to be "released"
Feb 23 21:12:26.844: INFO: Pod "adopt-release-2lkcp": Phase="Running", Reason="", readiness=true. Elapsed: 6.587582ms
Feb 23 21:12:28.847: INFO: Pod "adopt-release-2lkcp": Phase="Running", Reason="", readiness=true. Elapsed: 2.009378306s
Feb 23 21:12:28.847: INFO: Pod "adopt-release-2lkcp" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:12:28.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3194" for this suite.

â€¢ [SLOW TEST:7.094 seconds]
[sig-apps] Job
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":280,"completed":158,"skipped":2592,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:12:28.863: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-e2587515-39b5-4245-926a-42dfe73d5fc4
STEP: Creating a pod to test consume secrets
Feb 23 21:12:28.905: INFO: Waiting up to 5m0s for pod "pod-secrets-5b051a7a-57f2-4a19-a7f7-c691b8bc893b" in namespace "secrets-5789" to be "success or failure"
Feb 23 21:12:28.909: INFO: Pod "pod-secrets-5b051a7a-57f2-4a19-a7f7-c691b8bc893b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.878934ms
Feb 23 21:12:30.912: INFO: Pod "pod-secrets-5b051a7a-57f2-4a19-a7f7-c691b8bc893b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007392468s
STEP: Saw pod success
Feb 23 21:12:30.912: INFO: Pod "pod-secrets-5b051a7a-57f2-4a19-a7f7-c691b8bc893b" satisfied condition "success or failure"
Feb 23 21:12:30.916: INFO: Trying to get logs from node worker01 pod pod-secrets-5b051a7a-57f2-4a19-a7f7-c691b8bc893b container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 21:12:30.937: INFO: Waiting for pod pod-secrets-5b051a7a-57f2-4a19-a7f7-c691b8bc893b to disappear
Feb 23 21:12:30.941: INFO: Pod pod-secrets-5b051a7a-57f2-4a19-a7f7-c691b8bc893b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:12:30.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5789" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":159,"skipped":2629,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:12:30.950: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:12:42.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9899" for this suite.

â€¢ [SLOW TEST:11.073 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":280,"completed":160,"skipped":2670,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:12:42.024: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-f587d975-6101-407e-98e4-2085ae31912a
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:12:42.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4857" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":280,"completed":161,"skipped":2686,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:12:42.066: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-4e7b4687-a5f9-4d5b-b6d3-24f147cde086
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-4e7b4687-a5f9-4d5b-b6d3-24f147cde086
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:12:46.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1544" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":162,"skipped":2690,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:12:46.152: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 23 21:12:46.195: INFO: Waiting up to 5m0s for pod "pod-d21825b0-5457-41d6-897b-a63d99ea733c" in namespace "emptydir-4602" to be "success or failure"
Feb 23 21:12:46.198: INFO: Pod "pod-d21825b0-5457-41d6-897b-a63d99ea733c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.708473ms
Feb 23 21:12:48.202: INFO: Pod "pod-d21825b0-5457-41d6-897b-a63d99ea733c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007099715s
STEP: Saw pod success
Feb 23 21:12:48.202: INFO: Pod "pod-d21825b0-5457-41d6-897b-a63d99ea733c" satisfied condition "success or failure"
Feb 23 21:12:48.206: INFO: Trying to get logs from node worker01 pod pod-d21825b0-5457-41d6-897b-a63d99ea733c container test-container: <nil>
STEP: delete the pod
Feb 23 21:12:48.224: INFO: Waiting for pod pod-d21825b0-5457-41d6-897b-a63d99ea733c to disappear
Feb 23 21:12:48.227: INFO: Pod pod-d21825b0-5457-41d6-897b-a63d99ea733c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:12:48.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4602" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":163,"skipped":2690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:12:48.236: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 21:12:48.283: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ecd38bdf-7b40-4f74-8bee-81b00cab0b47" in namespace "downward-api-6664" to be "success or failure"
Feb 23 21:12:48.288: INFO: Pod "downwardapi-volume-ecd38bdf-7b40-4f74-8bee-81b00cab0b47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.381245ms
Feb 23 21:12:50.294: INFO: Pod "downwardapi-volume-ecd38bdf-7b40-4f74-8bee-81b00cab0b47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010819953s
STEP: Saw pod success
Feb 23 21:12:50.295: INFO: Pod "downwardapi-volume-ecd38bdf-7b40-4f74-8bee-81b00cab0b47" satisfied condition "success or failure"
Feb 23 21:12:50.298: INFO: Trying to get logs from node worker01 pod downwardapi-volume-ecd38bdf-7b40-4f74-8bee-81b00cab0b47 container client-container: <nil>
STEP: delete the pod
Feb 23 21:12:50.319: INFO: Waiting for pod downwardapi-volume-ecd38bdf-7b40-4f74-8bee-81b00cab0b47 to disappear
Feb 23 21:12:50.323: INFO: Pod downwardapi-volume-ecd38bdf-7b40-4f74-8bee-81b00cab0b47 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:12:50.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6664" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":164,"skipped":2724,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:12:50.333: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-1114
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 23 21:12:50.357: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 23 21:13:10.426: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.5.49:8080/dial?request=hostname&protocol=http&host=10.200.131.143&port=8080&tries=1'] Namespace:pod-network-test-1114 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:13:10.426: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:13:10.522: INFO: Waiting for responses: map[]
Feb 23 21:13:10.526: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.200.5.49:8080/dial?request=hostname&protocol=http&host=10.200.5.40&port=8080&tries=1'] Namespace:pod-network-test-1114 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:13:10.526: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:13:10.616: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:13:10.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1114" for this suite.

â€¢ [SLOW TEST:20.291 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":165,"skipped":2775,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:13:10.624: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-c6ee9a67-23fd-4acf-95bc-db65979aa67e
STEP: Creating a pod to test consume secrets
Feb 23 21:13:10.663: INFO: Waiting up to 5m0s for pod "pod-secrets-83b7daad-909f-4a2b-94f0-3a0b50e03315" in namespace "secrets-6507" to be "success or failure"
Feb 23 21:13:10.666: INFO: Pod "pod-secrets-83b7daad-909f-4a2b-94f0-3a0b50e03315": Phase="Pending", Reason="", readiness=false. Elapsed: 3.170749ms
Feb 23 21:13:12.673: INFO: Pod "pod-secrets-83b7daad-909f-4a2b-94f0-3a0b50e03315": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009981506s
STEP: Saw pod success
Feb 23 21:13:12.673: INFO: Pod "pod-secrets-83b7daad-909f-4a2b-94f0-3a0b50e03315" satisfied condition "success or failure"
Feb 23 21:13:12.679: INFO: Trying to get logs from node worker01 pod pod-secrets-83b7daad-909f-4a2b-94f0-3a0b50e03315 container secret-volume-test: <nil>
STEP: delete the pod
Feb 23 21:13:12.697: INFO: Waiting for pod pod-secrets-83b7daad-909f-4a2b-94f0-3a0b50e03315 to disappear
Feb 23 21:13:12.702: INFO: Pod pod-secrets-83b7daad-909f-4a2b-94f0-3a0b50e03315 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:13:12.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6507" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":166,"skipped":2788,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:13:12.710: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:13:14.783: INFO: Waiting up to 5m0s for pod "client-envvars-d4fc77fb-6b52-49d7-b9cb-4c6453c60f43" in namespace "pods-5668" to be "success or failure"
Feb 23 21:13:14.790: INFO: Pod "client-envvars-d4fc77fb-6b52-49d7-b9cb-4c6453c60f43": Phase="Pending", Reason="", readiness=false. Elapsed: 6.523428ms
Feb 23 21:13:16.793: INFO: Pod "client-envvars-d4fc77fb-6b52-49d7-b9cb-4c6453c60f43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01009755s
STEP: Saw pod success
Feb 23 21:13:16.793: INFO: Pod "client-envvars-d4fc77fb-6b52-49d7-b9cb-4c6453c60f43" satisfied condition "success or failure"
Feb 23 21:13:16.796: INFO: Trying to get logs from node worker01 pod client-envvars-d4fc77fb-6b52-49d7-b9cb-4c6453c60f43 container env3cont: <nil>
STEP: delete the pod
Feb 23 21:13:16.818: INFO: Waiting for pod client-envvars-d4fc77fb-6b52-49d7-b9cb-4c6453c60f43 to disappear
Feb 23 21:13:16.821: INFO: Pod client-envvars-d4fc77fb-6b52-49d7-b9cb-4c6453c60f43 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:13:16.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5668" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":280,"completed":167,"skipped":2789,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:13:16.831: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 23 21:13:16.871: INFO: Waiting up to 5m0s for pod "downward-api-87490f10-30f0-4356-9bda-5e823cc3d4af" in namespace "downward-api-8756" to be "success or failure"
Feb 23 21:13:16.876: INFO: Pod "downward-api-87490f10-30f0-4356-9bda-5e823cc3d4af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.4382ms
Feb 23 21:13:18.883: INFO: Pod "downward-api-87490f10-30f0-4356-9bda-5e823cc3d4af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011602206s
STEP: Saw pod success
Feb 23 21:13:18.883: INFO: Pod "downward-api-87490f10-30f0-4356-9bda-5e823cc3d4af" satisfied condition "success or failure"
Feb 23 21:13:18.886: INFO: Trying to get logs from node worker01 pod downward-api-87490f10-30f0-4356-9bda-5e823cc3d4af container dapi-container: <nil>
STEP: delete the pod
Feb 23 21:13:18.907: INFO: Waiting for pod downward-api-87490f10-30f0-4356-9bda-5e823cc3d4af to disappear
Feb 23 21:13:18.915: INFO: Pod downward-api-87490f10-30f0-4356-9bda-5e823cc3d4af no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:13:18.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8756" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":280,"completed":168,"skipped":2799,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:13:18.922: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Feb 23 21:13:18.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 create -f - --namespace=kubectl-466'
Feb 23 21:13:19.997: INFO: stderr: ""
Feb 23 21:13:19.997: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 21:13:19.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-466'
Feb 23 21:13:20.082: INFO: stderr: ""
Feb 23 21:13:20.082: INFO: stdout: "update-demo-nautilus-lv5jd update-demo-nautilus-wbndv "
Feb 23 21:13:20.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-lv5jd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-466'
Feb 23 21:13:20.137: INFO: stderr: ""
Feb 23 21:13:20.137: INFO: stdout: ""
Feb 23 21:13:20.137: INFO: update-demo-nautilus-lv5jd is created but not running
Feb 23 21:13:25.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-466'
Feb 23 21:13:25.197: INFO: stderr: ""
Feb 23 21:13:25.197: INFO: stdout: "update-demo-nautilus-lv5jd update-demo-nautilus-wbndv "
Feb 23 21:13:25.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-lv5jd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-466'
Feb 23 21:13:25.249: INFO: stderr: ""
Feb 23 21:13:25.249: INFO: stdout: "true"
Feb 23 21:13:25.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-lv5jd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-466'
Feb 23 21:13:25.306: INFO: stderr: ""
Feb 23 21:13:25.306: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 21:13:25.307: INFO: validating pod update-demo-nautilus-lv5jd
Feb 23 21:13:25.316: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 21:13:25.316: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 21:13:25.316: INFO: update-demo-nautilus-lv5jd is verified up and running
Feb 23 21:13:25.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-wbndv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-466'
Feb 23 21:13:25.372: INFO: stderr: ""
Feb 23 21:13:25.372: INFO: stdout: "true"
Feb 23 21:13:25.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-wbndv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-466'
Feb 23 21:13:25.424: INFO: stderr: ""
Feb 23 21:13:25.424: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 21:13:25.424: INFO: validating pod update-demo-nautilus-wbndv
Feb 23 21:13:25.428: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 21:13:25.428: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 21:13:25.428: INFO: update-demo-nautilus-wbndv is verified up and running
STEP: rolling-update to new replication controller
Feb 23 21:13:25.431: INFO: scanned /root for discovery docs: <nil>
Feb 23 21:13:25.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-466'
Feb 23 21:13:49.561: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 23 21:13:49.561: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 21:13:49.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-466'
Feb 23 21:13:49.624: INFO: stderr: ""
Feb 23 21:13:49.624: INFO: stdout: "update-demo-kitten-gdpcb update-demo-kitten-skcp9 "
Feb 23 21:13:49.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-kitten-gdpcb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-466'
Feb 23 21:13:49.679: INFO: stderr: ""
Feb 23 21:13:49.679: INFO: stdout: "true"
Feb 23 21:13:49.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-kitten-gdpcb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-466'
Feb 23 21:13:49.740: INFO: stderr: ""
Feb 23 21:13:49.740: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 23 21:13:49.740: INFO: validating pod update-demo-kitten-gdpcb
Feb 23 21:13:49.744: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 23 21:13:49.744: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 23 21:13:49.744: INFO: update-demo-kitten-gdpcb is verified up and running
Feb 23 21:13:49.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-kitten-skcp9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-466'
Feb 23 21:13:49.803: INFO: stderr: ""
Feb 23 21:13:49.803: INFO: stdout: "true"
Feb 23 21:13:49.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-kitten-skcp9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-466'
Feb 23 21:13:49.865: INFO: stderr: ""
Feb 23 21:13:49.865: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 23 21:13:49.865: INFO: validating pod update-demo-kitten-skcp9
Feb 23 21:13:49.868: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 23 21:13:49.868: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 23 21:13:49.868: INFO: update-demo-kitten-skcp9 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:13:49.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-466" for this suite.

â€¢ [SLOW TEST:30.955 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":280,"completed":169,"skipped":2820,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:13:49.878: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Feb 23 21:13:59.954: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:13:59.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0223 21:13:59.954459      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2398" for this suite.

â€¢ [SLOW TEST:10.085 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":280,"completed":170,"skipped":2831,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:13:59.964: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:15:00.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6178" for this suite.

â€¢ [SLOW TEST:60.053 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":280,"completed":171,"skipped":2856,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:15:00.018: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Feb 23 21:15:00.050: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:15:19.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8585" for this suite.

â€¢ [SLOW TEST:19.081 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":280,"completed":172,"skipped":2861,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:15:19.099: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:15:19.136: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 23 21:15:24.139: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 23 21:15:24.139: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 23 21:15:26.144: INFO: Creating deployment "test-rollover-deployment"
Feb 23 21:15:26.155: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 23 21:15:28.188: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 23 21:15:28.202: INFO: Ensure that both replica sets have 1 created replica
Feb 23 21:15:28.206: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 23 21:15:28.213: INFO: Updating deployment test-rollover-deployment
Feb 23 21:15:28.213: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 23 21:15:30.219: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 23 21:15:30.223: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 23 21:15:30.228: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 21:15:30.228: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089326, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089326, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089329, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089326, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 21:15:32.235: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 21:15:32.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089326, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089326, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089329, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089326, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 21:15:34.235: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 21:15:34.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089326, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089326, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089329, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089326, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 21:15:36.238: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 21:15:36.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089326, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089326, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089329, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089326, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 21:15:38.237: INFO: all replica sets need to contain the pod-template-hash label
Feb 23 21:15:38.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089326, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089326, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089329, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718089326, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 21:15:40.251: INFO: 
Feb 23 21:15:40.251: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 23 21:15:40.258: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8630 /apis/apps/v1/namespaces/deployment-8630/deployments/test-rollover-deployment 4b247e05-682c-44ef-86d2-82aea0d94a1e 20929 2 2020-02-23 21:15:26 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0041ea308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-23 21:15:26 +0000 UTC,LastTransitionTime:2020-02-23 21:15:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-02-23 21:15:39 +0000 UTC,LastTransitionTime:2020-02-23 21:15:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 23 21:15:40.260: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-8630 /apis/apps/v1/namespaces/deployment-8630/replicasets/test-rollover-deployment-574d6dfbff 8b10f477-a4d1-4547-bde3-e13c59750af7 20918 2 2020-02-23 21:15:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4b247e05-682c-44ef-86d2-82aea0d94a1e 0xc0041ea877 0xc0041ea878}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0041ea908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 23 21:15:40.260: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 23 21:15:40.261: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8630 /apis/apps/v1/namespaces/deployment-8630/replicasets/test-rollover-controller 879c3388-0867-418a-8c78-6864dfb9364e 20928 2 2020-02-23 21:15:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4b247e05-682c-44ef-86d2-82aea0d94a1e 0xc0041ea777 0xc0041ea778}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0041ea7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 23 21:15:40.261: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-8630 /apis/apps/v1/namespaces/deployment-8630/replicasets/test-rollover-deployment-f6c94f66c d08fa207-ed9c-4684-9b7e-3e91acf201e9 20870 2 2020-02-23 21:15:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4b247e05-682c-44ef-86d2-82aea0d94a1e 0xc0041ea9c0 0xc0041ea9c1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0041eaa58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 23 21:15:40.264: INFO: Pod "test-rollover-deployment-574d6dfbff-rqgwt" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-rqgwt test-rollover-deployment-574d6dfbff- deployment-8630 /api/v1/namespaces/deployment-8630/pods/test-rollover-deployment-574d6dfbff-rqgwt de60216e-d5ef-482b-98fa-803a5e711a61 20885 0 2020-02-23 21:15:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[cni.projectcalico.org/podIP:10.200.5.57/32] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff 8b10f477-a4d1-4547-bde3-e13c59750af7 0xc0041eb147 0xc0041eb148}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2tjlv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2tjlv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2tjlv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 21:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 21:15:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 21:15:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 21:15:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:10.200.5.57,StartTime:2020-02-23 21:15:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 21:15:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://b6e8f4cb3214ee0637b32d9fa04263ccf0d706d9d2eedd471f842e96cd36d7ce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:15:40.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8630" for this suite.

â€¢ [SLOW TEST:21.174 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":280,"completed":173,"skipped":2862,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:15:40.274: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:16:09.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2090" for this suite.
STEP: Destroying namespace "nsdeletetest-229" for this suite.
Feb 23 21:16:09.415: INFO: Namespace nsdeletetest-229 was already deleted
STEP: Destroying namespace "nsdeletetest-2115" for this suite.

â€¢ [SLOW TEST:29.148 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":280,"completed":174,"skipped":2864,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:16:09.424: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 21:16:09.841: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 21:16:12.863: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:16:13.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1361" for this suite.
STEP: Destroying namespace "webhook-1361-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":280,"completed":175,"skipped":2866,"failed":0}

------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:16:13.093: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:16:13.161: INFO: (0) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 13.842498ms)
Feb 23 21:16:13.165: INFO: (1) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.657778ms)
Feb 23 21:16:13.173: INFO: (2) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 8.021698ms)
Feb 23 21:16:13.177: INFO: (3) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.129837ms)
Feb 23 21:16:13.183: INFO: (4) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.402661ms)
Feb 23 21:16:13.187: INFO: (5) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.084488ms)
Feb 23 21:16:13.191: INFO: (6) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.078646ms)
Feb 23 21:16:13.194: INFO: (7) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.878084ms)
Feb 23 21:16:13.197: INFO: (8) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.32616ms)
Feb 23 21:16:13.201: INFO: (9) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.617225ms)
Feb 23 21:16:13.207: INFO: (10) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.08064ms)
Feb 23 21:16:13.210: INFO: (11) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.988098ms)
Feb 23 21:16:13.214: INFO: (12) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.289581ms)
Feb 23 21:16:13.216: INFO: (13) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.694878ms)
Feb 23 21:16:13.219: INFO: (14) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.950946ms)
Feb 23 21:16:13.222: INFO: (15) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.02695ms)
Feb 23 21:16:13.226: INFO: (16) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.571899ms)
Feb 23 21:16:13.229: INFO: (17) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.099063ms)
Feb 23 21:16:13.236: INFO: (18) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.664295ms)
Feb 23 21:16:13.239: INFO: (19) /api/v1/nodes/worker01:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.232371ms)
[AfterEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:16:13.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2950" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":280,"completed":176,"skipped":2866,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:16:13.251: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 23 21:16:13.290: INFO: Waiting up to 5m0s for pod "downward-api-f6754756-03f1-4315-be24-8d81ec195940" in namespace "downward-api-9401" to be "success or failure"
Feb 23 21:16:13.296: INFO: Pod "downward-api-f6754756-03f1-4315-be24-8d81ec195940": Phase="Pending", Reason="", readiness=false. Elapsed: 5.451772ms
Feb 23 21:16:15.300: INFO: Pod "downward-api-f6754756-03f1-4315-be24-8d81ec195940": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009971426s
STEP: Saw pod success
Feb 23 21:16:15.300: INFO: Pod "downward-api-f6754756-03f1-4315-be24-8d81ec195940" satisfied condition "success or failure"
Feb 23 21:16:15.305: INFO: Trying to get logs from node worker01 pod downward-api-f6754756-03f1-4315-be24-8d81ec195940 container dapi-container: <nil>
STEP: delete the pod
Feb 23 21:16:15.329: INFO: Waiting for pod downward-api-f6754756-03f1-4315-be24-8d81ec195940 to disappear
Feb 23 21:16:15.332: INFO: Pod downward-api-f6754756-03f1-4315-be24-8d81ec195940 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:16:15.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9401" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":280,"completed":177,"skipped":2866,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:16:15.340: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 23 21:16:15.375: INFO: Waiting up to 5m0s for pod "pod-818c4b09-6c3a-4140-a582-ff73375a749c" in namespace "emptydir-3075" to be "success or failure"
Feb 23 21:16:15.379: INFO: Pod "pod-818c4b09-6c3a-4140-a582-ff73375a749c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.506223ms
Feb 23 21:16:17.411: INFO: Pod "pod-818c4b09-6c3a-4140-a582-ff73375a749c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03530246s
STEP: Saw pod success
Feb 23 21:16:17.411: INFO: Pod "pod-818c4b09-6c3a-4140-a582-ff73375a749c" satisfied condition "success or failure"
Feb 23 21:16:17.415: INFO: Trying to get logs from node worker01 pod pod-818c4b09-6c3a-4140-a582-ff73375a749c container test-container: <nil>
STEP: delete the pod
Feb 23 21:16:17.432: INFO: Waiting for pod pod-818c4b09-6c3a-4140-a582-ff73375a749c to disappear
Feb 23 21:16:17.438: INFO: Pod pod-818c4b09-6c3a-4140-a582-ff73375a749c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:16:17.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3075" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":178,"skipped":2870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:16:17.449: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:16:17.477: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8803
I0223 21:16:17.492310      23 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8803, replica count: 1
I0223 21:16:18.543410      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0223 21:16:19.545684      23 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 23 21:16:19.660: INFO: Created: latency-svc-sd4c5
Feb 23 21:16:19.676: INFO: Got endpoints: latency-svc-sd4c5 [30.201637ms]
Feb 23 21:16:19.713: INFO: Created: latency-svc-f5hbr
Feb 23 21:16:19.722: INFO: Got endpoints: latency-svc-f5hbr [46.112481ms]
Feb 23 21:16:19.733: INFO: Created: latency-svc-6zzbp
Feb 23 21:16:19.739: INFO: Got endpoints: latency-svc-6zzbp [62.955656ms]
Feb 23 21:16:19.745: INFO: Created: latency-svc-gzc4h
Feb 23 21:16:19.750: INFO: Got endpoints: latency-svc-gzc4h [73.202615ms]
Feb 23 21:16:19.755: INFO: Created: latency-svc-p8dwk
Feb 23 21:16:19.762: INFO: Got endpoints: latency-svc-p8dwk [85.608276ms]
Feb 23 21:16:19.773: INFO: Created: latency-svc-nr65m
Feb 23 21:16:19.780: INFO: Got endpoints: latency-svc-nr65m [103.168252ms]
Feb 23 21:16:19.787: INFO: Created: latency-svc-hpvdp
Feb 23 21:16:19.793: INFO: Got endpoints: latency-svc-hpvdp [116.386339ms]
Feb 23 21:16:19.800: INFO: Created: latency-svc-q6xsr
Feb 23 21:16:19.813: INFO: Got endpoints: latency-svc-q6xsr [136.383971ms]
Feb 23 21:16:19.826: INFO: Created: latency-svc-ch925
Feb 23 21:16:19.833: INFO: Got endpoints: latency-svc-ch925 [156.36063ms]
Feb 23 21:16:19.840: INFO: Created: latency-svc-glzpr
Feb 23 21:16:19.846: INFO: Got endpoints: latency-svc-glzpr [168.609562ms]
Feb 23 21:16:19.854: INFO: Created: latency-svc-5hj9p
Feb 23 21:16:19.859: INFO: Created: latency-svc-cwdg5
Feb 23 21:16:19.859: INFO: Got endpoints: latency-svc-5hj9p [182.130538ms]
Feb 23 21:16:19.869: INFO: Got endpoints: latency-svc-cwdg5 [192.350993ms]
Feb 23 21:16:19.872: INFO: Created: latency-svc-rq59s
Feb 23 21:16:19.882: INFO: Got endpoints: latency-svc-rq59s [204.711594ms]
Feb 23 21:16:19.885: INFO: Created: latency-svc-962g4
Feb 23 21:16:19.890: INFO: Got endpoints: latency-svc-962g4 [213.794186ms]
Feb 23 21:16:19.903: INFO: Created: latency-svc-698cg
Feb 23 21:16:19.911: INFO: Got endpoints: latency-svc-698cg [233.652893ms]
Feb 23 21:16:19.919: INFO: Created: latency-svc-bd57k
Feb 23 21:16:19.926: INFO: Got endpoints: latency-svc-bd57k [248.775ms]
Feb 23 21:16:19.934: INFO: Created: latency-svc-sfnln
Feb 23 21:16:19.943: INFO: Got endpoints: latency-svc-sfnln [221.264312ms]
Feb 23 21:16:19.949: INFO: Created: latency-svc-gk2sm
Feb 23 21:16:19.958: INFO: Got endpoints: latency-svc-gk2sm [219.048977ms]
Feb 23 21:16:19.966: INFO: Created: latency-svc-4rcp7
Feb 23 21:16:19.966: INFO: Got endpoints: latency-svc-4rcp7 [215.839911ms]
Feb 23 21:16:19.969: INFO: Created: latency-svc-kgzfb
Feb 23 21:16:19.976: INFO: Got endpoints: latency-svc-kgzfb [214.130821ms]
Feb 23 21:16:19.997: INFO: Created: latency-svc-2kpw7
Feb 23 21:16:20.004: INFO: Got endpoints: latency-svc-2kpw7 [223.792847ms]
Feb 23 21:16:20.008: INFO: Created: latency-svc-j8hx8
Feb 23 21:16:20.014: INFO: Got endpoints: latency-svc-j8hx8 [220.461354ms]
Feb 23 21:16:20.024: INFO: Created: latency-svc-cjs7k
Feb 23 21:16:20.031: INFO: Got endpoints: latency-svc-cjs7k [217.941069ms]
Feb 23 21:16:20.032: INFO: Created: latency-svc-rtzj9
Feb 23 21:16:20.039: INFO: Got endpoints: latency-svc-rtzj9 [206.400445ms]
Feb 23 21:16:20.052: INFO: Created: latency-svc-j6mvw
Feb 23 21:16:20.062: INFO: Got endpoints: latency-svc-j6mvw [215.920768ms]
Feb 23 21:16:20.064: INFO: Created: latency-svc-msm87
Feb 23 21:16:20.071: INFO: Got endpoints: latency-svc-msm87 [212.238215ms]
Feb 23 21:16:20.082: INFO: Created: latency-svc-6mw9n
Feb 23 21:16:20.083: INFO: Got endpoints: latency-svc-6mw9n [213.95822ms]
Feb 23 21:16:20.086: INFO: Created: latency-svc-k8b52
Feb 23 21:16:20.093: INFO: Got endpoints: latency-svc-k8b52 [211.042235ms]
Feb 23 21:16:20.103: INFO: Created: latency-svc-xrxx7
Feb 23 21:16:20.117: INFO: Got endpoints: latency-svc-xrxx7 [226.915379ms]
Feb 23 21:16:20.126: INFO: Created: latency-svc-c6m6p
Feb 23 21:16:20.138: INFO: Got endpoints: latency-svc-c6m6p [227.327456ms]
Feb 23 21:16:20.146: INFO: Created: latency-svc-84fl2
Feb 23 21:16:20.151: INFO: Got endpoints: latency-svc-84fl2 [225.259759ms]
Feb 23 21:16:20.153: INFO: Created: latency-svc-6g5pc
Feb 23 21:16:20.159: INFO: Got endpoints: latency-svc-6g5pc [215.45731ms]
Feb 23 21:16:20.170: INFO: Created: latency-svc-hn8qv
Feb 23 21:16:20.170: INFO: Got endpoints: latency-svc-hn8qv [208.056827ms]
Feb 23 21:16:20.171: INFO: Created: latency-svc-q5g55
Feb 23 21:16:20.188: INFO: Got endpoints: latency-svc-q5g55 [221.449301ms]
Feb 23 21:16:20.195: INFO: Created: latency-svc-pqcj6
Feb 23 21:16:20.198: INFO: Got endpoints: latency-svc-pqcj6 [221.951322ms]
Feb 23 21:16:20.203: INFO: Created: latency-svc-dtjxl
Feb 23 21:16:20.209: INFO: Got endpoints: latency-svc-dtjxl [204.745885ms]
Feb 23 21:16:20.218: INFO: Created: latency-svc-4crs8
Feb 23 21:16:20.222: INFO: Got endpoints: latency-svc-4crs8 [207.92597ms]
Feb 23 21:16:20.224: INFO: Created: latency-svc-gh64k
Feb 23 21:16:20.229: INFO: Got endpoints: latency-svc-gh64k [197.566718ms]
Feb 23 21:16:20.237: INFO: Created: latency-svc-ntbv5
Feb 23 21:16:20.239: INFO: Got endpoints: latency-svc-ntbv5 [200.206874ms]
Feb 23 21:16:20.248: INFO: Created: latency-svc-qk8k9
Feb 23 21:16:20.253: INFO: Got endpoints: latency-svc-qk8k9 [191.247629ms]
Feb 23 21:16:20.261: INFO: Created: latency-svc-7z9hb
Feb 23 21:16:20.267: INFO: Created: latency-svc-ckdw8
Feb 23 21:16:20.271: INFO: Got endpoints: latency-svc-7z9hb [199.628077ms]
Feb 23 21:16:20.275: INFO: Got endpoints: latency-svc-ckdw8 [191.577518ms]
Feb 23 21:16:20.280: INFO: Created: latency-svc-j22ng
Feb 23 21:16:20.285: INFO: Created: latency-svc-wkl56
Feb 23 21:16:20.291: INFO: Created: latency-svc-8ln25
Feb 23 21:16:20.296: INFO: Created: latency-svc-b44tl
Feb 23 21:16:20.308: INFO: Created: latency-svc-dnmws
Feb 23 21:16:20.312: INFO: Created: latency-svc-qsnfh
Feb 23 21:16:20.318: INFO: Got endpoints: latency-svc-j22ng [224.657178ms]
Feb 23 21:16:20.325: INFO: Created: latency-svc-nslms
Feb 23 21:16:20.333: INFO: Created: latency-svc-lxcqj
Feb 23 21:16:20.348: INFO: Created: latency-svc-ttklj
Feb 23 21:16:20.362: INFO: Created: latency-svc-rvsfd
Feb 23 21:16:20.373: INFO: Got endpoints: latency-svc-wkl56 [255.415336ms]
Feb 23 21:16:20.380: INFO: Created: latency-svc-c4kz4
Feb 23 21:16:20.389: INFO: Created: latency-svc-ssjsc
Feb 23 21:16:20.397: INFO: Created: latency-svc-4cbrl
Feb 23 21:16:20.409: INFO: Created: latency-svc-5l57h
Feb 23 21:16:20.422: INFO: Got endpoints: latency-svc-8ln25 [284.251213ms]
Feb 23 21:16:20.435: INFO: Created: latency-svc-8bzxz
Feb 23 21:16:20.451: INFO: Created: latency-svc-q98k9
Feb 23 21:16:20.462: INFO: Created: latency-svc-ch7cn
Feb 23 21:16:20.471: INFO: Got endpoints: latency-svc-b44tl [319.159179ms]
Feb 23 21:16:20.481: INFO: Created: latency-svc-kx9xx
Feb 23 21:16:20.494: INFO: Created: latency-svc-h54wg
Feb 23 21:16:20.521: INFO: Got endpoints: latency-svc-dnmws [361.671351ms]
Feb 23 21:16:20.533: INFO: Created: latency-svc-x6h4v
Feb 23 21:16:20.569: INFO: Got endpoints: latency-svc-qsnfh [398.821913ms]
Feb 23 21:16:20.583: INFO: Created: latency-svc-hhr55
Feb 23 21:16:20.620: INFO: Got endpoints: latency-svc-nslms [432.221363ms]
Feb 23 21:16:20.634: INFO: Created: latency-svc-6g4t9
Feb 23 21:16:20.667: INFO: Got endpoints: latency-svc-lxcqj [468.315549ms]
Feb 23 21:16:20.682: INFO: Created: latency-svc-rkxpz
Feb 23 21:16:20.719: INFO: Got endpoints: latency-svc-ttklj [510.610003ms]
Feb 23 21:16:20.733: INFO: Created: latency-svc-5zz75
Feb 23 21:16:20.772: INFO: Got endpoints: latency-svc-rvsfd [549.044211ms]
Feb 23 21:16:20.791: INFO: Created: latency-svc-l77cv
Feb 23 21:16:20.819: INFO: Got endpoints: latency-svc-c4kz4 [590.298939ms]
Feb 23 21:16:20.835: INFO: Created: latency-svc-2wb2c
Feb 23 21:16:20.867: INFO: Got endpoints: latency-svc-ssjsc [627.739417ms]
Feb 23 21:16:20.879: INFO: Created: latency-svc-w8w2h
Feb 23 21:16:20.923: INFO: Got endpoints: latency-svc-4cbrl [670.207175ms]
Feb 23 21:16:20.943: INFO: Created: latency-svc-wj926
Feb 23 21:16:20.968: INFO: Got endpoints: latency-svc-5l57h [697.177239ms]
Feb 23 21:16:20.982: INFO: Created: latency-svc-z5gd9
Feb 23 21:16:21.018: INFO: Got endpoints: latency-svc-8bzxz [743.384065ms]
Feb 23 21:16:21.029: INFO: Created: latency-svc-gkj2m
Feb 23 21:16:21.068: INFO: Got endpoints: latency-svc-q98k9 [749.835656ms]
Feb 23 21:16:21.078: INFO: Created: latency-svc-q6htt
Feb 23 21:16:21.118: INFO: Got endpoints: latency-svc-ch7cn [744.380683ms]
Feb 23 21:16:21.128: INFO: Created: latency-svc-9g6cn
Feb 23 21:16:21.169: INFO: Got endpoints: latency-svc-kx9xx [746.514389ms]
Feb 23 21:16:21.182: INFO: Created: latency-svc-kbkkf
Feb 23 21:16:21.220: INFO: Got endpoints: latency-svc-h54wg [748.656287ms]
Feb 23 21:16:21.229: INFO: Created: latency-svc-jm5qw
Feb 23 21:16:21.270: INFO: Got endpoints: latency-svc-x6h4v [749.347875ms]
Feb 23 21:16:21.284: INFO: Created: latency-svc-jc5mh
Feb 23 21:16:21.319: INFO: Got endpoints: latency-svc-hhr55 [749.345626ms]
Feb 23 21:16:21.327: INFO: Created: latency-svc-j428m
Feb 23 21:16:21.372: INFO: Got endpoints: latency-svc-6g4t9 [751.662853ms]
Feb 23 21:16:21.387: INFO: Created: latency-svc-bwtqt
Feb 23 21:16:21.417: INFO: Got endpoints: latency-svc-rkxpz [750.388433ms]
Feb 23 21:16:21.428: INFO: Created: latency-svc-6455n
Feb 23 21:16:21.469: INFO: Got endpoints: latency-svc-5zz75 [749.802035ms]
Feb 23 21:16:21.481: INFO: Created: latency-svc-87pw5
Feb 23 21:16:21.518: INFO: Got endpoints: latency-svc-l77cv [746.426772ms]
Feb 23 21:16:21.528: INFO: Created: latency-svc-rmwpg
Feb 23 21:16:21.570: INFO: Got endpoints: latency-svc-2wb2c [750.163304ms]
Feb 23 21:16:21.582: INFO: Created: latency-svc-nppxf
Feb 23 21:16:21.620: INFO: Got endpoints: latency-svc-w8w2h [752.879929ms]
Feb 23 21:16:21.632: INFO: Created: latency-svc-gk9zf
Feb 23 21:16:21.673: INFO: Got endpoints: latency-svc-wj926 [749.871281ms]
Feb 23 21:16:21.686: INFO: Created: latency-svc-q6btc
Feb 23 21:16:21.720: INFO: Got endpoints: latency-svc-z5gd9 [751.662821ms]
Feb 23 21:16:21.730: INFO: Created: latency-svc-pvw89
Feb 23 21:16:21.771: INFO: Got endpoints: latency-svc-gkj2m [752.706547ms]
Feb 23 21:16:21.782: INFO: Created: latency-svc-whg2q
Feb 23 21:16:21.818: INFO: Got endpoints: latency-svc-q6htt [749.610367ms]
Feb 23 21:16:21.828: INFO: Created: latency-svc-vcmc7
Feb 23 21:16:21.868: INFO: Got endpoints: latency-svc-9g6cn [750.259387ms]
Feb 23 21:16:21.880: INFO: Created: latency-svc-csmq9
Feb 23 21:16:21.918: INFO: Got endpoints: latency-svc-kbkkf [749.209452ms]
Feb 23 21:16:21.932: INFO: Created: latency-svc-dfxh5
Feb 23 21:16:21.968: INFO: Got endpoints: latency-svc-jm5qw [748.964739ms]
Feb 23 21:16:21.981: INFO: Created: latency-svc-c7lw7
Feb 23 21:16:22.020: INFO: Got endpoints: latency-svc-jc5mh [749.654494ms]
Feb 23 21:16:22.031: INFO: Created: latency-svc-9klx6
Feb 23 21:16:22.067: INFO: Got endpoints: latency-svc-j428m [748.461057ms]
Feb 23 21:16:22.080: INFO: Created: latency-svc-fpwpj
Feb 23 21:16:22.118: INFO: Got endpoints: latency-svc-bwtqt [746.589913ms]
Feb 23 21:16:22.128: INFO: Created: latency-svc-qqnsb
Feb 23 21:16:22.169: INFO: Got endpoints: latency-svc-6455n [751.337709ms]
Feb 23 21:16:22.187: INFO: Created: latency-svc-vp95w
Feb 23 21:16:22.220: INFO: Got endpoints: latency-svc-87pw5 [750.512877ms]
Feb 23 21:16:22.241: INFO: Created: latency-svc-nb4lp
Feb 23 21:16:22.271: INFO: Got endpoints: latency-svc-rmwpg [752.577793ms]
Feb 23 21:16:22.283: INFO: Created: latency-svc-25t47
Feb 23 21:16:22.320: INFO: Got endpoints: latency-svc-nppxf [750.68708ms]
Feb 23 21:16:22.329: INFO: Created: latency-svc-5l79f
Feb 23 21:16:22.369: INFO: Got endpoints: latency-svc-gk9zf [748.320439ms]
Feb 23 21:16:22.380: INFO: Created: latency-svc-nbrd6
Feb 23 21:16:22.418: INFO: Got endpoints: latency-svc-q6btc [745.238117ms]
Feb 23 21:16:22.430: INFO: Created: latency-svc-q4zq7
Feb 23 21:16:22.470: INFO: Got endpoints: latency-svc-pvw89 [749.789798ms]
Feb 23 21:16:22.485: INFO: Created: latency-svc-llnjj
Feb 23 21:16:22.519: INFO: Got endpoints: latency-svc-whg2q [748.06791ms]
Feb 23 21:16:22.534: INFO: Created: latency-svc-dl2hb
Feb 23 21:16:22.570: INFO: Got endpoints: latency-svc-vcmc7 [751.851104ms]
Feb 23 21:16:22.586: INFO: Created: latency-svc-5gxx6
Feb 23 21:16:22.618: INFO: Got endpoints: latency-svc-csmq9 [749.802822ms]
Feb 23 21:16:22.633: INFO: Created: latency-svc-vzbp8
Feb 23 21:16:22.668: INFO: Got endpoints: latency-svc-dfxh5 [749.280419ms]
Feb 23 21:16:22.682: INFO: Created: latency-svc-l74bh
Feb 23 21:16:22.720: INFO: Got endpoints: latency-svc-c7lw7 [750.445774ms]
Feb 23 21:16:22.734: INFO: Created: latency-svc-p27n4
Feb 23 21:16:22.772: INFO: Got endpoints: latency-svc-9klx6 [752.119664ms]
Feb 23 21:16:22.789: INFO: Created: latency-svc-q5x4k
Feb 23 21:16:22.818: INFO: Got endpoints: latency-svc-fpwpj [750.894856ms]
Feb 23 21:16:22.832: INFO: Created: latency-svc-pbwl9
Feb 23 21:16:22.869: INFO: Got endpoints: latency-svc-qqnsb [750.337401ms]
Feb 23 21:16:22.882: INFO: Created: latency-svc-662vr
Feb 23 21:16:22.917: INFO: Got endpoints: latency-svc-vp95w [748.100236ms]
Feb 23 21:16:22.931: INFO: Created: latency-svc-n74v9
Feb 23 21:16:22.968: INFO: Got endpoints: latency-svc-nb4lp [747.582321ms]
Feb 23 21:16:22.985: INFO: Created: latency-svc-s6ld4
Feb 23 21:16:23.019: INFO: Got endpoints: latency-svc-25t47 [747.411919ms]
Feb 23 21:16:23.030: INFO: Created: latency-svc-hnw7b
Feb 23 21:16:23.068: INFO: Got endpoints: latency-svc-5l79f [747.492968ms]
Feb 23 21:16:23.088: INFO: Created: latency-svc-n66rz
Feb 23 21:16:23.118: INFO: Got endpoints: latency-svc-nbrd6 [749.159344ms]
Feb 23 21:16:23.129: INFO: Created: latency-svc-qzbkt
Feb 23 21:16:23.171: INFO: Got endpoints: latency-svc-q4zq7 [753.046761ms]
Feb 23 21:16:23.182: INFO: Created: latency-svc-2p6tp
Feb 23 21:16:23.223: INFO: Got endpoints: latency-svc-llnjj [752.17482ms]
Feb 23 21:16:23.234: INFO: Created: latency-svc-cc7s8
Feb 23 21:16:23.271: INFO: Got endpoints: latency-svc-dl2hb [751.485895ms]
Feb 23 21:16:23.280: INFO: Created: latency-svc-ztkm4
Feb 23 21:16:23.317: INFO: Got endpoints: latency-svc-5gxx6 [746.932109ms]
Feb 23 21:16:23.332: INFO: Created: latency-svc-wgvlr
Feb 23 21:16:23.368: INFO: Got endpoints: latency-svc-vzbp8 [749.496402ms]
Feb 23 21:16:23.384: INFO: Created: latency-svc-sr5mn
Feb 23 21:16:23.419: INFO: Got endpoints: latency-svc-l74bh [750.938093ms]
Feb 23 21:16:23.431: INFO: Created: latency-svc-x4w2b
Feb 23 21:16:23.471: INFO: Got endpoints: latency-svc-p27n4 [750.302028ms]
Feb 23 21:16:23.483: INFO: Created: latency-svc-92mtm
Feb 23 21:16:23.517: INFO: Got endpoints: latency-svc-q5x4k [745.03122ms]
Feb 23 21:16:23.526: INFO: Created: latency-svc-zkcmf
Feb 23 21:16:23.571: INFO: Got endpoints: latency-svc-pbwl9 [751.557652ms]
Feb 23 21:16:23.580: INFO: Created: latency-svc-fvm7n
Feb 23 21:16:23.620: INFO: Got endpoints: latency-svc-662vr [750.323744ms]
Feb 23 21:16:23.628: INFO: Created: latency-svc-855g8
Feb 23 21:16:23.667: INFO: Got endpoints: latency-svc-n74v9 [748.609896ms]
Feb 23 21:16:23.683: INFO: Created: latency-svc-wqqqq
Feb 23 21:16:23.717: INFO: Got endpoints: latency-svc-s6ld4 [749.394123ms]
Feb 23 21:16:23.727: INFO: Created: latency-svc-x2jbx
Feb 23 21:16:23.768: INFO: Got endpoints: latency-svc-hnw7b [749.491504ms]
Feb 23 21:16:23.780: INFO: Created: latency-svc-tb9d8
Feb 23 21:16:23.819: INFO: Got endpoints: latency-svc-n66rz [750.129876ms]
Feb 23 21:16:23.833: INFO: Created: latency-svc-zvzsx
Feb 23 21:16:23.872: INFO: Got endpoints: latency-svc-qzbkt [754.054308ms]
Feb 23 21:16:23.885: INFO: Created: latency-svc-9ctfc
Feb 23 21:16:23.919: INFO: Got endpoints: latency-svc-2p6tp [747.405543ms]
Feb 23 21:16:23.934: INFO: Created: latency-svc-f82cf
Feb 23 21:16:23.969: INFO: Got endpoints: latency-svc-cc7s8 [746.020308ms]
Feb 23 21:16:23.983: INFO: Created: latency-svc-x2td6
Feb 23 21:16:24.020: INFO: Got endpoints: latency-svc-ztkm4 [749.066244ms]
Feb 23 21:16:24.030: INFO: Created: latency-svc-vz75p
Feb 23 21:16:24.067: INFO: Got endpoints: latency-svc-wgvlr [749.337778ms]
Feb 23 21:16:24.078: INFO: Created: latency-svc-zmw5b
Feb 23 21:16:24.117: INFO: Got endpoints: latency-svc-sr5mn [748.629368ms]
Feb 23 21:16:24.129: INFO: Created: latency-svc-bn5dw
Feb 23 21:16:24.170: INFO: Got endpoints: latency-svc-x4w2b [751.295802ms]
Feb 23 21:16:24.181: INFO: Created: latency-svc-gfvfb
Feb 23 21:16:24.228: INFO: Got endpoints: latency-svc-92mtm [756.665541ms]
Feb 23 21:16:24.238: INFO: Created: latency-svc-bcxx4
Feb 23 21:16:24.268: INFO: Got endpoints: latency-svc-zkcmf [750.107206ms]
Feb 23 21:16:24.282: INFO: Created: latency-svc-tptkb
Feb 23 21:16:24.318: INFO: Got endpoints: latency-svc-fvm7n [747.339043ms]
Feb 23 21:16:24.332: INFO: Created: latency-svc-g95n9
Feb 23 21:16:24.367: INFO: Got endpoints: latency-svc-855g8 [746.984654ms]
Feb 23 21:16:24.378: INFO: Created: latency-svc-6gbrr
Feb 23 21:16:24.417: INFO: Got endpoints: latency-svc-wqqqq [749.610181ms]
Feb 23 21:16:24.428: INFO: Created: latency-svc-72xx8
Feb 23 21:16:24.468: INFO: Got endpoints: latency-svc-x2jbx [749.951312ms]
Feb 23 21:16:24.485: INFO: Created: latency-svc-vmgf5
Feb 23 21:16:24.523: INFO: Got endpoints: latency-svc-tb9d8 [754.899099ms]
Feb 23 21:16:24.536: INFO: Created: latency-svc-6fxvf
Feb 23 21:16:24.570: INFO: Got endpoints: latency-svc-zvzsx [751.55987ms]
Feb 23 21:16:24.587: INFO: Created: latency-svc-hl9pp
Feb 23 21:16:24.619: INFO: Got endpoints: latency-svc-9ctfc [744.834817ms]
Feb 23 21:16:24.629: INFO: Created: latency-svc-kh4td
Feb 23 21:16:24.668: INFO: Got endpoints: latency-svc-f82cf [748.677731ms]
Feb 23 21:16:24.678: INFO: Created: latency-svc-kx57s
Feb 23 21:16:24.719: INFO: Got endpoints: latency-svc-x2td6 [749.987237ms]
Feb 23 21:16:24.728: INFO: Created: latency-svc-6pmn8
Feb 23 21:16:24.770: INFO: Got endpoints: latency-svc-vz75p [749.364999ms]
Feb 23 21:16:24.779: INFO: Created: latency-svc-8fbb9
Feb 23 21:16:24.818: INFO: Got endpoints: latency-svc-zmw5b [750.781308ms]
Feb 23 21:16:24.832: INFO: Created: latency-svc-kfkk4
Feb 23 21:16:24.868: INFO: Got endpoints: latency-svc-bn5dw [750.600648ms]
Feb 23 21:16:24.878: INFO: Created: latency-svc-tvb6r
Feb 23 21:16:24.918: INFO: Got endpoints: latency-svc-gfvfb [747.816277ms]
Feb 23 21:16:24.926: INFO: Created: latency-svc-pnd2r
Feb 23 21:16:24.968: INFO: Got endpoints: latency-svc-bcxx4 [739.671505ms]
Feb 23 21:16:24.981: INFO: Created: latency-svc-hn8c4
Feb 23 21:16:25.020: INFO: Got endpoints: latency-svc-tptkb [752.706041ms]
Feb 23 21:16:25.029: INFO: Created: latency-svc-glbd5
Feb 23 21:16:25.071: INFO: Got endpoints: latency-svc-g95n9 [752.018611ms]
Feb 23 21:16:25.093: INFO: Created: latency-svc-qftrz
Feb 23 21:16:25.120: INFO: Got endpoints: latency-svc-6gbrr [753.175483ms]
Feb 23 21:16:25.130: INFO: Created: latency-svc-kj7ln
Feb 23 21:16:25.171: INFO: Got endpoints: latency-svc-72xx8 [753.197724ms]
Feb 23 21:16:25.180: INFO: Created: latency-svc-m4z87
Feb 23 21:16:25.218: INFO: Got endpoints: latency-svc-vmgf5 [750.138225ms]
Feb 23 21:16:25.231: INFO: Created: latency-svc-lg7bc
Feb 23 21:16:25.278: INFO: Got endpoints: latency-svc-6fxvf [754.851224ms]
Feb 23 21:16:25.293: INFO: Created: latency-svc-567rt
Feb 23 21:16:25.326: INFO: Got endpoints: latency-svc-hl9pp [755.828176ms]
Feb 23 21:16:25.337: INFO: Created: latency-svc-hjns7
Feb 23 21:16:25.368: INFO: Got endpoints: latency-svc-kh4td [748.562601ms]
Feb 23 21:16:25.387: INFO: Created: latency-svc-xc6fs
Feb 23 21:16:25.418: INFO: Got endpoints: latency-svc-kx57s [749.725943ms]
Feb 23 21:16:25.430: INFO: Created: latency-svc-sgdsx
Feb 23 21:16:25.468: INFO: Got endpoints: latency-svc-6pmn8 [748.589472ms]
Feb 23 21:16:25.480: INFO: Created: latency-svc-ccqvx
Feb 23 21:16:25.523: INFO: Got endpoints: latency-svc-8fbb9 [753.57094ms]
Feb 23 21:16:25.542: INFO: Created: latency-svc-sm6xx
Feb 23 21:16:25.569: INFO: Got endpoints: latency-svc-kfkk4 [751.322717ms]
Feb 23 21:16:25.581: INFO: Created: latency-svc-w4t65
Feb 23 21:16:25.619: INFO: Got endpoints: latency-svc-tvb6r [751.314773ms]
Feb 23 21:16:25.629: INFO: Created: latency-svc-rwr99
Feb 23 21:16:25.669: INFO: Got endpoints: latency-svc-pnd2r [751.120546ms]
Feb 23 21:16:25.687: INFO: Created: latency-svc-sh2zp
Feb 23 21:16:25.718: INFO: Got endpoints: latency-svc-hn8c4 [750.214986ms]
Feb 23 21:16:25.727: INFO: Created: latency-svc-bld27
Feb 23 21:16:25.769: INFO: Got endpoints: latency-svc-glbd5 [748.797036ms]
Feb 23 21:16:25.780: INFO: Created: latency-svc-9d8zw
Feb 23 21:16:25.817: INFO: Got endpoints: latency-svc-qftrz [745.916243ms]
Feb 23 21:16:25.835: INFO: Created: latency-svc-2rcpq
Feb 23 21:16:25.870: INFO: Got endpoints: latency-svc-kj7ln [750.146083ms]
Feb 23 21:16:25.881: INFO: Created: latency-svc-nmjxr
Feb 23 21:16:25.924: INFO: Got endpoints: latency-svc-m4z87 [753.325187ms]
Feb 23 21:16:25.948: INFO: Created: latency-svc-xmvsm
Feb 23 21:16:25.972: INFO: Got endpoints: latency-svc-lg7bc [754.414744ms]
Feb 23 21:16:25.982: INFO: Created: latency-svc-gxp6t
Feb 23 21:16:26.019: INFO: Got endpoints: latency-svc-567rt [740.710921ms]
Feb 23 21:16:26.034: INFO: Created: latency-svc-f9wcb
Feb 23 21:16:26.068: INFO: Got endpoints: latency-svc-hjns7 [741.590504ms]
Feb 23 21:16:26.078: INFO: Created: latency-svc-mvlmw
Feb 23 21:16:26.119: INFO: Got endpoints: latency-svc-xc6fs [751.104933ms]
Feb 23 21:16:26.134: INFO: Created: latency-svc-clb2k
Feb 23 21:16:26.171: INFO: Got endpoints: latency-svc-sgdsx [753.792102ms]
Feb 23 21:16:26.183: INFO: Created: latency-svc-9fmgr
Feb 23 21:16:26.219: INFO: Got endpoints: latency-svc-ccqvx [750.713865ms]
Feb 23 21:16:26.235: INFO: Created: latency-svc-jzrst
Feb 23 21:16:26.269: INFO: Got endpoints: latency-svc-sm6xx [745.571616ms]
Feb 23 21:16:26.283: INFO: Created: latency-svc-z2w47
Feb 23 21:16:26.321: INFO: Got endpoints: latency-svc-w4t65 [751.869038ms]
Feb 23 21:16:26.337: INFO: Created: latency-svc-8s8mn
Feb 23 21:16:26.372: INFO: Got endpoints: latency-svc-rwr99 [752.452357ms]
Feb 23 21:16:26.381: INFO: Created: latency-svc-5kmxh
Feb 23 21:16:26.420: INFO: Got endpoints: latency-svc-sh2zp [750.573002ms]
Feb 23 21:16:26.432: INFO: Created: latency-svc-h7mf4
Feb 23 21:16:26.469: INFO: Got endpoints: latency-svc-bld27 [750.33392ms]
Feb 23 21:16:26.486: INFO: Created: latency-svc-vd58v
Feb 23 21:16:26.518: INFO: Got endpoints: latency-svc-9d8zw [748.713481ms]
Feb 23 21:16:26.532: INFO: Created: latency-svc-79fx7
Feb 23 21:16:26.569: INFO: Got endpoints: latency-svc-2rcpq [751.805717ms]
Feb 23 21:16:26.579: INFO: Created: latency-svc-p28nl
Feb 23 21:16:26.617: INFO: Got endpoints: latency-svc-nmjxr [746.303521ms]
Feb 23 21:16:26.626: INFO: Created: latency-svc-pbnq7
Feb 23 21:16:26.668: INFO: Got endpoints: latency-svc-xmvsm [743.645591ms]
Feb 23 21:16:26.679: INFO: Created: latency-svc-z5mtp
Feb 23 21:16:26.732: INFO: Got endpoints: latency-svc-gxp6t [760.006652ms]
Feb 23 21:16:26.742: INFO: Created: latency-svc-4q4qp
Feb 23 21:16:26.771: INFO: Got endpoints: latency-svc-f9wcb [751.63598ms]
Feb 23 21:16:26.782: INFO: Created: latency-svc-jrw76
Feb 23 21:16:26.818: INFO: Got endpoints: latency-svc-mvlmw [750.513058ms]
Feb 23 21:16:26.829: INFO: Created: latency-svc-c4xgq
Feb 23 21:16:26.869: INFO: Got endpoints: latency-svc-clb2k [750.231167ms]
Feb 23 21:16:26.885: INFO: Created: latency-svc-9m5cn
Feb 23 21:16:26.918: INFO: Got endpoints: latency-svc-9fmgr [745.186085ms]
Feb 23 21:16:26.931: INFO: Created: latency-svc-qp49t
Feb 23 21:16:26.968: INFO: Got endpoints: latency-svc-jzrst [749.159572ms]
Feb 23 21:16:26.987: INFO: Created: latency-svc-4sngs
Feb 23 21:16:27.020: INFO: Got endpoints: latency-svc-z2w47 [751.121399ms]
Feb 23 21:16:27.036: INFO: Created: latency-svc-p5wlp
Feb 23 21:16:27.068: INFO: Got endpoints: latency-svc-8s8mn [747.049502ms]
Feb 23 21:16:27.080: INFO: Created: latency-svc-j2z7t
Feb 23 21:16:27.117: INFO: Got endpoints: latency-svc-5kmxh [744.717165ms]
Feb 23 21:16:27.127: INFO: Created: latency-svc-j7cpn
Feb 23 21:16:27.169: INFO: Got endpoints: latency-svc-h7mf4 [749.030326ms]
Feb 23 21:16:27.179: INFO: Created: latency-svc-q5k5g
Feb 23 21:16:27.218: INFO: Got endpoints: latency-svc-vd58v [749.518365ms]
Feb 23 21:16:27.229: INFO: Created: latency-svc-f2f9f
Feb 23 21:16:27.267: INFO: Got endpoints: latency-svc-79fx7 [749.022089ms]
Feb 23 21:16:27.276: INFO: Created: latency-svc-l97wd
Feb 23 21:16:27.318: INFO: Got endpoints: latency-svc-p28nl [748.908977ms]
Feb 23 21:16:27.332: INFO: Created: latency-svc-xc9jk
Feb 23 21:16:27.368: INFO: Got endpoints: latency-svc-pbnq7 [751.044068ms]
Feb 23 21:16:27.382: INFO: Created: latency-svc-8cz82
Feb 23 21:16:27.418: INFO: Got endpoints: latency-svc-z5mtp [749.553405ms]
Feb 23 21:16:27.428: INFO: Created: latency-svc-bwc2p
Feb 23 21:16:27.468: INFO: Got endpoints: latency-svc-4q4qp [735.732307ms]
Feb 23 21:16:27.477: INFO: Created: latency-svc-phr7w
Feb 23 21:16:27.518: INFO: Got endpoints: latency-svc-jrw76 [747.287906ms]
Feb 23 21:16:27.568: INFO: Got endpoints: latency-svc-c4xgq [749.512028ms]
Feb 23 21:16:27.618: INFO: Got endpoints: latency-svc-9m5cn [748.933132ms]
Feb 23 21:16:27.671: INFO: Got endpoints: latency-svc-qp49t [753.018231ms]
Feb 23 21:16:27.720: INFO: Got endpoints: latency-svc-4sngs [752.637993ms]
Feb 23 21:16:27.769: INFO: Got endpoints: latency-svc-p5wlp [748.464406ms]
Feb 23 21:16:27.818: INFO: Got endpoints: latency-svc-j2z7t [749.367786ms]
Feb 23 21:16:27.868: INFO: Got endpoints: latency-svc-j7cpn [750.302375ms]
Feb 23 21:16:27.918: INFO: Got endpoints: latency-svc-q5k5g [748.932648ms]
Feb 23 21:16:27.970: INFO: Got endpoints: latency-svc-f2f9f [751.511301ms]
Feb 23 21:16:28.023: INFO: Got endpoints: latency-svc-l97wd [755.761397ms]
Feb 23 21:16:28.069: INFO: Got endpoints: latency-svc-xc9jk [751.387416ms]
Feb 23 21:16:28.119: INFO: Got endpoints: latency-svc-8cz82 [750.9289ms]
Feb 23 21:16:28.170: INFO: Got endpoints: latency-svc-bwc2p [751.094167ms]
Feb 23 21:16:28.220: INFO: Got endpoints: latency-svc-phr7w [750.960689ms]
Feb 23 21:16:28.220: INFO: Latencies: [46.112481ms 62.955656ms 73.202615ms 85.608276ms 103.168252ms 116.386339ms 136.383971ms 156.36063ms 168.609562ms 182.130538ms 191.247629ms 191.577518ms 192.350993ms 197.566718ms 199.628077ms 200.206874ms 204.711594ms 204.745885ms 206.400445ms 207.92597ms 208.056827ms 211.042235ms 212.238215ms 213.794186ms 213.95822ms 214.130821ms 215.45731ms 215.839911ms 215.920768ms 217.941069ms 219.048977ms 220.461354ms 221.264312ms 221.449301ms 221.951322ms 223.792847ms 224.657178ms 225.259759ms 226.915379ms 227.327456ms 233.652893ms 248.775ms 255.415336ms 284.251213ms 319.159179ms 361.671351ms 398.821913ms 432.221363ms 468.315549ms 510.610003ms 549.044211ms 590.298939ms 627.739417ms 670.207175ms 697.177239ms 735.732307ms 739.671505ms 740.710921ms 741.590504ms 743.384065ms 743.645591ms 744.380683ms 744.717165ms 744.834817ms 745.03122ms 745.186085ms 745.238117ms 745.571616ms 745.916243ms 746.020308ms 746.303521ms 746.426772ms 746.514389ms 746.589913ms 746.932109ms 746.984654ms 747.049502ms 747.287906ms 747.339043ms 747.405543ms 747.411919ms 747.492968ms 747.582321ms 747.816277ms 748.06791ms 748.100236ms 748.320439ms 748.461057ms 748.464406ms 748.562601ms 748.589472ms 748.609896ms 748.629368ms 748.656287ms 748.677731ms 748.713481ms 748.797036ms 748.908977ms 748.932648ms 748.933132ms 748.964739ms 749.022089ms 749.030326ms 749.066244ms 749.159344ms 749.159572ms 749.209452ms 749.280419ms 749.337778ms 749.345626ms 749.347875ms 749.364999ms 749.367786ms 749.394123ms 749.491504ms 749.496402ms 749.512028ms 749.518365ms 749.553405ms 749.610181ms 749.610367ms 749.654494ms 749.725943ms 749.789798ms 749.802035ms 749.802822ms 749.835656ms 749.871281ms 749.951312ms 749.987237ms 750.107206ms 750.129876ms 750.138225ms 750.146083ms 750.163304ms 750.214986ms 750.231167ms 750.259387ms 750.302028ms 750.302375ms 750.323744ms 750.33392ms 750.337401ms 750.388433ms 750.445774ms 750.512877ms 750.513058ms 750.573002ms 750.600648ms 750.68708ms 750.713865ms 750.781308ms 750.894856ms 750.9289ms 750.938093ms 750.960689ms 751.044068ms 751.094167ms 751.104933ms 751.120546ms 751.121399ms 751.295802ms 751.314773ms 751.322717ms 751.337709ms 751.387416ms 751.485895ms 751.511301ms 751.557652ms 751.55987ms 751.63598ms 751.662821ms 751.662853ms 751.805717ms 751.851104ms 751.869038ms 752.018611ms 752.119664ms 752.17482ms 752.452357ms 752.577793ms 752.637993ms 752.706041ms 752.706547ms 752.879929ms 753.018231ms 753.046761ms 753.175483ms 753.197724ms 753.325187ms 753.57094ms 753.792102ms 754.054308ms 754.414744ms 754.851224ms 754.899099ms 755.761397ms 755.828176ms 756.665541ms 760.006652ms]
Feb 23 21:16:28.220: INFO: 50 %ile: 748.964739ms
Feb 23 21:16:28.220: INFO: 90 %ile: 752.577793ms
Feb 23 21:16:28.221: INFO: 99 %ile: 756.665541ms
Feb 23 21:16:28.221: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:16:28.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8803" for this suite.

â€¢ [SLOW TEST:10.785 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":280,"completed":179,"skipped":2903,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:16:28.235: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7030 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7030;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7030 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7030;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7030.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7030.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7030.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7030.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7030.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7030.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7030.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7030.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7030.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7030.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7030.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7030.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7030.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 164.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.164_udp@PTR;check="$$(dig +tcp +noall +answer +search 164.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.164_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7030 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7030;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7030 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7030;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7030.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7030.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7030.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7030.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7030.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7030.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7030.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7030.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7030.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7030.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7030.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7030.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7030.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 164.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.164_udp@PTR;check="$$(dig +tcp +noall +answer +search 164.0.32.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.32.0.164_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 21:16:30.360: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.364: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.368: INFO: Unable to read wheezy_udp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.371: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.374: INFO: Unable to read wheezy_udp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.377: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.381: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.384: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.388: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.391: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.396: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.399: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.403: INFO: Unable to read 10.32.0.164_udp@PTR from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.406: INFO: Unable to read 10.32.0.164_tcp@PTR from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.410: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.412: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.417: INFO: Unable to read jessie_udp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.421: INFO: Unable to read jessie_tcp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.425: INFO: Unable to read jessie_udp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.429: INFO: Unable to read jessie_tcp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.432: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.436: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.441: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.445: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.449: INFO: Unable to read jessie_udp@PodARecord from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.452: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.456: INFO: Unable to read 10.32.0.164_udp@PTR from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.460: INFO: Unable to read 10.32.0.164_tcp@PTR from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:30.460: INFO: Lookups using dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7030 wheezy_tcp@dns-test-service.dns-7030 wheezy_udp@dns-test-service.dns-7030.svc wheezy_tcp@dns-test-service.dns-7030.svc wheezy_udp@_http._tcp.dns-test-service.dns-7030.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7030.svc wheezy_udp@_http._tcp.test-service-2.dns-7030.svc wheezy_tcp@_http._tcp.test-service-2.dns-7030.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.32.0.164_udp@PTR 10.32.0.164_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7030 jessie_tcp@dns-test-service.dns-7030 jessie_udp@dns-test-service.dns-7030.svc jessie_tcp@dns-test-service.dns-7030.svc jessie_udp@_http._tcp.dns-test-service.dns-7030.svc jessie_tcp@_http._tcp.dns-test-service.dns-7030.svc jessie_udp@_http._tcp.test-service-2.dns-7030.svc jessie_tcp@_http._tcp.test-service-2.dns-7030.svc jessie_udp@PodARecord jessie_tcp@PodARecord 10.32.0.164_udp@PTR 10.32.0.164_tcp@PTR]

Feb 23 21:16:35.467: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.472: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.479: INFO: Unable to read wheezy_udp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.482: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.492: INFO: Unable to read wheezy_udp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.523: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.540: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.547: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.559: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.565: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.571: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.574: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.587: INFO: Unable to read 10.32.0.164_udp@PTR from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.596: INFO: Unable to read 10.32.0.164_tcp@PTR from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.602: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.611: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.620: INFO: Unable to read jessie_udp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.626: INFO: Unable to read jessie_tcp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.634: INFO: Unable to read jessie_udp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.638: INFO: Unable to read jessie_tcp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.647: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.653: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.660: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.665: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.670: INFO: Unable to read jessie_udp@PodARecord from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.678: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.688: INFO: Unable to read 10.32.0.164_udp@PTR from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.697: INFO: Unable to read 10.32.0.164_tcp@PTR from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:35.697: INFO: Lookups using dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7030 wheezy_tcp@dns-test-service.dns-7030 wheezy_udp@dns-test-service.dns-7030.svc wheezy_tcp@dns-test-service.dns-7030.svc wheezy_udp@_http._tcp.dns-test-service.dns-7030.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7030.svc wheezy_udp@_http._tcp.test-service-2.dns-7030.svc wheezy_tcp@_http._tcp.test-service-2.dns-7030.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.32.0.164_udp@PTR 10.32.0.164_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7030 jessie_tcp@dns-test-service.dns-7030 jessie_udp@dns-test-service.dns-7030.svc jessie_tcp@dns-test-service.dns-7030.svc jessie_udp@_http._tcp.dns-test-service.dns-7030.svc jessie_tcp@_http._tcp.dns-test-service.dns-7030.svc jessie_udp@_http._tcp.test-service-2.dns-7030.svc jessie_tcp@_http._tcp.test-service-2.dns-7030.svc jessie_udp@PodARecord jessie_tcp@PodARecord 10.32.0.164_udp@PTR 10.32.0.164_tcp@PTR]

Feb 23 21:16:40.465: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.470: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.475: INFO: Unable to read wheezy_udp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.479: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.483: INFO: Unable to read wheezy_udp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.492: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.496: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.500: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.504: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.509: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.513: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.519: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.523: INFO: Unable to read 10.32.0.164_udp@PTR from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.528: INFO: Unable to read 10.32.0.164_tcp@PTR from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.532: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.537: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.542: INFO: Unable to read jessie_udp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.547: INFO: Unable to read jessie_tcp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.551: INFO: Unable to read jessie_udp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.555: INFO: Unable to read jessie_tcp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.559: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.564: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.567: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.572: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.576: INFO: Unable to read jessie_udp@PodARecord from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.582: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.586: INFO: Unable to read 10.32.0.164_udp@PTR from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.590: INFO: Unable to read 10.32.0.164_tcp@PTR from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:40.590: INFO: Lookups using dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7030 wheezy_tcp@dns-test-service.dns-7030 wheezy_udp@dns-test-service.dns-7030.svc wheezy_tcp@dns-test-service.dns-7030.svc wheezy_udp@_http._tcp.dns-test-service.dns-7030.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7030.svc wheezy_udp@_http._tcp.test-service-2.dns-7030.svc wheezy_tcp@_http._tcp.test-service-2.dns-7030.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.32.0.164_udp@PTR 10.32.0.164_tcp@PTR jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7030 jessie_tcp@dns-test-service.dns-7030 jessie_udp@dns-test-service.dns-7030.svc jessie_tcp@dns-test-service.dns-7030.svc jessie_udp@_http._tcp.dns-test-service.dns-7030.svc jessie_tcp@_http._tcp.dns-test-service.dns-7030.svc jessie_udp@_http._tcp.test-service-2.dns-7030.svc jessie_tcp@_http._tcp.test-service-2.dns-7030.svc jessie_udp@PodARecord jessie_tcp@PodARecord 10.32.0.164_udp@PTR 10.32.0.164_tcp@PTR]

Feb 23 21:16:45.467: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:45.471: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:45.475: INFO: Unable to read wheezy_udp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:45.478: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:45.481: INFO: Unable to read wheezy_udp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:45.484: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:45.516: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:45.520: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:45.524: INFO: Unable to read jessie_udp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:45.527: INFO: Unable to read jessie_tcp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:45.530: INFO: Unable to read jessie_udp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:45.534: INFO: Unable to read jessie_tcp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:45.558: INFO: Lookups using dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7030 wheezy_tcp@dns-test-service.dns-7030 wheezy_udp@dns-test-service.dns-7030.svc wheezy_tcp@dns-test-service.dns-7030.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7030 jessie_tcp@dns-test-service.dns-7030 jessie_udp@dns-test-service.dns-7030.svc jessie_tcp@dns-test-service.dns-7030.svc]

Feb 23 21:16:50.463: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:50.467: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:50.471: INFO: Unable to read wheezy_udp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:50.474: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:50.477: INFO: Unable to read wheezy_udp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:50.481: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:50.513: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:50.516: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:50.519: INFO: Unable to read jessie_udp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:50.522: INFO: Unable to read jessie_tcp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:50.525: INFO: Unable to read jessie_udp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:50.528: INFO: Unable to read jessie_tcp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:50.552: INFO: Lookups using dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7030 wheezy_tcp@dns-test-service.dns-7030 wheezy_udp@dns-test-service.dns-7030.svc wheezy_tcp@dns-test-service.dns-7030.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7030 jessie_tcp@dns-test-service.dns-7030 jessie_udp@dns-test-service.dns-7030.svc jessie_tcp@dns-test-service.dns-7030.svc]

Feb 23 21:16:55.464: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:55.468: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:55.472: INFO: Unable to read wheezy_udp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:55.475: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:55.478: INFO: Unable to read wheezy_udp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:55.481: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:55.514: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:55.522: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:55.526: INFO: Unable to read jessie_udp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:55.529: INFO: Unable to read jessie_tcp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:55.534: INFO: Unable to read jessie_udp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:55.536: INFO: Unable to read jessie_tcp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:16:55.562: INFO: Lookups using dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7030 wheezy_tcp@dns-test-service.dns-7030 wheezy_udp@dns-test-service.dns-7030.svc wheezy_tcp@dns-test-service.dns-7030.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7030 jessie_tcp@dns-test-service.dns-7030 jessie_udp@dns-test-service.dns-7030.svc jessie_tcp@dns-test-service.dns-7030.svc]

Feb 23 21:17:00.465: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:17:00.469: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:17:00.473: INFO: Unable to read wheezy_udp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:17:00.476: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:17:00.480: INFO: Unable to read wheezy_udp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:17:00.483: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:17:00.516: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:17:00.519: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:17:00.523: INFO: Unable to read jessie_udp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:17:00.525: INFO: Unable to read jessie_tcp@dns-test-service.dns-7030 from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:17:00.529: INFO: Unable to read jessie_udp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:17:00.532: INFO: Unable to read jessie_tcp@dns-test-service.dns-7030.svc from pod dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586: the server could not find the requested resource (get pods dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586)
Feb 23 21:17:00.560: INFO: Lookups using dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7030 wheezy_tcp@dns-test-service.dns-7030 wheezy_udp@dns-test-service.dns-7030.svc wheezy_tcp@dns-test-service.dns-7030.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7030 jessie_tcp@dns-test-service.dns-7030 jessie_udp@dns-test-service.dns-7030.svc jessie_tcp@dns-test-service.dns-7030.svc]

Feb 23 21:17:05.559: INFO: DNS probes using dns-7030/dns-test-4ef6f27c-ce4f-4c38-be26-1ce8886b9586 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:17:05.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7030" for this suite.

â€¢ [SLOW TEST:37.456 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":280,"completed":180,"skipped":2908,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:17:05.692: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-7262
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 23 21:17:05.731: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 23 21:17:27.836: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.200.131.148 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7262 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:17:27.836: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:17:28.913: INFO: Found all expected endpoints: [netserver-0]
Feb 23 21:17:28.918: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.200.5.61 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7262 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:17:28.918: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:17:29.986: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:17:29.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7262" for this suite.

â€¢ [SLOW TEST:24.306 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":181,"skipped":2939,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:17:29.998: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Feb 23 21:17:30.050: INFO: Waiting up to 5m0s for pod "client-containers-981c17cc-cb30-489d-9447-ebe3477f688e" in namespace "containers-7484" to be "success or failure"
Feb 23 21:17:30.067: INFO: Pod "client-containers-981c17cc-cb30-489d-9447-ebe3477f688e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.19865ms
Feb 23 21:17:32.070: INFO: Pod "client-containers-981c17cc-cb30-489d-9447-ebe3477f688e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020364867s
STEP: Saw pod success
Feb 23 21:17:32.070: INFO: Pod "client-containers-981c17cc-cb30-489d-9447-ebe3477f688e" satisfied condition "success or failure"
Feb 23 21:17:32.074: INFO: Trying to get logs from node worker01 pod client-containers-981c17cc-cb30-489d-9447-ebe3477f688e container test-container: <nil>
STEP: delete the pod
Feb 23 21:17:32.091: INFO: Waiting for pod client-containers-981c17cc-cb30-489d-9447-ebe3477f688e to disappear
Feb 23 21:17:32.095: INFO: Pod client-containers-981c17cc-cb30-489d-9447-ebe3477f688e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:17:32.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7484" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":280,"completed":182,"skipped":2950,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:17:32.104: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 21:17:32.139: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c0fa15bb-c0a0-49c1-bbae-a8554b4f09c1" in namespace "projected-9946" to be "success or failure"
Feb 23 21:17:32.144: INFO: Pod "downwardapi-volume-c0fa15bb-c0a0-49c1-bbae-a8554b4f09c1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.451012ms
Feb 23 21:17:34.149: INFO: Pod "downwardapi-volume-c0fa15bb-c0a0-49c1-bbae-a8554b4f09c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010507482s
STEP: Saw pod success
Feb 23 21:17:34.149: INFO: Pod "downwardapi-volume-c0fa15bb-c0a0-49c1-bbae-a8554b4f09c1" satisfied condition "success or failure"
Feb 23 21:17:34.156: INFO: Trying to get logs from node worker01 pod downwardapi-volume-c0fa15bb-c0a0-49c1-bbae-a8554b4f09c1 container client-container: <nil>
STEP: delete the pod
Feb 23 21:17:34.170: INFO: Waiting for pod downwardapi-volume-c0fa15bb-c0a0-49c1-bbae-a8554b4f09c1 to disappear
Feb 23 21:17:34.175: INFO: Pod downwardapi-volume-c0fa15bb-c0a0-49c1-bbae-a8554b4f09c1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:17:34.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9946" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":280,"completed":183,"skipped":2962,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:17:34.183: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 21:17:34.763: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 21:17:37.784: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:17:37.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7252" for this suite.
STEP: Destroying namespace "webhook-7252-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":280,"completed":184,"skipped":2978,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:17:37.929: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1692
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 23 21:17:37.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-86'
Feb 23 21:17:38.722: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 23 21:17:38.722: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Feb 23 21:17:38.735: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Feb 23 21:17:38.735: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 23 21:17:38.760: INFO: scanned /root for discovery docs: <nil>
Feb 23 21:17:38.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-86'
Feb 23 21:17:54.549: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 23 21:17:54.549: INFO: stdout: "Created e2e-test-httpd-rc-67b9efc4b98404ca408f76b8ec2661c4\nScaling up e2e-test-httpd-rc-67b9efc4b98404ca408f76b8ec2661c4 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-67b9efc4b98404ca408f76b8ec2661c4 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-67b9efc4b98404ca408f76b8ec2661c4 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Feb 23 21:17:54.549: INFO: stdout: "Created e2e-test-httpd-rc-67b9efc4b98404ca408f76b8ec2661c4\nScaling up e2e-test-httpd-rc-67b9efc4b98404ca408f76b8ec2661c4 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-67b9efc4b98404ca408f76b8ec2661c4 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-67b9efc4b98404ca408f76b8ec2661c4 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Feb 23 21:17:54.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-86'
Feb 23 21:17:54.609: INFO: stderr: ""
Feb 23 21:17:54.609: INFO: stdout: "e2e-test-httpd-rc-67b9efc4b98404ca408f76b8ec2661c4-xp8lq "
Feb 23 21:17:54.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods e2e-test-httpd-rc-67b9efc4b98404ca408f76b8ec2661c4-xp8lq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-86'
Feb 23 21:17:54.665: INFO: stderr: ""
Feb 23 21:17:54.665: INFO: stdout: "true"
Feb 23 21:17:54.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods e2e-test-httpd-rc-67b9efc4b98404ca408f76b8ec2661c4-xp8lq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-86'
Feb 23 21:17:54.719: INFO: stderr: ""
Feb 23 21:17:54.719: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Feb 23 21:17:54.719: INFO: e2e-test-httpd-rc-67b9efc4b98404ca408f76b8ec2661c4-xp8lq is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1698
Feb 23 21:17:54.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete rc e2e-test-httpd-rc --namespace=kubectl-86'
Feb 23 21:17:54.784: INFO: stderr: ""
Feb 23 21:17:54.784: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:17:54.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-86" for this suite.

â€¢ [SLOW TEST:16.867 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":280,"completed":185,"skipped":2995,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:17:54.797: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:18:19.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2910" for this suite.

â€¢ [SLOW TEST:24.308 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":280,"completed":186,"skipped":2999,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:18:19.106: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:18:21.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1301" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":280,"completed":187,"skipped":3017,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:18:21.167: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:18:43.223: INFO: Container started at 2020-02-23 21:18:22 +0000 UTC, pod became ready at 2020-02-23 21:18:41 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:18:43.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5359" for this suite.

â€¢ [SLOW TEST:22.064 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":280,"completed":188,"skipped":3021,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:18:43.232: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1788
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 23 21:18:43.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3430'
Feb 23 21:18:43.348: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 23 21:18:43.349: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1793
Feb 23 21:18:43.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete jobs e2e-test-httpd-job --namespace=kubectl-3430'
Feb 23 21:18:43.440: INFO: stderr: ""
Feb 23 21:18:43.440: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:18:43.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3430" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":280,"completed":189,"skipped":3037,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:18:43.450: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 23 21:18:43.513: INFO: Number of nodes with available pods: 0
Feb 23 21:18:43.513: INFO: Node worker00 is running more than one daemon pod
Feb 23 21:18:44.522: INFO: Number of nodes with available pods: 0
Feb 23 21:18:44.523: INFO: Node worker00 is running more than one daemon pod
Feb 23 21:18:45.519: INFO: Number of nodes with available pods: 2
Feb 23 21:18:45.519: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 23 21:18:45.539: INFO: Number of nodes with available pods: 1
Feb 23 21:18:45.539: INFO: Node worker00 is running more than one daemon pod
Feb 23 21:18:46.554: INFO: Number of nodes with available pods: 1
Feb 23 21:18:46.554: INFO: Node worker00 is running more than one daemon pod
Feb 23 21:18:47.555: INFO: Number of nodes with available pods: 1
Feb 23 21:18:47.555: INFO: Node worker00 is running more than one daemon pod
Feb 23 21:18:48.548: INFO: Number of nodes with available pods: 1
Feb 23 21:18:48.548: INFO: Node worker00 is running more than one daemon pod
Feb 23 21:18:49.550: INFO: Number of nodes with available pods: 1
Feb 23 21:18:49.550: INFO: Node worker00 is running more than one daemon pod
Feb 23 21:18:50.550: INFO: Number of nodes with available pods: 1
Feb 23 21:18:50.550: INFO: Node worker00 is running more than one daemon pod
Feb 23 21:18:51.549: INFO: Number of nodes with available pods: 1
Feb 23 21:18:51.549: INFO: Node worker00 is running more than one daemon pod
Feb 23 21:18:52.566: INFO: Number of nodes with available pods: 1
Feb 23 21:18:52.566: INFO: Node worker00 is running more than one daemon pod
Feb 23 21:18:53.547: INFO: Number of nodes with available pods: 2
Feb 23 21:18:53.547: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7236, will wait for the garbage collector to delete the pods
Feb 23 21:18:53.615: INFO: Deleting DaemonSet.extensions daemon-set took: 7.775169ms
Feb 23 21:18:54.517: INFO: Terminating DaemonSet.extensions daemon-set pods took: 901.168796ms
Feb 23 21:19:00.822: INFO: Number of nodes with available pods: 0
Feb 23 21:19:00.822: INFO: Number of running nodes: 0, number of available pods: 0
Feb 23 21:19:00.826: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7236/daemonsets","resourceVersion":"23685"},"items":null}

Feb 23 21:19:00.830: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7236/pods","resourceVersion":"23685"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:19:00.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7236" for this suite.

â€¢ [SLOW TEST:17.399 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":280,"completed":190,"skipped":3043,"failed":0}
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:19:00.849: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-6210/configmap-test-14eff7fe-848e-4c6a-9446-9179506ca831
STEP: Creating a pod to test consume configMaps
Feb 23 21:19:00.888: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce3df58a-6888-48a8-82e5-a2a7d3337934" in namespace "configmap-6210" to be "success or failure"
Feb 23 21:19:00.902: INFO: Pod "pod-configmaps-ce3df58a-6888-48a8-82e5-a2a7d3337934": Phase="Pending", Reason="", readiness=false. Elapsed: 13.75175ms
Feb 23 21:19:02.905: INFO: Pod "pod-configmaps-ce3df58a-6888-48a8-82e5-a2a7d3337934": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017021081s
STEP: Saw pod success
Feb 23 21:19:02.906: INFO: Pod "pod-configmaps-ce3df58a-6888-48a8-82e5-a2a7d3337934" satisfied condition "success or failure"
Feb 23 21:19:02.908: INFO: Trying to get logs from node worker01 pod pod-configmaps-ce3df58a-6888-48a8-82e5-a2a7d3337934 container env-test: <nil>
STEP: delete the pod
Feb 23 21:19:02.923: INFO: Waiting for pod pod-configmaps-ce3df58a-6888-48a8-82e5-a2a7d3337934 to disappear
Feb 23 21:19:02.926: INFO: Pod pod-configmaps-ce3df58a-6888-48a8-82e5-a2a7d3337934 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:19:02.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6210" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":191,"skipped":3049,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:19:02.934: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:19:19.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7217" for this suite.

â€¢ [SLOW TEST:16.099 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":280,"completed":192,"skipped":3059,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:19:19.034: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:19:21.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5886" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":280,"completed":193,"skipped":3079,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:19:21.145: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-3ecba4bc-3a45-46bf-8209-0f96ef756311
STEP: Creating a pod to test consume secrets
Feb 23 21:19:21.185: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5639b595-9b66-40d0-84c5-75c8d5d35dd9" in namespace "projected-6030" to be "success or failure"
Feb 23 21:19:21.193: INFO: Pod "pod-projected-secrets-5639b595-9b66-40d0-84c5-75c8d5d35dd9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.891968ms
Feb 23 21:19:23.200: INFO: Pod "pod-projected-secrets-5639b595-9b66-40d0-84c5-75c8d5d35dd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014816613s
STEP: Saw pod success
Feb 23 21:19:23.200: INFO: Pod "pod-projected-secrets-5639b595-9b66-40d0-84c5-75c8d5d35dd9" satisfied condition "success or failure"
Feb 23 21:19:23.205: INFO: Trying to get logs from node worker01 pod pod-projected-secrets-5639b595-9b66-40d0-84c5-75c8d5d35dd9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 23 21:19:23.235: INFO: Waiting for pod pod-projected-secrets-5639b595-9b66-40d0-84c5-75c8d5d35dd9 to disappear
Feb 23 21:19:23.240: INFO: Pod pod-projected-secrets-5639b595-9b66-40d0-84c5-75c8d5d35dd9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:19:23.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6030" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":194,"skipped":3088,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:19:23.248: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-1d9fcaff-dcfe-458a-8954-10b110944d01
STEP: Creating configMap with name cm-test-opt-upd-c511c7d7-a251-4de7-bf06-c9687a664aa0
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1d9fcaff-dcfe-458a-8954-10b110944d01
STEP: Updating configmap cm-test-opt-upd-c511c7d7-a251-4de7-bf06-c9687a664aa0
STEP: Creating configMap with name cm-test-opt-create-339ed1cd-0686-4dcc-9a00-7c24ceac9415
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:20:57.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8112" for this suite.

â€¢ [SLOW TEST:94.625 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":195,"skipped":3094,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:20:57.874: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 21:20:57.910: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a878a01c-d298-46f6-a338-b1cd08c7487b" in namespace "downward-api-8468" to be "success or failure"
Feb 23 21:20:57.915: INFO: Pod "downwardapi-volume-a878a01c-d298-46f6-a338-b1cd08c7487b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.499937ms
Feb 23 21:20:59.920: INFO: Pod "downwardapi-volume-a878a01c-d298-46f6-a338-b1cd08c7487b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010698811s
STEP: Saw pod success
Feb 23 21:20:59.920: INFO: Pod "downwardapi-volume-a878a01c-d298-46f6-a338-b1cd08c7487b" satisfied condition "success or failure"
Feb 23 21:20:59.923: INFO: Trying to get logs from node worker01 pod downwardapi-volume-a878a01c-d298-46f6-a338-b1cd08c7487b container client-container: <nil>
STEP: delete the pod
Feb 23 21:20:59.945: INFO: Waiting for pod downwardapi-volume-a878a01c-d298-46f6-a338-b1cd08c7487b to disappear
Feb 23 21:20:59.949: INFO: Pod downwardapi-volume-a878a01c-d298-46f6-a338-b1cd08c7487b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:20:59.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8468" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":196,"skipped":3129,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:20:59.961: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-hbnx
STEP: Creating a pod to test atomic-volume-subpath
Feb 23 21:21:00.006: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hbnx" in namespace "subpath-9893" to be "success or failure"
Feb 23 21:21:00.009: INFO: Pod "pod-subpath-test-configmap-hbnx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.503973ms
Feb 23 21:21:02.016: INFO: Pod "pod-subpath-test-configmap-hbnx": Phase="Running", Reason="", readiness=true. Elapsed: 2.009080122s
Feb 23 21:21:04.022: INFO: Pod "pod-subpath-test-configmap-hbnx": Phase="Running", Reason="", readiness=true. Elapsed: 4.015745062s
Feb 23 21:21:06.028: INFO: Pod "pod-subpath-test-configmap-hbnx": Phase="Running", Reason="", readiness=true. Elapsed: 6.021663557s
Feb 23 21:21:08.034: INFO: Pod "pod-subpath-test-configmap-hbnx": Phase="Running", Reason="", readiness=true. Elapsed: 8.027514769s
Feb 23 21:21:10.040: INFO: Pod "pod-subpath-test-configmap-hbnx": Phase="Running", Reason="", readiness=true. Elapsed: 10.03369999s
Feb 23 21:21:12.049: INFO: Pod "pod-subpath-test-configmap-hbnx": Phase="Running", Reason="", readiness=true. Elapsed: 12.042307775s
Feb 23 21:21:14.053: INFO: Pod "pod-subpath-test-configmap-hbnx": Phase="Running", Reason="", readiness=true. Elapsed: 14.0462475s
Feb 23 21:21:16.056: INFO: Pod "pod-subpath-test-configmap-hbnx": Phase="Running", Reason="", readiness=true. Elapsed: 16.049725694s
Feb 23 21:21:18.061: INFO: Pod "pod-subpath-test-configmap-hbnx": Phase="Running", Reason="", readiness=true. Elapsed: 18.054068436s
Feb 23 21:21:20.064: INFO: Pod "pod-subpath-test-configmap-hbnx": Phase="Running", Reason="", readiness=true. Elapsed: 20.056986521s
Feb 23 21:21:22.067: INFO: Pod "pod-subpath-test-configmap-hbnx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.060506056s
STEP: Saw pod success
Feb 23 21:21:22.067: INFO: Pod "pod-subpath-test-configmap-hbnx" satisfied condition "success or failure"
Feb 23 21:21:22.071: INFO: Trying to get logs from node worker01 pod pod-subpath-test-configmap-hbnx container test-container-subpath-configmap-hbnx: <nil>
STEP: delete the pod
Feb 23 21:21:22.087: INFO: Waiting for pod pod-subpath-test-configmap-hbnx to disappear
Feb 23 21:21:22.090: INFO: Pod pod-subpath-test-configmap-hbnx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hbnx
Feb 23 21:21:22.090: INFO: Deleting pod "pod-subpath-test-configmap-hbnx" in namespace "subpath-9893"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:21:22.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9893" for this suite.

â€¢ [SLOW TEST:22.143 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":280,"completed":197,"skipped":3141,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:21:22.104: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:21:22.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5869" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":280,"completed":198,"skipped":3155,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:21:22.169: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 23 21:21:24.738: INFO: Successfully updated pod "pod-update-activedeadlineseconds-dc35af16-940f-4792-846c-9a919fa8eb02"
Feb 23 21:21:24.738: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-dc35af16-940f-4792-846c-9a919fa8eb02" in namespace "pods-5873" to be "terminated due to deadline exceeded"
Feb 23 21:21:24.741: INFO: Pod "pod-update-activedeadlineseconds-dc35af16-940f-4792-846c-9a919fa8eb02": Phase="Running", Reason="", readiness=true. Elapsed: 2.854961ms
Feb 23 21:21:26.746: INFO: Pod "pod-update-activedeadlineseconds-dc35af16-940f-4792-846c-9a919fa8eb02": Phase="Running", Reason="", readiness=true. Elapsed: 2.008219179s
Feb 23 21:21:28.750: INFO: Pod "pod-update-activedeadlineseconds-dc35af16-940f-4792-846c-9a919fa8eb02": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.012147177s
Feb 23 21:21:28.750: INFO: Pod "pod-update-activedeadlineseconds-dc35af16-940f-4792-846c-9a919fa8eb02" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:21:28.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5873" for this suite.

â€¢ [SLOW TEST:6.591 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":280,"completed":199,"skipped":3184,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:21:28.761: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 21:21:28.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19b3d7c3-79f9-4dd5-bf04-1aa8f52e4b2f" in namespace "projected-5412" to be "success or failure"
Feb 23 21:21:28.807: INFO: Pod "downwardapi-volume-19b3d7c3-79f9-4dd5-bf04-1aa8f52e4b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.560601ms
Feb 23 21:21:30.810: INFO: Pod "downwardapi-volume-19b3d7c3-79f9-4dd5-bf04-1aa8f52e4b2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010340532s
STEP: Saw pod success
Feb 23 21:21:30.810: INFO: Pod "downwardapi-volume-19b3d7c3-79f9-4dd5-bf04-1aa8f52e4b2f" satisfied condition "success or failure"
Feb 23 21:21:30.813: INFO: Trying to get logs from node worker01 pod downwardapi-volume-19b3d7c3-79f9-4dd5-bf04-1aa8f52e4b2f container client-container: <nil>
STEP: delete the pod
Feb 23 21:21:30.830: INFO: Waiting for pod downwardapi-volume-19b3d7c3-79f9-4dd5-bf04-1aa8f52e4b2f to disappear
Feb 23 21:21:30.836: INFO: Pod downwardapi-volume-19b3d7c3-79f9-4dd5-bf04-1aa8f52e4b2f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:21:30.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5412" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":200,"skipped":3189,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:21:30.844: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-2473
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-2473
Feb 23 21:21:30.892: INFO: Found 0 stateful pods, waiting for 1
Feb 23 21:21:40.907: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 23 21:21:40.923: INFO: Deleting all statefulset in ns statefulset-2473
Feb 23 21:21:40.928: INFO: Scaling statefulset ss to 0
Feb 23 21:22:00.962: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 21:22:00.968: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:22:00.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2473" for this suite.

â€¢ [SLOW TEST:30.152 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":280,"completed":201,"skipped":3210,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:22:00.999: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-4e522cad-2a95-4370-a174-287464930d6f
STEP: Creating a pod to test consume configMaps
Feb 23 21:22:01.052: INFO: Waiting up to 5m0s for pod "pod-configmaps-85907234-48ef-47b3-a91c-a402811ddd96" in namespace "configmap-4943" to be "success or failure"
Feb 23 21:22:01.056: INFO: Pod "pod-configmaps-85907234-48ef-47b3-a91c-a402811ddd96": Phase="Pending", Reason="", readiness=false. Elapsed: 3.90444ms
Feb 23 21:22:03.059: INFO: Pod "pod-configmaps-85907234-48ef-47b3-a91c-a402811ddd96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007419623s
STEP: Saw pod success
Feb 23 21:22:03.059: INFO: Pod "pod-configmaps-85907234-48ef-47b3-a91c-a402811ddd96" satisfied condition "success or failure"
Feb 23 21:22:03.062: INFO: Trying to get logs from node worker01 pod pod-configmaps-85907234-48ef-47b3-a91c-a402811ddd96 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 21:22:03.086: INFO: Waiting for pod pod-configmaps-85907234-48ef-47b3-a91c-a402811ddd96 to disappear
Feb 23 21:22:03.089: INFO: Pod pod-configmaps-85907234-48ef-47b3-a91c-a402811ddd96 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:22:03.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4943" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":202,"skipped":3245,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:22:03.100: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Feb 23 21:22:03.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=kubectl-6070 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 23 21:22:05.061: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 23 21:22:05.061: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:22:07.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6070" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":280,"completed":203,"skipped":3305,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:22:07.077: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 21:22:07.116: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f7b7c95-5418-4332-9a9a-6a921e0a1c19" in namespace "projected-228" to be "success or failure"
Feb 23 21:22:07.122: INFO: Pod "downwardapi-volume-3f7b7c95-5418-4332-9a9a-6a921e0a1c19": Phase="Pending", Reason="", readiness=false. Elapsed: 5.835312ms
Feb 23 21:22:09.126: INFO: Pod "downwardapi-volume-3f7b7c95-5418-4332-9a9a-6a921e0a1c19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00920072s
STEP: Saw pod success
Feb 23 21:22:09.126: INFO: Pod "downwardapi-volume-3f7b7c95-5418-4332-9a9a-6a921e0a1c19" satisfied condition "success or failure"
Feb 23 21:22:09.129: INFO: Trying to get logs from node worker01 pod downwardapi-volume-3f7b7c95-5418-4332-9a9a-6a921e0a1c19 container client-container: <nil>
STEP: delete the pod
Feb 23 21:22:09.146: INFO: Waiting for pod downwardapi-volume-3f7b7c95-5418-4332-9a9a-6a921e0a1c19 to disappear
Feb 23 21:22:09.148: INFO: Pod downwardapi-volume-3f7b7c95-5418-4332-9a9a-6a921e0a1c19 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:22:09.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-228" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":204,"skipped":3306,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:22:09.155: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2111.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2111.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2111.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2111.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2111.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2111.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 21:22:11.246: INFO: DNS probes using dns-2111/dns-test-1d45d06e-eaa6-40ae-833b-448c882ec44c succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:22:11.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2111" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":280,"completed":205,"skipped":3308,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:22:11.306: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-09197ec2-d74b-4b76-a2db-21471aff221a
STEP: Creating a pod to test consume configMaps
Feb 23 21:22:11.356: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ae4d70f6-28ac-4b7a-bb00-a9891326604f" in namespace "projected-5161" to be "success or failure"
Feb 23 21:22:11.359: INFO: Pod "pod-projected-configmaps-ae4d70f6-28ac-4b7a-bb00-a9891326604f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.535771ms
Feb 23 21:22:13.362: INFO: Pod "pod-projected-configmaps-ae4d70f6-28ac-4b7a-bb00-a9891326604f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005828217s
STEP: Saw pod success
Feb 23 21:22:13.362: INFO: Pod "pod-projected-configmaps-ae4d70f6-28ac-4b7a-bb00-a9891326604f" satisfied condition "success or failure"
Feb 23 21:22:13.366: INFO: Trying to get logs from node worker01 pod pod-projected-configmaps-ae4d70f6-28ac-4b7a-bb00-a9891326604f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 21:22:13.399: INFO: Waiting for pod pod-projected-configmaps-ae4d70f6-28ac-4b7a-bb00-a9891326604f to disappear
Feb 23 21:22:13.402: INFO: Pod pod-projected-configmaps-ae4d70f6-28ac-4b7a-bb00-a9891326604f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:22:13.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5161" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":206,"skipped":3334,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:22:13.414: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:22:13.446: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:22:19.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-146" for this suite.

â€¢ [SLOW TEST:6.167 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":280,"completed":207,"skipped":3369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:22:19.584: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:22:19.636: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 23 21:22:19.649: INFO: Number of nodes with available pods: 0
Feb 23 21:22:19.649: INFO: Node worker00 is running more than one daemon pod
Feb 23 21:22:20.660: INFO: Number of nodes with available pods: 0
Feb 23 21:22:20.660: INFO: Node worker00 is running more than one daemon pod
Feb 23 21:22:21.657: INFO: Number of nodes with available pods: 2
Feb 23 21:22:21.657: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 23 21:22:21.676: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:21.676: INFO: Wrong image for pod: daemon-set-nj6np. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:22.684: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:22.685: INFO: Wrong image for pod: daemon-set-nj6np. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:23.686: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:23.686: INFO: Wrong image for pod: daemon-set-nj6np. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:24.682: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:24.682: INFO: Wrong image for pod: daemon-set-nj6np. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:24.682: INFO: Pod daemon-set-nj6np is not available
Feb 23 21:22:25.683: INFO: Pod daemon-set-dpv8b is not available
Feb 23 21:22:25.683: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:26.683: INFO: Pod daemon-set-dpv8b is not available
Feb 23 21:22:26.683: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:27.683: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:28.683: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:28.683: INFO: Pod daemon-set-nb8fx is not available
Feb 23 21:22:29.684: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:29.684: INFO: Pod daemon-set-nb8fx is not available
Feb 23 21:22:30.684: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:30.684: INFO: Pod daemon-set-nb8fx is not available
Feb 23 21:22:31.686: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:31.686: INFO: Pod daemon-set-nb8fx is not available
Feb 23 21:22:32.685: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:32.685: INFO: Pod daemon-set-nb8fx is not available
Feb 23 21:22:33.686: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:33.687: INFO: Pod daemon-set-nb8fx is not available
Feb 23 21:22:34.685: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:34.685: INFO: Pod daemon-set-nb8fx is not available
Feb 23 21:22:35.686: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:35.686: INFO: Pod daemon-set-nb8fx is not available
Feb 23 21:22:36.683: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:36.683: INFO: Pod daemon-set-nb8fx is not available
Feb 23 21:22:37.689: INFO: Wrong image for pod: daemon-set-nb8fx. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 23 21:22:37.689: INFO: Pod daemon-set-nb8fx is not available
Feb 23 21:22:38.685: INFO: Pod daemon-set-nj87n is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 23 21:22:38.694: INFO: Number of nodes with available pods: 1
Feb 23 21:22:38.694: INFO: Node worker01 is running more than one daemon pod
Feb 23 21:22:39.706: INFO: Number of nodes with available pods: 2
Feb 23 21:22:39.706: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2997, will wait for the garbage collector to delete the pods
Feb 23 21:22:39.782: INFO: Deleting DaemonSet.extensions daemon-set took: 7.678179ms
Feb 23 21:22:40.682: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.202231ms
Feb 23 21:22:50.792: INFO: Number of nodes with available pods: 0
Feb 23 21:22:50.792: INFO: Number of running nodes: 0, number of available pods: 0
Feb 23 21:22:50.798: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2997/daemonsets","resourceVersion":"25116"},"items":null}

Feb 23 21:22:50.803: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2997/pods","resourceVersion":"25116"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:22:50.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2997" for this suite.

â€¢ [SLOW TEST:31.239 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":280,"completed":208,"skipped":3424,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:22:50.823: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 23 21:22:51.172: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 21:22:54.190: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:22:54.197: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:23:00.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7664" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

â€¢ [SLOW TEST:9.694 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":280,"completed":209,"skipped":3427,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:23:00.517: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 23 21:23:00.616: INFO: Waiting up to 5m0s for pod "downward-api-b4517cf8-2695-4b2c-a662-61bb1e8e9f23" in namespace "downward-api-4674" to be "success or failure"
Feb 23 21:23:00.620: INFO: Pod "downward-api-b4517cf8-2695-4b2c-a662-61bb1e8e9f23": Phase="Pending", Reason="", readiness=false. Elapsed: 3.962613ms
Feb 23 21:23:02.627: INFO: Pod "downward-api-b4517cf8-2695-4b2c-a662-61bb1e8e9f23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011491711s
STEP: Saw pod success
Feb 23 21:23:02.628: INFO: Pod "downward-api-b4517cf8-2695-4b2c-a662-61bb1e8e9f23" satisfied condition "success or failure"
Feb 23 21:23:02.632: INFO: Trying to get logs from node worker01 pod downward-api-b4517cf8-2695-4b2c-a662-61bb1e8e9f23 container dapi-container: <nil>
STEP: delete the pod
Feb 23 21:23:02.653: INFO: Waiting for pod downward-api-b4517cf8-2695-4b2c-a662-61bb1e8e9f23 to disappear
Feb 23 21:23:02.657: INFO: Pod downward-api-b4517cf8-2695-4b2c-a662-61bb1e8e9f23 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:23:02.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4674" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":280,"completed":210,"skipped":3437,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:23:02.670: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8972
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8972
STEP: creating replication controller externalsvc in namespace services-8972
I0223 21:23:02.755418      23 runners.go:189] Created replication controller with name: externalsvc, namespace: services-8972, replica count: 2
I0223 21:23:05.808948      23 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Feb 23 21:23:05.827: INFO: Creating new exec pod
Feb 23 21:23:07.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=services-8972 execpodg9mnq -- /bin/sh -x -c nslookup clusterip-service'
Feb 23 21:23:08.047: INFO: stderr: "+ nslookup clusterip-service\n"
Feb 23 21:23:08.047: INFO: stdout: "Server:\t\t10.32.0.10\nAddress:\t10.32.0.10#53\n\nclusterip-service.services-8972.svc.cluster.local\tcanonical name = externalsvc.services-8972.svc.cluster.local.\nName:\texternalsvc.services-8972.svc.cluster.local\nAddress: 10.32.0.160\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8972, will wait for the garbage collector to delete the pods
Feb 23 21:23:08.109: INFO: Deleting ReplicationController externalsvc took: 8.139461ms
Feb 23 21:23:08.210: INFO: Terminating ReplicationController externalsvc pods took: 100.427359ms
Feb 23 21:23:12.340: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:23:12.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8972" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:9.697 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":280,"completed":211,"skipped":3479,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:23:12.368: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:23:12.413: INFO: (0) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 12.057655ms)
Feb 23 21:23:12.416: INFO: (1) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.800416ms)
Feb 23 21:23:12.419: INFO: (2) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.173569ms)
Feb 23 21:23:12.421: INFO: (3) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.369382ms)
Feb 23 21:23:12.424: INFO: (4) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.041847ms)
Feb 23 21:23:12.427: INFO: (5) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.416355ms)
Feb 23 21:23:12.430: INFO: (6) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.205902ms)
Feb 23 21:23:12.434: INFO: (7) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.634473ms)
Feb 23 21:23:12.438: INFO: (8) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.889152ms)
Feb 23 21:23:12.441: INFO: (9) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.228679ms)
Feb 23 21:23:12.445: INFO: (10) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.869456ms)
Feb 23 21:23:12.448: INFO: (11) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.130314ms)
Feb 23 21:23:12.452: INFO: (12) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.993231ms)
Feb 23 21:23:12.455: INFO: (13) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.442564ms)
Feb 23 21:23:12.459: INFO: (14) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.884196ms)
Feb 23 21:23:12.463: INFO: (15) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.405144ms)
Feb 23 21:23:12.467: INFO: (16) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.028728ms)
Feb 23 21:23:12.470: INFO: (17) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.167677ms)
Feb 23 21:23:12.475: INFO: (18) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.408033ms)
Feb 23 21:23:12.478: INFO: (19) /api/v1/nodes/worker00/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.979155ms)
[AfterEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:23:12.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5023" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":280,"completed":212,"skipped":3496,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:23:12.487: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:23:17.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2667" for this suite.

â€¢ [SLOW TEST:5.260 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":280,"completed":213,"skipped":3514,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:23:17.747: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-3debfc04-3c16-4f94-9de2-9dcdddb3b758 in namespace container-probe-3556
Feb 23 21:23:19.821: INFO: Started pod test-webserver-3debfc04-3c16-4f94-9de2-9dcdddb3b758 in namespace container-probe-3556
STEP: checking the pod's current state and verifying that restartCount is present
Feb 23 21:23:19.825: INFO: Initial restart count of pod test-webserver-3debfc04-3c16-4f94-9de2-9dcdddb3b758 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:27:20.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3556" for this suite.

â€¢ [SLOW TEST:242.858 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":214,"skipped":3524,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:27:20.611: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 23 21:27:23.204: INFO: Successfully updated pod "annotationupdate3be97699-c325-4efb-bcd9-331b12c284f7"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:27:27.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2920" for this suite.

â€¢ [SLOW TEST:6.640 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":215,"skipped":3549,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:27:27.251: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 23 21:27:27.285: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:27:30.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9752" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":280,"completed":216,"skipped":3586,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:27:30.069: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1861
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 23 21:27:30.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5029'
Feb 23 21:27:30.187: INFO: stderr: ""
Feb 23 21:27:30.188: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1866
Feb 23 21:27:30.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete pods e2e-test-httpd-pod --namespace=kubectl-5029'
Feb 23 21:27:37.742: INFO: stderr: ""
Feb 23 21:27:37.742: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:27:37.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5029" for this suite.

â€¢ [SLOW TEST:7.683 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1857
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":280,"completed":217,"skipped":3614,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:27:37.753: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
STEP: creating the pod
Feb 23 21:27:37.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 create -f - --namespace=kubectl-2730'
Feb 23 21:27:40.199: INFO: stderr: ""
Feb 23 21:27:40.199: INFO: stdout: "pod/pause created\n"
Feb 23 21:27:40.199: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 23 21:27:40.199: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2730" to be "running and ready"
Feb 23 21:27:40.205: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.269828ms
Feb 23 21:27:42.210: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.011576829s
Feb 23 21:27:42.211: INFO: Pod "pause" satisfied condition "running and ready"
Feb 23 21:27:42.211: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 23 21:27:42.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 label pods pause testing-label=testing-label-value --namespace=kubectl-2730'
Feb 23 21:27:42.276: INFO: stderr: ""
Feb 23 21:27:42.276: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 23 21:27:42.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pod pause -L testing-label --namespace=kubectl-2730'
Feb 23 21:27:42.338: INFO: stderr: ""
Feb 23 21:27:42.338: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 23 21:27:42.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 label pods pause testing-label- --namespace=kubectl-2730'
Feb 23 21:27:42.402: INFO: stderr: ""
Feb 23 21:27:42.402: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 23 21:27:42.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pod pause -L testing-label --namespace=kubectl-2730'
Feb 23 21:27:42.451: INFO: stderr: ""
Feb 23 21:27:42.451: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
STEP: using delete to clean up resources
Feb 23 21:27:42.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete --grace-period=0 --force -f - --namespace=kubectl-2730'
Feb 23 21:27:42.527: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 21:27:42.527: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 23 21:27:42.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get rc,svc -l name=pause --no-headers --namespace=kubectl-2730'
Feb 23 21:27:42.599: INFO: stderr: "No resources found in kubectl-2730 namespace.\n"
Feb 23 21:27:42.599: INFO: stdout: ""
Feb 23 21:27:42.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -l name=pause --namespace=kubectl-2730 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 21:27:42.650: INFO: stderr: ""
Feb 23 21:27:42.650: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:27:42.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2730" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":280,"completed":218,"skipped":3629,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:27:42.657: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:27:42.688: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 23 21:27:42.703: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 23 21:27:47.707: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 23 21:27:47.707: INFO: Creating deployment "test-rolling-update-deployment"
Feb 23 21:27:47.715: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 23 21:27:47.723: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 23 21:27:49.729: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 23 21:27:49.732: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 23 21:27:49.740: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6356 /apis/apps/v1/namespaces/deployment-6356/deployments/test-rolling-update-deployment 7e3356d3-4110-48aa-9f30-0100cfba91c2 26613 1 2020-02-23 21:27:47 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0042a1018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-23 21:27:47 +0000 UTC,LastTransitionTime:2020-02-23 21:27:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-02-23 21:27:49 +0000 UTC,LastTransitionTime:2020-02-23 21:27:47 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 23 21:27:49.746: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-6356 /apis/apps/v1/namespaces/deployment-6356/replicasets/test-rolling-update-deployment-67cf4f6444 f74333ab-2fc3-4077-a4e5-3c3e56357104 26602 1 2020-02-23 21:27:47 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 7e3356d3-4110-48aa-9f30-0100cfba91c2 0xc0042a14d7 0xc0042a14d8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0042a1548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 23 21:27:49.746: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 23 21:27:49.746: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6356 /apis/apps/v1/namespaces/deployment-6356/replicasets/test-rolling-update-controller 684c29b2-b37b-45a3-9e23-a799c2e45c28 26612 2 2020-02-23 21:27:42 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 7e3356d3-4110-48aa-9f30-0100cfba91c2 0xc0042a1407 0xc0042a1408}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0042a1468 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 23 21:27:49.749: INFO: Pod "test-rolling-update-deployment-67cf4f6444-nw28q" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-nw28q test-rolling-update-deployment-67cf4f6444- deployment-6356 /api/v1/namespaces/deployment-6356/pods/test-rolling-update-deployment-67cf4f6444-nw28q 2ed2440f-a67c-4804-9cb1-c8b10ca8dbaf 26601 0 2020-02-23 21:27:47 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[cni.projectcalico.org/podIP:10.200.5.47/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 f74333ab-2fc3-4077-a4e5-3c3e56357104 0xc0042a19c7 0xc0042a19c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gk7fw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gk7fw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gk7fw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker01,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 21:27:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 21:27:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 21:27:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-23 21:27:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.180.101,PodIP:10.200.5.47,StartTime:2020-02-23 21:27:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-23 21:27:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:containerd://20bfdc8dd3307e7d97fbd4bc07919e05e01d143836e3436db0b8d8b2badd12d0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.200.5.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:27:49.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6356" for this suite.

â€¢ [SLOW TEST:7.106 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":219,"skipped":3634,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:27:49.764: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:27:53.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2546" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":280,"completed":220,"skipped":3647,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:27:53.823: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 21:27:54.226: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 21:27:57.247: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:27:57.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4791" for this suite.
STEP: Destroying namespace "webhook-4791-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":280,"completed":221,"skipped":3669,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:27:57.520: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-29636ed8-d2bf-453a-81ac-6265088cd5e1
STEP: Creating a pod to test consume configMaps
Feb 23 21:27:57.592: INFO: Waiting up to 5m0s for pod "pod-configmaps-87b90d13-a679-4176-a791-f4e369eaa345" in namespace "configmap-2385" to be "success or failure"
Feb 23 21:27:57.595: INFO: Pod "pod-configmaps-87b90d13-a679-4176-a791-f4e369eaa345": Phase="Pending", Reason="", readiness=false. Elapsed: 3.042269ms
Feb 23 21:27:59.601: INFO: Pod "pod-configmaps-87b90d13-a679-4176-a791-f4e369eaa345": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009044136s
STEP: Saw pod success
Feb 23 21:27:59.602: INFO: Pod "pod-configmaps-87b90d13-a679-4176-a791-f4e369eaa345" satisfied condition "success or failure"
Feb 23 21:27:59.607: INFO: Trying to get logs from node worker01 pod pod-configmaps-87b90d13-a679-4176-a791-f4e369eaa345 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 21:27:59.625: INFO: Waiting for pod pod-configmaps-87b90d13-a679-4176-a791-f4e369eaa345 to disappear
Feb 23 21:27:59.628: INFO: Pod pod-configmaps-87b90d13-a679-4176-a791-f4e369eaa345 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:27:59.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2385" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":222,"skipped":3687,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:27:59.638: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-611.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-611.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-611.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-611.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-611.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-611.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 21:28:01.723: INFO: DNS probes using dns-611/dns-test-699ebb9d-3cd1-41fc-bc1d-a352ad9b0d38 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:28:01.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-611" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":280,"completed":223,"skipped":3731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:28:01.765: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Feb 23 21:28:01.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 api-versions'
Feb 23 21:28:01.857: INFO: stderr: ""
Feb 23 21:28:01.857: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauditregistration.k8s.io/v1alpha1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1alpha1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1alpha1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nsnapshot.storage.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1alpha1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:28:01.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2839" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":280,"completed":224,"skipped":3767,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:28:01.869: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 23 21:28:03.918: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:28:03.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9931" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":225,"skipped":3789,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:28:03.953: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-9568
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-9568
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9568
Feb 23 21:28:04.001: INFO: Found 0 stateful pods, waiting for 1
Feb 23 21:28:14.004: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 23 21:28:14.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 21:28:14.162: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 21:28:14.162: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 21:28:14.162: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 21:28:14.166: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 23 21:28:24.173: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 21:28:24.173: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 21:28:24.185: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 21:28:24.185: INFO: ss-0  worker01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  }]
Feb 23 21:28:24.185: INFO: 
Feb 23 21:28:24.185: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 23 21:28:25.191: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995023591s
Feb 23 21:28:26.196: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990425631s
Feb 23 21:28:27.204: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984741031s
Feb 23 21:28:28.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977659624s
Feb 23 21:28:29.216: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972202726s
Feb 23 21:28:30.225: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964235191s
Feb 23 21:28:31.228: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.955942778s
Feb 23 21:28:32.235: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.952531795s
Feb 23 21:28:33.245: INFO: Verifying statefulset ss doesn't scale past 3 for another 940.334338ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9568
Feb 23 21:28:34.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:28:34.376: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 23 21:28:34.377: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 21:28:34.377: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 21:28:34.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:28:34.502: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 23 21:28:34.502: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 21:28:34.502: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 21:28:34.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:28:34.642: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 23 21:28:34.642: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 23 21:28:34.642: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 23 21:28:34.646: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 21:28:34.646: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 21:28:34.646: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 23 21:28:34.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 21:28:34.766: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 21:28:34.766: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 21:28:34.766: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 21:28:34.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 21:28:34.900: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 21:28:34.900: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 21:28:34.900: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 21:28:34.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 23 21:28:35.047: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 23 21:28:35.047: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 23 21:28:35.047: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 23 21:28:35.047: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 21:28:35.051: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 23 21:28:45.063: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 21:28:45.063: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 21:28:45.063: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 23 21:28:45.073: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 21:28:45.073: INFO: ss-0  worker01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  }]
Feb 23 21:28:45.073: INFO: ss-1  worker00  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:24 +0000 UTC  }]
Feb 23 21:28:45.073: INFO: ss-2  worker01  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:24 +0000 UTC  }]
Feb 23 21:28:45.073: INFO: 
Feb 23 21:28:45.073: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 23 21:28:46.080: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 21:28:46.080: INFO: ss-0  worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  }]
Feb 23 21:28:46.080: INFO: ss-1  worker00  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:24 +0000 UTC  }]
Feb 23 21:28:46.080: INFO: ss-2  worker01  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:24 +0000 UTC  }]
Feb 23 21:28:46.080: INFO: 
Feb 23 21:28:46.080: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 23 21:28:47.086: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 21:28:47.086: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  }]
Feb 23 21:28:47.086: INFO: ss-2  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:24 +0000 UTC  }]
Feb 23 21:28:47.086: INFO: 
Feb 23 21:28:47.086: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 23 21:28:48.089: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 21:28:48.089: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  }]
Feb 23 21:28:48.089: INFO: ss-2  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:24 +0000 UTC  }]
Feb 23 21:28:48.089: INFO: 
Feb 23 21:28:48.089: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 23 21:28:49.094: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 21:28:49.094: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  }]
Feb 23 21:28:49.094: INFO: 
Feb 23 21:28:49.094: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 23 21:28:50.097: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 21:28:50.097: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  }]
Feb 23 21:28:50.097: INFO: 
Feb 23 21:28:50.097: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 23 21:28:51.101: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 21:28:51.101: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  }]
Feb 23 21:28:51.101: INFO: 
Feb 23 21:28:51.101: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 23 21:28:52.105: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 21:28:52.105: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  }]
Feb 23 21:28:52.105: INFO: 
Feb 23 21:28:52.105: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 23 21:28:53.108: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 21:28:53.108: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  }]
Feb 23 21:28:53.108: INFO: 
Feb 23 21:28:53.108: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 23 21:28:54.111: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
Feb 23 21:28:54.111: INFO: ss-0  worker01  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-23 21:28:04 +0000 UTC  }]
Feb 23 21:28:54.111: INFO: 
Feb 23 21:28:54.111: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9568
Feb 23 21:28:55.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:28:55.191: INFO: rc: 1
Feb 23 21:28:55.191: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Feb 23 21:29:05.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:29:05.254: INFO: rc: 1
Feb 23 21:29:05.254: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:29:15.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:29:15.332: INFO: rc: 1
Feb 23 21:29:15.332: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:29:25.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:29:25.392: INFO: rc: 1
Feb 23 21:29:25.392: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:29:35.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:29:35.453: INFO: rc: 1
Feb 23 21:29:35.453: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:29:45.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:29:45.522: INFO: rc: 1
Feb 23 21:29:45.522: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:29:55.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:29:55.579: INFO: rc: 1
Feb 23 21:29:55.579: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:30:05.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:30:05.640: INFO: rc: 1
Feb 23 21:30:05.640: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:30:15.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:30:15.709: INFO: rc: 1
Feb 23 21:30:15.709: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:30:25.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:30:25.766: INFO: rc: 1
Feb 23 21:30:25.766: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:30:35.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:30:35.828: INFO: rc: 1
Feb 23 21:30:35.828: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:30:45.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:30:45.888: INFO: rc: 1
Feb 23 21:30:45.888: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:30:55.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:30:55.944: INFO: rc: 1
Feb 23 21:30:55.944: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:31:05.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:31:06.010: INFO: rc: 1
Feb 23 21:31:06.010: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:31:16.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:31:16.069: INFO: rc: 1
Feb 23 21:31:16.069: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:31:26.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:31:26.133: INFO: rc: 1
Feb 23 21:31:26.133: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:31:36.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:31:36.193: INFO: rc: 1
Feb 23 21:31:36.193: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:31:46.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:31:46.258: INFO: rc: 1
Feb 23 21:31:46.258: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:31:56.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:31:56.320: INFO: rc: 1
Feb 23 21:31:56.320: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:32:06.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:32:06.381: INFO: rc: 1
Feb 23 21:32:06.381: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:32:16.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:32:16.443: INFO: rc: 1
Feb 23 21:32:16.443: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:32:26.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:32:26.524: INFO: rc: 1
Feb 23 21:32:26.524: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:32:36.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:32:36.587: INFO: rc: 1
Feb 23 21:32:36.587: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:32:46.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:32:46.650: INFO: rc: 1
Feb 23 21:32:46.650: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:32:56.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:32:56.711: INFO: rc: 1
Feb 23 21:32:56.711: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:33:06.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:33:06.768: INFO: rc: 1
Feb 23 21:33:06.768: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:33:16.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:33:16.826: INFO: rc: 1
Feb 23 21:33:16.826: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:33:26.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:33:26.894: INFO: rc: 1
Feb 23 21:33:26.895: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:33:36.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:33:36.960: INFO: rc: 1
Feb 23 21:33:36.960: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:33:46.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:33:47.023: INFO: rc: 1
Feb 23 21:33:47.023: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 23 21:33:57.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 exec --namespace=statefulset-9568 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 23 21:33:57.087: INFO: rc: 1
Feb 23 21:33:57.087: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Feb 23 21:33:57.087: INFO: Scaling statefulset ss to 0
Feb 23 21:33:57.096: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 23 21:33:57.098: INFO: Deleting all statefulset in ns statefulset-9568
Feb 23 21:33:57.100: INFO: Scaling statefulset ss to 0
Feb 23 21:33:57.106: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 21:33:57.108: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:33:57.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9568" for this suite.

â€¢ [SLOW TEST:353.175 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":280,"completed":226,"skipped":3797,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:33:57.128: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Feb 23 21:33:57.166: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:34:05.033: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:34:21.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7305" for this suite.

â€¢ [SLOW TEST:24.228 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":280,"completed":227,"skipped":3797,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:34:21.357: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 21:34:21.397: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99ffd3e0-a691-45e7-8901-5433d11dfaa9" in namespace "projected-3407" to be "success or failure"
Feb 23 21:34:21.399: INFO: Pod "downwardapi-volume-99ffd3e0-a691-45e7-8901-5433d11dfaa9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.475146ms
Feb 23 21:34:23.403: INFO: Pod "downwardapi-volume-99ffd3e0-a691-45e7-8901-5433d11dfaa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005853757s
STEP: Saw pod success
Feb 23 21:34:23.403: INFO: Pod "downwardapi-volume-99ffd3e0-a691-45e7-8901-5433d11dfaa9" satisfied condition "success or failure"
Feb 23 21:34:23.406: INFO: Trying to get logs from node worker01 pod downwardapi-volume-99ffd3e0-a691-45e7-8901-5433d11dfaa9 container client-container: <nil>
STEP: delete the pod
Feb 23 21:34:23.437: INFO: Waiting for pod downwardapi-volume-99ffd3e0-a691-45e7-8901-5433d11dfaa9 to disappear
Feb 23 21:34:23.440: INFO: Pod downwardapi-volume-99ffd3e0-a691-45e7-8901-5433d11dfaa9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:34:23.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3407" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":228,"skipped":3798,"failed":0}
SS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:34:23.449: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 23 21:34:27.524: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3512 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:34:27.524: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:34:27.586: INFO: Exec stderr: ""
Feb 23 21:34:27.586: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3512 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:34:27.587: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:34:27.646: INFO: Exec stderr: ""
Feb 23 21:34:27.647: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3512 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:34:27.647: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:34:27.697: INFO: Exec stderr: ""
Feb 23 21:34:27.697: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3512 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:34:27.697: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:34:27.749: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 23 21:34:27.749: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3512 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:34:27.749: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:34:27.807: INFO: Exec stderr: ""
Feb 23 21:34:27.807: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3512 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:34:27.807: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:34:27.862: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 23 21:34:27.862: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3512 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:34:27.862: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:34:27.912: INFO: Exec stderr: ""
Feb 23 21:34:27.912: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3512 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:34:27.912: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:34:27.959: INFO: Exec stderr: ""
Feb 23 21:34:27.959: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3512 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:34:27.959: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:34:28.026: INFO: Exec stderr: ""
Feb 23 21:34:28.026: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3512 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:34:28.026: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:34:28.090: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:34:28.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3512" for this suite.
â€¢{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":229,"skipped":3800,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:34:28.099: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:34:41.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5711" for this suite.

â€¢ [SLOW TEST:13.089 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":280,"completed":230,"skipped":3820,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:34:41.192: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Feb 23 21:35:21.257: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:35:21.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0223 21:35:21.257936      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3327" for this suite.

â€¢ [SLOW TEST:40.075 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":280,"completed":231,"skipped":3826,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:35:21.268: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 21:35:21.306: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a245cd0-c999-4ec7-91e0-0794c01c3504" in namespace "downward-api-1537" to be "success or failure"
Feb 23 21:35:21.310: INFO: Pod "downwardapi-volume-8a245cd0-c999-4ec7-91e0-0794c01c3504": Phase="Pending", Reason="", readiness=false. Elapsed: 3.50235ms
Feb 23 21:35:23.312: INFO: Pod "downwardapi-volume-8a245cd0-c999-4ec7-91e0-0794c01c3504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005983812s
STEP: Saw pod success
Feb 23 21:35:23.312: INFO: Pod "downwardapi-volume-8a245cd0-c999-4ec7-91e0-0794c01c3504" satisfied condition "success or failure"
Feb 23 21:35:23.315: INFO: Trying to get logs from node worker01 pod downwardapi-volume-8a245cd0-c999-4ec7-91e0-0794c01c3504 container client-container: <nil>
STEP: delete the pod
Feb 23 21:35:23.333: INFO: Waiting for pod downwardapi-volume-8a245cd0-c999-4ec7-91e0-0794c01c3504 to disappear
Feb 23 21:35:23.336: INFO: Pod downwardapi-volume-8a245cd0-c999-4ec7-91e0-0794c01c3504 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:35:23.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1537" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":280,"completed":232,"skipped":3832,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:35:23.347: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-316
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-316 to expose endpoints map[]
Feb 23 21:35:23.393: INFO: Get endpoints failed (7.312705ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Feb 23 21:35:24.396: INFO: successfully validated that service multi-endpoint-test in namespace services-316 exposes endpoints map[] (1.010575493s elapsed)
STEP: Creating pod pod1 in namespace services-316
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-316 to expose endpoints map[pod1:[100]]
Feb 23 21:35:26.431: INFO: successfully validated that service multi-endpoint-test in namespace services-316 exposes endpoints map[pod1:[100]] (2.027952441s elapsed)
STEP: Creating pod pod2 in namespace services-316
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-316 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 23 21:35:28.532: INFO: successfully validated that service multi-endpoint-test in namespace services-316 exposes endpoints map[pod1:[100] pod2:[101]] (2.077351093s elapsed)
STEP: Deleting pod pod1 in namespace services-316
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-316 to expose endpoints map[pod2:[101]]
Feb 23 21:35:28.564: INFO: successfully validated that service multi-endpoint-test in namespace services-316 exposes endpoints map[pod2:[101]] (18.562388ms elapsed)
STEP: Deleting pod pod2 in namespace services-316
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-316 to expose endpoints map[]
Feb 23 21:35:29.588: INFO: successfully validated that service multi-endpoint-test in namespace services-316 exposes endpoints map[] (1.01677364s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:35:29.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-316" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:6.283 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":280,"completed":233,"skipped":3840,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:35:29.630: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 23 21:35:29.672: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 23 21:35:29.688: INFO: Waiting for terminating namespaces to be deleted...
Feb 23 21:35:29.696: INFO: 
Logging pods the kubelet thinks is on node worker00 before test
Feb 23 21:35:29.718: INFO: etcd-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container etcd ready: true, restart count 0
Feb 23 21:35:29.719: INFO: ceph-osd-worker00-556546b495-tmvn5 from storage started at 2020-02-23 20:28:14 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 21:35:29.719: INFO: sonobuoy-systemd-logs-daemon-set-975a2eaddfc64d32-ct4lk from sonobuoy started at 2020-02-23 20:38:34 +0000 UTC (2 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 21:35:29.719: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 21:35:29.719: INFO: kube-scheduler-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container kube-scheduler ready: true, restart count 2
Feb 23 21:35:29.719: INFO: calico-node-st65h from networking started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 21:35:29.719: INFO: ceph-mgr-94b9dd996-ggbsc from storage started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container ceph-mgr ready: true, restart count 0
Feb 23 21:35:29.719: INFO: csi-rbdplugin-provisioner-7494f65674-7tn5d from storage started at 2020-02-23 20:28:35 +0000 UTC (5 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 21:35:29.719: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:35:29.719: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:35:29.719: INFO: 	Container csi-snapshotter ready: true, restart count 2
Feb 23 21:35:29.719: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:35:29.719: INFO: csi-rbdplugin-provisioner-7494f65674-2h6xd from storage started at 2020-02-23 20:28:35 +0000 UTC (5 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 21:35:29.719: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 21:35:29.719: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:35:29.719: INFO: 	Container csi-snapshotter ready: true, restart count 2
Feb 23 21:35:29.719: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:35:29.719: INFO: ceph-mds-worker00-6f479b4486-bdmhh from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container ceph-mds ready: true, restart count 3
Feb 23 21:35:29.719: INFO: ceph-mon-worker00-5cf654d469-bcbdt from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 21:35:29.719: INFO: coredns-676544c7b9-j9z7b from kube-system started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container coredns ready: true, restart count 0
Feb 23 21:35:29.719: INFO: calico-kube-controllers-7cd585bcd-vmw47 from networking started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container calico-kube-controllers ready: true, restart count 2
Feb 23 21:35:29.719: INFO: csi-rbdplugin-z2bdm from storage started at 2020-02-23 20:28:35 +0000 UTC (3 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:35:29.719: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 21:35:29.719: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:35:29.719: INFO: csi-cephfsplugin-q68vh from storage started at 2020-02-23 20:28:35 +0000 UTC (3 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:35:29.719: INFO: 	Container driver-registrar ready: true, restart count 1
Feb 23 21:35:29.719: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:35:29.719: INFO: csi-cephfsplugin-provisioner-6cd7596f75-tw7n4 from storage started at 2020-02-23 20:28:35 +0000 UTC (4 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:35:29.719: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 21:35:29.719: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 21:35:29.719: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:35:29.719: INFO: csi-rbdplugin-provisioner-7494f65674-ddxmx from storage started at 2020-02-23 20:28:35 +0000 UTC (5 container statuses recorded)
Feb 23 21:35:29.719: INFO: 	Container csi-attacher ready: true, restart count 1
Feb 23 21:35:29.720: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:35:29.720: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:35:29.720: INFO: 	Container csi-snapshotter ready: true, restart count 2
Feb 23 21:35:29.720: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:35:29.720: INFO: csi-cephfsplugin-provisioner-6cd7596f75-9hfw2 from storage started at 2020-02-23 20:28:35 +0000 UTC (4 container statuses recorded)
Feb 23 21:35:29.720: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:35:29.720: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 21:35:29.720: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:35:29.720: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:35:29.720: INFO: ceph-setup-6j8xr from storage started at 2020-02-23 20:28:36 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.720: INFO: 	Container ceph ready: false, restart count 2
Feb 23 21:35:29.720: INFO: kube-apiserver-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.720: INFO: 	Container kube-apiserver ready: true, restart count 1
Feb 23 21:35:29.720: INFO: csi-cephfsplugin-provisioner-6cd7596f75-492rn from storage started at 2020-02-23 20:28:35 +0000 UTC (4 container statuses recorded)
Feb 23 21:35:29.720: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:35:29.720: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 21:35:29.720: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:35:29.720: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:35:29.720: INFO: metallb-controller-b96bfbbf8-p9lnh from networking started at 2020-02-23 20:28:36 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.722: INFO: 	Container controller ready: true, restart count 0
Feb 23 21:35:29.722: INFO: pod2 from services-316 started at 2020-02-23 21:35:26 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.722: INFO: 	Container pause ready: true, restart count 0
Feb 23 21:35:29.722: INFO: gobetween-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.722: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 21:35:29.722: INFO: kube-proxy-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.722: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 21:35:29.722: INFO: coredns-676544c7b9-q29b9 from kube-system started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.722: INFO: 	Container coredns ready: true, restart count 0
Feb 23 21:35:29.722: INFO: ceph-rgw-57cd48f74c-lf65j from storage started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.722: INFO: 	Container ceph-rgw ready: true, restart count 0
Feb 23 21:35:29.722: INFO: metallb-speaker-74sbt from networking started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.722: INFO: 	Container speaker ready: true, restart count 0
Feb 23 21:35:29.722: INFO: kube-controller-manager-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.722: INFO: 	Container kube-controller-manager ready: true, restart count 2
Feb 23 21:35:29.722: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
Feb 23 21:35:29.732: INFO: kubernetes-dashboard-f957cddcb-xf79k from kube-system started at 2020-02-23 20:33:06 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 23 21:35:29.732: INFO: pod1 from services-316 started at 2020-02-23 21:35:24 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container pause ready: false, restart count 0
Feb 23 21:35:29.732: INFO: sonobuoy from sonobuoy started at 2020-02-23 20:38:32 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 23 21:35:29.732: INFO: kube-apiserver-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 23 21:35:29.732: INFO: csi-rbdplugin-c6629 from storage started at 2020-02-23 20:31:01 +0000 UTC (3 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:35:29.732: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 21:35:29.732: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:35:29.732: INFO: kube-scheduler-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container kube-scheduler ready: true, restart count 2
Feb 23 21:35:29.732: INFO: kube-controller-manager-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container kube-controller-manager ready: true, restart count 2
Feb 23 21:35:29.732: INFO: sonobuoy-e2e-job-5674935bca8a45cb from sonobuoy started at 2020-02-23 20:38:33 +0000 UTC (2 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container e2e ready: true, restart count 0
Feb 23 21:35:29.732: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 21:35:29.732: INFO: sonobuoy-systemd-logs-daemon-set-975a2eaddfc64d32-jk65g from sonobuoy started at 2020-02-23 20:38:33 +0000 UTC (2 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 21:35:29.732: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 21:35:29.732: INFO: calico-node-wlnml from networking started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 21:35:29.732: INFO: metallb-speaker-nx7ns from networking started at 2020-02-23 20:31:01 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container speaker ready: true, restart count 0
Feb 23 21:35:29.732: INFO: ceph-mon-worker01-bdb694876-bwgrw from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 21:35:29.732: INFO: csi-cephfsplugin-7cvst from storage started at 2020-02-23 20:31:01 +0000 UTC (3 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:35:29.732: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 21:35:29.732: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:35:29.732: INFO: gobetween-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 21:35:29.732: INFO: kube-proxy-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 21:35:29.732: INFO: etcd-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container etcd ready: true, restart count 1
Feb 23 21:35:29.732: INFO: ceph-mds-worker01-7f5fdb58c6-vz8jk from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container ceph-mds ready: true, restart count 2
Feb 23 21:35:29.732: INFO: ceph-osd-worker01-67947c799-h78d6 from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 21:35:29.732: INFO: dashboard-metrics-scraper-58475bc987-qhvsn from kube-system started at 2020-02-23 20:33:06 +0000 UTC (1 container statuses recorded)
Feb 23 21:35:29.732: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-051eeb97-9274-4806-bb15-be7cb37f0087 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-051eeb97-9274-4806-bb15-be7cb37f0087 off the node worker01
STEP: verifying the node doesn't have the label kubernetes.io/e2e-051eeb97-9274-4806-bb15-be7cb37f0087
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:40:33.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6865" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

â€¢ [SLOW TEST:304.217 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":280,"completed":234,"skipped":3843,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:40:33.848: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Feb 23 21:40:35.900: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-277121970 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 23 21:40:50.975: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:40:50.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9660" for this suite.

â€¢ [SLOW TEST:17.140 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":280,"completed":235,"skipped":3845,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:40:50.988: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:40:51.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-710" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
â€¢{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":280,"completed":236,"skipped":3861,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:40:51.034: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-6f9e38bd-c048-4aa5-96fa-868b5b09c5bf
Feb 23 21:40:51.076: INFO: Pod name my-hostname-basic-6f9e38bd-c048-4aa5-96fa-868b5b09c5bf: Found 0 pods out of 1
Feb 23 21:40:56.095: INFO: Pod name my-hostname-basic-6f9e38bd-c048-4aa5-96fa-868b5b09c5bf: Found 1 pods out of 1
Feb 23 21:40:56.095: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6f9e38bd-c048-4aa5-96fa-868b5b09c5bf" are running
Feb 23 21:40:56.098: INFO: Pod "my-hostname-basic-6f9e38bd-c048-4aa5-96fa-868b5b09c5bf-429nc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 21:40:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 21:40:52 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 21:40:52 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-23 21:40:51 +0000 UTC Reason: Message:}])
Feb 23 21:40:56.098: INFO: Trying to dial the pod
Feb 23 21:41:01.109: INFO: Controller my-hostname-basic-6f9e38bd-c048-4aa5-96fa-868b5b09c5bf: Got expected result from replica 1 [my-hostname-basic-6f9e38bd-c048-4aa5-96fa-868b5b09c5bf-429nc]: "my-hostname-basic-6f9e38bd-c048-4aa5-96fa-868b5b09c5bf-429nc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:41:01.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9947" for this suite.

â€¢ [SLOW TEST:10.084 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":237,"skipped":3866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:41:01.118: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Feb 23 21:41:01.145: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-277121970 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:41:01.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8204" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":280,"completed":238,"skipped":3897,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:41:01.209: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Feb 23 21:41:02.327: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0223 21:41:02.327295      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 23 21:41:02.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-685" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":280,"completed":239,"skipped":3898,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:41:02.336: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-78be2c3f-a240-4a28-a65a-37c4667d39e3
STEP: Creating a pod to test consume configMaps
Feb 23 21:41:02.377: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c2d89d86-a27c-4bca-92de-594878ca334e" in namespace "projected-3103" to be "success or failure"
Feb 23 21:41:02.383: INFO: Pod "pod-projected-configmaps-c2d89d86-a27c-4bca-92de-594878ca334e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.421828ms
Feb 23 21:41:04.387: INFO: Pod "pod-projected-configmaps-c2d89d86-a27c-4bca-92de-594878ca334e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00986177s
STEP: Saw pod success
Feb 23 21:41:04.387: INFO: Pod "pod-projected-configmaps-c2d89d86-a27c-4bca-92de-594878ca334e" satisfied condition "success or failure"
Feb 23 21:41:04.391: INFO: Trying to get logs from node worker01 pod pod-projected-configmaps-c2d89d86-a27c-4bca-92de-594878ca334e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 21:41:04.406: INFO: Waiting for pod pod-projected-configmaps-c2d89d86-a27c-4bca-92de-594878ca334e to disappear
Feb 23 21:41:04.411: INFO: Pod pod-projected-configmaps-c2d89d86-a27c-4bca-92de-594878ca334e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:41:04.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3103" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":240,"skipped":3926,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:41:04.423: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:41:10.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9263" for this suite.
STEP: Destroying namespace "nsdeletetest-2028" for this suite.
Feb 23 21:41:10.532: INFO: Namespace nsdeletetest-2028 was already deleted
STEP: Destroying namespace "nsdeletetest-755" for this suite.

â€¢ [SLOW TEST:6.114 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":280,"completed":241,"skipped":4008,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:41:10.537: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7967, will wait for the garbage collector to delete the pods
Feb 23 21:41:12.636: INFO: Deleting Job.batch foo took: 6.183755ms
Feb 23 21:41:12.738: INFO: Terminating Job.batch foo pods took: 102.17458ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:41:57.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7967" for this suite.

â€¢ [SLOW TEST:47.217 seconds]
[sig-apps] Job
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":280,"completed":242,"skipped":4018,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:41:57.755: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 21:41:58.313: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 21:42:01.330: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:42:01.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6312" for this suite.
STEP: Destroying namespace "webhook-6312-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":280,"completed":243,"skipped":4023,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:42:01.492: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 23 21:42:01.531: INFO: Waiting up to 5m0s for pod "pod-578635c7-9be7-4e4e-aa7f-b53e57c768d0" in namespace "emptydir-8933" to be "success or failure"
Feb 23 21:42:01.534: INFO: Pod "pod-578635c7-9be7-4e4e-aa7f-b53e57c768d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.579797ms
Feb 23 21:42:03.537: INFO: Pod "pod-578635c7-9be7-4e4e-aa7f-b53e57c768d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006170454s
STEP: Saw pod success
Feb 23 21:42:03.537: INFO: Pod "pod-578635c7-9be7-4e4e-aa7f-b53e57c768d0" satisfied condition "success or failure"
Feb 23 21:42:03.540: INFO: Trying to get logs from node worker01 pod pod-578635c7-9be7-4e4e-aa7f-b53e57c768d0 container test-container: <nil>
STEP: delete the pod
Feb 23 21:42:03.553: INFO: Waiting for pod pod-578635c7-9be7-4e4e-aa7f-b53e57c768d0 to disappear
Feb 23 21:42:03.556: INFO: Pod pod-578635c7-9be7-4e4e-aa7f-b53e57c768d0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:42:03.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8933" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":244,"skipped":4027,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:42:03.565: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:42:03.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1618" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":280,"completed":245,"skipped":4069,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:42:03.604: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1596
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 23 21:42:03.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9844'
Feb 23 21:42:04.382: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 23 21:42:04.382: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1602
Feb 23 21:42:04.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete deployment e2e-test-httpd-deployment --namespace=kubectl-9844'
Feb 23 21:42:04.467: INFO: stderr: ""
Feb 23 21:42:04.467: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:42:04.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9844" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":280,"completed":246,"skipped":4104,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:42:04.478: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 21:42:04.521: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1fa63ee-2a07-443e-8718-16896197a3d0" in namespace "projected-6971" to be "success or failure"
Feb 23 21:42:04.524: INFO: Pod "downwardapi-volume-f1fa63ee-2a07-443e-8718-16896197a3d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.832446ms
Feb 23 21:42:06.529: INFO: Pod "downwardapi-volume-f1fa63ee-2a07-443e-8718-16896197a3d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007080939s
STEP: Saw pod success
Feb 23 21:42:06.529: INFO: Pod "downwardapi-volume-f1fa63ee-2a07-443e-8718-16896197a3d0" satisfied condition "success or failure"
Feb 23 21:42:06.535: INFO: Trying to get logs from node worker01 pod downwardapi-volume-f1fa63ee-2a07-443e-8718-16896197a3d0 container client-container: <nil>
STEP: delete the pod
Feb 23 21:42:06.555: INFO: Waiting for pod downwardapi-volume-f1fa63ee-2a07-443e-8718-16896197a3d0 to disappear
Feb 23 21:42:06.558: INFO: Pod downwardapi-volume-f1fa63ee-2a07-443e-8718-16896197a3d0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:42:06.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6971" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":247,"skipped":4105,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:42:06.575: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0223 21:42:16.728611      23 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 23 21:42:16.728: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:42:16.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9574" for this suite.

â€¢ [SLOW TEST:10.162 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":280,"completed":248,"skipped":4119,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:42:16.738: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 23 21:42:16.777: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 23 21:42:21.780: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:42:22.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2433" for this suite.

â€¢ [SLOW TEST:6.075 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":280,"completed":249,"skipped":4162,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:42:22.813: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 23 21:42:22.897: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1326 /api/v1/namespaces/watch-1326/configmaps/e2e-watch-test-label-changed 8d126f02-478a-4f43-9326-b15a822a8d36 31104 0 2020-02-23 21:42:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 23 21:42:22.897: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1326 /api/v1/namespaces/watch-1326/configmaps/e2e-watch-test-label-changed 8d126f02-478a-4f43-9326-b15a822a8d36 31105 0 2020-02-23 21:42:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 23 21:42:22.898: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1326 /api/v1/namespaces/watch-1326/configmaps/e2e-watch-test-label-changed 8d126f02-478a-4f43-9326-b15a822a8d36 31107 0 2020-02-23 21:42:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 23 21:42:32.924: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1326 /api/v1/namespaces/watch-1326/configmaps/e2e-watch-test-label-changed 8d126f02-478a-4f43-9326-b15a822a8d36 31173 0 2020-02-23 21:42:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 23 21:42:32.924: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1326 /api/v1/namespaces/watch-1326/configmaps/e2e-watch-test-label-changed 8d126f02-478a-4f43-9326-b15a822a8d36 31174 0 2020-02-23 21:42:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 23 21:42:32.925: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1326 /api/v1/namespaces/watch-1326/configmaps/e2e-watch-test-label-changed 8d126f02-478a-4f43-9326-b15a822a8d36 31175 0 2020-02-23 21:42:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:42:32.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1326" for this suite.

â€¢ [SLOW TEST:10.122 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":280,"completed":250,"skipped":4196,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:42:32.937: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-3046
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Feb 23 21:42:32.978: INFO: Found 0 stateful pods, waiting for 3
Feb 23 21:42:42.982: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 21:42:42.982: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 21:42:42.983: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 23 21:42:43.011: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 23 21:42:53.049: INFO: Updating stateful set ss2
Feb 23 21:42:53.057: INFO: Waiting for Pod statefulset-3046/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Feb 23 21:43:03.148: INFO: Found 2 stateful pods, waiting for 3
Feb 23 21:43:13.154: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 21:43:13.154: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 23 21:43:13.154: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 23 21:43:13.177: INFO: Updating stateful set ss2
Feb 23 21:43:13.185: INFO: Waiting for Pod statefulset-3046/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 23 21:43:23.209: INFO: Updating stateful set ss2
Feb 23 21:43:23.223: INFO: Waiting for StatefulSet statefulset-3046/ss2 to complete update
Feb 23 21:43:23.223: INFO: Waiting for Pod statefulset-3046/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 23 21:43:33.230: INFO: Deleting all statefulset in ns statefulset-3046
Feb 23 21:43:33.232: INFO: Scaling statefulset ss2 to 0
Feb 23 21:43:43.249: INFO: Waiting for statefulset status.replicas updated to 0
Feb 23 21:43:43.252: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:43:43.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3046" for this suite.

â€¢ [SLOW TEST:70.349 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":280,"completed":251,"skipped":4198,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:43:43.290: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:43:43.323: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 23 21:43:51.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-5911 create -f -'
Feb 23 21:43:52.054: INFO: stderr: ""
Feb 23 21:43:52.054: INFO: stdout: "e2e-test-crd-publish-openapi-6670-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 23 21:43:52.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-5911 delete e2e-test-crd-publish-openapi-6670-crds test-cr'
Feb 23 21:43:52.124: INFO: stderr: ""
Feb 23 21:43:52.124: INFO: stdout: "e2e-test-crd-publish-openapi-6670-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 23 21:43:52.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-5911 apply -f -'
Feb 23 21:43:52.306: INFO: stderr: ""
Feb 23 21:43:52.306: INFO: stdout: "e2e-test-crd-publish-openapi-6670-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 23 21:43:52.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 --namespace=crd-publish-openapi-5911 delete e2e-test-crd-publish-openapi-6670-crds test-cr'
Feb 23 21:43:52.373: INFO: stderr: ""
Feb 23 21:43:52.373: INFO: stdout: "e2e-test-crd-publish-openapi-6670-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 23 21:43:52.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 explain e2e-test-crd-publish-openapi-6670-crds'
Feb 23 21:43:52.493: INFO: stderr: ""
Feb 23 21:43:52.493: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6670-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:43:55.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5911" for this suite.

â€¢ [SLOW TEST:12.088 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":280,"completed":252,"skipped":4208,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:43:55.378: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 21:43:55.927: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 21:43:58.943: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:43:58.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2003" for this suite.
STEP: Destroying namespace "webhook-2003-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":280,"completed":253,"skipped":4216,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:43:59.020: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 23 21:43:59.058: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:44:02.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2455" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":280,"completed":254,"skipped":4229,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:44:02.922: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:44:13.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8249" for this suite.

â€¢ [SLOW TEST:11.075 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":280,"completed":255,"skipped":4232,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:44:13.997: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 23 21:44:14.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e9389a3-2c3f-4b6a-83a1-a16accc5bc52" in namespace "projected-9806" to be "success or failure"
Feb 23 21:44:14.034: INFO: Pod "downwardapi-volume-5e9389a3-2c3f-4b6a-83a1-a16accc5bc52": Phase="Pending", Reason="", readiness=false. Elapsed: 3.383193ms
Feb 23 21:44:16.037: INFO: Pod "downwardapi-volume-5e9389a3-2c3f-4b6a-83a1-a16accc5bc52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0062243s
STEP: Saw pod success
Feb 23 21:44:16.037: INFO: Pod "downwardapi-volume-5e9389a3-2c3f-4b6a-83a1-a16accc5bc52" satisfied condition "success or failure"
Feb 23 21:44:16.042: INFO: Trying to get logs from node worker01 pod downwardapi-volume-5e9389a3-2c3f-4b6a-83a1-a16accc5bc52 container client-container: <nil>
STEP: delete the pod
Feb 23 21:44:16.068: INFO: Waiting for pod downwardapi-volume-5e9389a3-2c3f-4b6a-83a1-a16accc5bc52 to disappear
Feb 23 21:44:16.072: INFO: Pod downwardapi-volume-5e9389a3-2c3f-4b6a-83a1-a16accc5bc52 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:44:16.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9806" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":256,"skipped":4233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:44:16.084: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:163
Feb 23 21:44:16.111: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 23 21:45:16.124: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:45:16.126: INFO: Starting informer...
STEP: Starting pod...
Feb 23 21:45:16.338: INFO: Pod is running on worker01. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Feb 23 21:45:16.364: INFO: Pod wasn't evicted. Proceeding
Feb 23 21:45:16.364: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Feb 23 21:46:31.448: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:46:31.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3821" for this suite.

â€¢ [SLOW TEST:135.373 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":280,"completed":257,"skipped":4257,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:46:31.457: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Feb 23 21:46:31.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 create -f - --namespace=kubectl-6196'
Feb 23 21:46:31.690: INFO: stderr: ""
Feb 23 21:46:31.690: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 21:46:31.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6196'
Feb 23 21:46:31.771: INFO: stderr: ""
Feb 23 21:46:31.771: INFO: stdout: "update-demo-nautilus-kph4h update-demo-nautilus-q9mlg "
Feb 23 21:46:31.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-kph4h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6196'
Feb 23 21:46:31.824: INFO: stderr: ""
Feb 23 21:46:31.824: INFO: stdout: ""
Feb 23 21:46:31.824: INFO: update-demo-nautilus-kph4h is created but not running
Feb 23 21:46:36.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6196'
Feb 23 21:46:36.889: INFO: stderr: ""
Feb 23 21:46:36.889: INFO: stdout: "update-demo-nautilus-kph4h update-demo-nautilus-q9mlg "
Feb 23 21:46:36.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-kph4h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6196'
Feb 23 21:46:36.941: INFO: stderr: ""
Feb 23 21:46:36.941: INFO: stdout: "true"
Feb 23 21:46:36.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-kph4h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6196'
Feb 23 21:46:36.998: INFO: stderr: ""
Feb 23 21:46:36.998: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 21:46:36.998: INFO: validating pod update-demo-nautilus-kph4h
Feb 23 21:46:37.002: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 21:46:37.002: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 21:46:37.002: INFO: update-demo-nautilus-kph4h is verified up and running
Feb 23 21:46:37.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-q9mlg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6196'
Feb 23 21:46:37.052: INFO: stderr: ""
Feb 23 21:46:37.052: INFO: stdout: "true"
Feb 23 21:46:37.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-q9mlg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6196'
Feb 23 21:46:37.101: INFO: stderr: ""
Feb 23 21:46:37.101: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 21:46:37.101: INFO: validating pod update-demo-nautilus-q9mlg
Feb 23 21:46:37.110: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 21:46:37.110: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 21:46:37.110: INFO: update-demo-nautilus-q9mlg is verified up and running
STEP: using delete to clean up resources
Feb 23 21:46:37.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete --grace-period=0 --force -f - --namespace=kubectl-6196'
Feb 23 21:46:37.177: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 21:46:37.177: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 23 21:46:37.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6196'
Feb 23 21:46:37.238: INFO: stderr: "No resources found in kubectl-6196 namespace.\n"
Feb 23 21:46:37.238: INFO: stdout: ""
Feb 23 21:46:37.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -l name=update-demo --namespace=kubectl-6196 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 21:46:37.292: INFO: stderr: ""
Feb 23 21:46:37.292: INFO: stdout: "update-demo-nautilus-kph4h\nupdate-demo-nautilus-q9mlg\n"
Feb 23 21:46:37.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6196'
Feb 23 21:46:37.859: INFO: stderr: "No resources found in kubectl-6196 namespace.\n"
Feb 23 21:46:37.859: INFO: stdout: ""
Feb 23 21:46:37.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -l name=update-demo --namespace=kubectl-6196 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 21:46:37.924: INFO: stderr: ""
Feb 23 21:46:37.924: INFO: stdout: "update-demo-nautilus-kph4h\nupdate-demo-nautilus-q9mlg\n"
Feb 23 21:46:38.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6196'
Feb 23 21:46:38.373: INFO: stderr: "No resources found in kubectl-6196 namespace.\n"
Feb 23 21:46:38.373: INFO: stdout: ""
Feb 23 21:46:38.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -l name=update-demo --namespace=kubectl-6196 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 21:46:38.487: INFO: stderr: ""
Feb 23 21:46:38.487: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:46:38.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6196" for this suite.

â€¢ [SLOW TEST:7.039 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":280,"completed":258,"skipped":4257,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:46:38.496: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:46:55.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3452" for this suite.

â€¢ [SLOW TEST:17.076 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":280,"completed":259,"skipped":4258,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:46:55.573: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 23 21:46:55.598: INFO: PodSpec: initContainers in spec.initContainers
Feb 23 21:47:38.540: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-be491a3f-fe84-4215-95fa-3980122315d1", GenerateName:"", Namespace:"init-container-1749", SelfLink:"/api/v1/namespaces/init-container-1749/pods/pod-init-be491a3f-fe84-4215-95fa-3980122315d1", UID:"811d04ed-5d7b-4232-b079-8199b42f8827", ResourceVersion:"32994", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63718091215, loc:(*time.Location)(0x7db7bc0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"598622300"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.200.5.32/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-lhx9b", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc005a348c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lhx9b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lhx9b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-lhx9b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004baafa8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker01", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0031229c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004bab030)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004bab050)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004bab058), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004bab05c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091215, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091215, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091215, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091215, loc:(*time.Location)(0x7db7bc0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.180.101", PodIP:"10.200.5.32", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.200.5.32"}}, StartTime:(*v1.Time)(0xc004f3cd80), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000b8e310)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000b8e3f0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://68eaea22590532a28190ae16cabf164567622f77114fc0a1a538b32f6ec2ed80", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004f3cdc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004f3cda0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc004bab0df)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:47:38.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1749" for this suite.

â€¢ [SLOW TEST:42.979 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":280,"completed":260,"skipped":4322,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:47:38.552: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-848a799a-cb13-40cc-844c-391a2283c3c3
STEP: Creating a pod to test consume configMaps
Feb 23 21:47:38.593: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8343fc21-b105-4aaf-8cc8-8ccc05991733" in namespace "projected-7752" to be "success or failure"
Feb 23 21:47:38.597: INFO: Pod "pod-projected-configmaps-8343fc21-b105-4aaf-8cc8-8ccc05991733": Phase="Pending", Reason="", readiness=false. Elapsed: 3.419886ms
Feb 23 21:47:40.600: INFO: Pod "pod-projected-configmaps-8343fc21-b105-4aaf-8cc8-8ccc05991733": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006464744s
STEP: Saw pod success
Feb 23 21:47:40.600: INFO: Pod "pod-projected-configmaps-8343fc21-b105-4aaf-8cc8-8ccc05991733" satisfied condition "success or failure"
Feb 23 21:47:40.602: INFO: Trying to get logs from node worker01 pod pod-projected-configmaps-8343fc21-b105-4aaf-8cc8-8ccc05991733 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 21:47:40.627: INFO: Waiting for pod pod-projected-configmaps-8343fc21-b105-4aaf-8cc8-8ccc05991733 to disappear
Feb 23 21:47:40.629: INFO: Pod pod-projected-configmaps-8343fc21-b105-4aaf-8cc8-8ccc05991733 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:47:40.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7752" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":261,"skipped":4323,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:47:40.637: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-4053f0ab-2a26-41ee-aa79-97ee1abcfc3c
STEP: Creating a pod to test consume secrets
Feb 23 21:47:40.676: INFO: Waiting up to 5m0s for pod "pod-secrets-5999b535-d859-4929-9806-4122486bb5d7" in namespace "secrets-5830" to be "success or failure"
Feb 23 21:47:40.679: INFO: Pod "pod-secrets-5999b535-d859-4929-9806-4122486bb5d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.905306ms
Feb 23 21:47:42.683: INFO: Pod "pod-secrets-5999b535-d859-4929-9806-4122486bb5d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006886542s
STEP: Saw pod success
Feb 23 21:47:42.683: INFO: Pod "pod-secrets-5999b535-d859-4929-9806-4122486bb5d7" satisfied condition "success or failure"
Feb 23 21:47:42.688: INFO: Trying to get logs from node worker01 pod pod-secrets-5999b535-d859-4929-9806-4122486bb5d7 container secret-env-test: <nil>
STEP: delete the pod
Feb 23 21:47:42.705: INFO: Waiting for pod pod-secrets-5999b535-d859-4929-9806-4122486bb5d7 to disappear
Feb 23 21:47:42.715: INFO: Pod pod-secrets-5999b535-d859-4929-9806-4122486bb5d7 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:47:42.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5830" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":280,"completed":262,"skipped":4350,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:47:42.724: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Feb 23 21:47:42.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 create -f - --namespace=kubectl-9510'
Feb 23 21:47:42.973: INFO: stderr: ""
Feb 23 21:47:42.973: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 21:47:42.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9510'
Feb 23 21:47:43.054: INFO: stderr: ""
Feb 23 21:47:43.054: INFO: stdout: "update-demo-nautilus-kglkq update-demo-nautilus-ks6hp "
Feb 23 21:47:43.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-kglkq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9510'
Feb 23 21:47:43.112: INFO: stderr: ""
Feb 23 21:47:43.112: INFO: stdout: ""
Feb 23 21:47:43.112: INFO: update-demo-nautilus-kglkq is created but not running
Feb 23 21:47:48.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9510'
Feb 23 21:47:48.164: INFO: stderr: ""
Feb 23 21:47:48.164: INFO: stdout: "update-demo-nautilus-kglkq update-demo-nautilus-ks6hp "
Feb 23 21:47:48.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-kglkq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9510'
Feb 23 21:47:48.223: INFO: stderr: ""
Feb 23 21:47:48.223: INFO: stdout: "true"
Feb 23 21:47:48.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-kglkq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9510'
Feb 23 21:47:48.272: INFO: stderr: ""
Feb 23 21:47:48.272: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 21:47:48.272: INFO: validating pod update-demo-nautilus-kglkq
Feb 23 21:47:48.275: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 21:47:48.275: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 21:47:48.275: INFO: update-demo-nautilus-kglkq is verified up and running
Feb 23 21:47:48.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-ks6hp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9510'
Feb 23 21:47:48.334: INFO: stderr: ""
Feb 23 21:47:48.334: INFO: stdout: "true"
Feb 23 21:47:48.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-ks6hp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9510'
Feb 23 21:47:48.389: INFO: stderr: ""
Feb 23 21:47:48.389: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 21:47:48.389: INFO: validating pod update-demo-nautilus-ks6hp
Feb 23 21:47:48.392: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 21:47:48.392: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 21:47:48.392: INFO: update-demo-nautilus-ks6hp is verified up and running
STEP: scaling down the replication controller
Feb 23 21:47:48.394: INFO: scanned /root for discovery docs: <nil>
Feb 23 21:47:48.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9510'
Feb 23 21:47:49.480: INFO: stderr: ""
Feb 23 21:47:49.480: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 21:47:49.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9510'
Feb 23 21:47:49.557: INFO: stderr: ""
Feb 23 21:47:49.557: INFO: stdout: "update-demo-nautilus-kglkq update-demo-nautilus-ks6hp "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 23 21:47:54.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9510'
Feb 23 21:47:54.612: INFO: stderr: ""
Feb 23 21:47:54.612: INFO: stdout: "update-demo-nautilus-kglkq "
Feb 23 21:47:54.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-kglkq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9510'
Feb 23 21:47:54.676: INFO: stderr: ""
Feb 23 21:47:54.676: INFO: stdout: "true"
Feb 23 21:47:54.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-kglkq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9510'
Feb 23 21:47:54.737: INFO: stderr: ""
Feb 23 21:47:54.737: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 21:47:54.738: INFO: validating pod update-demo-nautilus-kglkq
Feb 23 21:47:54.741: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 21:47:54.741: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 21:47:54.741: INFO: update-demo-nautilus-kglkq is verified up and running
STEP: scaling up the replication controller
Feb 23 21:47:54.744: INFO: scanned /root for discovery docs: <nil>
Feb 23 21:47:54.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9510'
Feb 23 21:47:55.823: INFO: stderr: ""
Feb 23 21:47:55.823: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 23 21:47:55.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9510'
Feb 23 21:47:55.884: INFO: stderr: ""
Feb 23 21:47:55.884: INFO: stdout: "update-demo-nautilus-9bfxt update-demo-nautilus-kglkq "
Feb 23 21:47:55.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-9bfxt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9510'
Feb 23 21:47:55.935: INFO: stderr: ""
Feb 23 21:47:55.935: INFO: stdout: ""
Feb 23 21:47:55.935: INFO: update-demo-nautilus-9bfxt is created but not running
Feb 23 21:48:00.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9510'
Feb 23 21:48:00.992: INFO: stderr: ""
Feb 23 21:48:00.992: INFO: stdout: "update-demo-nautilus-9bfxt update-demo-nautilus-kglkq "
Feb 23 21:48:00.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-9bfxt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9510'
Feb 23 21:48:01.047: INFO: stderr: ""
Feb 23 21:48:01.047: INFO: stdout: "true"
Feb 23 21:48:01.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-9bfxt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9510'
Feb 23 21:48:01.097: INFO: stderr: ""
Feb 23 21:48:01.097: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 21:48:01.097: INFO: validating pod update-demo-nautilus-9bfxt
Feb 23 21:48:01.101: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 21:48:01.101: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 21:48:01.101: INFO: update-demo-nautilus-9bfxt is verified up and running
Feb 23 21:48:01.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-kglkq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9510'
Feb 23 21:48:01.154: INFO: stderr: ""
Feb 23 21:48:01.154: INFO: stdout: "true"
Feb 23 21:48:01.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods update-demo-nautilus-kglkq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9510'
Feb 23 21:48:01.209: INFO: stderr: ""
Feb 23 21:48:01.209: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 23 21:48:01.209: INFO: validating pod update-demo-nautilus-kglkq
Feb 23 21:48:01.212: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 23 21:48:01.212: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 23 21:48:01.212: INFO: update-demo-nautilus-kglkq is verified up and running
STEP: using delete to clean up resources
Feb 23 21:48:01.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete --grace-period=0 --force -f - --namespace=kubectl-9510'
Feb 23 21:48:01.267: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 23 21:48:01.267: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 23 21:48:01.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9510'
Feb 23 21:48:01.334: INFO: stderr: "No resources found in kubectl-9510 namespace.\n"
Feb 23 21:48:01.334: INFO: stdout: ""
Feb 23 21:48:01.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -l name=update-demo --namespace=kubectl-9510 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 21:48:01.394: INFO: stderr: ""
Feb 23 21:48:01.394: INFO: stdout: "update-demo-nautilus-9bfxt\nupdate-demo-nautilus-kglkq\n"
Feb 23 21:48:01.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9510'
Feb 23 21:48:01.959: INFO: stderr: "No resources found in kubectl-9510 namespace.\n"
Feb 23 21:48:01.959: INFO: stdout: ""
Feb 23 21:48:01.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -l name=update-demo --namespace=kubectl-9510 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 21:48:02.026: INFO: stderr: ""
Feb 23 21:48:02.026: INFO: stdout: "update-demo-nautilus-9bfxt\nupdate-demo-nautilus-kglkq\n"
Feb 23 21:48:02.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9510'
Feb 23 21:48:02.463: INFO: stderr: "No resources found in kubectl-9510 namespace.\n"
Feb 23 21:48:02.463: INFO: stdout: ""
Feb 23 21:48:02.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 get pods -l name=update-demo --namespace=kubectl-9510 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 23 21:48:02.530: INFO: stderr: ""
Feb 23 21:48:02.530: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:48:02.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9510" for this suite.

â€¢ [SLOW TEST:19.819 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":280,"completed":263,"skipped":4357,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:48:02.543: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 23 21:48:06.619: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 21:48:06.623: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 21:48:08.623: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 21:48:08.626: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 21:48:10.623: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 21:48:10.627: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 21:48:12.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 21:48:12.627: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 21:48:14.623: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 21:48:14.627: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 21:48:16.623: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 21:48:16.626: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 23 21:48:18.624: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 23 21:48:18.627: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:48:18.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2532" for this suite.

â€¢ [SLOW TEST:16.100 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":280,"completed":264,"skipped":4371,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:48:18.643: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 23 21:48:19.354: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 23 21:48:22.370: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Feb 23 21:48:22.386: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:48:22.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1281" for this suite.
STEP: Destroying namespace "webhook-1281-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":280,"completed":265,"skipped":4372,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:48:22.460: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 23 21:48:24.527: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:48:24.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6190" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":266,"skipped":4383,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:48:24.553: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 23 21:48:24.583: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 23 21:48:24.590: INFO: Waiting for terminating namespaces to be deleted...
Feb 23 21:48:24.593: INFO: 
Logging pods the kubelet thinks is on node worker00 before test
Feb 23 21:48:24.611: INFO: ceph-mgr-94b9dd996-ggbsc from storage started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container ceph-mgr ready: true, restart count 0
Feb 23 21:48:24.611: INFO: csi-rbdplugin-provisioner-7494f65674-7tn5d from storage started at 2020-02-23 20:28:35 +0000 UTC (5 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container csi-snapshotter ready: true, restart count 2
Feb 23 21:48:24.611: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:48:24.611: INFO: csi-rbdplugin-provisioner-7494f65674-2h6xd from storage started at 2020-02-23 20:28:35 +0000 UTC (5 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container csi-attacher ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 21:48:24.611: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container csi-snapshotter ready: true, restart count 2
Feb 23 21:48:24.611: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:48:24.611: INFO: kube-scheduler-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container kube-scheduler ready: true, restart count 2
Feb 23 21:48:24.611: INFO: calico-node-st65h from networking started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 21:48:24.611: INFO: coredns-676544c7b9-j9z7b from kube-system started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container coredns ready: true, restart count 0
Feb 23 21:48:24.611: INFO: calico-kube-controllers-7cd585bcd-vmw47 from networking started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container calico-kube-controllers ready: true, restart count 2
Feb 23 21:48:24.611: INFO: csi-rbdplugin-z2bdm from storage started at 2020-02-23 20:28:35 +0000 UTC (3 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:48:24.611: INFO: csi-cephfsplugin-q68vh from storage started at 2020-02-23 20:28:35 +0000 UTC (3 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container driver-registrar ready: true, restart count 1
Feb 23 21:48:24.611: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:48:24.611: INFO: ceph-mds-worker00-6f479b4486-bdmhh from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container ceph-mds ready: true, restart count 3
Feb 23 21:48:24.611: INFO: ceph-mon-worker00-5cf654d469-bcbdt from storage started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 21:48:24.611: INFO: csi-cephfsplugin-provisioner-6cd7596f75-9hfw2 from storage started at 2020-02-23 20:28:35 +0000 UTC (4 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:48:24.611: INFO: ceph-setup-6j8xr from storage started at 2020-02-23 20:28:36 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container ceph ready: false, restart count 2
Feb 23 21:48:24.611: INFO: csi-cephfsplugin-provisioner-6cd7596f75-tw7n4 from storage started at 2020-02-23 20:28:35 +0000 UTC (4 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container csi-provisioner ready: true, restart count 1
Feb 23 21:48:24.611: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:48:24.611: INFO: csi-rbdplugin-provisioner-7494f65674-ddxmx from storage started at 2020-02-23 20:28:35 +0000 UTC (5 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container csi-attacher ready: true, restart count 1
Feb 23 21:48:24.611: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container csi-snapshotter ready: true, restart count 2
Feb 23 21:48:24.611: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:48:24.611: INFO: metallb-controller-b96bfbbf8-p9lnh from networking started at 2020-02-23 20:28:36 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container controller ready: true, restart count 0
Feb 23 21:48:24.611: INFO: kube-apiserver-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container kube-apiserver ready: true, restart count 1
Feb 23 21:48:24.611: INFO: csi-cephfsplugin-provisioner-6cd7596f75-492rn from storage started at 2020-02-23 20:28:35 +0000 UTC (4 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container csi-cephfsplugin-attacher ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container csi-provisioner ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:48:24.611: INFO: coredns-676544c7b9-q29b9 from kube-system started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container coredns ready: true, restart count 0
Feb 23 21:48:24.611: INFO: ceph-rgw-57cd48f74c-lf65j from storage started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container ceph-rgw ready: true, restart count 0
Feb 23 21:48:24.611: INFO: metallb-speaker-74sbt from networking started at 2020-02-23 20:28:35 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container speaker ready: true, restart count 0
Feb 23 21:48:24.611: INFO: gobetween-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 21:48:24.611: INFO: kube-proxy-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 21:48:24.611: INFO: kube-controller-manager-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container kube-controller-manager ready: true, restart count 2
Feb 23 21:48:24.611: INFO: sonobuoy-systemd-logs-daemon-set-975a2eaddfc64d32-ct4lk from sonobuoy started at 2020-02-23 20:38:34 +0000 UTC (2 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 23 21:48:24.611: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 21:48:24.611: INFO: etcd-worker00 from kube-system started at 2020-02-23 20:26:50 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container etcd ready: true, restart count 0
Feb 23 21:48:24.611: INFO: ceph-osd-worker00-556546b495-tmvn5 from storage started at 2020-02-23 20:28:14 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.611: INFO: 	Container ceph-osd ready: true, restart count 0
Feb 23 21:48:24.611: INFO: 
Logging pods the kubelet thinks is on node worker01 before test
Feb 23 21:48:24.621: INFO: dashboard-metrics-scraper-58475bc987-xsfzf from kube-system started at 2020-02-23 21:45:16 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Feb 23 21:48:24.621: INFO: metallb-speaker-kl257 from networking started at 2020-02-23 21:45:16 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container speaker ready: true, restart count 0
Feb 23 21:48:24.621: INFO: kube-apiserver-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container kube-apiserver ready: true, restart count 0
Feb 23 21:48:24.621: INFO: sonobuoy from sonobuoy started at 2020-02-23 20:38:32 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 23 21:48:24.621: INFO: kubernetes-dashboard-f957cddcb-pfxt5 from kube-system started at 2020-02-23 21:45:16 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 23 21:48:24.621: INFO: kube-scheduler-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container kube-scheduler ready: true, restart count 2
Feb 23 21:48:24.621: INFO: kube-controller-manager-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container kube-controller-manager ready: true, restart count 2
Feb 23 21:48:24.621: INFO: ceph-osd-worker01-67947c799-dl7k6 from storage started at 2020-02-23 21:45:16 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container ceph-osd ready: true, restart count 2
Feb 23 21:48:24.621: INFO: csi-rbdplugin-j5ljp from storage started at 2020-02-23 21:45:23 +0000 UTC (3 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container csi-rbdplugin ready: true, restart count 0
Feb 23 21:48:24.621: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 21:48:24.621: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:48:24.621: INFO: calico-node-wlnml from networking started at 2020-02-23 20:28:13 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container calico-node ready: true, restart count 0
Feb 23 21:48:24.621: INFO: ceph-mon-worker01-bdb694876-fkv82 from storage started at 2020-02-23 21:45:16 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container ceph-mon ready: true, restart count 0
Feb 23 21:48:24.621: INFO: sonobuoy-e2e-job-5674935bca8a45cb from sonobuoy started at 2020-02-23 20:38:33 +0000 UTC (2 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container e2e ready: true, restart count 0
Feb 23 21:48:24.621: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 23 21:48:24.621: INFO: sonobuoy-systemd-logs-daemon-set-975a2eaddfc64d32-jk65g from sonobuoy started at 2020-02-23 20:38:33 +0000 UTC (2 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 23 21:48:24.621: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 23 21:48:24.621: INFO: gobetween-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container gobetween ready: true, restart count 0
Feb 23 21:48:24.621: INFO: kube-proxy-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 23 21:48:24.621: INFO: etcd-worker01 from kube-system started at 2020-02-23 20:27:47 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container etcd ready: true, restart count 1
Feb 23 21:48:24.621: INFO: pod-handle-http-request from container-lifecycle-hook-2532 started at 2020-02-23 21:48:02 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container pod-handle-http-request ready: true, restart count 0
Feb 23 21:48:24.621: INFO: csi-cephfsplugin-9zrbs from storage started at 2020-02-23 21:45:22 +0000 UTC (3 container statuses recorded)
Feb 23 21:48:24.621: INFO: 	Container csi-cephfsplugin ready: true, restart count 0
Feb 23 21:48:24.621: INFO: 	Container driver-registrar ready: true, restart count 0
Feb 23 21:48:24.621: INFO: 	Container liveness-prometheus ready: true, restart count 0
Feb 23 21:48:24.622: INFO: ceph-mds-worker01-7f5fdb58c6-tct54 from storage started at 2020-02-23 21:45:48 +0000 UTC (1 container statuses recorded)
Feb 23 21:48:24.622: INFO: 	Container ceph-mds ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f6265e1a2c65ea], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f6265e1ada641a], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:48:25.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6925" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":280,"completed":267,"skipped":4383,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:48:25.657: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:344
Feb 23 21:48:25.693: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 23 21:49:25.708: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 23 21:49:25.711: INFO: Starting informer...
STEP: Starting pods...
Feb 23 21:49:25.925: INFO: Pod1 is running on worker01. Tainting Node
Feb 23 21:49:28.143: INFO: Pod2 is running on worker01. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Feb 23 21:49:35.979: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 23 21:49:57.699: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:49:57.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-3536" for this suite.

â€¢ [SLOW TEST:92.142 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":280,"completed":268,"skipped":4392,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:49:57.799: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 23 21:49:57.865: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-734 /api/v1/namespaces/watch-734/configmaps/e2e-watch-test-watch-closed 44a1636c-fb05-47b9-84d4-04b1a8aa64bf 33994 0 2020-02-23 21:49:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 23 21:49:57.865: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-734 /api/v1/namespaces/watch-734/configmaps/e2e-watch-test-watch-closed 44a1636c-fb05-47b9-84d4-04b1a8aa64bf 33995 0 2020-02-23 21:49:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 23 21:49:57.878: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-734 /api/v1/namespaces/watch-734/configmaps/e2e-watch-test-watch-closed 44a1636c-fb05-47b9-84d4-04b1a8aa64bf 33996 0 2020-02-23 21:49:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 23 21:49:57.879: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-734 /api/v1/namespaces/watch-734/configmaps/e2e-watch-test-watch-closed 44a1636c-fb05-47b9-84d4-04b1a8aa64bf 33997 0 2020-02-23 21:49:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:49:57.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-734" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":280,"completed":269,"skipped":4395,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:49:57.890: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Feb 23 21:49:57.918: INFO: namespace kubectl-4248
Feb 23 21:49:57.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 create -f - --namespace=kubectl-4248'
Feb 23 21:49:58.072: INFO: stderr: ""
Feb 23 21:49:58.072: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Feb 23 21:49:59.075: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 21:49:59.075: INFO: Found 0 / 1
Feb 23 21:50:00.077: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 21:50:00.077: INFO: Found 0 / 1
Feb 23 21:50:01.076: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 21:50:01.076: INFO: Found 0 / 1
Feb 23 21:50:02.075: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 21:50:02.075: INFO: Found 0 / 1
Feb 23 21:50:03.076: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 21:50:03.076: INFO: Found 0 / 1
Feb 23 21:50:04.075: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 21:50:04.075: INFO: Found 0 / 1
Feb 23 21:50:05.075: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 21:50:05.075: INFO: Found 0 / 1
Feb 23 21:50:06.078: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 21:50:06.078: INFO: Found 0 / 1
Feb 23 21:50:07.079: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 21:50:07.079: INFO: Found 1 / 1
Feb 23 21:50:07.079: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 23 21:50:07.082: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 23 21:50:07.082: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 23 21:50:07.082: INFO: wait on agnhost-master startup in kubectl-4248 
Feb 23 21:50:07.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 logs agnhost-master-wbrv6 agnhost-master --namespace=kubectl-4248'
Feb 23 21:50:07.153: INFO: stderr: ""
Feb 23 21:50:07.153: INFO: stdout: "Paused\n"
STEP: exposing RC
Feb 23 21:50:07.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-4248'
Feb 23 21:50:07.223: INFO: stderr: ""
Feb 23 21:50:07.223: INFO: stdout: "service/rm2 exposed\n"
Feb 23 21:50:07.228: INFO: Service rm2 in namespace kubectl-4248 found.
STEP: exposing service
Feb 23 21:50:09.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-4248'
Feb 23 21:50:09.307: INFO: stderr: ""
Feb 23 21:50:09.307: INFO: stdout: "service/rm3 exposed\n"
Feb 23 21:50:09.313: INFO: Service rm3 in namespace kubectl-4248 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:50:11.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4248" for this suite.

â€¢ [SLOW TEST:13.434 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1295
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":280,"completed":270,"skipped":4400,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:50:11.325: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Feb 23 21:50:13.379: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-892 PodName:pod-sharedvolume-29dc7d8e-a90e-4925-93c5-924690c24ef5 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 23 21:50:13.379: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
Feb 23 21:50:13.426: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:50:13.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-892" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":280,"completed":271,"skipped":4414,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:50:13.434: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1733
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 23 21:50:13.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-5584'
Feb 23 21:50:13.533: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 23 21:50:13.533: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1738
Feb 23 21:50:15.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-277121970 delete deployment e2e-test-httpd-deployment --namespace=kubectl-5584'
Feb 23 21:50:15.608: INFO: stderr: ""
Feb 23 21:50:15.608: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:50:15.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5584" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":280,"completed":272,"skipped":4427,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:50:15.619: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-ca5ef4d5-106f-4b96-8ebd-c782258e4a2b
STEP: Creating a pod to test consume configMaps
Feb 23 21:50:15.686: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-de9e4675-8b70-4c2d-8440-f15a4f1f1eaf" in namespace "projected-2712" to be "success or failure"
Feb 23 21:50:15.693: INFO: Pod "pod-projected-configmaps-de9e4675-8b70-4c2d-8440-f15a4f1f1eaf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.767688ms
Feb 23 21:50:17.697: INFO: Pod "pod-projected-configmaps-de9e4675-8b70-4c2d-8440-f15a4f1f1eaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010916915s
STEP: Saw pod success
Feb 23 21:50:17.697: INFO: Pod "pod-projected-configmaps-de9e4675-8b70-4c2d-8440-f15a4f1f1eaf" satisfied condition "success or failure"
Feb 23 21:50:17.701: INFO: Trying to get logs from node worker01 pod pod-projected-configmaps-de9e4675-8b70-4c2d-8440-f15a4f1f1eaf container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 23 21:50:17.723: INFO: Waiting for pod pod-projected-configmaps-de9e4675-8b70-4c2d-8440-f15a4f1f1eaf to disappear
Feb 23 21:50:17.726: INFO: Pod pod-projected-configmaps-de9e4675-8b70-4c2d-8440-f15a4f1f1eaf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:50:17.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2712" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":280,"completed":273,"skipped":4431,"failed":0}

------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:50:17.735: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-3faef919-ca6d-41e9-b581-8160e7c741a6 in namespace container-probe-8084
Feb 23 21:50:19.777: INFO: Started pod liveness-3faef919-ca6d-41e9-b581-8160e7c741a6 in namespace container-probe-8084
STEP: checking the pod's current state and verifying that restartCount is present
Feb 23 21:50:19.781: INFO: Initial restart count of pod liveness-3faef919-ca6d-41e9-b581-8160e7c741a6 is 0
Feb 23 21:50:35.820: INFO: Restart count of pod container-probe-8084/liveness-3faef919-ca6d-41e9-b581-8160e7c741a6 is now 1 (16.038170575s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:50:35.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8084" for this suite.

â€¢ [SLOW TEST:18.114 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":274,"skipped":4431,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:50:35.849: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3680.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3680.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3680.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3680.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3680.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3680.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3680.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3680.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3680.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3680.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 23 21:50:39.917: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:39.922: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:39.925: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:39.928: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:39.936: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:39.939: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:39.941: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:39.944: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:39.950: INFO: Lookups using dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3680.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3680.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local jessie_udp@dns-test-service-2.dns-3680.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3680.svc.cluster.local]

Feb 23 21:50:44.953: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:44.956: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:44.959: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:44.962: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:44.969: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:44.971: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:44.974: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:44.977: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:44.983: INFO: Lookups using dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3680.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3680.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local jessie_udp@dns-test-service-2.dns-3680.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3680.svc.cluster.local]

Feb 23 21:50:49.954: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:49.956: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:49.959: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:49.964: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:49.971: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:49.974: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:49.977: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:49.979: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:49.984: INFO: Lookups using dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3680.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3680.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local jessie_udp@dns-test-service-2.dns-3680.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3680.svc.cluster.local]

Feb 23 21:50:54.953: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:54.956: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:54.960: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:54.963: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:54.970: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:54.972: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:54.983: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:54.986: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:54.994: INFO: Lookups using dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3680.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3680.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local jessie_udp@dns-test-service-2.dns-3680.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3680.svc.cluster.local]

Feb 23 21:50:59.955: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:59.958: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:59.961: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:59.964: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:59.970: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:59.973: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:59.975: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:59.978: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:50:59.983: INFO: Lookups using dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3680.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3680.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local jessie_udp@dns-test-service-2.dns-3680.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3680.svc.cluster.local]

Feb 23 21:51:04.953: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:51:04.956: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:51:04.959: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:51:04.962: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:51:04.970: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:51:04.974: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:51:04.979: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:51:04.983: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3680.svc.cluster.local from pod dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d: the server could not find the requested resource (get pods dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d)
Feb 23 21:51:04.988: INFO: Lookups using dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3680.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3680.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3680.svc.cluster.local jessie_udp@dns-test-service-2.dns-3680.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3680.svc.cluster.local]

Feb 23 21:51:09.979: INFO: DNS probes using dns-3680/dns-test-21a09acf-5e6a-416b-a324-4a5af7af4b7d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:51:10.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3680" for this suite.

â€¢ [SLOW TEST:34.206 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":280,"completed":275,"skipped":4473,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:51:10.058: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Feb 23 21:51:10.104: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Feb 23 21:51:10.927: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb 23 21:51:12.974: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 21:51:14.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 21:51:16.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 21:51:18.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 21:51:20.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 21:51:22.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 21:51:24.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 21:51:26.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 21:51:28.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 21:51:30.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63718091470, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 23 21:51:33.610: INFO: Waited 618.544732ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:51:34.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7154" for this suite.

â€¢ [SLOW TEST:24.697 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":280,"completed":276,"skipped":4482,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:51:34.755: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Feb 23 21:51:34.799: INFO: Waiting up to 5m0s for pod "var-expansion-7cef400d-d702-4d15-ba5f-8462d1faf1bc" in namespace "var-expansion-7720" to be "success or failure"
Feb 23 21:51:34.802: INFO: Pod "var-expansion-7cef400d-d702-4d15-ba5f-8462d1faf1bc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.240807ms
Feb 23 21:51:36.805: INFO: Pod "var-expansion-7cef400d-d702-4d15-ba5f-8462d1faf1bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005857493s
STEP: Saw pod success
Feb 23 21:51:36.805: INFO: Pod "var-expansion-7cef400d-d702-4d15-ba5f-8462d1faf1bc" satisfied condition "success or failure"
Feb 23 21:51:36.808: INFO: Trying to get logs from node worker01 pod var-expansion-7cef400d-d702-4d15-ba5f-8462d1faf1bc container dapi-container: <nil>
STEP: delete the pod
Feb 23 21:51:36.823: INFO: Waiting for pod var-expansion-7cef400d-d702-4d15-ba5f-8462d1faf1bc to disappear
Feb 23 21:51:36.828: INFO: Pod var-expansion-7cef400d-d702-4d15-ba5f-8462d1faf1bc no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:51:36.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7720" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":280,"completed":277,"skipped":4495,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:51:36.836: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 23 21:51:40.905: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 23 21:51:40.908: INFO: Pod pod-with-prestop-http-hook still exists
Feb 23 21:51:42.910: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 23 21:51:42.912: INFO: Pod pod-with-prestop-http-hook still exists
Feb 23 21:51:44.910: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 23 21:51:44.914: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:51:44.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2300" for this suite.

â€¢ [SLOW TEST:8.090 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":280,"completed":278,"skipped":4521,"failed":0}
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:51:44.926: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-q2cd9 in namespace proxy-2209
I0223 21:51:44.973426      23 runners.go:189] Created replication controller with name: proxy-service-q2cd9, namespace: proxy-2209, replica count: 1
I0223 21:51:46.023886      23 runners.go:189] proxy-service-q2cd9 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0223 21:51:47.027284      23 runners.go:189] proxy-service-q2cd9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0223 21:51:48.030070      23 runners.go:189] proxy-service-q2cd9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0223 21:51:49.030279      23 runners.go:189] proxy-service-q2cd9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0223 21:51:50.030569      23 runners.go:189] proxy-service-q2cd9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0223 21:51:51.030814      23 runners.go:189] proxy-service-q2cd9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0223 21:51:52.030900      23 runners.go:189] proxy-service-q2cd9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0223 21:51:53.031211      23 runners.go:189] proxy-service-q2cd9 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 23 21:51:53.033: INFO: setup took 8.073991523s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 23 21:51:53.039: INFO: (0) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 5.782624ms)
Feb 23 21:51:53.039: INFO: (0) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 5.702462ms)
Feb 23 21:51:53.039: INFO: (0) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 5.798882ms)
Feb 23 21:51:53.041: INFO: (0) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 7.686292ms)
Feb 23 21:51:53.041: INFO: (0) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 7.507873ms)
Feb 23 21:51:53.043: INFO: (0) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 9.831659ms)
Feb 23 21:51:53.044: INFO: (0) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 10.645285ms)
Feb 23 21:51:53.045: INFO: (0) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 11.774005ms)
Feb 23 21:51:53.049: INFO: (0) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 15.962256ms)
Feb 23 21:51:53.050: INFO: (0) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 16.717559ms)
Feb 23 21:51:53.050: INFO: (0) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 16.948782ms)
Feb 23 21:51:53.050: INFO: (0) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 16.944028ms)
Feb 23 21:51:53.050: INFO: (0) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 17.088825ms)
Feb 23 21:51:53.051: INFO: (0) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 17.897103ms)
Feb 23 21:51:53.052: INFO: (0) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 18.981808ms)
Feb 23 21:51:53.053: INFO: (0) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 19.278626ms)
Feb 23 21:51:53.057: INFO: (1) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 3.590799ms)
Feb 23 21:51:53.058: INFO: (1) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 4.476397ms)
Feb 23 21:51:53.059: INFO: (1) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 5.816352ms)
Feb 23 21:51:53.059: INFO: (1) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 5.182594ms)
Feb 23 21:51:53.059: INFO: (1) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 4.479522ms)
Feb 23 21:51:53.059: INFO: (1) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 5.150793ms)
Feb 23 21:51:53.062: INFO: (1) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 8.077922ms)
Feb 23 21:51:53.063: INFO: (1) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 8.080718ms)
Feb 23 21:51:53.063: INFO: (1) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 8.836229ms)
Feb 23 21:51:53.063: INFO: (1) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 9.450869ms)
Feb 23 21:51:53.064: INFO: (1) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 11.345473ms)
Feb 23 21:51:53.066: INFO: (1) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 10.876819ms)
Feb 23 21:51:53.066: INFO: (1) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 11.964009ms)
Feb 23 21:51:53.066: INFO: (1) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 11.22041ms)
Feb 23 21:51:53.067: INFO: (1) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 12.048207ms)
Feb 23 21:51:53.068: INFO: (1) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 13.275163ms)
Feb 23 21:51:53.073: INFO: (2) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 5.33269ms)
Feb 23 21:51:53.074: INFO: (2) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 5.710077ms)
Feb 23 21:51:53.074: INFO: (2) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 6.06694ms)
Feb 23 21:51:53.074: INFO: (2) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 6.319395ms)
Feb 23 21:51:53.075: INFO: (2) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 6.212919ms)
Feb 23 21:51:53.075: INFO: (2) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 6.284698ms)
Feb 23 21:51:53.077: INFO: (2) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 9.041121ms)
Feb 23 21:51:53.077: INFO: (2) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 9.04424ms)
Feb 23 21:51:53.077: INFO: (2) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 9.259132ms)
Feb 23 21:51:53.078: INFO: (2) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 9.176634ms)
Feb 23 21:51:53.078: INFO: (2) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 9.182773ms)
Feb 23 21:51:53.078: INFO: (2) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 10.088548ms)
Feb 23 21:51:53.080: INFO: (2) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 11.826478ms)
Feb 23 21:51:53.081: INFO: (2) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 13.029219ms)
Feb 23 21:51:53.081: INFO: (2) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 12.711297ms)
Feb 23 21:51:53.081: INFO: (2) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 13.10591ms)
Feb 23 21:51:53.086: INFO: (3) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 4.669863ms)
Feb 23 21:51:53.087: INFO: (3) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 5.967345ms)
Feb 23 21:51:53.091: INFO: (3) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 9.139408ms)
Feb 23 21:51:53.091: INFO: (3) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 8.967445ms)
Feb 23 21:51:53.091: INFO: (3) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 9.143654ms)
Feb 23 21:51:53.091: INFO: (3) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 9.138298ms)
Feb 23 21:51:53.091: INFO: (3) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 9.029314ms)
Feb 23 21:51:53.091: INFO: (3) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 9.315253ms)
Feb 23 21:51:53.091: INFO: (3) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 9.13222ms)
Feb 23 21:51:53.091: INFO: (3) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 9.305145ms)
Feb 23 21:51:53.092: INFO: (3) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 10.559888ms)
Feb 23 21:51:53.093: INFO: (3) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 11.467667ms)
Feb 23 21:51:53.094: INFO: (3) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 11.710662ms)
Feb 23 21:51:53.094: INFO: (3) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 12.512308ms)
Feb 23 21:51:53.095: INFO: (3) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 13.621254ms)
Feb 23 21:51:53.095: INFO: (3) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 13.566727ms)
Feb 23 21:51:53.101: INFO: (4) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 4.843978ms)
Feb 23 21:51:53.103: INFO: (4) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 7.432824ms)
Feb 23 21:51:53.103: INFO: (4) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 7.255078ms)
Feb 23 21:51:53.103: INFO: (4) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 7.5558ms)
Feb 23 21:51:53.103: INFO: (4) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 7.42865ms)
Feb 23 21:51:53.103: INFO: (4) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 7.51198ms)
Feb 23 21:51:53.103: INFO: (4) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 7.624407ms)
Feb 23 21:51:53.104: INFO: (4) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 7.603756ms)
Feb 23 21:51:53.104: INFO: (4) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 7.925772ms)
Feb 23 21:51:53.106: INFO: (4) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 10.025472ms)
Feb 23 21:51:53.106: INFO: (4) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 9.592971ms)
Feb 23 21:51:53.106: INFO: (4) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 9.646332ms)
Feb 23 21:51:53.106: INFO: (4) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 10.079603ms)
Feb 23 21:51:53.106: INFO: (4) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 10.459977ms)
Feb 23 21:51:53.109: INFO: (4) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 13.510114ms)
Feb 23 21:51:53.109: INFO: (4) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 13.64541ms)
Feb 23 21:51:53.117: INFO: (5) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 7.26245ms)
Feb 23 21:51:53.117: INFO: (5) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 7.625703ms)
Feb 23 21:51:53.117: INFO: (5) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 7.781821ms)
Feb 23 21:51:53.118: INFO: (5) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 7.851305ms)
Feb 23 21:51:53.120: INFO: (5) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 9.625482ms)
Feb 23 21:51:53.120: INFO: (5) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 10.02976ms)
Feb 23 21:51:53.121: INFO: (5) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 10.42634ms)
Feb 23 21:51:53.121: INFO: (5) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 10.507562ms)
Feb 23 21:51:53.121: INFO: (5) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 10.454013ms)
Feb 23 21:51:53.122: INFO: (5) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 12.222974ms)
Feb 23 21:51:53.122: INFO: (5) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 11.420502ms)
Feb 23 21:51:53.122: INFO: (5) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 11.819473ms)
Feb 23 21:51:53.122: INFO: (5) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 11.823429ms)
Feb 23 21:51:53.125: INFO: (5) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 15.079686ms)
Feb 23 21:51:53.125: INFO: (5) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 14.516375ms)
Feb 23 21:51:53.125: INFO: (5) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 15.475654ms)
Feb 23 21:51:53.130: INFO: (6) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 4.209656ms)
Feb 23 21:51:53.132: INFO: (6) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 4.836301ms)
Feb 23 21:51:53.134: INFO: (6) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 7.270404ms)
Feb 23 21:51:53.135: INFO: (6) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 7.678978ms)
Feb 23 21:51:53.135: INFO: (6) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 8.283781ms)
Feb 23 21:51:53.135: INFO: (6) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 8.361198ms)
Feb 23 21:51:53.136: INFO: (6) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 8.956613ms)
Feb 23 21:51:53.136: INFO: (6) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 9.293524ms)
Feb 23 21:51:53.136: INFO: (6) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 9.40839ms)
Feb 23 21:51:53.136: INFO: (6) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 9.526461ms)
Feb 23 21:51:53.136: INFO: (6) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 9.299659ms)
Feb 23 21:51:53.138: INFO: (6) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 11.309258ms)
Feb 23 21:51:53.138: INFO: (6) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 10.963146ms)
Feb 23 21:51:53.139: INFO: (6) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 11.865657ms)
Feb 23 21:51:53.141: INFO: (6) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 15.49444ms)
Feb 23 21:51:53.141: INFO: (6) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 13.982883ms)
Feb 23 21:51:53.148: INFO: (7) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 6.981974ms)
Feb 23 21:51:53.148: INFO: (7) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 6.509501ms)
Feb 23 21:51:53.148: INFO: (7) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 6.685759ms)
Feb 23 21:51:53.149: INFO: (7) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 6.976893ms)
Feb 23 21:51:53.149: INFO: (7) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 7.218343ms)
Feb 23 21:51:53.151: INFO: (7) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 9.503968ms)
Feb 23 21:51:53.151: INFO: (7) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 9.455863ms)
Feb 23 21:51:53.151: INFO: (7) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 9.865175ms)
Feb 23 21:51:53.152: INFO: (7) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 10.867069ms)
Feb 23 21:51:53.152: INFO: (7) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 10.224528ms)
Feb 23 21:51:53.152: INFO: (7) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 10.471353ms)
Feb 23 21:51:53.152: INFO: (7) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 10.540531ms)
Feb 23 21:51:53.152: INFO: (7) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 10.535431ms)
Feb 23 21:51:53.153: INFO: (7) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 11.852217ms)
Feb 23 21:51:53.154: INFO: (7) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 12.547815ms)
Feb 23 21:51:53.155: INFO: (7) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 12.835115ms)
Feb 23 21:51:53.160: INFO: (8) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 4.784098ms)
Feb 23 21:51:53.162: INFO: (8) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 7.241696ms)
Feb 23 21:51:53.162: INFO: (8) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 7.581717ms)
Feb 23 21:51:53.163: INFO: (8) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 7.703399ms)
Feb 23 21:51:53.163: INFO: (8) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 7.718695ms)
Feb 23 21:51:53.163: INFO: (8) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 7.667512ms)
Feb 23 21:51:53.163: INFO: (8) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 8.177026ms)
Feb 23 21:51:53.164: INFO: (8) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 9.127297ms)
Feb 23 21:51:53.164: INFO: (8) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 9.189121ms)
Feb 23 21:51:53.164: INFO: (8) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 9.083119ms)
Feb 23 21:51:53.165: INFO: (8) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 9.53143ms)
Feb 23 21:51:53.166: INFO: (8) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 11.309872ms)
Feb 23 21:51:53.167: INFO: (8) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 11.841505ms)
Feb 23 21:51:53.168: INFO: (8) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 12.993704ms)
Feb 23 21:51:53.169: INFO: (8) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 14.232801ms)
Feb 23 21:51:53.169: INFO: (8) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 14.300249ms)
Feb 23 21:51:53.176: INFO: (9) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 6.147032ms)
Feb 23 21:51:53.177: INFO: (9) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 6.65667ms)
Feb 23 21:51:53.177: INFO: (9) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 6.96296ms)
Feb 23 21:51:53.179: INFO: (9) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 8.739231ms)
Feb 23 21:51:53.179: INFO: (9) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 8.862223ms)
Feb 23 21:51:53.179: INFO: (9) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 8.676208ms)
Feb 23 21:51:53.179: INFO: (9) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 9.113039ms)
Feb 23 21:51:53.179: INFO: (9) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 9.152106ms)
Feb 23 21:51:53.179: INFO: (9) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 9.488686ms)
Feb 23 21:51:53.179: INFO: (9) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 9.045754ms)
Feb 23 21:51:53.179: INFO: (9) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 9.229416ms)
Feb 23 21:51:53.180: INFO: (9) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 9.215186ms)
Feb 23 21:51:53.181: INFO: (9) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 10.410945ms)
Feb 23 21:51:53.182: INFO: (9) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 11.569714ms)
Feb 23 21:51:53.183: INFO: (9) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 12.982069ms)
Feb 23 21:51:53.185: INFO: (9) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 14.459149ms)
Feb 23 21:51:53.191: INFO: (10) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 6.057931ms)
Feb 23 21:51:53.191: INFO: (10) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 6.523562ms)
Feb 23 21:51:53.192: INFO: (10) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 6.56903ms)
Feb 23 21:51:53.192: INFO: (10) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 6.531439ms)
Feb 23 21:51:53.192: INFO: (10) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 6.601363ms)
Feb 23 21:51:53.192: INFO: (10) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 6.25568ms)
Feb 23 21:51:53.192: INFO: (10) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 6.601212ms)
Feb 23 21:51:53.192: INFO: (10) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 6.54072ms)
Feb 23 21:51:53.192: INFO: (10) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 7.063252ms)
Feb 23 21:51:53.193: INFO: (10) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 8.237616ms)
Feb 23 21:51:53.193: INFO: (10) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 8.114792ms)
Feb 23 21:51:53.194: INFO: (10) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 8.593777ms)
Feb 23 21:51:53.196: INFO: (10) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 10.615376ms)
Feb 23 21:51:53.196: INFO: (10) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 10.809319ms)
Feb 23 21:51:53.196: INFO: (10) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 10.731307ms)
Feb 23 21:51:53.198: INFO: (10) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 12.46561ms)
Feb 23 21:51:53.202: INFO: (11) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 4.838647ms)
Feb 23 21:51:53.206: INFO: (11) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 8.206616ms)
Feb 23 21:51:53.206: INFO: (11) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 8.2464ms)
Feb 23 21:51:53.206: INFO: (11) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 8.208469ms)
Feb 23 21:51:53.206: INFO: (11) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 8.244959ms)
Feb 23 21:51:53.206: INFO: (11) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 8.238939ms)
Feb 23 21:51:53.206: INFO: (11) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 8.375307ms)
Feb 23 21:51:53.206: INFO: (11) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 8.39391ms)
Feb 23 21:51:53.206: INFO: (11) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 8.289993ms)
Feb 23 21:51:53.206: INFO: (11) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 8.42745ms)
Feb 23 21:51:53.206: INFO: (11) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 8.402756ms)
Feb 23 21:51:53.207: INFO: (11) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 9.521179ms)
Feb 23 21:51:53.207: INFO: (11) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 9.72401ms)
Feb 23 21:51:53.209: INFO: (11) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 10.930147ms)
Feb 23 21:51:53.210: INFO: (11) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 11.915863ms)
Feb 23 21:51:53.210: INFO: (11) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 11.886418ms)
Feb 23 21:51:53.216: INFO: (12) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 6.442794ms)
Feb 23 21:51:53.217: INFO: (12) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 6.626022ms)
Feb 23 21:51:53.218: INFO: (12) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 7.657445ms)
Feb 23 21:51:53.221: INFO: (12) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 11.31213ms)
Feb 23 21:51:53.221: INFO: (12) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 11.351457ms)
Feb 23 21:51:53.221: INFO: (12) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 11.215075ms)
Feb 23 21:51:53.221: INFO: (12) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 11.050975ms)
Feb 23 21:51:53.221: INFO: (12) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 11.032303ms)
Feb 23 21:51:53.221: INFO: (12) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 11.390473ms)
Feb 23 21:51:53.221: INFO: (12) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 11.398176ms)
Feb 23 21:51:53.221: INFO: (12) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 11.028801ms)
Feb 23 21:51:53.222: INFO: (12) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 11.65798ms)
Feb 23 21:51:53.222: INFO: (12) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 12.272355ms)
Feb 23 21:51:53.222: INFO: (12) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 12.438226ms)
Feb 23 21:51:53.223: INFO: (12) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 12.580346ms)
Feb 23 21:51:53.224: INFO: (12) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 13.803813ms)
Feb 23 21:51:53.230: INFO: (13) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 5.953963ms)
Feb 23 21:51:53.230: INFO: (13) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 6.129022ms)
Feb 23 21:51:53.230: INFO: (13) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 6.172766ms)
Feb 23 21:51:53.230: INFO: (13) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 6.281558ms)
Feb 23 21:51:53.231: INFO: (13) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 6.869273ms)
Feb 23 21:51:53.231: INFO: (13) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 7.340217ms)
Feb 23 21:51:53.231: INFO: (13) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 7.157423ms)
Feb 23 21:51:53.231: INFO: (13) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 7.011098ms)
Feb 23 21:51:53.231: INFO: (13) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 7.166577ms)
Feb 23 21:51:53.231: INFO: (13) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 7.392476ms)
Feb 23 21:51:53.233: INFO: (13) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 9.223814ms)
Feb 23 21:51:53.234: INFO: (13) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 10.209194ms)
Feb 23 21:51:53.235: INFO: (13) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 10.572712ms)
Feb 23 21:51:53.235: INFO: (13) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 10.962163ms)
Feb 23 21:51:53.235: INFO: (13) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 11.068044ms)
Feb 23 21:51:53.235: INFO: (13) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 11.151922ms)
Feb 23 21:51:53.240: INFO: (14) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 4.437831ms)
Feb 23 21:51:53.241: INFO: (14) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 5.10378ms)
Feb 23 21:51:53.243: INFO: (14) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 6.707807ms)
Feb 23 21:51:53.244: INFO: (14) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 8.040872ms)
Feb 23 21:51:53.244: INFO: (14) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 8.410469ms)
Feb 23 21:51:53.244: INFO: (14) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 8.258818ms)
Feb 23 21:51:53.244: INFO: (14) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 8.075499ms)
Feb 23 21:51:53.244: INFO: (14) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 7.915165ms)
Feb 23 21:51:53.244: INFO: (14) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 8.065533ms)
Feb 23 21:51:53.244: INFO: (14) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 7.937997ms)
Feb 23 21:51:53.245: INFO: (14) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 8.994588ms)
Feb 23 21:51:53.246: INFO: (14) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 9.901536ms)
Feb 23 21:51:53.246: INFO: (14) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 10.014254ms)
Feb 23 21:51:53.248: INFO: (14) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 12.219168ms)
Feb 23 21:51:53.248: INFO: (14) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 11.997744ms)
Feb 23 21:51:53.248: INFO: (14) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 12.395231ms)
Feb 23 21:51:53.255: INFO: (15) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 5.979498ms)
Feb 23 21:51:53.255: INFO: (15) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 6.830542ms)
Feb 23 21:51:53.255: INFO: (15) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 6.403963ms)
Feb 23 21:51:53.255: INFO: (15) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 5.978059ms)
Feb 23 21:51:53.255: INFO: (15) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 6.244872ms)
Feb 23 21:51:53.255: INFO: (15) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 6.382851ms)
Feb 23 21:51:53.255: INFO: (15) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 6.724591ms)
Feb 23 21:51:53.255: INFO: (15) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 6.613506ms)
Feb 23 21:51:53.255: INFO: (15) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 6.488682ms)
Feb 23 21:51:53.255: INFO: (15) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 6.113286ms)
Feb 23 21:51:53.257: INFO: (15) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 8.531231ms)
Feb 23 21:51:53.258: INFO: (15) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 9.33547ms)
Feb 23 21:51:53.258: INFO: (15) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 9.502348ms)
Feb 23 21:51:53.258: INFO: (15) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 8.858423ms)
Feb 23 21:51:53.259: INFO: (15) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 10.252834ms)
Feb 23 21:51:53.260: INFO: (15) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 11.041695ms)
Feb 23 21:51:53.269: INFO: (16) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 6.188597ms)
Feb 23 21:51:53.269: INFO: (16) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 8.249ms)
Feb 23 21:51:53.269: INFO: (16) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 6.468331ms)
Feb 23 21:51:53.269: INFO: (16) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 7.917055ms)
Feb 23 21:51:53.269: INFO: (16) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 7.62686ms)
Feb 23 21:51:53.269: INFO: (16) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 6.633ms)
Feb 23 21:51:53.272: INFO: (16) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 9.190785ms)
Feb 23 21:51:53.272: INFO: (16) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 9.288445ms)
Feb 23 21:51:53.272: INFO: (16) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 9.512242ms)
Feb 23 21:51:53.273: INFO: (16) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 11.116551ms)
Feb 23 21:51:53.273: INFO: (16) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 12.050933ms)
Feb 23 21:51:53.273: INFO: (16) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 12.4877ms)
Feb 23 21:51:53.275: INFO: (16) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 13.429207ms)
Feb 23 21:51:53.275: INFO: (16) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 14.448935ms)
Feb 23 21:51:53.275: INFO: (16) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 12.556251ms)
Feb 23 21:51:53.276: INFO: (16) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 13.378995ms)
Feb 23 21:51:53.281: INFO: (17) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 5.309253ms)
Feb 23 21:51:53.283: INFO: (17) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 7.451217ms)
Feb 23 21:51:53.284: INFO: (17) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 7.290453ms)
Feb 23 21:51:53.284: INFO: (17) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 7.535569ms)
Feb 23 21:51:53.285: INFO: (17) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 8.725562ms)
Feb 23 21:51:53.285: INFO: (17) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 8.796232ms)
Feb 23 21:51:53.286: INFO: (17) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 10.073468ms)
Feb 23 21:51:53.286: INFO: (17) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 9.850044ms)
Feb 23 21:51:53.286: INFO: (17) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 10.121259ms)
Feb 23 21:51:53.286: INFO: (17) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 9.851443ms)
Feb 23 21:51:53.287: INFO: (17) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 10.872867ms)
Feb 23 21:51:53.289: INFO: (17) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 12.744953ms)
Feb 23 21:51:53.291: INFO: (17) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 14.161891ms)
Feb 23 21:51:53.291: INFO: (17) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 14.695318ms)
Feb 23 21:51:53.290: INFO: (17) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 14.711246ms)
Feb 23 21:51:53.290: INFO: (17) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 14.000475ms)
Feb 23 21:51:53.298: INFO: (18) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 6.635138ms)
Feb 23 21:51:53.299: INFO: (18) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 7.916334ms)
Feb 23 21:51:53.299: INFO: (18) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 7.613048ms)
Feb 23 21:51:53.299: INFO: (18) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 8.147259ms)
Feb 23 21:51:53.300: INFO: (18) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 8.264298ms)
Feb 23 21:51:53.300: INFO: (18) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 8.347846ms)
Feb 23 21:51:53.300: INFO: (18) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 9.487128ms)
Feb 23 21:51:53.300: INFO: (18) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 9.218116ms)
Feb 23 21:51:53.300: INFO: (18) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 9.228773ms)
Feb 23 21:51:53.300: INFO: (18) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 9.315487ms)
Feb 23 21:51:53.300: INFO: (18) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 9.214064ms)
Feb 23 21:51:53.302: INFO: (18) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 10.88066ms)
Feb 23 21:51:53.302: INFO: (18) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 11.186407ms)
Feb 23 21:51:53.304: INFO: (18) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 13.089618ms)
Feb 23 21:51:53.304: INFO: (18) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 13.117702ms)
Feb 23 21:51:53.304: INFO: (18) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 12.581799ms)
Feb 23 21:51:53.310: INFO: (19) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:460/proxy/: tls baz (200; 5.057924ms)
Feb 23 21:51:53.314: INFO: (19) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 9.113185ms)
Feb 23 21:51:53.314: INFO: (19) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">... (200; 8.931876ms)
Feb 23 21:51:53.314: INFO: (19) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4/proxy/rewriteme">test</a> (200; 8.942064ms)
Feb 23 21:51:53.314: INFO: (19) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:162/proxy/: bar (200; 9.363376ms)
Feb 23 21:51:53.314: INFO: (19) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:443/proxy/tlsrewritem... (200; 9.465374ms)
Feb 23 21:51:53.314: INFO: (19) /api/v1/namespaces/proxy-2209/pods/http:proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 9.322471ms)
Feb 23 21:51:53.315: INFO: (19) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname1/proxy/: tls baz (200; 9.411659ms)
Feb 23 21:51:53.315: INFO: (19) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:1080/proxy/rewriteme">test<... (200; 9.365229ms)
Feb 23 21:51:53.315: INFO: (19) /api/v1/namespaces/proxy-2209/pods/proxy-service-q2cd9-6pjx4:160/proxy/: foo (200; 10.701467ms)
Feb 23 21:51:53.315: INFO: (19) /api/v1/namespaces/proxy-2209/pods/https:proxy-service-q2cd9-6pjx4:462/proxy/: tls qux (200; 10.010157ms)
Feb 23 21:51:53.318: INFO: (19) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname1/proxy/: foo (200; 13.471602ms)
Feb 23 21:51:53.323: INFO: (19) /api/v1/namespaces/proxy-2209/services/https:proxy-service-q2cd9:tlsportname2/proxy/: tls qux (200; 17.392412ms)
Feb 23 21:51:53.323: INFO: (19) /api/v1/namespaces/proxy-2209/services/http:proxy-service-q2cd9:portname2/proxy/: bar (200; 17.369269ms)
Feb 23 21:51:53.323: INFO: (19) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname1/proxy/: foo (200; 17.429679ms)
Feb 23 21:51:53.323: INFO: (19) /api/v1/namespaces/proxy-2209/services/proxy-service-q2cd9:portname2/proxy/: bar (200; 17.369029ms)
STEP: deleting ReplicationController proxy-service-q2cd9 in namespace proxy-2209, will wait for the garbage collector to delete the pods
Feb 23 21:51:53.382: INFO: Deleting ReplicationController proxy-service-q2cd9 took: 6.890511ms
Feb 23 21:51:54.282: INFO: Terminating ReplicationController proxy-service-q2cd9 pods took: 900.120927ms
[AfterEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:51:56.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2209" for this suite.

â€¢ [SLOW TEST:11.376 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":280,"completed":279,"skipped":4524,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 23 21:51:56.303: INFO: >>> kubeConfig: /tmp/kubeconfig-277121970
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 23 21:51:56.339: INFO: Waiting up to 5m0s for pod "downward-api-ce4377cd-1203-4db3-876f-825d5e1e226b" in namespace "downward-api-4637" to be "success or failure"
Feb 23 21:51:56.343: INFO: Pod "downward-api-ce4377cd-1203-4db3-876f-825d5e1e226b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.940443ms
Feb 23 21:51:58.345: INFO: Pod "downward-api-ce4377cd-1203-4db3-876f-825d5e1e226b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006535963s
STEP: Saw pod success
Feb 23 21:51:58.345: INFO: Pod "downward-api-ce4377cd-1203-4db3-876f-825d5e1e226b" satisfied condition "success or failure"
Feb 23 21:51:58.348: INFO: Trying to get logs from node worker01 pod downward-api-ce4377cd-1203-4db3-876f-825d5e1e226b container dapi-container: <nil>
STEP: delete the pod
Feb 23 21:51:58.369: INFO: Waiting for pod downward-api-ce4377cd-1203-4db3-876f-825d5e1e226b to disappear
Feb 23 21:51:58.372: INFO: Pod downward-api-ce4377cd-1203-4db3-876f-825d5e1e226b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 23 21:51:58.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4637" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":280,"completed":280,"skipped":4529,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSFeb 23 21:51:58.383: INFO: Running AfterSuite actions on all nodes
Feb 23 21:51:58.383: INFO: Running AfterSuite actions on node 1
Feb 23 21:51:58.383: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":280,"completed":280,"skipped":4563,"failed":0}

Ran 280 of 4843 Specs in 4401.347 seconds
SUCCESS! -- 280 Passed | 0 Failed | 0 Pending | 4563 Skipped
PASS

Ginkgo ran 1 suite in 1h13m22.352868915s
Test Suite Passed
