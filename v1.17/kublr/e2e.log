I0217 18:03:39.021983      26 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-713147157
I0217 18:03:39.022003      26 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0217 18:03:39.022089      26 e2e.go:109] Starting e2e run "4311e4a8-a409-4e97-b195-a6af62106842" on Ginkgo node 1
{"msg":"Test Suite starting","total":278,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1581962617 - Will randomize all specs
Will run 278 of 4843 specs

Feb 17 18:03:39.077: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:03:39.079: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 17 18:03:39.099: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 17 18:03:39.145: INFO: 51 / 51 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 17 18:03:39.145: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Feb 17 18:03:39.145: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 17 18:03:39.155: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Feb 17 18:03:39.155: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Feb 17 18:03:39.155: INFO: e2e test version: v1.17.3
Feb 17 18:03:39.156: INFO: kube-apiserver version: v1.17.3
Feb 17 18:03:39.156: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:03:39.162: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:03:39.162: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename resourcequota
Feb 17 18:03:39.209: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Feb 17 18:03:39.224: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3212
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:03:50.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3212" for this suite.

• [SLOW TEST:11.252 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":278,"completed":1,"skipped":24,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:03:50.414: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2037
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-ca15a862-8485-41dd-92e0-1b51149c0fd6
STEP: Creating a pod to test consume secrets
Feb 17 18:03:50.590: INFO: Waiting up to 5m0s for pod "pod-secrets-bdd1de77-36d6-4981-904c-9be6f6d7e7b2" in namespace "secrets-2037" to be "success or failure"
Feb 17 18:03:50.597: INFO: Pod "pod-secrets-bdd1de77-36d6-4981-904c-9be6f6d7e7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.30039ms
Feb 17 18:03:52.601: INFO: Pod "pod-secrets-bdd1de77-36d6-4981-904c-9be6f6d7e7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01064553s
Feb 17 18:03:54.606: INFO: Pod "pod-secrets-bdd1de77-36d6-4981-904c-9be6f6d7e7b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016119441s
STEP: Saw pod success
Feb 17 18:03:54.606: INFO: Pod "pod-secrets-bdd1de77-36d6-4981-904c-9be6f6d7e7b2" satisfied condition "success or failure"
Feb 17 18:03:54.610: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-secrets-bdd1de77-36d6-4981-904c-9be6f6d7e7b2 container secret-env-test: <nil>
STEP: delete the pod
Feb 17 18:03:54.649: INFO: Waiting for pod pod-secrets-bdd1de77-36d6-4981-904c-9be6f6d7e7b2 to disappear
Feb 17 18:03:54.653: INFO: Pod pod-secrets-bdd1de77-36d6-4981-904c-9be6f6d7e7b2 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:03:54.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2037" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":278,"completed":2,"skipped":28,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:03:54.666: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7019
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7019.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7019.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7019.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7019.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7019.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7019.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7019.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7019.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7019.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7019.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 18:04:06.868: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:06.873: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:06.877: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:06.882: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:06.895: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:06.901: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:06.905: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:06.909: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:06.919: INFO: Lookups using dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7019.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7019.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local jessie_udp@dns-test-service-2.dns-7019.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7019.svc.cluster.local]

Feb 17 18:04:11.925: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:11.931: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:11.935: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:11.939: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:11.953: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:11.958: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:11.962: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:11.966: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:11.976: INFO: Lookups using dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7019.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7019.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local jessie_udp@dns-test-service-2.dns-7019.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7019.svc.cluster.local]

Feb 17 18:04:16.925: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:16.930: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:16.935: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:16.942: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:16.956: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:16.960: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:16.965: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:16.970: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:16.980: INFO: Lookups using dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7019.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7019.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local jessie_udp@dns-test-service-2.dns-7019.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7019.svc.cluster.local]

Feb 17 18:04:21.926: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:21.930: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:21.935: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:21.940: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:21.954: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:21.959: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:21.963: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:21.967: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:21.976: INFO: Lookups using dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7019.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7019.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local jessie_udp@dns-test-service-2.dns-7019.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7019.svc.cluster.local]

Feb 17 18:04:26.925: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:26.930: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:26.934: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:26.940: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:26.957: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:26.962: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:26.966: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:26.971: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7019.svc.cluster.local from pod dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654: the server could not find the requested resource (get pods dns-test-8063af57-dbad-42cf-b86b-7f4af0769654)
Feb 17 18:04:26.980: INFO: Lookups using dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7019.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7019.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7019.svc.cluster.local jessie_udp@dns-test-service-2.dns-7019.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7019.svc.cluster.local]

Feb 17 18:04:31.975: INFO: DNS probes using dns-7019/dns-test-8063af57-dbad-42cf-b86b-7f4af0769654 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:04:32.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7019" for this suite.

• [SLOW TEST:37.396 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":278,"completed":3,"skipped":43,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:04:32.063: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7118
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:04:32.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7118" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":278,"completed":4,"skipped":55,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:04:32.237: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 17 18:04:36.928: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6412 pod-service-account-28c55945-6f95-4dbc-a63f-85069ad1b5ba -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 17 18:04:37.853: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6412 pod-service-account-28c55945-6f95-4dbc-a63f-85069ad1b5ba -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 17 18:04:38.034: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6412 pod-service-account-28c55945-6f95-4dbc-a63f-85069ad1b5ba -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:04:38.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6412" for this suite.

• [SLOW TEST:6.002 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":278,"completed":5,"skipped":58,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:04:38.239: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2592
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:04:38.417: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 17 18:04:38.426: INFO: Number of nodes with available pods: 0
Feb 17 18:04:38.426: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 17 18:04:38.452: INFO: Number of nodes with available pods: 0
Feb 17 18:04:38.452: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:39.458: INFO: Number of nodes with available pods: 0
Feb 17 18:04:39.458: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:40.457: INFO: Number of nodes with available pods: 0
Feb 17 18:04:40.458: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:41.457: INFO: Number of nodes with available pods: 0
Feb 17 18:04:41.457: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:42.458: INFO: Number of nodes with available pods: 0
Feb 17 18:04:42.458: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:43.458: INFO: Number of nodes with available pods: 0
Feb 17 18:04:43.458: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:44.457: INFO: Number of nodes with available pods: 0
Feb 17 18:04:44.457: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:45.458: INFO: Number of nodes with available pods: 1
Feb 17 18:04:45.458: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 17 18:04:45.482: INFO: Number of nodes with available pods: 1
Feb 17 18:04:45.482: INFO: Number of running nodes: 0, number of available pods: 1
Feb 17 18:04:46.487: INFO: Number of nodes with available pods: 0
Feb 17 18:04:46.487: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 17 18:04:46.501: INFO: Number of nodes with available pods: 0
Feb 17 18:04:46.501: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:47.506: INFO: Number of nodes with available pods: 0
Feb 17 18:04:47.506: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:48.505: INFO: Number of nodes with available pods: 0
Feb 17 18:04:48.505: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:49.505: INFO: Number of nodes with available pods: 0
Feb 17 18:04:49.505: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:50.506: INFO: Number of nodes with available pods: 0
Feb 17 18:04:50.506: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:51.506: INFO: Number of nodes with available pods: 0
Feb 17 18:04:51.506: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:52.506: INFO: Number of nodes with available pods: 0
Feb 17 18:04:52.506: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:53.506: INFO: Number of nodes with available pods: 0
Feb 17 18:04:53.506: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:54.506: INFO: Number of nodes with available pods: 0
Feb 17 18:04:54.506: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:55.505: INFO: Number of nodes with available pods: 0
Feb 17 18:04:55.505: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:56.506: INFO: Number of nodes with available pods: 0
Feb 17 18:04:56.506: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:57.506: INFO: Number of nodes with available pods: 0
Feb 17 18:04:57.506: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:58.505: INFO: Number of nodes with available pods: 0
Feb 17 18:04:58.505: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:04:59.506: INFO: Number of nodes with available pods: 0
Feb 17 18:04:59.506: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:05:00.506: INFO: Number of nodes with available pods: 0
Feb 17 18:05:00.506: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:05:01.506: INFO: Number of nodes with available pods: 1
Feb 17 18:05:01.506: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2592, will wait for the garbage collector to delete the pods
Feb 17 18:05:01.581: INFO: Deleting DaemonSet.extensions daemon-set took: 11.820303ms
Feb 17 18:05:02.081: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.269178ms
Feb 17 18:05:05.286: INFO: Number of nodes with available pods: 0
Feb 17 18:05:05.286: INFO: Number of running nodes: 0, number of available pods: 0
Feb 17 18:05:05.292: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2592/daemonsets","resourceVersion":"3922"},"items":null}

Feb 17 18:05:05.296: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2592/pods","resourceVersion":"3922"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:05:05.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2592" for this suite.

• [SLOW TEST:27.098 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":278,"completed":6,"skipped":83,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:05:05.336: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-21e0d07c-b9b0-46cb-86bd-7077d391906b
STEP: Creating a pod to test consume configMaps
Feb 17 18:05:05.505: INFO: Waiting up to 5m0s for pod "pod-configmaps-92e5ec18-720e-4e73-b292-b921c4119ec3" in namespace "configmap-4964" to be "success or failure"
Feb 17 18:05:05.520: INFO: Pod "pod-configmaps-92e5ec18-720e-4e73-b292-b921c4119ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.995382ms
Feb 17 18:05:07.525: INFO: Pod "pod-configmaps-92e5ec18-720e-4e73-b292-b921c4119ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019777645s
Feb 17 18:05:09.529: INFO: Pod "pod-configmaps-92e5ec18-720e-4e73-b292-b921c4119ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024366862s
Feb 17 18:05:11.534: INFO: Pod "pod-configmaps-92e5ec18-720e-4e73-b292-b921c4119ec3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029238755s
STEP: Saw pod success
Feb 17 18:05:11.534: INFO: Pod "pod-configmaps-92e5ec18-720e-4e73-b292-b921c4119ec3" satisfied condition "success or failure"
Feb 17 18:05:11.538: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-configmaps-92e5ec18-720e-4e73-b292-b921c4119ec3 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:05:11.563: INFO: Waiting for pod pod-configmaps-92e5ec18-720e-4e73-b292-b921c4119ec3 to disappear
Feb 17 18:05:11.566: INFO: Pod pod-configmaps-92e5ec18-720e-4e73-b292-b921c4119ec3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:05:11.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4964" for this suite.

• [SLOW TEST:6.242 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":7,"skipped":83,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:05:11.579: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8963
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 17 18:05:11.731: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:05:15.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8963" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":278,"completed":8,"skipped":85,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:05:15.887: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2060
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1897
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 18:05:16.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-2060'
Feb 17 18:05:16.122: INFO: stderr: ""
Feb 17 18:05:16.122: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Feb 17 18:05:26.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pod e2e-test-httpd-pod --namespace=kubectl-2060 -o json'
Feb 17 18:05:26.237: INFO: stderr: ""
Feb 17 18:05:26.237: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.8/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2020-02-17T18:05:16Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2060\",\n        \"resourceVersion\": \"4091\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2060/pods/e2e-test-httpd-pod\",\n        \"uid\": \"0094db08-a05a-47f7-a137-fbc8c3513f4a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-qvs9f\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-16-58-212.ec2.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-qvs9f\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-qvs9f\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-17T18:05:16Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-17T18:05:22Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-17T18:05:22Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-17T18:05:16Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://353888856f0a3a50743eaec2d5b5247fd4b8aa5613f80bb968fa3b179d2f77da\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-02-17T18:05:21Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.58.212\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.8\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.96.1.8\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-02-17T18:05:16Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 17 18:05:26.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 replace -f - --namespace=kubectl-2060'
Feb 17 18:05:26.435: INFO: stderr: ""
Feb 17 18:05:26.435: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1902
Feb 17 18:05:26.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete pods e2e-test-httpd-pod --namespace=kubectl-2060'
Feb 17 18:05:28.267: INFO: stderr: ""
Feb 17 18:05:28.267: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:05:28.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2060" for this suite.

• [SLOW TEST:12.398 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1893
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":278,"completed":9,"skipped":111,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:05:28.286: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3283
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:05:28.473: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a6bdd653-9274-4d59-9100-07b8f88d5175" in namespace "projected-3283" to be "success or failure"
Feb 17 18:05:28.477: INFO: Pod "downwardapi-volume-a6bdd653-9274-4d59-9100-07b8f88d5175": Phase="Pending", Reason="", readiness=false. Elapsed: 3.833451ms
Feb 17 18:05:30.481: INFO: Pod "downwardapi-volume-a6bdd653-9274-4d59-9100-07b8f88d5175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007959392s
STEP: Saw pod success
Feb 17 18:05:30.481: INFO: Pod "downwardapi-volume-a6bdd653-9274-4d59-9100-07b8f88d5175" satisfied condition "success or failure"
Feb 17 18:05:30.525: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downwardapi-volume-a6bdd653-9274-4d59-9100-07b8f88d5175 container client-container: <nil>
STEP: delete the pod
Feb 17 18:05:30.581: INFO: Waiting for pod downwardapi-volume-a6bdd653-9274-4d59-9100-07b8f88d5175 to disappear
Feb 17 18:05:30.585: INFO: Pod downwardapi-volume-a6bdd653-9274-4d59-9100-07b8f88d5175 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:05:30.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3283" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":278,"completed":10,"skipped":138,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:05:30.598: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9307
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-9307, will wait for the garbage collector to delete the pods
Feb 17 18:05:34.877: INFO: Deleting Job.batch foo took: 14.79984ms
Feb 17 18:05:35.378: INFO: Terminating Job.batch foo pods took: 500.331909ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:06:17.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9307" for this suite.

• [SLOW TEST:47.198 seconds]
[sig-apps] Job
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":278,"completed":11,"skipped":140,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:06:17.796: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4157
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:06:17.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4157" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":278,"completed":12,"skipped":165,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:06:18.010: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2450
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:06:18.214: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:07:18.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2450" for this suite.

• [SLOW TEST:60.880 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":278,"completed":13,"skipped":168,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:07:18.891: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9906
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:07:19.562: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 17 18:07:20.695: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:07:21.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9906" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":278,"completed":14,"skipped":182,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:07:21.724: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8567
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:07:21.882: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-ff23a18f-b2f8-4bc0-94c5-21a36eabde0c" in namespace "security-context-test-8567" to be "success or failure"
Feb 17 18:07:21.894: INFO: Pod "alpine-nnp-false-ff23a18f-b2f8-4bc0-94c5-21a36eabde0c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.321598ms
Feb 17 18:07:23.899: INFO: Pod "alpine-nnp-false-ff23a18f-b2f8-4bc0-94c5-21a36eabde0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017326767s
Feb 17 18:07:25.905: INFO: Pod "alpine-nnp-false-ff23a18f-b2f8-4bc0-94c5-21a36eabde0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022699959s
Feb 17 18:07:25.905: INFO: Pod "alpine-nnp-false-ff23a18f-b2f8-4bc0-94c5-21a36eabde0c" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:07:25.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8567" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":15,"skipped":194,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:07:25.935: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4203
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 17 18:07:26.097: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4203 /api/v1/namespaces/watch-4203/configmaps/e2e-watch-test-configmap-a dde680f2-df64-4da7-8025-18ee9e089e95 4754 0 2020-02-17 18:07:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 17 18:07:26.097: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4203 /api/v1/namespaces/watch-4203/configmaps/e2e-watch-test-configmap-a dde680f2-df64-4da7-8025-18ee9e089e95 4754 0 2020-02-17 18:07:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 17 18:07:36.107: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4203 /api/v1/namespaces/watch-4203/configmaps/e2e-watch-test-configmap-a dde680f2-df64-4da7-8025-18ee9e089e95 4828 0 2020-02-17 18:07:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 17 18:07:36.107: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4203 /api/v1/namespaces/watch-4203/configmaps/e2e-watch-test-configmap-a dde680f2-df64-4da7-8025-18ee9e089e95 4828 0 2020-02-17 18:07:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 17 18:07:46.117: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4203 /api/v1/namespaces/watch-4203/configmaps/e2e-watch-test-configmap-a dde680f2-df64-4da7-8025-18ee9e089e95 4863 0 2020-02-17 18:07:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 17 18:07:46.117: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4203 /api/v1/namespaces/watch-4203/configmaps/e2e-watch-test-configmap-a dde680f2-df64-4da7-8025-18ee9e089e95 4863 0 2020-02-17 18:07:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 17 18:07:56.129: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4203 /api/v1/namespaces/watch-4203/configmaps/e2e-watch-test-configmap-a dde680f2-df64-4da7-8025-18ee9e089e95 4899 0 2020-02-17 18:07:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 17 18:07:56.129: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4203 /api/v1/namespaces/watch-4203/configmaps/e2e-watch-test-configmap-a dde680f2-df64-4da7-8025-18ee9e089e95 4899 0 2020-02-17 18:07:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 17 18:08:06.140: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4203 /api/v1/namespaces/watch-4203/configmaps/e2e-watch-test-configmap-b d5a3cbcb-026f-4d94-b8ba-07babfa0fd92 4933 0 2020-02-17 18:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 17 18:08:06.140: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4203 /api/v1/namespaces/watch-4203/configmaps/e2e-watch-test-configmap-b d5a3cbcb-026f-4d94-b8ba-07babfa0fd92 4933 0 2020-02-17 18:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 17 18:08:16.151: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4203 /api/v1/namespaces/watch-4203/configmaps/e2e-watch-test-configmap-b d5a3cbcb-026f-4d94-b8ba-07babfa0fd92 4967 0 2020-02-17 18:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 17 18:08:16.151: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4203 /api/v1/namespaces/watch-4203/configmaps/e2e-watch-test-configmap-b d5a3cbcb-026f-4d94-b8ba-07babfa0fd92 4967 0 2020-02-17 18:08:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:08:26.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4203" for this suite.

• [SLOW TEST:60.232 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":278,"completed":16,"skipped":199,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:08:26.167: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8119
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-8119
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8119 to expose endpoints map[]
Feb 17 18:08:26.348: INFO: successfully validated that service endpoint-test2 in namespace services-8119 exposes endpoints map[] (6.017429ms elapsed)
STEP: Creating pod pod1 in namespace services-8119
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8119 to expose endpoints map[pod1:[80]]
Feb 17 18:08:30.404: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.045961435s elapsed, will retry)
Feb 17 18:08:32.421: INFO: successfully validated that service endpoint-test2 in namespace services-8119 exposes endpoints map[pod1:[80]] (6.063026162s elapsed)
STEP: Creating pod pod2 in namespace services-8119
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8119 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 17 18:08:36.492: INFO: Unexpected endpoints: found map[7655da63-b441-4d9d-a0ed-d43917a5def7:[80]], expected map[pod1:[80] pod2:[80]] (4.06381757s elapsed, will retry)
Feb 17 18:08:37.504: INFO: successfully validated that service endpoint-test2 in namespace services-8119 exposes endpoints map[pod1:[80] pod2:[80]] (5.076485375s elapsed)
STEP: Deleting pod pod1 in namespace services-8119
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8119 to expose endpoints map[pod2:[80]]
Feb 17 18:08:38.534: INFO: successfully validated that service endpoint-test2 in namespace services-8119 exposes endpoints map[pod2:[80]] (1.019208194s elapsed)
STEP: Deleting pod pod2 in namespace services-8119
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8119 to expose endpoints map[]
Feb 17 18:08:39.551: INFO: successfully validated that service endpoint-test2 in namespace services-8119 exposes endpoints map[] (1.008845521s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:08:39.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8119" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:13.428 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":278,"completed":17,"skipped":228,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:08:39.596: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7823
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7823
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7823
I0217 18:08:39.795450      26 runners.go:189] Created replication controller with name: externalname-service, namespace: services-7823, replica count: 2
I0217 18:08:42.845896      26 runners.go:189] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 18:08:45.846: INFO: Creating new exec pod
I0217 18:08:45.846160      26 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 18:08:50.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-7823 execpodvjxxq -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 17 18:08:52.108: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 17 18:08:52.108: INFO: stdout: ""
Feb 17 18:08:52.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-7823 execpodvjxxq -- /bin/sh -x -c nc -zv -t -w 2 100.71.177.116 80'
Feb 17 18:08:52.301: INFO: stderr: "+ nc -zv -t -w 2 100.71.177.116 80\nConnection to 100.71.177.116 80 port [tcp/http] succeeded!\n"
Feb 17 18:08:52.301: INFO: stdout: ""
Feb 17 18:08:52.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-7823 execpodvjxxq -- /bin/sh -x -c nc -zv -t -w 2 172.16.86.181 31528'
Feb 17 18:08:53.514: INFO: stderr: "+ nc -zv -t -w 2 172.16.86.181 31528\nConnection to 172.16.86.181 31528 port [tcp/31528] succeeded!\n"
Feb 17 18:08:53.514: INFO: stdout: ""
Feb 17 18:08:53.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-7823 execpodvjxxq -- /bin/sh -x -c nc -zv -t -w 2 172.16.58.212 31528'
Feb 17 18:08:53.708: INFO: stderr: "+ nc -zv -t -w 2 172.16.58.212 31528\nConnection to 172.16.58.212 31528 port [tcp/31528] succeeded!\n"
Feb 17 18:08:53.708: INFO: stdout: ""
Feb 17 18:08:53.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-7823 execpodvjxxq -- /bin/sh -x -c nc -zv -t -w 2 54.157.144.225 31528'
Feb 17 18:08:53.906: INFO: stderr: "+ nc -zv -t -w 2 54.157.144.225 31528\nConnection to 54.157.144.225 31528 port [tcp/31528] succeeded!\n"
Feb 17 18:08:53.906: INFO: stdout: ""
Feb 17 18:08:53.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-7823 execpodvjxxq -- /bin/sh -x -c nc -zv -t -w 2 52.91.6.143 31528'
Feb 17 18:08:54.102: INFO: stderr: "+ nc -zv -t -w 2 52.91.6.143 31528\nConnection to 52.91.6.143 31528 port [tcp/31528] succeeded!\n"
Feb 17 18:08:54.102: INFO: stdout: ""
Feb 17 18:08:54.102: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:08:54.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7823" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:14.561 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":278,"completed":18,"skipped":245,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:08:54.157: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7711
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:08:54.325: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-651e8dae-df9e-4b10-8525-6a218148c7ef" in namespace "security-context-test-7711" to be "success or failure"
Feb 17 18:08:54.330: INFO: Pod "busybox-privileged-false-651e8dae-df9e-4b10-8525-6a218148c7ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.532092ms
Feb 17 18:08:56.334: INFO: Pod "busybox-privileged-false-651e8dae-df9e-4b10-8525-6a218148c7ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009248577s
Feb 17 18:08:56.334: INFO: Pod "busybox-privileged-false-651e8dae-df9e-4b10-8525-6a218148c7ef" satisfied condition "success or failure"
Feb 17 18:08:56.354: INFO: Got logs for pod "busybox-privileged-false-651e8dae-df9e-4b10-8525-6a218148c7ef": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:08:56.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7711" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":19,"skipped":316,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:08:56.368: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2379
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Feb 17 18:08:56.532: INFO: Waiting up to 5m0s for pod "var-expansion-339436ee-6c3e-483b-9427-7cf061e118df" in namespace "var-expansion-2379" to be "success or failure"
Feb 17 18:08:56.538: INFO: Pod "var-expansion-339436ee-6c3e-483b-9427-7cf061e118df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.533471ms
Feb 17 18:08:58.543: INFO: Pod "var-expansion-339436ee-6c3e-483b-9427-7cf061e118df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011273226s
STEP: Saw pod success
Feb 17 18:08:58.543: INFO: Pod "var-expansion-339436ee-6c3e-483b-9427-7cf061e118df" satisfied condition "success or failure"
Feb 17 18:08:58.547: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod var-expansion-339436ee-6c3e-483b-9427-7cf061e118df container dapi-container: <nil>
STEP: delete the pod
Feb 17 18:08:58.573: INFO: Waiting for pod var-expansion-339436ee-6c3e-483b-9427-7cf061e118df to disappear
Feb 17 18:08:58.577: INFO: Pod var-expansion-339436ee-6c3e-483b-9427-7cf061e118df no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:08:58.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2379" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":278,"completed":20,"skipped":329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:08:58.591: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3928
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 17 18:08:58.754: INFO: Waiting up to 5m0s for pod "pod-ec7aca3d-44a3-44b2-90c6-a4dce8cec691" in namespace "emptydir-3928" to be "success or failure"
Feb 17 18:08:58.758: INFO: Pod "pod-ec7aca3d-44a3-44b2-90c6-a4dce8cec691": Phase="Pending", Reason="", readiness=false. Elapsed: 4.515697ms
Feb 17 18:09:00.768: INFO: Pod "pod-ec7aca3d-44a3-44b2-90c6-a4dce8cec691": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014235663s
Feb 17 18:09:02.774: INFO: Pod "pod-ec7aca3d-44a3-44b2-90c6-a4dce8cec691": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019997796s
STEP: Saw pod success
Feb 17 18:09:02.774: INFO: Pod "pod-ec7aca3d-44a3-44b2-90c6-a4dce8cec691" satisfied condition "success or failure"
Feb 17 18:09:02.778: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-ec7aca3d-44a3-44b2-90c6-a4dce8cec691 container test-container: <nil>
STEP: delete the pod
Feb 17 18:09:02.807: INFO: Waiting for pod pod-ec7aca3d-44a3-44b2-90c6-a4dce8cec691 to disappear
Feb 17 18:09:02.813: INFO: Pod pod-ec7aca3d-44a3-44b2-90c6-a4dce8cec691 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:09:02.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3928" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":21,"skipped":359,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:09:02.826: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5486
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:09:03.805: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:09:05.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559743, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559743, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559743, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559743, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:09:08.846: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
Feb 17 18:09:10.007: INFO: Waiting for webhook configuration to be ready...
Feb 17 18:09:11.150: INFO: Waiting for webhook configuration to be ready...
Feb 17 18:09:12.267: INFO: Waiting for webhook configuration to be ready...
Feb 17 18:09:13.355: INFO: Waiting for webhook configuration to be ready...
Feb 17 18:09:14.442: INFO: Waiting for webhook configuration to be ready...
Feb 17 18:09:15.563: INFO: Waiting for webhook configuration to be ready...
Feb 17 18:09:16.652: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:09:19.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5486" for this suite.
STEP: Destroying namespace "webhook-5486-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.146 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":278,"completed":22,"skipped":390,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:09:19.973: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7468
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-eb8d30d7-4e72-476b-a503-162a8939cc9d
STEP: Creating a pod to test consume secrets
Feb 17 18:09:20.140: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-27cd152e-84db-48e0-836e-3f5da841eef3" in namespace "projected-7468" to be "success or failure"
Feb 17 18:09:20.148: INFO: Pod "pod-projected-secrets-27cd152e-84db-48e0-836e-3f5da841eef3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.427038ms
Feb 17 18:09:22.153: INFO: Pod "pod-projected-secrets-27cd152e-84db-48e0-836e-3f5da841eef3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013289371s
Feb 17 18:09:24.158: INFO: Pod "pod-projected-secrets-27cd152e-84db-48e0-836e-3f5da841eef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017772659s
STEP: Saw pod success
Feb 17 18:09:24.158: INFO: Pod "pod-projected-secrets-27cd152e-84db-48e0-836e-3f5da841eef3" satisfied condition "success or failure"
Feb 17 18:09:24.162: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod pod-projected-secrets-27cd152e-84db-48e0-836e-3f5da841eef3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 17 18:09:24.195: INFO: Waiting for pod pod-projected-secrets-27cd152e-84db-48e0-836e-3f5da841eef3 to disappear
Feb 17 18:09:24.200: INFO: Pod pod-projected-secrets-27cd152e-84db-48e0-836e-3f5da841eef3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:09:24.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7468" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":23,"skipped":412,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:09:24.215: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8691
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 17 18:09:34.488: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0217 18:09:34.488280      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:09:34.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8691" for this suite.

• [SLOW TEST:10.286 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":278,"completed":24,"skipped":421,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:09:34.501: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8047
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 17 18:09:34.661: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 17 18:09:34.677: INFO: Waiting for terminating namespaces to be deleted...
Feb 17 18:09:34.681: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-58-212.ec2.internal before test
Feb 17 18:09:34.688: INFO: k8s-api-haproxy-b9ba254b202c3ae80f0c096eb09e3391b752ed191356ffd2fb26712eff44ce1b-ip-172-16-58-212.ec2.internal from kube-system started at 2020-02-17 17:52:08 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.688: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Feb 17 18:09:34.688: INFO: kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-58-212.ec2.internal from kube-system started at 2020-02-17 17:52:08 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.688: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:09:34.688: INFO: canal-v8cfc from kube-system started at 2020-02-17 17:53:10 +0000 UTC (4 container statuses recorded)
Feb 17 18:09:34.688: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:09:34.688: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 17 18:09:34.688: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Feb 17 18:09:34.688: INFO: 	Container update-network-condition ready: true, restart count 0
Feb 17 18:09:34.688: INFO: node-local-dns-rvfkr from kube-system started at 2020-02-17 17:53:20 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.688: INFO: 	Container node-cache ready: true, restart count 0
Feb 17 18:09:34.688: INFO: sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-zlh59 from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:09:34.688: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:09:34.688: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:09:34.688: INFO: simpletest-rc-to-be-deleted-6ml7n from gc-8691 started at 2020-02-17 18:09:24 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.688: INFO: 	Container nginx ready: true, restart count 0
Feb 17 18:09:34.688: INFO: kublr-node-name-reporter-2e4a883ef03ca181a5002964babb2c122cef2ef3f405e26061a11af75a5d837f-ip-172-16-58-212.ec2.internal from kube-system started at 2020-02-17 17:52:08 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.688: INFO: 	Container main ready: true, restart count 0
Feb 17 18:09:34.688: INFO: coredns-6c578c8bf7-88w67 from kube-system started at 2020-02-17 17:53:27 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.688: INFO: 	Container coredns ready: true, restart count 0
Feb 17 18:09:34.688: INFO: simpletest-rc-to-be-deleted-9r59p from gc-8691 started at 2020-02-17 18:09:24 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.688: INFO: 	Container nginx ready: true, restart count 0
Feb 17 18:09:34.688: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-71-125.ec2.internal before test
Feb 17 18:09:34.707: INFO: sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-d99zx from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:09:34.707: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:09:34.707: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:09:34.707: INFO: canal-9jpnm from kube-system started at 2020-02-17 17:53:09 +0000 UTC (4 container statuses recorded)
Feb 17 18:09:34.707: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:09:34.707: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 17 18:09:34.707: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Feb 17 18:09:34.707: INFO: 	Container update-network-condition ready: true, restart count 0
Feb 17 18:09:34.707: INFO: kublr-system-shell-5b9bd5c865-lmlsp from kube-system started at 2020-02-17 17:54:02 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.707: INFO: 	Container shell ready: true, restart count 0
Feb 17 18:09:34.707: INFO: sonobuoy-e2e-job-20e37349fda64c29 from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:09:34.707: INFO: 	Container e2e ready: true, restart count 0
Feb 17 18:09:34.707: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:09:34.707: INFO: node-local-dns-b4j9f from kube-system started at 2020-02-17 17:53:19 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.707: INFO: 	Container node-cache ready: true, restart count 0
Feb 17 18:09:34.708: INFO: sonobuoy from sonobuoy started at 2020-02-17 18:03:16 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.708: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 17 18:09:34.708: INFO: simpletest-rc-to-be-deleted-5zq27 from gc-8691 started at 2020-02-17 18:09:24 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.708: INFO: 	Container nginx ready: true, restart count 0
Feb 17 18:09:34.708: INFO: simpletest-rc-to-be-deleted-825j6 from gc-8691 started at 2020-02-17 18:09:24 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.708: INFO: 	Container nginx ready: true, restart count 0
Feb 17 18:09:34.708: INFO: kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-71-125.ec2.internal from kube-system started at 2020-02-17 17:51:59 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.708: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:09:34.708: INFO: kublr-node-name-reporter-3cd39b06f3a526166a2bdbb7d0aa396f06e89576f16de214708f3a78d146fdd2-ip-172-16-71-125.ec2.internal from kube-system started at 2020-02-17 17:51:59 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.708: INFO: 	Container main ready: true, restart count 0
Feb 17 18:09:34.708: INFO: k8s-api-haproxy-92dea82227153ce8905e2adce84945a2a46a1ff902bbfb5846f597d0315ec0e1-ip-172-16-71-125.ec2.internal from kube-system started at 2020-02-17 17:51:59 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.708: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Feb 17 18:09:34.708: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-86-181.ec2.internal before test
Feb 17 18:09:34.715: INFO: k8s-api-haproxy-b3a73b75b08bcd3ee17799fee24d51365c32935f7350cdcde3e2adb2935e8f65-ip-172-16-86-181.ec2.internal from kube-system started at 2020-02-17 17:51:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.715: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Feb 17 18:09:34.715: INFO: canal-ng4fk from kube-system started at 2020-02-17 17:53:14 +0000 UTC (4 container statuses recorded)
Feb 17 18:09:34.715: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:09:34.715: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 17 18:09:34.715: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Feb 17 18:09:34.715: INFO: 	Container update-network-condition ready: true, restart count 0
Feb 17 18:09:34.715: INFO: node-local-dns-7prxg from kube-system started at 2020-02-17 17:53:34 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.715: INFO: 	Container node-cache ready: true, restart count 0
Feb 17 18:09:34.715: INFO: sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-q7qcp from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:09:34.715: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:09:34.715: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:09:34.715: INFO: simpletest-rc-to-be-deleted-6qcm5 from gc-8691 started at 2020-02-17 18:09:24 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.715: INFO: 	Container nginx ready: true, restart count 0
Feb 17 18:09:34.715: INFO: kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-86-181.ec2.internal from kube-system started at 2020-02-17 17:51:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.715: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:09:34.715: INFO: kublr-node-name-reporter-d9895b0065d8993619f85e1df198fa878363f17b2ecacbc683c208bcec5934f7-ip-172-16-86-181.ec2.internal from kube-system started at 2020-02-17 17:51:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.715: INFO: 	Container main ready: true, restart count 0
Feb 17 18:09:34.715: INFO: coredns-6c578c8bf7-bwb2d from kube-system started at 2020-02-17 17:53:34 +0000 UTC (1 container statuses recorded)
Feb 17 18:09:34.715: INFO: 	Container coredns ready: true, restart count 0
Feb 17 18:09:34.715: INFO: metrics-server-v0.3.6-7b66f9c8dc-n74zk from kube-system started at 2020-02-17 17:53:36 +0000 UTC (2 container statuses recorded)
Feb 17 18:09:34.715: INFO: 	Container metrics-server ready: true, restart count 0
Feb 17 18:09:34.715: INFO: 	Container metrics-server-nanny ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node ip-172-16-58-212.ec2.internal
STEP: verifying the node has the label node ip-172-16-71-125.ec2.internal
STEP: verifying the node has the label node ip-172-16-86-181.ec2.internal
Feb 17 18:09:34.795: INFO: Pod simpletest-rc-to-be-deleted-5zq27 requesting resource cpu=0m on Node ip-172-16-71-125.ec2.internal
Feb 17 18:09:34.795: INFO: Pod simpletest-rc-to-be-deleted-6ml7n requesting resource cpu=0m on Node ip-172-16-58-212.ec2.internal
Feb 17 18:09:34.795: INFO: Pod simpletest-rc-to-be-deleted-6qcm5 requesting resource cpu=0m on Node ip-172-16-86-181.ec2.internal
Feb 17 18:09:34.795: INFO: Pod simpletest-rc-to-be-deleted-825j6 requesting resource cpu=0m on Node ip-172-16-71-125.ec2.internal
Feb 17 18:09:34.795: INFO: Pod simpletest-rc-to-be-deleted-9r59p requesting resource cpu=0m on Node ip-172-16-58-212.ec2.internal
Feb 17 18:09:34.795: INFO: Pod canal-9jpnm requesting resource cpu=50m on Node ip-172-16-71-125.ec2.internal
Feb 17 18:09:34.795: INFO: Pod canal-ng4fk requesting resource cpu=50m on Node ip-172-16-86-181.ec2.internal
Feb 17 18:09:34.795: INFO: Pod canal-v8cfc requesting resource cpu=50m on Node ip-172-16-58-212.ec2.internal
Feb 17 18:09:34.795: INFO: Pod coredns-6c578c8bf7-88w67 requesting resource cpu=100m on Node ip-172-16-58-212.ec2.internal
Feb 17 18:09:34.795: INFO: Pod coredns-6c578c8bf7-bwb2d requesting resource cpu=100m on Node ip-172-16-86-181.ec2.internal
Feb 17 18:09:34.795: INFO: Pod k8s-api-haproxy-92dea82227153ce8905e2adce84945a2a46a1ff902bbfb5846f597d0315ec0e1-ip-172-16-71-125.ec2.internal requesting resource cpu=1m on Node ip-172-16-71-125.ec2.internal
Feb 17 18:09:34.795: INFO: Pod k8s-api-haproxy-b3a73b75b08bcd3ee17799fee24d51365c32935f7350cdcde3e2adb2935e8f65-ip-172-16-86-181.ec2.internal requesting resource cpu=1m on Node ip-172-16-86-181.ec2.internal
Feb 17 18:09:34.795: INFO: Pod k8s-api-haproxy-b9ba254b202c3ae80f0c096eb09e3391b752ed191356ffd2fb26712eff44ce1b-ip-172-16-58-212.ec2.internal requesting resource cpu=1m on Node ip-172-16-58-212.ec2.internal
Feb 17 18:09:34.795: INFO: Pod kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-58-212.ec2.internal requesting resource cpu=5m on Node ip-172-16-58-212.ec2.internal
Feb 17 18:09:34.795: INFO: Pod kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-71-125.ec2.internal requesting resource cpu=5m on Node ip-172-16-71-125.ec2.internal
Feb 17 18:09:34.795: INFO: Pod kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-86-181.ec2.internal requesting resource cpu=5m on Node ip-172-16-86-181.ec2.internal
Feb 17 18:09:34.795: INFO: Pod kublr-node-name-reporter-2e4a883ef03ca181a5002964babb2c122cef2ef3f405e26061a11af75a5d837f-ip-172-16-58-212.ec2.internal requesting resource cpu=0m on Node ip-172-16-58-212.ec2.internal
Feb 17 18:09:34.795: INFO: Pod kublr-node-name-reporter-3cd39b06f3a526166a2bdbb7d0aa396f06e89576f16de214708f3a78d146fdd2-ip-172-16-71-125.ec2.internal requesting resource cpu=0m on Node ip-172-16-71-125.ec2.internal
Feb 17 18:09:34.795: INFO: Pod kublr-node-name-reporter-d9895b0065d8993619f85e1df198fa878363f17b2ecacbc683c208bcec5934f7-ip-172-16-86-181.ec2.internal requesting resource cpu=0m on Node ip-172-16-86-181.ec2.internal
Feb 17 18:09:34.795: INFO: Pod kublr-system-shell-5b9bd5c865-lmlsp requesting resource cpu=10m on Node ip-172-16-71-125.ec2.internal
Feb 17 18:09:34.795: INFO: Pod metrics-server-v0.3.6-7b66f9c8dc-n74zk requesting resource cpu=98m on Node ip-172-16-86-181.ec2.internal
Feb 17 18:09:34.795: INFO: Pod node-local-dns-7prxg requesting resource cpu=25m on Node ip-172-16-86-181.ec2.internal
Feb 17 18:09:34.795: INFO: Pod node-local-dns-b4j9f requesting resource cpu=25m on Node ip-172-16-71-125.ec2.internal
Feb 17 18:09:34.795: INFO: Pod node-local-dns-rvfkr requesting resource cpu=25m on Node ip-172-16-58-212.ec2.internal
Feb 17 18:09:34.795: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-16-71-125.ec2.internal
Feb 17 18:09:34.795: INFO: Pod sonobuoy-e2e-job-20e37349fda64c29 requesting resource cpu=0m on Node ip-172-16-71-125.ec2.internal
Feb 17 18:09:34.795: INFO: Pod sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-d99zx requesting resource cpu=0m on Node ip-172-16-71-125.ec2.internal
Feb 17 18:09:34.795: INFO: Pod sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-q7qcp requesting resource cpu=0m on Node ip-172-16-86-181.ec2.internal
Feb 17 18:09:34.795: INFO: Pod sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-zlh59 requesting resource cpu=0m on Node ip-172-16-58-212.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
Feb 17 18:09:34.795: INFO: Creating a pod which consumes cpu=2603m on Node ip-172-16-58-212.ec2.internal
Feb 17 18:09:34.804: INFO: Creating a pod which consumes cpu=2666m on Node ip-172-16-71-125.ec2.internal
Feb 17 18:09:34.820: INFO: Creating a pod which consumes cpu=2534m on Node ip-172-16-86-181.ec2.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2dae570-6d3f-4e22-9b84-9e8bcc28b382.15f442f1abc5c9cc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8047/filler-pod-c2dae570-6d3f-4e22-9b84-9e8bcc28b382 to ip-172-16-58-212.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2dae570-6d3f-4e22-9b84-9e8bcc28b382.15f442f1e797299f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2dae570-6d3f-4e22-9b84-9e8bcc28b382.15f442f1ee85f623], Reason = [Created], Message = [Created container filler-pod-c2dae570-6d3f-4e22-9b84-9e8bcc28b382]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2dae570-6d3f-4e22-9b84-9e8bcc28b382.15f442f1fbb9f5dd], Reason = [Started], Message = [Started container filler-pod-c2dae570-6d3f-4e22-9b84-9e8bcc28b382]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7a10e9c-6dd9-43de-9f75-a784a902d966.15f442f1ad0f62f0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8047/filler-pod-c7a10e9c-6dd9-43de-9f75-a784a902d966 to ip-172-16-71-125.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7a10e9c-6dd9-43de-9f75-a784a902d966.15f442f1f4a8bb9e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7a10e9c-6dd9-43de-9f75-a784a902d966.15f442f1fb3f027e], Reason = [Created], Message = [Created container filler-pod-c7a10e9c-6dd9-43de-9f75-a784a902d966]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c7a10e9c-6dd9-43de-9f75-a784a902d966.15f442f208b9bd38], Reason = [Started], Message = [Started container filler-pod-c7a10e9c-6dd9-43de-9f75-a784a902d966]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ee7ca74c-e9f4-4ad5-a933-4f2ab38b2590.15f442f1ad8e1b2f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8047/filler-pod-ee7ca74c-e9f4-4ad5-a933-4f2ab38b2590 to ip-172-16-86-181.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ee7ca74c-e9f4-4ad5-a933-4f2ab38b2590.15f442f1ecab7b52], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ee7ca74c-e9f4-4ad5-a933-4f2ab38b2590.15f442f1f3d6c9fa], Reason = [Created], Message = [Created container filler-pod-ee7ca74c-e9f4-4ad5-a933-4f2ab38b2590]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ee7ca74c-e9f4-4ad5-a933-4f2ab38b2590.15f442f1ff5059b4], Reason = [Started], Message = [Started container filler-pod-ee7ca74c-e9f4-4ad5-a933-4f2ab38b2590]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f442f29d676cc3], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 Insufficient cpu.]
STEP: removing the label node off the node ip-172-16-58-212.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-16-71-125.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-16-86-181.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:09:39.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8047" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:5.452 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":278,"completed":25,"skipped":424,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:09:39.953: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8476
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-fe74fc85-a282-4fd2-8771-08bf7fb5ffec
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-fe74fc85-a282-4fd2-8771-08bf7fb5ffec
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:11:04.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8476" for this suite.

• [SLOW TEST:84.690 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":26,"skipped":430,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:11:04.644: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-160
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 17 18:11:04.831: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-160 /api/v1/namespaces/watch-160/configmaps/e2e-watch-test-resource-version 552ddd70-9e51-4716-b8e4-5b1b43334735 6246 0 2020-02-17 18:11:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 17 18:11:04.831: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-160 /api/v1/namespaces/watch-160/configmaps/e2e-watch-test-resource-version 552ddd70-9e51-4716-b8e4-5b1b43334735 6247 0 2020-02-17 18:11:04 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:11:04.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-160" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":278,"completed":27,"skipped":433,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:11:04.844: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6851
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-6851/secret-test-4638be9d-d4fd-40f7-8c47-96c8fa68b6ad
STEP: Creating a pod to test consume secrets
Feb 17 18:11:05.016: INFO: Waiting up to 5m0s for pod "pod-configmaps-3353c827-146f-40e9-beb3-03a02f6072d9" in namespace "secrets-6851" to be "success or failure"
Feb 17 18:11:05.028: INFO: Pod "pod-configmaps-3353c827-146f-40e9-beb3-03a02f6072d9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.578552ms
Feb 17 18:11:07.033: INFO: Pod "pod-configmaps-3353c827-146f-40e9-beb3-03a02f6072d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016946873s
Feb 17 18:11:09.038: INFO: Pod "pod-configmaps-3353c827-146f-40e9-beb3-03a02f6072d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022058617s
STEP: Saw pod success
Feb 17 18:11:09.038: INFO: Pod "pod-configmaps-3353c827-146f-40e9-beb3-03a02f6072d9" satisfied condition "success or failure"
Feb 17 18:11:09.042: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-configmaps-3353c827-146f-40e9-beb3-03a02f6072d9 container env-test: <nil>
STEP: delete the pod
Feb 17 18:11:09.074: INFO: Waiting for pod pod-configmaps-3353c827-146f-40e9-beb3-03a02f6072d9 to disappear
Feb 17 18:11:09.079: INFO: Pod pod-configmaps-3353c827-146f-40e9-beb3-03a02f6072d9 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:11:09.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6851" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":278,"completed":28,"skipped":439,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:11:09.097: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-8203
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 17 18:11:15.305: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8203 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:11:15.305: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:11:15.425: INFO: Exec stderr: ""
Feb 17 18:11:15.425: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8203 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:11:15.425: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:11:15.554: INFO: Exec stderr: ""
Feb 17 18:11:15.554: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8203 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:11:15.554: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:11:15.678: INFO: Exec stderr: ""
Feb 17 18:11:15.678: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8203 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:11:15.678: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:11:15.805: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 17 18:11:15.805: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8203 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:11:15.805: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:11:15.920: INFO: Exec stderr: ""
Feb 17 18:11:15.921: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8203 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:11:15.921: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:11:16.048: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 17 18:11:16.048: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8203 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:11:16.048: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:11:16.177: INFO: Exec stderr: ""
Feb 17 18:11:16.177: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8203 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:11:16.177: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:11:16.296: INFO: Exec stderr: ""
Feb 17 18:11:16.296: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8203 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:11:16.296: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:11:16.416: INFO: Exec stderr: ""
Feb 17 18:11:16.416: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8203 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:11:16.416: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:11:16.540: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:11:16.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8203" for this suite.

• [SLOW TEST:7.463 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":29,"skipped":453,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:11:16.561: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4946
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 17 18:11:16.753: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:16.753: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:16.753: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:16.757: INFO: Number of nodes with available pods: 0
Feb 17 18:11:16.757: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:11:17.764: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:17.764: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:17.764: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:17.769: INFO: Number of nodes with available pods: 0
Feb 17 18:11:17.769: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:11:18.764: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:18.764: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:18.764: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:18.768: INFO: Number of nodes with available pods: 0
Feb 17 18:11:18.768: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:11:19.763: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:19.763: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:19.764: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:19.768: INFO: Number of nodes with available pods: 3
Feb 17 18:11:19.768: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 17 18:11:19.790: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:19.790: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:19.790: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:19.795: INFO: Number of nodes with available pods: 2
Feb 17 18:11:19.795: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:11:20.803: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:20.803: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:20.803: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:20.808: INFO: Number of nodes with available pods: 2
Feb 17 18:11:20.808: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:11:21.802: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:21.802: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:21.802: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:21.806: INFO: Number of nodes with available pods: 2
Feb 17 18:11:21.806: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:11:22.802: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:22.802: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:22.802: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:22.807: INFO: Number of nodes with available pods: 2
Feb 17 18:11:22.807: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:11:23.803: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:23.803: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:23.803: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:23.808: INFO: Number of nodes with available pods: 2
Feb 17 18:11:23.808: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:11:24.802: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:24.802: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:24.802: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:24.806: INFO: Number of nodes with available pods: 2
Feb 17 18:11:24.806: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:11:25.802: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:25.802: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:25.802: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:25.807: INFO: Number of nodes with available pods: 2
Feb 17 18:11:25.807: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:11:26.803: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:26.804: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:26.804: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:26.810: INFO: Number of nodes with available pods: 2
Feb 17 18:11:26.810: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:11:27.802: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:27.803: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:27.803: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:27.807: INFO: Number of nodes with available pods: 2
Feb 17 18:11:27.807: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:11:28.802: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:28.802: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:28.802: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:28.807: INFO: Number of nodes with available pods: 2
Feb 17 18:11:28.807: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:11:29.803: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:29.803: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:29.803: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:29.807: INFO: Number of nodes with available pods: 2
Feb 17 18:11:29.807: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:11:30.802: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:30.802: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:30.802: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:11:30.807: INFO: Number of nodes with available pods: 3
Feb 17 18:11:30.807: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4946, will wait for the garbage collector to delete the pods
Feb 17 18:11:30.877: INFO: Deleting DaemonSet.extensions daemon-set took: 11.116749ms
Feb 17 18:11:31.377: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.286804ms
Feb 17 18:11:39.983: INFO: Number of nodes with available pods: 0
Feb 17 18:11:39.983: INFO: Number of running nodes: 0, number of available pods: 0
Feb 17 18:11:39.986: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4946/daemonsets","resourceVersion":"6543"},"items":null}

Feb 17 18:11:39.990: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4946/pods","resourceVersion":"6543"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:11:40.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4946" for this suite.

• [SLOW TEST:23.462 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":278,"completed":30,"skipped":479,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:11:40.023: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-989
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-sfrl
STEP: Creating a pod to test atomic-volume-subpath
Feb 17 18:11:40.194: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-sfrl" in namespace "subpath-989" to be "success or failure"
Feb 17 18:11:40.197: INFO: Pod "pod-subpath-test-configmap-sfrl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.551891ms
Feb 17 18:11:42.203: INFO: Pod "pod-subpath-test-configmap-sfrl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009120729s
Feb 17 18:11:44.209: INFO: Pod "pod-subpath-test-configmap-sfrl": Phase="Running", Reason="", readiness=true. Elapsed: 4.014760287s
Feb 17 18:11:46.213: INFO: Pod "pod-subpath-test-configmap-sfrl": Phase="Running", Reason="", readiness=true. Elapsed: 6.018880275s
Feb 17 18:11:48.219: INFO: Pod "pod-subpath-test-configmap-sfrl": Phase="Running", Reason="", readiness=true. Elapsed: 8.02484497s
Feb 17 18:11:50.224: INFO: Pod "pod-subpath-test-configmap-sfrl": Phase="Running", Reason="", readiness=true. Elapsed: 10.030290358s
Feb 17 18:11:52.229: INFO: Pod "pod-subpath-test-configmap-sfrl": Phase="Running", Reason="", readiness=true. Elapsed: 12.034895876s
Feb 17 18:11:54.234: INFO: Pod "pod-subpath-test-configmap-sfrl": Phase="Running", Reason="", readiness=true. Elapsed: 14.040407898s
Feb 17 18:11:56.239: INFO: Pod "pod-subpath-test-configmap-sfrl": Phase="Running", Reason="", readiness=true. Elapsed: 16.045003481s
Feb 17 18:11:58.244: INFO: Pod "pod-subpath-test-configmap-sfrl": Phase="Running", Reason="", readiness=true. Elapsed: 18.049738972s
Feb 17 18:12:00.249: INFO: Pod "pod-subpath-test-configmap-sfrl": Phase="Running", Reason="", readiness=true. Elapsed: 20.055075072s
Feb 17 18:12:02.256: INFO: Pod "pod-subpath-test-configmap-sfrl": Phase="Running", Reason="", readiness=true. Elapsed: 22.061849246s
Feb 17 18:12:04.261: INFO: Pod "pod-subpath-test-configmap-sfrl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.067582975s
STEP: Saw pod success
Feb 17 18:12:04.261: INFO: Pod "pod-subpath-test-configmap-sfrl" satisfied condition "success or failure"
Feb 17 18:12:04.265: INFO: Trying to get logs from node ip-172-16-71-125.ec2.internal pod pod-subpath-test-configmap-sfrl container test-container-subpath-configmap-sfrl: <nil>
STEP: delete the pod
Feb 17 18:12:04.303: INFO: Waiting for pod pod-subpath-test-configmap-sfrl to disappear
Feb 17 18:12:04.307: INFO: Pod pod-subpath-test-configmap-sfrl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-sfrl
Feb 17 18:12:04.307: INFO: Deleting pod "pod-subpath-test-configmap-sfrl" in namespace "subpath-989"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:12:04.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-989" for this suite.

• [SLOW TEST:24.302 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":278,"completed":31,"skipped":490,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:12:04.325: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2959
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-c096216c-4948-4cb3-bd93-54426c3bb848
STEP: Creating a pod to test consume configMaps
Feb 17 18:12:04.493: INFO: Waiting up to 5m0s for pod "pod-configmaps-f2e459c1-acdd-4e32-a15b-f8b670d127ce" in namespace "configmap-2959" to be "success or failure"
Feb 17 18:12:04.501: INFO: Pod "pod-configmaps-f2e459c1-acdd-4e32-a15b-f8b670d127ce": Phase="Pending", Reason="", readiness=false. Elapsed: 8.175236ms
Feb 17 18:12:06.506: INFO: Pod "pod-configmaps-f2e459c1-acdd-4e32-a15b-f8b670d127ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012728015s
STEP: Saw pod success
Feb 17 18:12:06.506: INFO: Pod "pod-configmaps-f2e459c1-acdd-4e32-a15b-f8b670d127ce" satisfied condition "success or failure"
Feb 17 18:12:06.510: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-configmaps-f2e459c1-acdd-4e32-a15b-f8b670d127ce container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:12:06.538: INFO: Waiting for pod pod-configmaps-f2e459c1-acdd-4e32-a15b-f8b670d127ce to disappear
Feb 17 18:12:06.541: INFO: Pod pod-configmaps-f2e459c1-acdd-4e32-a15b-f8b670d127ce no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:12:06.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2959" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":32,"skipped":506,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:12:06.555: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1507
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:12:10.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1507" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":278,"completed":33,"skipped":536,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:12:10.747: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2232
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Feb 17 18:12:12.958: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-713147157 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 17 18:12:28.030: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:12:28.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2232" for this suite.

• [SLOW TEST:17.300 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":278,"completed":34,"skipped":581,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:12:28.047: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4440
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 17 18:12:28.210: INFO: Waiting up to 5m0s for pod "pod-1725af2e-89dc-4d44-95fa-e84b442451cd" in namespace "emptydir-4440" to be "success or failure"
Feb 17 18:12:28.216: INFO: Pod "pod-1725af2e-89dc-4d44-95fa-e84b442451cd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.581732ms
Feb 17 18:12:30.220: INFO: Pod "pod-1725af2e-89dc-4d44-95fa-e84b442451cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010430946s
Feb 17 18:12:32.225: INFO: Pod "pod-1725af2e-89dc-4d44-95fa-e84b442451cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015136642s
STEP: Saw pod success
Feb 17 18:12:32.225: INFO: Pod "pod-1725af2e-89dc-4d44-95fa-e84b442451cd" satisfied condition "success or failure"
Feb 17 18:12:32.229: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-1725af2e-89dc-4d44-95fa-e84b442451cd container test-container: <nil>
STEP: delete the pod
Feb 17 18:12:32.263: INFO: Waiting for pod pod-1725af2e-89dc-4d44-95fa-e84b442451cd to disappear
Feb 17 18:12:32.266: INFO: Pod pod-1725af2e-89dc-4d44-95fa-e84b442451cd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:12:32.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4440" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":35,"skipped":586,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:12:32.282: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-2b565d9a-a799-49d8-baf5-05c1dd3ca886
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:12:32.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3470" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":278,"completed":36,"skipped":590,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:12:32.463: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-83
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 17 18:12:32.624: INFO: Waiting up to 5m0s for pod "pod-7a53d1c1-71eb-4751-beff-74876c438105" in namespace "emptydir-83" to be "success or failure"
Feb 17 18:12:32.628: INFO: Pod "pod-7a53d1c1-71eb-4751-beff-74876c438105": Phase="Pending", Reason="", readiness=false. Elapsed: 4.110881ms
Feb 17 18:12:34.633: INFO: Pod "pod-7a53d1c1-71eb-4751-beff-74876c438105": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008696681s
Feb 17 18:12:36.638: INFO: Pod "pod-7a53d1c1-71eb-4751-beff-74876c438105": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014197982s
STEP: Saw pod success
Feb 17 18:12:36.638: INFO: Pod "pod-7a53d1c1-71eb-4751-beff-74876c438105" satisfied condition "success or failure"
Feb 17 18:12:36.642: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-7a53d1c1-71eb-4751-beff-74876c438105 container test-container: <nil>
STEP: delete the pod
Feb 17 18:12:36.667: INFO: Waiting for pod pod-7a53d1c1-71eb-4751-beff-74876c438105 to disappear
Feb 17 18:12:36.671: INFO: Pod pod-7a53d1c1-71eb-4751-beff-74876c438105 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:12:36.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-83" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":37,"skipped":621,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:12:36.686: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-429
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 17 18:12:36.855: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 17 18:12:41.860: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:12:42.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-429" for this suite.

• [SLOW TEST:6.211 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":278,"completed":38,"skipped":634,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:12:42.897: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4819
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-c19d1252-8b44-4604-93ec-089b862f13f2
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:12:43.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4819" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":278,"completed":39,"skipped":651,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:12:43.065: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1657
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-g4q8
STEP: Creating a pod to test atomic-volume-subpath
Feb 17 18:12:43.236: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-g4q8" in namespace "subpath-1657" to be "success or failure"
Feb 17 18:12:43.241: INFO: Pod "pod-subpath-test-secret-g4q8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.828251ms
Feb 17 18:12:45.247: INFO: Pod "pod-subpath-test-secret-g4q8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011000284s
Feb 17 18:12:47.252: INFO: Pod "pod-subpath-test-secret-g4q8": Phase="Running", Reason="", readiness=true. Elapsed: 4.016251586s
Feb 17 18:12:49.257: INFO: Pod "pod-subpath-test-secret-g4q8": Phase="Running", Reason="", readiness=true. Elapsed: 6.021015305s
Feb 17 18:12:51.262: INFO: Pod "pod-subpath-test-secret-g4q8": Phase="Running", Reason="", readiness=true. Elapsed: 8.025845363s
Feb 17 18:12:53.269: INFO: Pod "pod-subpath-test-secret-g4q8": Phase="Running", Reason="", readiness=true. Elapsed: 10.03286129s
Feb 17 18:12:55.274: INFO: Pod "pod-subpath-test-secret-g4q8": Phase="Running", Reason="", readiness=true. Elapsed: 12.037605046s
Feb 17 18:12:57.280: INFO: Pod "pod-subpath-test-secret-g4q8": Phase="Running", Reason="", readiness=true. Elapsed: 14.043537114s
Feb 17 18:12:59.285: INFO: Pod "pod-subpath-test-secret-g4q8": Phase="Running", Reason="", readiness=true. Elapsed: 16.049284359s
Feb 17 18:13:01.291: INFO: Pod "pod-subpath-test-secret-g4q8": Phase="Running", Reason="", readiness=true. Elapsed: 18.055262511s
Feb 17 18:13:03.297: INFO: Pod "pod-subpath-test-secret-g4q8": Phase="Running", Reason="", readiness=true. Elapsed: 20.061109054s
Feb 17 18:13:05.303: INFO: Pod "pod-subpath-test-secret-g4q8": Phase="Running", Reason="", readiness=true. Elapsed: 22.066657067s
Feb 17 18:13:07.307: INFO: Pod "pod-subpath-test-secret-g4q8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.071506541s
STEP: Saw pod success
Feb 17 18:13:07.308: INFO: Pod "pod-subpath-test-secret-g4q8" satisfied condition "success or failure"
Feb 17 18:13:07.312: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-subpath-test-secret-g4q8 container test-container-subpath-secret-g4q8: <nil>
STEP: delete the pod
Feb 17 18:13:07.338: INFO: Waiting for pod pod-subpath-test-secret-g4q8 to disappear
Feb 17 18:13:07.342: INFO: Pod pod-subpath-test-secret-g4q8 no longer exists
STEP: Deleting pod pod-subpath-test-secret-g4q8
Feb 17 18:13:07.342: INFO: Deleting pod "pod-subpath-test-secret-g4q8" in namespace "subpath-1657"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:13:07.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1657" for this suite.

• [SLOW TEST:24.294 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":278,"completed":40,"skipped":706,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:13:07.360: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1790
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:13:07.522: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 17 18:13:12.527: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 17 18:13:12.528: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 17 18:13:12.555: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1790 /apis/apps/v1/namespaces/deployment-1790/deployments/test-cleanup-deployment e88e2cbb-547c-468b-be03-4662090974b0 7197 1 2020-02-17 18:13:12 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0029fed38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Feb 17 18:13:12.559: INFO: New ReplicaSet "test-cleanup-deployment-55ffc6b7b6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6  deployment-1790 /apis/apps/v1/namespaces/deployment-1790/replicasets/test-cleanup-deployment-55ffc6b7b6 b4e83024-008c-4d52-bca1-7de60908ad39 7199 1 2020-02-17 18:13:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment e88e2cbb-547c-468b-be03-4662090974b0 0xc0029ff167 0xc0029ff168}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55ffc6b7b6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0029ff1d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 18:13:12.559: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Feb 17 18:13:12.560: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1790 /apis/apps/v1/namespaces/deployment-1790/replicasets/test-cleanup-controller b61e7c47-bbb6-4e4c-9b61-c8ba3fd2bf9c 7198 1 2020-02-17 18:13:07 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment e88e2cbb-547c-468b-be03-4662090974b0 0xc0029ff08f 0xc0029ff0a0}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0029ff108 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 17 18:13:12.571: INFO: Pod "test-cleanup-controller-sp8z8" is available:
&Pod{ObjectMeta:{test-cleanup-controller-sp8z8 test-cleanup-controller- deployment-1790 /api/v1/namespaces/deployment-1790/pods/test-cleanup-controller-sp8z8 6a83a46f-2af1-4158-a886-ea8808ae218b 7175 0 2020-02-17 18:13:07 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:100.96.1.37/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller b61e7c47-bbb6-4e4c-9b61-c8ba3fd2bf9c 0xc0029ff72f 0xc0029ff750}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9zzsh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9zzsh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9zzsh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-58-212.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:13:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:13:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.58.212,PodIP:100.96.1.37,StartTime:2020-02-17 18:13:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 18:13:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a64d47df6ab24e7a51c7c2e0ad52c591e6c771d1230de9b924a95c81eb34faad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 18:13:12.571: INFO: Pod "test-cleanup-deployment-55ffc6b7b6-jmtct" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6-jmtct test-cleanup-deployment-55ffc6b7b6- deployment-1790 /api/v1/namespaces/deployment-1790/pods/test-cleanup-deployment-55ffc6b7b6-jmtct 4763d4f3-a59d-4649-a3e3-f597aa6d747a 7200 0 2020-02-17 18:13:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-55ffc6b7b6 b4e83024-008c-4d52-bca1-7de60908ad39 0xc0029ff8b7 0xc0029ff8b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9zzsh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9zzsh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9zzsh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:13:12.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1790" for this suite.

• [SLOW TEST:5.251 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":278,"completed":41,"skipped":722,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:13:12.611: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2162
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-55812cf7-5485-4e00-8160-d0db040c626a
STEP: Creating a pod to test consume secrets
Feb 17 18:13:12.786: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b7edc6dd-e278-48ac-9526-573b5355663e" in namespace "projected-2162" to be "success or failure"
Feb 17 18:13:12.795: INFO: Pod "pod-projected-secrets-b7edc6dd-e278-48ac-9526-573b5355663e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.261094ms
Feb 17 18:13:14.799: INFO: Pod "pod-projected-secrets-b7edc6dd-e278-48ac-9526-573b5355663e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012841856s
STEP: Saw pod success
Feb 17 18:13:14.799: INFO: Pod "pod-projected-secrets-b7edc6dd-e278-48ac-9526-573b5355663e" satisfied condition "success or failure"
Feb 17 18:13:14.805: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-projected-secrets-b7edc6dd-e278-48ac-9526-573b5355663e container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 17 18:13:14.832: INFO: Waiting for pod pod-projected-secrets-b7edc6dd-e278-48ac-9526-573b5355663e to disappear
Feb 17 18:13:14.836: INFO: Pod pod-projected-secrets-b7edc6dd-e278-48ac-9526-573b5355663e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:13:14.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2162" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":42,"skipped":738,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:13:14.850: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4265
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4265
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4265
I0217 18:13:15.050133      26 runners.go:189] Created replication controller with name: externalname-service, namespace: services-4265, replica count: 2
I0217 18:13:18.100576      26 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 18:13:18.100: INFO: Creating new exec pod
Feb 17 18:13:23.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-4265 execpodxzmzc -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 17 18:13:25.314: INFO: rc: 1
Feb 17 18:13:25.314: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-4265 execpodxzmzc -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 externalname-service 80
nc: connect to externalname-service port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Feb 17 18:13:26.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-4265 execpodxzmzc -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 17 18:13:26.509: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 17 18:13:26.509: INFO: stdout: ""
Feb 17 18:13:26.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-4265 execpodxzmzc -- /bin/sh -x -c nc -zv -t -w 2 100.69.216.46 80'
Feb 17 18:13:26.704: INFO: stderr: "+ nc -zv -t -w 2 100.69.216.46 80\nConnection to 100.69.216.46 80 port [tcp/http] succeeded!\n"
Feb 17 18:13:26.704: INFO: stdout: ""
Feb 17 18:13:26.704: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:13:26.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4265" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:11.909 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":278,"completed":43,"skipped":739,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:13:26.759: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-955
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:13:27.380: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:13:29.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560007, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560007, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560007, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560007, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:13:32.421: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:13:32.426: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6105-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:13:39.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-955" for this suite.
STEP: Destroying namespace "webhook-955-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.082 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":278,"completed":44,"skipped":758,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:13:39.842: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3031
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:13:40.405: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:13:42.417: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560020, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560020, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560020, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560020, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:13:45.443: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:13:45.449: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:13:52.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3031" for this suite.
STEP: Destroying namespace "webhook-3031-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:12.884 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":278,"completed":45,"skipped":758,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:13:52.726: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6657
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 17 18:13:54.931: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:13:54.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6657" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":278,"completed":46,"skipped":771,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:13:54.970: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1692
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 18:13:55.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-9331'
Feb 17 18:13:55.208: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 17 18:13:55.208: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Feb 17 18:13:55.214: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Feb 17 18:13:55.219: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 17 18:13:55.241: INFO: scanned /root for discovery docs: <nil>
Feb 17 18:13:55.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-9331'
Feb 17 18:14:11.049: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 17 18:14:11.049: INFO: stdout: "Created e2e-test-httpd-rc-84414c8621a8f33fc115cf46e4c52d72\nScaling up e2e-test-httpd-rc-84414c8621a8f33fc115cf46e4c52d72 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-84414c8621a8f33fc115cf46e4c52d72 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-84414c8621a8f33fc115cf46e4c52d72 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Feb 17 18:14:11.049: INFO: stdout: "Created e2e-test-httpd-rc-84414c8621a8f33fc115cf46e4c52d72\nScaling up e2e-test-httpd-rc-84414c8621a8f33fc115cf46e4c52d72 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-84414c8621a8f33fc115cf46e4c52d72 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-84414c8621a8f33fc115cf46e4c52d72 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Feb 17 18:14:11.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-9331'
Feb 17 18:14:11.122: INFO: stderr: ""
Feb 17 18:14:11.122: INFO: stdout: "e2e-test-httpd-rc-84414c8621a8f33fc115cf46e4c52d72-q5s28 "
Feb 17 18:14:11.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods e2e-test-httpd-rc-84414c8621a8f33fc115cf46e4c52d72-q5s28 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9331'
Feb 17 18:14:11.187: INFO: stderr: ""
Feb 17 18:14:11.187: INFO: stdout: "true"
Feb 17 18:14:11.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods e2e-test-httpd-rc-84414c8621a8f33fc115cf46e4c52d72-q5s28 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9331'
Feb 17 18:14:11.250: INFO: stderr: ""
Feb 17 18:14:11.250: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Feb 17 18:14:11.250: INFO: e2e-test-httpd-rc-84414c8621a8f33fc115cf46e4c52d72-q5s28 is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1698
Feb 17 18:14:11.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete rc e2e-test-httpd-rc --namespace=kubectl-9331'
Feb 17 18:14:11.326: INFO: stderr: ""
Feb 17 18:14:11.326: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:14:11.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9331" for this suite.

• [SLOW TEST:16.373 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":278,"completed":47,"skipped":823,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:14:11.343: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7439
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-9aa87b1c-9311-40a6-a267-bbf581d03761
STEP: Creating secret with name s-test-opt-upd-be387d4e-cb0a-42e9-9f15-323cadc5fae5
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9aa87b1c-9311-40a6-a267-bbf581d03761
STEP: Updating secret s-test-opt-upd-be387d4e-cb0a-42e9-9f15-323cadc5fae5
STEP: Creating secret with name s-test-opt-create-dce967d6-5cd7-4521-a361-8edce2384179
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:14:19.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7439" for this suite.

• [SLOW TEST:8.312 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":48,"skipped":825,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:14:19.656: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9693
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-b75ebd8b-32e1-49af-96bd-5acb66a494a7
Feb 17 18:14:19.833: INFO: Pod name my-hostname-basic-b75ebd8b-32e1-49af-96bd-5acb66a494a7: Found 0 pods out of 1
Feb 17 18:14:24.838: INFO: Pod name my-hostname-basic-b75ebd8b-32e1-49af-96bd-5acb66a494a7: Found 1 pods out of 1
Feb 17 18:14:24.838: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b75ebd8b-32e1-49af-96bd-5acb66a494a7" are running
Feb 17 18:14:24.844: INFO: Pod "my-hostname-basic-b75ebd8b-32e1-49af-96bd-5acb66a494a7-h9zgd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 18:14:19 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 18:14:21 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 18:14:21 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 18:14:19 +0000 UTC Reason: Message:}])
Feb 17 18:14:24.844: INFO: Trying to dial the pod
Feb 17 18:14:29.859: INFO: Controller my-hostname-basic-b75ebd8b-32e1-49af-96bd-5acb66a494a7: Got expected result from replica 1 [my-hostname-basic-b75ebd8b-32e1-49af-96bd-5acb66a494a7-h9zgd]: "my-hostname-basic-b75ebd8b-32e1-49af-96bd-5acb66a494a7-h9zgd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:14:29.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9693" for this suite.

• [SLOW TEST:10.218 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":278,"completed":49,"skipped":869,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:14:29.874: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5602
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-06c33fdc-4dac-436a-b604-0bd4aef57fe7
STEP: Creating configMap with name cm-test-opt-upd-d5c3717c-e336-4ead-afc1-603aaa617917
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-06c33fdc-4dac-436a-b604-0bd4aef57fe7
STEP: Updating configmap cm-test-opt-upd-d5c3717c-e336-4ead-afc1-603aaa617917
STEP: Creating configMap with name cm-test-opt-create-e5b50f2b-4586-4e3a-ad4b-d678ab6992c5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:14:38.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5602" for this suite.

• [SLOW TEST:8.391 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":50,"skipped":882,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:14:38.264: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9264
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:14:38.840: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:14:40.854: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560078, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560078, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560078, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560078, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:14:43.880: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:14:44.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9264" for this suite.
STEP: Destroying namespace "webhook-9264-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.800 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":278,"completed":51,"skipped":900,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:14:45.065: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3130
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-6117
STEP: Creating secret with name secret-test-8cd8c5ac-10f4-4041-b1c4-141979351207
STEP: Creating a pod to test consume secrets
Feb 17 18:14:45.409: INFO: Waiting up to 5m0s for pod "pod-secrets-d28aac94-48a3-4ff9-86ce-c03ddcdb7e8f" in namespace "secrets-3130" to be "success or failure"
Feb 17 18:14:45.414: INFO: Pod "pod-secrets-d28aac94-48a3-4ff9-86ce-c03ddcdb7e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.98042ms
Feb 17 18:14:47.420: INFO: Pod "pod-secrets-d28aac94-48a3-4ff9-86ce-c03ddcdb7e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010970411s
Feb 17 18:14:49.425: INFO: Pod "pod-secrets-d28aac94-48a3-4ff9-86ce-c03ddcdb7e8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015637555s
STEP: Saw pod success
Feb 17 18:14:49.425: INFO: Pod "pod-secrets-d28aac94-48a3-4ff9-86ce-c03ddcdb7e8f" satisfied condition "success or failure"
Feb 17 18:14:49.429: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-secrets-d28aac94-48a3-4ff9-86ce-c03ddcdb7e8f container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 18:14:49.455: INFO: Waiting for pod pod-secrets-d28aac94-48a3-4ff9-86ce-c03ddcdb7e8f to disappear
Feb 17 18:14:49.459: INFO: Pod pod-secrets-d28aac94-48a3-4ff9-86ce-c03ddcdb7e8f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:14:49.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3130" for this suite.
STEP: Destroying namespace "secret-namespace-6117" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":278,"completed":52,"skipped":915,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:14:49.482: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2455
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 17 18:14:49.644: INFO: Waiting up to 5m0s for pod "pod-1c74b3ad-98ad-498a-94d2-9c7cc1c84db4" in namespace "emptydir-2455" to be "success or failure"
Feb 17 18:14:49.649: INFO: Pod "pod-1c74b3ad-98ad-498a-94d2-9c7cc1c84db4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.248968ms
Feb 17 18:14:51.654: INFO: Pod "pod-1c74b3ad-98ad-498a-94d2-9c7cc1c84db4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010148191s
STEP: Saw pod success
Feb 17 18:14:51.654: INFO: Pod "pod-1c74b3ad-98ad-498a-94d2-9c7cc1c84db4" satisfied condition "success or failure"
Feb 17 18:14:51.658: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-1c74b3ad-98ad-498a-94d2-9c7cc1c84db4 container test-container: <nil>
STEP: delete the pod
Feb 17 18:14:51.683: INFO: Waiting for pod pod-1c74b3ad-98ad-498a-94d2-9c7cc1c84db4 to disappear
Feb 17 18:14:51.688: INFO: Pod pod-1c74b3ad-98ad-498a-94d2-9c7cc1c84db4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:14:51.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2455" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":53,"skipped":980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:14:51.700: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8114
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:14:51.881: INFO: (0) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 15.289258ms)
Feb 17 18:14:51.886: INFO: (1) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.3709ms)
Feb 17 18:14:51.892: INFO: (2) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.529055ms)
Feb 17 18:14:51.897: INFO: (3) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.985393ms)
Feb 17 18:14:51.902: INFO: (4) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.042137ms)
Feb 17 18:14:51.908: INFO: (5) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.583912ms)
Feb 17 18:14:51.913: INFO: (6) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.037094ms)
Feb 17 18:14:51.917: INFO: (7) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.588996ms)
Feb 17 18:14:51.925: INFO: (8) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 7.725748ms)
Feb 17 18:14:51.930: INFO: (9) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.809709ms)
Feb 17 18:14:51.935: INFO: (10) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.49965ms)
Feb 17 18:14:51.940: INFO: (11) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.471687ms)
Feb 17 18:14:51.945: INFO: (12) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.877836ms)
Feb 17 18:14:51.949: INFO: (13) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.46303ms)
Feb 17 18:14:51.955: INFO: (14) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.369312ms)
Feb 17 18:14:51.960: INFO: (15) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.932385ms)
Feb 17 18:14:51.964: INFO: (16) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.542995ms)
Feb 17 18:14:51.970: INFO: (17) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.450189ms)
Feb 17 18:14:51.975: INFO: (18) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.819583ms)
Feb 17 18:14:51.979: INFO: (19) /api/v1/nodes/ip-172-16-86-181.ec2.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.646774ms)
[AfterEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:14:51.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8114" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":278,"completed":54,"skipped":1004,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:14:51.993: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7091
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7156
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7367
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:15:07.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7091" for this suite.
STEP: Destroying namespace "nsdeletetest-7156" for this suite.
Feb 17 18:15:07.507: INFO: Namespace nsdeletetest-7156 was already deleted
STEP: Destroying namespace "nsdeletetest-7367" for this suite.

• [SLOW TEST:15.529 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":278,"completed":55,"skipped":1008,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:15:07.523: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8346
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Feb 17 18:15:07.682: INFO: Waiting up to 5m0s for pod "client-containers-1eb0f4b8-e291-4dca-a090-9403ed22442f" in namespace "containers-8346" to be "success or failure"
Feb 17 18:15:07.687: INFO: Pod "client-containers-1eb0f4b8-e291-4dca-a090-9403ed22442f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.78426ms
Feb 17 18:15:09.691: INFO: Pod "client-containers-1eb0f4b8-e291-4dca-a090-9403ed22442f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009447159s
Feb 17 18:15:11.697: INFO: Pod "client-containers-1eb0f4b8-e291-4dca-a090-9403ed22442f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015502611s
STEP: Saw pod success
Feb 17 18:15:11.697: INFO: Pod "client-containers-1eb0f4b8-e291-4dca-a090-9403ed22442f" satisfied condition "success or failure"
Feb 17 18:15:11.701: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod client-containers-1eb0f4b8-e291-4dca-a090-9403ed22442f container test-container: <nil>
STEP: delete the pod
Feb 17 18:15:11.728: INFO: Waiting for pod client-containers-1eb0f4b8-e291-4dca-a090-9403ed22442f to disappear
Feb 17 18:15:11.732: INFO: Pod client-containers-1eb0f4b8-e291-4dca-a090-9403ed22442f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:15:11.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8346" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":278,"completed":56,"skipped":1018,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:15:11.747: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2115
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2115.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2115.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2115.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2115.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2115.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2115.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2115.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2115.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2115.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2115.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2115.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2115.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2115.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 132.185.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.185.132_udp@PTR;check="$$(dig +tcp +noall +answer +search 132.185.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.185.132_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2115.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2115.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2115.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2115.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2115.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2115.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2115.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2115.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2115.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2115.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2115.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2115.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2115.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 132.185.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.185.132_udp@PTR;check="$$(dig +tcp +noall +answer +search 132.185.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.185.132_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 18:15:16.050: INFO: DNS probes using dns-2115/dns-test-2d9c3bc0-1b3a-48a4-9e44-fce6cf4c17e9 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:15:16.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2115" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":278,"completed":57,"skipped":1021,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:15:16.143: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1730
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-37fff0f8-b87d-48a8-9fcc-84408aa40e80
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-37fff0f8-b87d-48a8-9fcc-84408aa40e80
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:16:26.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1730" for this suite.

• [SLOW TEST:70.594 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":58,"skipped":1021,"failed":0}
SSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:16:26.738: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8196
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-521f2612-980c-46cb-8492-96a7137f65da
STEP: Creating secret with name secret-projected-all-test-volume-526a2293-5ced-4c11-865d-ffd7da1223ad
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 17 18:16:26.916: INFO: Waiting up to 5m0s for pod "projected-volume-f6cd6222-2200-4a04-b3e7-41917edc9ad8" in namespace "projected-8196" to be "success or failure"
Feb 17 18:16:26.921: INFO: Pod "projected-volume-f6cd6222-2200-4a04-b3e7-41917edc9ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.816371ms
Feb 17 18:16:28.926: INFO: Pod "projected-volume-f6cd6222-2200-4a04-b3e7-41917edc9ad8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009803003s
Feb 17 18:16:30.932: INFO: Pod "projected-volume-f6cd6222-2200-4a04-b3e7-41917edc9ad8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015879549s
STEP: Saw pod success
Feb 17 18:16:30.932: INFO: Pod "projected-volume-f6cd6222-2200-4a04-b3e7-41917edc9ad8" satisfied condition "success or failure"
Feb 17 18:16:30.936: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod projected-volume-f6cd6222-2200-4a04-b3e7-41917edc9ad8 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 17 18:16:30.967: INFO: Waiting for pod projected-volume-f6cd6222-2200-4a04-b3e7-41917edc9ad8 to disappear
Feb 17 18:16:30.970: INFO: Pod projected-volume-f6cd6222-2200-4a04-b3e7-41917edc9ad8 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:16:30.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8196" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":278,"completed":59,"skipped":1024,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:16:30.985: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7127
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:16:34.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7127" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":278,"completed":60,"skipped":1039,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:16:34.208: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3297
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-8ad1b6c0-1f5b-4d54-8640-68ede4757a49
STEP: Creating a pod to test consume secrets
Feb 17 18:16:34.395: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a3bd836f-7bb6-4c5a-bd5b-edf6516266a1" in namespace "projected-3297" to be "success or failure"
Feb 17 18:16:34.401: INFO: Pod "pod-projected-secrets-a3bd836f-7bb6-4c5a-bd5b-edf6516266a1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.479628ms
Feb 17 18:16:36.405: INFO: Pod "pod-projected-secrets-a3bd836f-7bb6-4c5a-bd5b-edf6516266a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0099503s
STEP: Saw pod success
Feb 17 18:16:36.405: INFO: Pod "pod-projected-secrets-a3bd836f-7bb6-4c5a-bd5b-edf6516266a1" satisfied condition "success or failure"
Feb 17 18:16:36.409: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-projected-secrets-a3bd836f-7bb6-4c5a-bd5b-edf6516266a1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 17 18:16:36.436: INFO: Waiting for pod pod-projected-secrets-a3bd836f-7bb6-4c5a-bd5b-edf6516266a1 to disappear
Feb 17 18:16:36.439: INFO: Pod pod-projected-secrets-a3bd836f-7bb6-4c5a-bd5b-edf6516266a1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:16:36.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3297" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":61,"skipped":1051,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:16:36.453: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 17 18:16:36.613: INFO: Waiting up to 5m0s for pod "pod-afbe812f-a301-4da7-b976-b4ec2f282c0d" in namespace "emptydir-5990" to be "success or failure"
Feb 17 18:16:36.617: INFO: Pod "pod-afbe812f-a301-4da7-b976-b4ec2f282c0d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.672835ms
Feb 17 18:16:38.622: INFO: Pod "pod-afbe812f-a301-4da7-b976-b4ec2f282c0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009090396s
STEP: Saw pod success
Feb 17 18:16:38.622: INFO: Pod "pod-afbe812f-a301-4da7-b976-b4ec2f282c0d" satisfied condition "success or failure"
Feb 17 18:16:38.626: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-afbe812f-a301-4da7-b976-b4ec2f282c0d container test-container: <nil>
STEP: delete the pod
Feb 17 18:16:38.651: INFO: Waiting for pod pod-afbe812f-a301-4da7-b976-b4ec2f282c0d to disappear
Feb 17 18:16:38.656: INFO: Pod pod-afbe812f-a301-4da7-b976-b4ec2f282c0d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:16:38.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5990" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":62,"skipped":1089,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:16:38.668: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4321
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-a3d2d498-50d5-44f1-a62f-d09809c9d8b2
STEP: Creating a pod to test consume configMaps
Feb 17 18:16:38.845: INFO: Waiting up to 5m0s for pod "pod-configmaps-fb66385b-30d3-4f2d-bc4f-f4eb590def10" in namespace "configmap-4321" to be "success or failure"
Feb 17 18:16:38.851: INFO: Pod "pod-configmaps-fb66385b-30d3-4f2d-bc4f-f4eb590def10": Phase="Pending", Reason="", readiness=false. Elapsed: 6.133851ms
Feb 17 18:16:40.856: INFO: Pod "pod-configmaps-fb66385b-30d3-4f2d-bc4f-f4eb590def10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011162587s
STEP: Saw pod success
Feb 17 18:16:40.857: INFO: Pod "pod-configmaps-fb66385b-30d3-4f2d-bc4f-f4eb590def10" satisfied condition "success or failure"
Feb 17 18:16:40.860: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-configmaps-fb66385b-30d3-4f2d-bc4f-f4eb590def10 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:16:40.885: INFO: Waiting for pod pod-configmaps-fb66385b-30d3-4f2d-bc4f-f4eb590def10 to disappear
Feb 17 18:16:40.898: INFO: Pod pod-configmaps-fb66385b-30d3-4f2d-bc4f-f4eb590def10 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:16:40.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4321" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":63,"skipped":1092,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:16:40.912: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6365
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Feb 17 18:16:41.591: INFO: created pod pod-service-account-defaultsa
Feb 17 18:16:41.591: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 17 18:16:41.602: INFO: created pod pod-service-account-mountsa
Feb 17 18:16:41.602: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 17 18:16:41.614: INFO: created pod pod-service-account-nomountsa
Feb 17 18:16:41.614: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 17 18:16:41.635: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 17 18:16:41.635: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 17 18:16:41.644: INFO: created pod pod-service-account-mountsa-mountspec
Feb 17 18:16:41.644: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 17 18:16:41.659: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 17 18:16:41.659: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 17 18:16:41.673: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 17 18:16:41.673: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 17 18:16:41.682: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 17 18:16:41.682: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 17 18:16:41.688: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 17 18:16:41.688: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:16:41.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6365" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":278,"completed":64,"skipped":1125,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:16:41.739: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5791
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 17 18:16:50.067: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 18:16:50.071: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 17 18:16:52.071: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 18:16:52.076: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 17 18:16:54.071: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 18:16:54.076: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:16:54.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5791" for this suite.

• [SLOW TEST:12.350 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":278,"completed":65,"skipped":1144,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:16:54.089: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7901
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 17 18:16:56.284: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:16:56.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7901" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":278,"completed":66,"skipped":1151,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:16:56.317: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4764
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4764
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4764
Feb 17 18:16:56.490: INFO: Found 0 stateful pods, waiting for 1
Feb 17 18:17:06.495: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 17 18:17:06.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-4764 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 18:17:07.438: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 18:17:07.438: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 18:17:07.438: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 18:17:07.442: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 17 18:17:17.449: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 18:17:17.449: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 18:17:17.466: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998566s
Feb 17 18:17:18.471: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99569685s
Feb 17 18:17:19.477: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99076904s
Feb 17 18:17:20.482: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.985086271s
Feb 17 18:17:21.487: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979969206s
Feb 17 18:17:22.493: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974730518s
Feb 17 18:17:23.498: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.968777792s
Feb 17 18:17:24.503: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.963708375s
Feb 17 18:17:25.509: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.958521433s
Feb 17 18:17:26.514: INFO: Verifying statefulset ss doesn't scale past 1 for another 952.640073ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4764
Feb 17 18:17:27.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-4764 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:17:27.710: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 18:17:27.710: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 18:17:27.710: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 18:17:27.714: INFO: Found 1 stateful pods, waiting for 3
Feb 17 18:17:37.720: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 18:17:37.720: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 18:17:37.720: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 17 18:17:37.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-4764 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 18:17:37.925: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 18:17:37.925: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 18:17:37.925: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 18:17:37.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-4764 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 18:17:38.122: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 18:17:38.122: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 18:17:38.122: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 18:17:38.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-4764 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 18:17:38.328: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 18:17:38.328: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 18:17:38.328: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 18:17:38.328: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 18:17:38.333: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 17 18:17:48.341: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 18:17:48.341: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 18:17:48.341: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 18:17:48.356: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998547s
Feb 17 18:17:49.362: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995887478s
Feb 17 18:17:50.367: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989918455s
Feb 17 18:17:51.372: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984595028s
Feb 17 18:17:52.379: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979346805s
Feb 17 18:17:53.384: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.973002677s
Feb 17 18:17:54.389: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967967072s
Feb 17 18:17:55.396: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.963062742s
Feb 17 18:17:56.401: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.956078937s
Feb 17 18:17:57.406: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.69655ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4764
Feb 17 18:17:58.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-4764 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:17:58.603: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 18:17:58.603: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 18:17:58.603: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 18:17:58.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-4764 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:17:58.795: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 18:17:58.795: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 18:17:58.795: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 18:17:58.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-4764 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:17:58.999: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 18:17:58.999: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 18:17:58.999: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 18:17:58.999: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 17 18:18:19.018: INFO: Deleting all statefulset in ns statefulset-4764
Feb 17 18:18:19.022: INFO: Scaling statefulset ss to 0
Feb 17 18:18:19.034: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 18:18:19.038: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:18:19.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4764" for this suite.

• [SLOW TEST:82.753 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":278,"completed":67,"skipped":1161,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:18:19.071: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1774
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:18:19.245: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 17 18:18:19.256: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:19.256: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:19.256: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:19.268: INFO: Number of nodes with available pods: 0
Feb 17 18:18:19.268: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:18:20.275: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:20.275: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:20.275: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:20.280: INFO: Number of nodes with available pods: 0
Feb 17 18:18:20.280: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:18:21.275: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:21.275: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:21.275: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:21.279: INFO: Number of nodes with available pods: 1
Feb 17 18:18:21.279: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:18:22.276: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:22.276: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:22.276: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:22.281: INFO: Number of nodes with available pods: 3
Feb 17 18:18:22.281: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 17 18:18:22.312: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:22.312: INFO: Wrong image for pod: daemon-set-jntlr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:22.312: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:22.323: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:22.323: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:22.323: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:23.331: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:23.331: INFO: Wrong image for pod: daemon-set-jntlr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:23.331: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:23.339: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:23.339: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:23.339: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:24.327: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:24.327: INFO: Wrong image for pod: daemon-set-jntlr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:24.327: INFO: Pod daemon-set-jntlr is not available
Feb 17 18:18:24.327: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:24.334: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:24.334: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:24.334: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:25.327: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:25.327: INFO: Wrong image for pod: daemon-set-jntlr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:25.327: INFO: Pod daemon-set-jntlr is not available
Feb 17 18:18:25.327: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:25.333: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:25.333: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:25.333: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:26.328: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:26.328: INFO: Wrong image for pod: daemon-set-jntlr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:26.328: INFO: Pod daemon-set-jntlr is not available
Feb 17 18:18:26.328: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:26.335: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:26.335: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:26.335: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:27.328: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:27.328: INFO: Wrong image for pod: daemon-set-jntlr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:27.328: INFO: Pod daemon-set-jntlr is not available
Feb 17 18:18:27.328: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:27.335: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:27.335: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:27.335: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:28.327: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:28.327: INFO: Pod daemon-set-rgw4q is not available
Feb 17 18:18:28.327: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:28.333: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:28.333: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:28.333: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:29.328: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:29.328: INFO: Pod daemon-set-rgw4q is not available
Feb 17 18:18:29.328: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:29.334: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:29.334: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:29.334: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:30.328: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:30.328: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:30.334: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:30.334: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:30.334: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:31.328: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:31.328: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:31.328: INFO: Pod daemon-set-zh94p is not available
Feb 17 18:18:31.334: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:31.334: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:31.334: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:32.328: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:32.328: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:32.328: INFO: Pod daemon-set-zh94p is not available
Feb 17 18:18:32.333: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:32.333: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:32.333: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:33.328: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:33.328: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:33.328: INFO: Pod daemon-set-zh94p is not available
Feb 17 18:18:33.333: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:33.333: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:33.333: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:34.328: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:34.328: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:34.328: INFO: Pod daemon-set-zh94p is not available
Feb 17 18:18:34.334: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:34.334: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:34.334: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:35.327: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:35.327: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:35.327: INFO: Pod daemon-set-zh94p is not available
Feb 17 18:18:35.333: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:35.333: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:35.333: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:36.327: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:36.327: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:36.327: INFO: Pod daemon-set-zh94p is not available
Feb 17 18:18:36.334: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:36.334: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:36.334: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:37.328: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:37.329: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:37.329: INFO: Pod daemon-set-zh94p is not available
Feb 17 18:18:37.334: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:37.334: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:37.334: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:38.328: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:38.328: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:38.328: INFO: Pod daemon-set-zh94p is not available
Feb 17 18:18:38.335: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:38.335: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:38.335: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:39.328: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:39.328: INFO: Wrong image for pod: daemon-set-zh94p. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:39.328: INFO: Pod daemon-set-zh94p is not available
Feb 17 18:18:39.333: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:39.333: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:39.333: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:40.328: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:40.328: INFO: Pod daemon-set-vhct8 is not available
Feb 17 18:18:40.334: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:40.334: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:40.334: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:41.331: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:41.331: INFO: Pod daemon-set-vhct8 is not available
Feb 17 18:18:41.338: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:41.338: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:41.338: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:42.327: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:42.333: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:42.333: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:42.333: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:43.332: INFO: Wrong image for pod: daemon-set-c5fjt. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 18:18:43.332: INFO: Pod daemon-set-c5fjt is not available
Feb 17 18:18:43.339: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:43.339: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:43.339: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:44.328: INFO: Pod daemon-set-776w2 is not available
Feb 17 18:18:44.333: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:44.334: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:44.334: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 17 18:18:44.340: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:44.340: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:44.340: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:44.344: INFO: Number of nodes with available pods: 2
Feb 17 18:18:44.344: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:18:45.352: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:45.352: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:45.352: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:45.357: INFO: Number of nodes with available pods: 2
Feb 17 18:18:45.357: INFO: Node ip-172-16-71-125.ec2.internal is running more than one daemon pod
Feb 17 18:18:46.351: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:46.351: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:46.351: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:18:46.356: INFO: Number of nodes with available pods: 3
Feb 17 18:18:46.356: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1774, will wait for the garbage collector to delete the pods
Feb 17 18:18:46.439: INFO: Deleting DaemonSet.extensions daemon-set took: 10.306065ms
Feb 17 18:18:46.940: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.304463ms
Feb 17 18:19:00.045: INFO: Number of nodes with available pods: 0
Feb 17 18:19:00.045: INFO: Number of running nodes: 0, number of available pods: 0
Feb 17 18:19:00.051: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1774/daemonsets","resourceVersion":"9900"},"items":null}

Feb 17 18:19:00.055: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1774/pods","resourceVersion":"9900"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:19:00.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1774" for this suite.

• [SLOW TEST:41.014 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":278,"completed":68,"skipped":1165,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:19:00.085: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6454
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:19:00.681: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:19:02.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560340, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560340, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560340, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560340, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:19:05.720: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:19:19.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6454" for this suite.
STEP: Destroying namespace "webhook-6454-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:19.928 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":278,"completed":69,"skipped":1175,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:19:20.013: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 17 18:19:20.179: INFO: Waiting up to 5m0s for pod "pod-b411270d-46ae-49f7-b2af-c0be61ccefb0" in namespace "emptydir-2470" to be "success or failure"
Feb 17 18:19:20.191: INFO: Pod "pod-b411270d-46ae-49f7-b2af-c0be61ccefb0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.600162ms
Feb 17 18:19:22.195: INFO: Pod "pod-b411270d-46ae-49f7-b2af-c0be61ccefb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015999602s
Feb 17 18:19:24.201: INFO: Pod "pod-b411270d-46ae-49f7-b2af-c0be61ccefb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021877625s
STEP: Saw pod success
Feb 17 18:19:24.201: INFO: Pod "pod-b411270d-46ae-49f7-b2af-c0be61ccefb0" satisfied condition "success or failure"
Feb 17 18:19:24.205: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod pod-b411270d-46ae-49f7-b2af-c0be61ccefb0 container test-container: <nil>
STEP: delete the pod
Feb 17 18:19:24.247: INFO: Waiting for pod pod-b411270d-46ae-49f7-b2af-c0be61ccefb0 to disappear
Feb 17 18:19:24.251: INFO: Pod pod-b411270d-46ae-49f7-b2af-c0be61ccefb0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:19:24.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2470" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":70,"skipped":1181,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:19:24.268: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-42
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-8c94f069-9423-4965-b109-4bf54437ec1d
STEP: Creating a pod to test consume configMaps
Feb 17 18:19:24.428: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1af1bb6c-c06b-41cd-9ac1-76f399657219" in namespace "projected-42" to be "success or failure"
Feb 17 18:19:24.433: INFO: Pod "pod-projected-configmaps-1af1bb6c-c06b-41cd-9ac1-76f399657219": Phase="Pending", Reason="", readiness=false. Elapsed: 4.805331ms
Feb 17 18:19:26.437: INFO: Pod "pod-projected-configmaps-1af1bb6c-c06b-41cd-9ac1-76f399657219": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009486411s
STEP: Saw pod success
Feb 17 18:19:26.438: INFO: Pod "pod-projected-configmaps-1af1bb6c-c06b-41cd-9ac1-76f399657219" satisfied condition "success or failure"
Feb 17 18:19:26.441: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-projected-configmaps-1af1bb6c-c06b-41cd-9ac1-76f399657219 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:19:26.476: INFO: Waiting for pod pod-projected-configmaps-1af1bb6c-c06b-41cd-9ac1-76f399657219 to disappear
Feb 17 18:19:26.480: INFO: Pod pod-projected-configmaps-1af1bb6c-c06b-41cd-9ac1-76f399657219 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:19:26.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-42" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":71,"skipped":1182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:19:26.495: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1553
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Feb 17 18:19:26.646: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Feb 17 18:19:43.487: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:19:51.469: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:20:08.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1553" for this suite.

• [SLOW TEST:42.383 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":278,"completed":72,"skipped":1219,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:20:08.878: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3002
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:20:09.036: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 17 18:20:14.041: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 17 18:20:14.041: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 17 18:20:16.047: INFO: Creating deployment "test-rollover-deployment"
Feb 17 18:20:16.057: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 17 18:20:18.067: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 17 18:20:18.075: INFO: Ensure that both replica sets have 1 created replica
Feb 17 18:20:18.083: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 17 18:20:18.092: INFO: Updating deployment test-rollover-deployment
Feb 17 18:20:18.092: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 17 18:20:20.100: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 17 18:20:20.109: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 17 18:20:20.116: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 18:20:20.116: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560418, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:20:22.126: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 18:20:22.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560420, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:20:24.126: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 18:20:24.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560420, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:20:26.125: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 18:20:26.125: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560420, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:20:28.126: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 18:20:28.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560420, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:20:30.126: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 18:20:30.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560420, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:20:32.126: INFO: 
Feb 17 18:20:32.126: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 17 18:20:32.137: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3002 /apis/apps/v1/namespaces/deployment-3002/deployments/test-rollover-deployment a24012b5-57a9-49c3-91e2-5cf84d50e13b 10500 2 2020-02-17 18:20:16 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0065a8358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-17 18:20:16 +0000 UTC,LastTransitionTime:2020-02-17 18:20:16 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-02-17 18:20:30 +0000 UTC,LastTransitionTime:2020-02-17 18:20:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 17 18:20:32.141: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-3002 /apis/apps/v1/namespaces/deployment-3002/replicasets/test-rollover-deployment-574d6dfbff 4a86d9c5-960d-48a3-997a-fdb189c0a784 10489 2 2020-02-17 18:20:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment a24012b5-57a9-49c3-91e2-5cf84d50e13b 0xc0065a87f7 0xc0065a87f8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0065a8868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 17 18:20:32.141: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 17 18:20:32.141: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3002 /apis/apps/v1/namespaces/deployment-3002/replicasets/test-rollover-controller e74de1d1-be70-4fd3-91fe-83fe70397391 10499 2 2020-02-17 18:20:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment a24012b5-57a9-49c3-91e2-5cf84d50e13b 0xc0065a8727 0xc0065a8728}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0065a8788 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 18:20:32.141: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-3002 /apis/apps/v1/namespaces/deployment-3002/replicasets/test-rollover-deployment-f6c94f66c bcca35d4-e26d-4649-ba8b-c38ca8a756e4 10443 2 2020-02-17 18:20:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment a24012b5-57a9-49c3-91e2-5cf84d50e13b 0xc0065a8910 0xc0065a8911}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0065a8998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 18:20:32.145: INFO: Pod "test-rollover-deployment-574d6dfbff-rcxqh" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-rcxqh test-rollover-deployment-574d6dfbff- deployment-3002 /api/v1/namespaces/deployment-3002/pods/test-rollover-deployment-574d6dfbff-rcxqh 1ee4f89d-71bb-4b0f-87e7-69beaf755f11 10456 0 2020-02-17 18:20:18 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[cni.projectcalico.org/podIP:100.96.5.26/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff 4a86d9c5-960d-48a3-997a-fdb189c0a784 0xc0065a9037 0xc0065a9038}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-swpdd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-swpdd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-swpdd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-86-181.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:20:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:20:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:20:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:20:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.86.181,PodIP:100.96.5.26,StartTime:2020-02-17 18:20:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 18:20:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://6e1db9664cfeac32b8eb36903847c1040b8c38eee872dd0784408167f4961d81,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.5.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:20:32.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3002" for this suite.

• [SLOW TEST:23.282 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":278,"completed":73,"skipped":1224,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:20:32.161: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 17 18:20:32.321: INFO: Waiting up to 5m0s for pod "pod-e11ca4d7-62c7-47f3-9ca2-4b8e64f57a23" in namespace "emptydir-5470" to be "success or failure"
Feb 17 18:20:32.325: INFO: Pod "pod-e11ca4d7-62c7-47f3-9ca2-4b8e64f57a23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.29714ms
Feb 17 18:20:34.330: INFO: Pod "pod-e11ca4d7-62c7-47f3-9ca2-4b8e64f57a23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008917965s
Feb 17 18:20:36.334: INFO: Pod "pod-e11ca4d7-62c7-47f3-9ca2-4b8e64f57a23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013245488s
STEP: Saw pod success
Feb 17 18:20:36.334: INFO: Pod "pod-e11ca4d7-62c7-47f3-9ca2-4b8e64f57a23" satisfied condition "success or failure"
Feb 17 18:20:36.339: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-e11ca4d7-62c7-47f3-9ca2-4b8e64f57a23 container test-container: <nil>
STEP: delete the pod
Feb 17 18:20:36.366: INFO: Waiting for pod pod-e11ca4d7-62c7-47f3-9ca2-4b8e64f57a23 to disappear
Feb 17 18:20:36.371: INFO: Pod pod-e11ca4d7-62c7-47f3-9ca2-4b8e64f57a23 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:20:36.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5470" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":74,"skipped":1241,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:20:36.384: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8946
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:20:36.549: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15b9bd86-ede3-47c3-8734-6a9ba0230153" in namespace "downward-api-8946" to be "success or failure"
Feb 17 18:20:36.557: INFO: Pod "downwardapi-volume-15b9bd86-ede3-47c3-8734-6a9ba0230153": Phase="Pending", Reason="", readiness=false. Elapsed: 7.652324ms
Feb 17 18:20:38.563: INFO: Pod "downwardapi-volume-15b9bd86-ede3-47c3-8734-6a9ba0230153": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013631231s
STEP: Saw pod success
Feb 17 18:20:38.563: INFO: Pod "downwardapi-volume-15b9bd86-ede3-47c3-8734-6a9ba0230153" satisfied condition "success or failure"
Feb 17 18:20:38.566: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downwardapi-volume-15b9bd86-ede3-47c3-8734-6a9ba0230153 container client-container: <nil>
STEP: delete the pod
Feb 17 18:20:38.594: INFO: Waiting for pod downwardapi-volume-15b9bd86-ede3-47c3-8734-6a9ba0230153 to disappear
Feb 17 18:20:38.598: INFO: Pod downwardapi-volume-15b9bd86-ede3-47c3-8734-6a9ba0230153 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:20:38.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8946" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":278,"completed":75,"skipped":1255,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:20:38.614: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3186
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 17 18:20:38.775: INFO: Waiting up to 5m0s for pod "downward-api-c53858bf-680b-4c12-8a03-f8c8d2ff967f" in namespace "downward-api-3186" to be "success or failure"
Feb 17 18:20:38.784: INFO: Pod "downward-api-c53858bf-680b-4c12-8a03-f8c8d2ff967f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.505451ms
Feb 17 18:20:40.790: INFO: Pod "downward-api-c53858bf-680b-4c12-8a03-f8c8d2ff967f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014252233s
STEP: Saw pod success
Feb 17 18:20:40.790: INFO: Pod "downward-api-c53858bf-680b-4c12-8a03-f8c8d2ff967f" satisfied condition "success or failure"
Feb 17 18:20:40.795: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downward-api-c53858bf-680b-4c12-8a03-f8c8d2ff967f container dapi-container: <nil>
STEP: delete the pod
Feb 17 18:20:40.829: INFO: Waiting for pod downward-api-c53858bf-680b-4c12-8a03-f8c8d2ff967f to disappear
Feb 17 18:20:40.834: INFO: Pod downward-api-c53858bf-680b-4c12-8a03-f8c8d2ff967f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:20:40.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3186" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":278,"completed":76,"skipped":1271,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:20:40.848: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2424
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:20:41.012: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2d24efa-a9e6-42fc-a61e-f531410a808f" in namespace "downward-api-2424" to be "success or failure"
Feb 17 18:20:41.018: INFO: Pod "downwardapi-volume-e2d24efa-a9e6-42fc-a61e-f531410a808f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.831631ms
Feb 17 18:20:43.023: INFO: Pod "downwardapi-volume-e2d24efa-a9e6-42fc-a61e-f531410a808f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010282024s
STEP: Saw pod success
Feb 17 18:20:43.023: INFO: Pod "downwardapi-volume-e2d24efa-a9e6-42fc-a61e-f531410a808f" satisfied condition "success or failure"
Feb 17 18:20:43.026: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downwardapi-volume-e2d24efa-a9e6-42fc-a61e-f531410a808f container client-container: <nil>
STEP: delete the pod
Feb 17 18:20:43.102: INFO: Waiting for pod downwardapi-volume-e2d24efa-a9e6-42fc-a61e-f531410a808f to disappear
Feb 17 18:20:43.107: INFO: Pod downwardapi-volume-e2d24efa-a9e6-42fc-a61e-f531410a808f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:20:43.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2424" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":77,"skipped":1283,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:20:43.121: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6632
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:20:43.272: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:20:47.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6632" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":278,"completed":78,"skipped":1288,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:20:47.323: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6921
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:20:47.839: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 17 18:20:49.854: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560447, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560447, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560447, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560447, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:20:52.879: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:20:52.884: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2065-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:21:00.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6921" for this suite.
STEP: Destroying namespace "webhook-6921-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.037 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":278,"completed":79,"skipped":1299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:21:00.360: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2994
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:21:00.531: INFO: (0) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 15.123235ms)
Feb 17 18:21:00.536: INFO: (1) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.938531ms)
Feb 17 18:21:00.542: INFO: (2) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.495805ms)
Feb 17 18:21:00.547: INFO: (3) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.017881ms)
Feb 17 18:21:00.552: INFO: (4) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.483267ms)
Feb 17 18:21:00.557: INFO: (5) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.542714ms)
Feb 17 18:21:00.562: INFO: (6) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.773757ms)
Feb 17 18:21:00.566: INFO: (7) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.471251ms)
Feb 17 18:21:00.572: INFO: (8) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.469936ms)
Feb 17 18:21:00.577: INFO: (9) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.788285ms)
Feb 17 18:21:00.581: INFO: (10) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.543881ms)
Feb 17 18:21:00.587: INFO: (11) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 6.1505ms)
Feb 17 18:21:00.592: INFO: (12) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.872075ms)
Feb 17 18:21:00.597: INFO: (13) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.677391ms)
Feb 17 18:21:00.603: INFO: (14) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.543715ms)
Feb 17 18:21:00.607: INFO: (15) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.768946ms)
Feb 17 18:21:00.612: INFO: (16) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.687184ms)
Feb 17 18:21:00.618: INFO: (17) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 6.163047ms)
Feb 17 18:21:00.623: INFO: (18) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.820187ms)
Feb 17 18:21:00.628: INFO: (19) /api/v1/nodes/ip-172-16-86-181.ec2.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.471564ms)
[AfterEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:21:00.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2994" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":278,"completed":80,"skipped":1326,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:21:00.641: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-806
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:21:00.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-806" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":278,"completed":81,"skipped":1336,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:21:00.845: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:21:19.058: INFO: Container started at 2020-02-17 18:21:03 +0000 UTC, pod became ready at 2020-02-17 18:21:18 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:21:19.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4255" for this suite.

• [SLOW TEST:18.229 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":278,"completed":82,"skipped":1342,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:21:19.074: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8836
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 17 18:21:27.288: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 18:21:27.298: INFO: Pod pod-with-poststart-http-hook still exists
Feb 17 18:21:29.298: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 18:21:29.303: INFO: Pod pod-with-poststart-http-hook still exists
Feb 17 18:21:31.298: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 18:21:31.304: INFO: Pod pod-with-poststart-http-hook still exists
Feb 17 18:21:33.298: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 18:21:33.304: INFO: Pod pod-with-poststart-http-hook still exists
Feb 17 18:21:35.298: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 18:21:35.304: INFO: Pod pod-with-poststart-http-hook still exists
Feb 17 18:21:37.298: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 18:21:37.304: INFO: Pod pod-with-poststart-http-hook still exists
Feb 17 18:21:39.298: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 18:21:39.303: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:21:39.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8836" for this suite.

• [SLOW TEST:20.243 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":278,"completed":83,"skipped":1353,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:21:39.318: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5143
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:21:43.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5143" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":278,"completed":84,"skipped":1373,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:21:43.517: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-a74a7f0b-39a3-4367-9dc8-38677d04f95e
STEP: Creating a pod to test consume configMaps
Feb 17 18:21:43.684: INFO: Waiting up to 5m0s for pod "pod-configmaps-b66cd1bc-951c-4df2-990e-bbceeee23432" in namespace "configmap-9909" to be "success or failure"
Feb 17 18:21:43.688: INFO: Pod "pod-configmaps-b66cd1bc-951c-4df2-990e-bbceeee23432": Phase="Pending", Reason="", readiness=false. Elapsed: 3.356762ms
Feb 17 18:21:45.693: INFO: Pod "pod-configmaps-b66cd1bc-951c-4df2-990e-bbceeee23432": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008273482s
Feb 17 18:21:47.698: INFO: Pod "pod-configmaps-b66cd1bc-951c-4df2-990e-bbceeee23432": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013887803s
STEP: Saw pod success
Feb 17 18:21:47.698: INFO: Pod "pod-configmaps-b66cd1bc-951c-4df2-990e-bbceeee23432" satisfied condition "success or failure"
Feb 17 18:21:47.703: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-configmaps-b66cd1bc-951c-4df2-990e-bbceeee23432 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:21:47.730: INFO: Waiting for pod pod-configmaps-b66cd1bc-951c-4df2-990e-bbceeee23432 to disappear
Feb 17 18:21:47.734: INFO: Pod pod-configmaps-b66cd1bc-951c-4df2-990e-bbceeee23432 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:21:47.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9909" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":85,"skipped":1379,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:21:47.748: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5870
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:21:47.913: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87043e49-c420-49c2-9d65-5632e52daec7" in namespace "downward-api-5870" to be "success or failure"
Feb 17 18:21:47.919: INFO: Pod "downwardapi-volume-87043e49-c420-49c2-9d65-5632e52daec7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.415183ms
Feb 17 18:21:49.924: INFO: Pod "downwardapi-volume-87043e49-c420-49c2-9d65-5632e52daec7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010245042s
Feb 17 18:21:51.929: INFO: Pod "downwardapi-volume-87043e49-c420-49c2-9d65-5632e52daec7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015735637s
STEP: Saw pod success
Feb 17 18:21:51.929: INFO: Pod "downwardapi-volume-87043e49-c420-49c2-9d65-5632e52daec7" satisfied condition "success or failure"
Feb 17 18:21:51.933: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downwardapi-volume-87043e49-c420-49c2-9d65-5632e52daec7 container client-container: <nil>
STEP: delete the pod
Feb 17 18:21:51.957: INFO: Waiting for pod downwardapi-volume-87043e49-c420-49c2-9d65-5632e52daec7 to disappear
Feb 17 18:21:51.961: INFO: Pod downwardapi-volume-87043e49-c420-49c2-9d65-5632e52daec7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:21:51.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5870" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":86,"skipped":1388,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:21:51.975: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2711
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-39abcc70-f100-4ef9-bfe6-31df8f91f0dd
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:21:56.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2711" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":87,"skipped":1388,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:21:56.202: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4147
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 17 18:21:58.409: INFO: &Pod{ObjectMeta:{send-events-cfb8d5a4-9fac-40d6-a12a-c3c5881b3de1  events-4147 /api/v1/namespaces/events-4147/pods/send-events-cfb8d5a4-9fac-40d6-a12a-c3c5881b3de1 d575249d-f154-40e1-a914-daba579ad02a 11283 0 2020-02-17 18:21:56 +0000 UTC <nil> <nil> map[name:foo time:361931797] map[cni.projectcalico.org/podIP:100.96.1.80/32 kubernetes.io/psp:e2e-test-privileged-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2jnt4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2jnt4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2jnt4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-58-212.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:21:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:21:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.58.212,PodIP:100.96.1.80,StartTime:2020-02-17 18:21:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 18:21:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://15348f4250afbad6f8ba87f018154d1c105b304b0c3538eac2e4fa7118282318,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Feb 17 18:22:00.414: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 17 18:22:02.419: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:22:02.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4147" for this suite.

• [SLOW TEST:6.242 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":278,"completed":88,"skipped":1405,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:22:02.444: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:22:02.596: INFO: Creating deployment "test-recreate-deployment"
Feb 17 18:22:02.605: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 17 18:22:02.613: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 17 18:22:04.621: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 17 18:22:04.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560522, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560522, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560522, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560522, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:22:06.631: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 17 18:22:06.640: INFO: Updating deployment test-recreate-deployment
Feb 17 18:22:06.640: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 17 18:22:06.779: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7988 /apis/apps/v1/namespaces/deployment-7988/deployments/test-recreate-deployment bcc45d71-3073-4872-bc7f-4bfcabd5c217 11382 2 2020-02-17 18:22:02 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004c0fb08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-02-17 18:22:06 +0000 UTC,LastTransitionTime:2020-02-17 18:22:06 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-02-17 18:22:06 +0000 UTC,LastTransitionTime:2020-02-17 18:22:02 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 17 18:22:06.784: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-7988 /apis/apps/v1/namespaces/deployment-7988/replicasets/test-recreate-deployment-5f94c574ff f9110a21-4e6c-4d7e-930a-9c2ce3e1495f 11380 1 2020-02-17 18:22:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment bcc45d71-3073-4872-bc7f-4bfcabd5c217 0xc004b68117 0xc004b68118}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004b68178 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 18:22:06.784: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 17 18:22:06.784: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-7988 /apis/apps/v1/namespaces/deployment-7988/replicasets/test-recreate-deployment-799c574856 53c1e92b-3ba4-49c7-b52a-5b4cb606c778 11370 2 2020-02-17 18:22:02 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment bcc45d71-3073-4872-bc7f-4bfcabd5c217 0xc004b681f7 0xc004b681f8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004b68268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 18:22:06.788: INFO: Pod "test-recreate-deployment-5f94c574ff-8krxq" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-8krxq test-recreate-deployment-5f94c574ff- deployment-7988 /api/v1/namespaces/deployment-7988/pods/test-recreate-deployment-5f94c574ff-8krxq 03cb9d99-3b5c-4e09-9858-2841c4f03723 11381 0 2020-02-17 18:22:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff f9110a21-4e6c-4d7e-930a-9c2ce3e1495f 0xc004b68837 0xc004b68838}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ks46l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ks46l,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ks46l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-58-212.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:22:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:22:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:22:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:22:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.58.212,PodIP:,StartTime:2020-02-17 18:22:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:22:06.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7988" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":278,"completed":89,"skipped":1428,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:22:06.803: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7599
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-9a001c4f-fd75-4c8a-8e47-4b2741966bdc
STEP: Creating a pod to test consume configMaps
Feb 17 18:22:07.007: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e087485d-f52a-4874-b083-b125632a7a0b" in namespace "projected-7599" to be "success or failure"
Feb 17 18:22:07.012: INFO: Pod "pod-projected-configmaps-e087485d-f52a-4874-b083-b125632a7a0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.471963ms
Feb 17 18:22:09.018: INFO: Pod "pod-projected-configmaps-e087485d-f52a-4874-b083-b125632a7a0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011191965s
Feb 17 18:22:11.024: INFO: Pod "pod-projected-configmaps-e087485d-f52a-4874-b083-b125632a7a0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016973579s
STEP: Saw pod success
Feb 17 18:22:11.024: INFO: Pod "pod-projected-configmaps-e087485d-f52a-4874-b083-b125632a7a0b" satisfied condition "success or failure"
Feb 17 18:22:11.028: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod pod-projected-configmaps-e087485d-f52a-4874-b083-b125632a7a0b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:22:11.055: INFO: Waiting for pod pod-projected-configmaps-e087485d-f52a-4874-b083-b125632a7a0b to disappear
Feb 17 18:22:11.059: INFO: Pod pod-projected-configmaps-e087485d-f52a-4874-b083-b125632a7a0b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:22:11.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7599" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":90,"skipped":1429,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:22:11.073: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-275
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:22:11.233: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:22:15.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-275" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":278,"completed":91,"skipped":1434,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:22:15.386: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:22:15.989: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:22:18.001: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560536, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560536, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560536, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560536, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:22:21.029: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:22:22.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-336" for this suite.
STEP: Destroying namespace "webhook-336-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.842 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":278,"completed":92,"skipped":1470,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:22:22.228: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2609
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-b8xzh in namespace proxy-2609
I0217 18:22:22.416518      26 runners.go:189] Created replication controller with name: proxy-service-b8xzh, namespace: proxy-2609, replica count: 1
I0217 18:22:23.467002      26 runners.go:189] proxy-service-b8xzh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 18:22:24.467286      26 runners.go:189] proxy-service-b8xzh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0217 18:22:25.467503      26 runners.go:189] proxy-service-b8xzh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0217 18:22:26.467768      26 runners.go:189] proxy-service-b8xzh Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 18:22:26.473: INFO: setup took 4.082036863s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 17 18:22:26.482: INFO: (0) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 9.115463ms)
Feb 17 18:22:26.482: INFO: (0) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 9.511716ms)
Feb 17 18:22:26.483: INFO: (0) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 10.064218ms)
Feb 17 18:22:26.484: INFO: (0) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 10.866615ms)
Feb 17 18:22:26.484: INFO: (0) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 10.860903ms)
Feb 17 18:22:26.484: INFO: (0) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 10.832078ms)
Feb 17 18:22:26.485: INFO: (0) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 12.309954ms)
Feb 17 18:22:26.485: INFO: (0) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 12.597293ms)
Feb 17 18:22:26.489: INFO: (0) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 15.932792ms)
Feb 17 18:22:26.489: INFO: (0) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 16.429114ms)
Feb 17 18:22:26.489: INFO: (0) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 16.525578ms)
Feb 17 18:22:26.490: INFO: (0) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 16.68122ms)
Feb 17 18:22:26.490: INFO: (0) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 17.403558ms)
Feb 17 18:22:26.490: INFO: (0) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 17.385032ms)
Feb 17 18:22:26.493: INFO: (0) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 19.988274ms)
Feb 17 18:22:26.493: INFO: (0) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 19.964954ms)
Feb 17 18:22:26.498: INFO: (1) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 4.880897ms)
Feb 17 18:22:26.500: INFO: (1) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 6.560597ms)
Feb 17 18:22:26.500: INFO: (1) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 6.915141ms)
Feb 17 18:22:26.500: INFO: (1) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 6.993286ms)
Feb 17 18:22:26.500: INFO: (1) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 7.277306ms)
Feb 17 18:22:26.502: INFO: (1) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 9.119028ms)
Feb 17 18:22:26.502: INFO: (1) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 9.329217ms)
Feb 17 18:22:26.503: INFO: (1) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 9.408006ms)
Feb 17 18:22:26.503: INFO: (1) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 9.891517ms)
Feb 17 18:22:26.503: INFO: (1) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 10.061878ms)
Feb 17 18:22:26.503: INFO: (1) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 10.140397ms)
Feb 17 18:22:26.504: INFO: (1) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 10.616398ms)
Feb 17 18:22:26.505: INFO: (1) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 11.794935ms)
Feb 17 18:22:26.506: INFO: (1) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 13.354276ms)
Feb 17 18:22:26.507: INFO: (1) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 13.416086ms)
Feb 17 18:22:26.508: INFO: (1) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 15.236164ms)
Feb 17 18:22:26.515: INFO: (2) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 6.411762ms)
Feb 17 18:22:26.515: INFO: (2) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 6.712216ms)
Feb 17 18:22:26.517: INFO: (2) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 8.73974ms)
Feb 17 18:22:26.517: INFO: (2) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 9.073568ms)
Feb 17 18:22:26.518: INFO: (2) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 9.275683ms)
Feb 17 18:22:26.518: INFO: (2) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 9.465647ms)
Feb 17 18:22:26.520: INFO: (2) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 11.637653ms)
Feb 17 18:22:26.520: INFO: (2) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 11.620735ms)
Feb 17 18:22:26.520: INFO: (2) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 11.760899ms)
Feb 17 18:22:26.520: INFO: (2) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 11.759958ms)
Feb 17 18:22:26.520: INFO: (2) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 12.232283ms)
Feb 17 18:22:26.520: INFO: (2) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 12.046573ms)
Feb 17 18:22:26.521: INFO: (2) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 13.019609ms)
Feb 17 18:22:26.522: INFO: (2) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 13.758476ms)
Feb 17 18:22:26.522: INFO: (2) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 13.799212ms)
Feb 17 18:22:26.523: INFO: (2) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 14.929479ms)
Feb 17 18:22:26.530: INFO: (3) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 6.180667ms)
Feb 17 18:22:26.530: INFO: (3) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 6.267434ms)
Feb 17 18:22:26.532: INFO: (3) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 8.309347ms)
Feb 17 18:22:26.533: INFO: (3) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 9.56327ms)
Feb 17 18:22:26.534: INFO: (3) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 10.843435ms)
Feb 17 18:22:26.535: INFO: (3) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 11.562813ms)
Feb 17 18:22:26.536: INFO: (3) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 12.525649ms)
Feb 17 18:22:26.536: INFO: (3) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 12.727143ms)
Feb 17 18:22:26.536: INFO: (3) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 12.65579ms)
Feb 17 18:22:26.537: INFO: (3) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 13.140198ms)
Feb 17 18:22:26.537: INFO: (3) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 13.919126ms)
Feb 17 18:22:26.538: INFO: (3) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 14.142719ms)
Feb 17 18:22:26.538: INFO: (3) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 14.449226ms)
Feb 17 18:22:26.538: INFO: (3) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 14.754461ms)
Feb 17 18:22:26.539: INFO: (3) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 15.501721ms)
Feb 17 18:22:26.539: INFO: (3) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 15.506389ms)
Feb 17 18:22:26.545: INFO: (4) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 5.699595ms)
Feb 17 18:22:26.546: INFO: (4) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 6.488722ms)
Feb 17 18:22:26.546: INFO: (4) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 7.363609ms)
Feb 17 18:22:26.548: INFO: (4) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 9.185509ms)
Feb 17 18:22:26.549: INFO: (4) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 9.398794ms)
Feb 17 18:22:26.549: INFO: (4) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 9.967228ms)
Feb 17 18:22:26.550: INFO: (4) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 11.169751ms)
Feb 17 18:22:26.551: INFO: (4) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 11.535926ms)
Feb 17 18:22:26.551: INFO: (4) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 11.873485ms)
Feb 17 18:22:26.551: INFO: (4) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 12.33053ms)
Feb 17 18:22:26.552: INFO: (4) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 12.452037ms)
Feb 17 18:22:26.552: INFO: (4) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 13.043238ms)
Feb 17 18:22:26.552: INFO: (4) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 13.007076ms)
Feb 17 18:22:26.552: INFO: (4) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 13.329814ms)
Feb 17 18:22:26.554: INFO: (4) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 14.738563ms)
Feb 17 18:22:26.554: INFO: (4) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 15.003583ms)
Feb 17 18:22:26.559: INFO: (5) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 5.024654ms)
Feb 17 18:22:26.561: INFO: (5) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 6.533868ms)
Feb 17 18:22:26.561: INFO: (5) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 6.779466ms)
Feb 17 18:22:26.561: INFO: (5) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 6.750046ms)
Feb 17 18:22:26.563: INFO: (5) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 8.437537ms)
Feb 17 18:22:26.563: INFO: (5) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 9.061769ms)
Feb 17 18:22:26.564: INFO: (5) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 9.600739ms)
Feb 17 18:22:26.564: INFO: (5) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 10.099756ms)
Feb 17 18:22:26.564: INFO: (5) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 10.052001ms)
Feb 17 18:22:26.565: INFO: (5) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 10.589166ms)
Feb 17 18:22:26.566: INFO: (5) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 11.540468ms)
Feb 17 18:22:26.566: INFO: (5) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 11.815915ms)
Feb 17 18:22:26.566: INFO: (5) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 11.790624ms)
Feb 17 18:22:26.566: INFO: (5) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 11.958339ms)
Feb 17 18:22:26.567: INFO: (5) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 12.40939ms)
Feb 17 18:22:26.568: INFO: (5) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 13.050842ms)
Feb 17 18:22:26.572: INFO: (6) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 4.739154ms)
Feb 17 18:22:26.573: INFO: (6) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 5.735587ms)
Feb 17 18:22:26.574: INFO: (6) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 6.294479ms)
Feb 17 18:22:26.574: INFO: (6) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 6.37954ms)
Feb 17 18:22:26.576: INFO: (6) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 7.983185ms)
Feb 17 18:22:26.577: INFO: (6) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 9.066911ms)
Feb 17 18:22:26.577: INFO: (6) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 9.549186ms)
Feb 17 18:22:26.578: INFO: (6) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 10.377742ms)
Feb 17 18:22:26.578: INFO: (6) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 10.546543ms)
Feb 17 18:22:26.578: INFO: (6) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 10.526489ms)
Feb 17 18:22:26.578: INFO: (6) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 10.578502ms)
Feb 17 18:22:26.579: INFO: (6) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 11.745699ms)
Feb 17 18:22:26.580: INFO: (6) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 12.003363ms)
Feb 17 18:22:26.580: INFO: (6) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 12.137119ms)
Feb 17 18:22:26.581: INFO: (6) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 13.109135ms)
Feb 17 18:22:26.581: INFO: (6) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 13.208792ms)
Feb 17 18:22:26.585: INFO: (7) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 4.397719ms)
Feb 17 18:22:26.588: INFO: (7) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 6.951514ms)
Feb 17 18:22:26.589: INFO: (7) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 7.472587ms)
Feb 17 18:22:26.589: INFO: (7) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 7.553122ms)
Feb 17 18:22:26.589: INFO: (7) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 7.575058ms)
Feb 17 18:22:26.591: INFO: (7) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 10.120077ms)
Feb 17 18:22:26.591: INFO: (7) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 10.170049ms)
Feb 17 18:22:26.592: INFO: (7) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 10.431305ms)
Feb 17 18:22:26.592: INFO: (7) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 10.790339ms)
Feb 17 18:22:26.592: INFO: (7) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 10.743198ms)
Feb 17 18:22:26.593: INFO: (7) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 11.418707ms)
Feb 17 18:22:26.593: INFO: (7) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 11.831863ms)
Feb 17 18:22:26.593: INFO: (7) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 12.178362ms)
Feb 17 18:22:26.594: INFO: (7) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 12.674596ms)
Feb 17 18:22:26.595: INFO: (7) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 13.499574ms)
Feb 17 18:22:26.596: INFO: (7) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 14.884997ms)
Feb 17 18:22:26.602: INFO: (8) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 5.55179ms)
Feb 17 18:22:26.604: INFO: (8) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 8.14976ms)
Feb 17 18:22:26.606: INFO: (8) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 9.904351ms)
Feb 17 18:22:26.607: INFO: (8) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 10.415992ms)
Feb 17 18:22:26.607: INFO: (8) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 11.18729ms)
Feb 17 18:22:26.608: INFO: (8) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 11.526638ms)
Feb 17 18:22:26.608: INFO: (8) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 11.956317ms)
Feb 17 18:22:26.610: INFO: (8) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 13.928985ms)
Feb 17 18:22:26.610: INFO: (8) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 14.166197ms)
Feb 17 18:22:26.610: INFO: (8) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 14.058593ms)
Feb 17 18:22:26.610: INFO: (8) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 13.992454ms)
Feb 17 18:22:26.610: INFO: (8) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 14.138938ms)
Feb 17 18:22:26.610: INFO: (8) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 14.189329ms)
Feb 17 18:22:26.611: INFO: (8) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 14.492881ms)
Feb 17 18:22:26.612: INFO: (8) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 15.844007ms)
Feb 17 18:22:26.612: INFO: (8) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 16.165993ms)
Feb 17 18:22:26.622: INFO: (9) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 9.590115ms)
Feb 17 18:22:26.624: INFO: (9) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 11.125155ms)
Feb 17 18:22:26.624: INFO: (9) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 11.622054ms)
Feb 17 18:22:26.624: INFO: (9) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 11.922219ms)
Feb 17 18:22:26.625: INFO: (9) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 12.308784ms)
Feb 17 18:22:26.625: INFO: (9) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 12.915041ms)
Feb 17 18:22:26.626: INFO: (9) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 13.825053ms)
Feb 17 18:22:26.626: INFO: (9) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 14.101585ms)
Feb 17 18:22:26.627: INFO: (9) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 14.169778ms)
Feb 17 18:22:26.627: INFO: (9) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 14.423705ms)
Feb 17 18:22:26.627: INFO: (9) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 14.686631ms)
Feb 17 18:22:26.628: INFO: (9) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 15.683266ms)
Feb 17 18:22:26.629: INFO: (9) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 16.347207ms)
Feb 17 18:22:26.629: INFO: (9) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 16.763067ms)
Feb 17 18:22:26.629: INFO: (9) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 16.9429ms)
Feb 17 18:22:26.630: INFO: (9) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 18.006687ms)
Feb 17 18:22:26.635: INFO: (10) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 4.603215ms)
Feb 17 18:22:26.636: INFO: (10) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 5.406595ms)
Feb 17 18:22:26.637: INFO: (10) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 6.365558ms)
Feb 17 18:22:26.638: INFO: (10) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 7.916202ms)
Feb 17 18:22:26.639: INFO: (10) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 7.911407ms)
Feb 17 18:22:26.639: INFO: (10) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 7.617058ms)
Feb 17 18:22:26.639: INFO: (10) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 7.819249ms)
Feb 17 18:22:26.640: INFO: (10) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 9.122284ms)
Feb 17 18:22:26.640: INFO: (10) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 9.423716ms)
Feb 17 18:22:26.640: INFO: (10) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 9.201641ms)
Feb 17 18:22:26.641: INFO: (10) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 10.586344ms)
Feb 17 18:22:26.642: INFO: (10) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 10.807204ms)
Feb 17 18:22:26.642: INFO: (10) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 11.260974ms)
Feb 17 18:22:26.643: INFO: (10) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 11.955223ms)
Feb 17 18:22:26.643: INFO: (10) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 12.297503ms)
Feb 17 18:22:26.643: INFO: (10) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 12.67863ms)
Feb 17 18:22:26.649: INFO: (11) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 5.2815ms)
Feb 17 18:22:26.650: INFO: (11) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 6.86221ms)
Feb 17 18:22:26.651: INFO: (11) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 7.302461ms)
Feb 17 18:22:26.651: INFO: (11) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 7.271182ms)
Feb 17 18:22:26.651: INFO: (11) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 7.683254ms)
Feb 17 18:22:26.651: INFO: (11) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 7.846511ms)
Feb 17 18:22:26.652: INFO: (11) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 8.701726ms)
Feb 17 18:22:26.653: INFO: (11) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 9.355265ms)
Feb 17 18:22:26.653: INFO: (11) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 9.559275ms)
Feb 17 18:22:26.654: INFO: (11) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 10.073971ms)
Feb 17 18:22:26.654: INFO: (11) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 10.182826ms)
Feb 17 18:22:26.654: INFO: (11) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 10.715897ms)
Feb 17 18:22:26.655: INFO: (11) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 11.611941ms)
Feb 17 18:22:26.656: INFO: (11) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 12.624751ms)
Feb 17 18:22:26.657: INFO: (11) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 12.887833ms)
Feb 17 18:22:26.657: INFO: (11) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 13.430828ms)
Feb 17 18:22:26.665: INFO: (12) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 7.319207ms)
Feb 17 18:22:26.665: INFO: (12) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 7.255747ms)
Feb 17 18:22:26.665: INFO: (12) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 7.446078ms)
Feb 17 18:22:26.665: INFO: (12) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 7.641615ms)
Feb 17 18:22:26.668: INFO: (12) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 10.375665ms)
Feb 17 18:22:26.668: INFO: (12) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 10.892292ms)
Feb 17 18:22:26.668: INFO: (12) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 10.952894ms)
Feb 17 18:22:26.669: INFO: (12) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 11.266547ms)
Feb 17 18:22:26.669: INFO: (12) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 11.638198ms)
Feb 17 18:22:26.669: INFO: (12) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 11.704465ms)
Feb 17 18:22:26.669: INFO: (12) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 11.766305ms)
Feb 17 18:22:26.669: INFO: (12) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 12.088682ms)
Feb 17 18:22:26.671: INFO: (12) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 13.128833ms)
Feb 17 18:22:26.671: INFO: (12) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 13.466585ms)
Feb 17 18:22:26.672: INFO: (12) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 14.986351ms)
Feb 17 18:22:26.673: INFO: (12) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 15.340438ms)
Feb 17 18:22:26.677: INFO: (13) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 4.509499ms)
Feb 17 18:22:26.678: INFO: (13) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 5.193288ms)
Feb 17 18:22:26.679: INFO: (13) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 5.970365ms)
Feb 17 18:22:26.680: INFO: (13) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 6.321194ms)
Feb 17 18:22:26.680: INFO: (13) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 7.153336ms)
Feb 17 18:22:26.680: INFO: (13) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 7.146994ms)
Feb 17 18:22:26.680: INFO: (13) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 7.464352ms)
Feb 17 18:22:26.681: INFO: (13) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 7.901977ms)
Feb 17 18:22:26.682: INFO: (13) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 9.103181ms)
Feb 17 18:22:26.683: INFO: (13) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 9.992358ms)
Feb 17 18:22:26.684: INFO: (13) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 10.540183ms)
Feb 17 18:22:26.684: INFO: (13) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 10.471314ms)
Feb 17 18:22:26.684: INFO: (13) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 10.763618ms)
Feb 17 18:22:26.686: INFO: (13) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 12.54392ms)
Feb 17 18:22:26.686: INFO: (13) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 12.637827ms)
Feb 17 18:22:26.686: INFO: (13) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 13.098058ms)
Feb 17 18:22:26.691: INFO: (14) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 4.870899ms)
Feb 17 18:22:26.692: INFO: (14) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 5.850331ms)
Feb 17 18:22:26.693: INFO: (14) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 6.291139ms)
Feb 17 18:22:26.693: INFO: (14) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 6.82012ms)
Feb 17 18:22:26.693: INFO: (14) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 6.697696ms)
Feb 17 18:22:26.696: INFO: (14) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 9.192469ms)
Feb 17 18:22:26.696: INFO: (14) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 9.38556ms)
Feb 17 18:22:26.696: INFO: (14) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 9.432917ms)
Feb 17 18:22:26.696: INFO: (14) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 9.495765ms)
Feb 17 18:22:26.696: INFO: (14) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 9.643791ms)
Feb 17 18:22:26.697: INFO: (14) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 9.916526ms)
Feb 17 18:22:26.697: INFO: (14) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 10.541314ms)
Feb 17 18:22:26.697: INFO: (14) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 11.010721ms)
Feb 17 18:22:26.698: INFO: (14) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 11.675483ms)
Feb 17 18:22:26.698: INFO: (14) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 11.686039ms)
Feb 17 18:22:26.700: INFO: (14) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 13.03192ms)
Feb 17 18:22:26.705: INFO: (15) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 4.857446ms)
Feb 17 18:22:26.708: INFO: (15) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 7.805903ms)
Feb 17 18:22:26.709: INFO: (15) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 8.921647ms)
Feb 17 18:22:26.709: INFO: (15) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 9.010968ms)
Feb 17 18:22:26.709: INFO: (15) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 9.207253ms)
Feb 17 18:22:26.709: INFO: (15) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 9.486387ms)
Feb 17 18:22:26.710: INFO: (15) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 10.118977ms)
Feb 17 18:22:26.710: INFO: (15) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 10.030716ms)
Feb 17 18:22:26.711: INFO: (15) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 10.999114ms)
Feb 17 18:22:26.711: INFO: (15) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 10.825965ms)
Feb 17 18:22:26.711: INFO: (15) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 10.99339ms)
Feb 17 18:22:26.711: INFO: (15) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 11.21248ms)
Feb 17 18:22:26.718: INFO: (15) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 17.963339ms)
Feb 17 18:22:26.718: INFO: (15) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 18.567729ms)
Feb 17 18:22:26.721: INFO: (15) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 21.414236ms)
Feb 17 18:22:26.723: INFO: (15) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 23.053177ms)
Feb 17 18:22:26.728: INFO: (16) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 5.580535ms)
Feb 17 18:22:26.731: INFO: (16) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 7.710179ms)
Feb 17 18:22:26.731: INFO: (16) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 8.047114ms)
Feb 17 18:22:26.732: INFO: (16) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 9.324793ms)
Feb 17 18:22:26.732: INFO: (16) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 9.299107ms)
Feb 17 18:22:26.733: INFO: (16) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 9.751496ms)
Feb 17 18:22:26.733: INFO: (16) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 10.224657ms)
Feb 17 18:22:26.733: INFO: (16) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 10.430681ms)
Feb 17 18:22:26.733: INFO: (16) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 10.340102ms)
Feb 17 18:22:26.734: INFO: (16) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 10.652248ms)
Feb 17 18:22:26.734: INFO: (16) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 10.894897ms)
Feb 17 18:22:26.735: INFO: (16) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 11.813332ms)
Feb 17 18:22:26.736: INFO: (16) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 12.905787ms)
Feb 17 18:22:26.736: INFO: (16) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 12.915026ms)
Feb 17 18:22:26.736: INFO: (16) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 13.085985ms)
Feb 17 18:22:26.736: INFO: (16) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 13.065566ms)
Feb 17 18:22:26.743: INFO: (17) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 6.938779ms)
Feb 17 18:22:26.745: INFO: (17) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 8.760827ms)
Feb 17 18:22:26.745: INFO: (17) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 8.793246ms)
Feb 17 18:22:26.748: INFO: (17) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 11.157504ms)
Feb 17 18:22:26.748: INFO: (17) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 11.573612ms)
Feb 17 18:22:26.748: INFO: (17) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 11.791989ms)
Feb 17 18:22:26.749: INFO: (17) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 12.352402ms)
Feb 17 18:22:26.749: INFO: (17) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 12.235437ms)
Feb 17 18:22:26.749: INFO: (17) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 12.361374ms)
Feb 17 18:22:26.749: INFO: (17) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 13.139949ms)
Feb 17 18:22:26.750: INFO: (17) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 13.449595ms)
Feb 17 18:22:26.750: INFO: (17) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 13.954286ms)
Feb 17 18:22:26.751: INFO: (17) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 14.583547ms)
Feb 17 18:22:26.752: INFO: (17) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 15.21768ms)
Feb 17 18:22:26.752: INFO: (17) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 15.57081ms)
Feb 17 18:22:26.753: INFO: (17) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 17.313246ms)
Feb 17 18:22:26.761: INFO: (18) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 7.067235ms)
Feb 17 18:22:26.761: INFO: (18) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 7.321159ms)
Feb 17 18:22:26.762: INFO: (18) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 8.314314ms)
Feb 17 18:22:26.763: INFO: (18) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 9.022457ms)
Feb 17 18:22:26.763: INFO: (18) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 9.412477ms)
Feb 17 18:22:26.764: INFO: (18) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 10.397834ms)
Feb 17 18:22:26.764: INFO: (18) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 10.643717ms)
Feb 17 18:22:26.765: INFO: (18) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 11.136858ms)
Feb 17 18:22:26.765: INFO: (18) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 11.437676ms)
Feb 17 18:22:26.765: INFO: (18) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 11.531551ms)
Feb 17 18:22:26.765: INFO: (18) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 11.519957ms)
Feb 17 18:22:26.766: INFO: (18) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 12.141148ms)
Feb 17 18:22:26.766: INFO: (18) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 12.529523ms)
Feb 17 18:22:26.767: INFO: (18) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 13.223845ms)
Feb 17 18:22:26.767: INFO: (18) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 13.840346ms)
Feb 17 18:22:26.769: INFO: (18) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 14.935757ms)
Feb 17 18:22:26.774: INFO: (19) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:462/proxy/: tls qux (200; 5.550241ms)
Feb 17 18:22:26.775: INFO: (19) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:443/proxy/tlsrewritem... (200; 6.089776ms)
Feb 17 18:22:26.776: INFO: (19) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 6.603149ms)
Feb 17 18:22:26.776: INFO: (19) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj/proxy/rewriteme">test</a> (200; 7.223228ms)
Feb 17 18:22:26.777: INFO: (19) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">test<... (200; 7.626377ms)
Feb 17 18:22:26.777: INFO: (19) /api/v1/namespaces/proxy-2609/pods/proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 7.932301ms)
Feb 17 18:22:26.778: INFO: (19) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:160/proxy/: foo (200; 9.183401ms)
Feb 17 18:22:26.778: INFO: (19) /api/v1/namespaces/proxy-2609/pods/https:proxy-service-b8xzh-qmnwj:460/proxy/: tls baz (200; 9.279507ms)
Feb 17 18:22:26.779: INFO: (19) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/: <a href="/api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:1080/proxy/rewriteme">... (200; 10.142888ms)
Feb 17 18:22:26.779: INFO: (19) /api/v1/namespaces/proxy-2609/pods/http:proxy-service-b8xzh-qmnwj:162/proxy/: bar (200; 10.111709ms)
Feb 17 18:22:26.779: INFO: (19) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname1/proxy/: foo (200; 10.203677ms)
Feb 17 18:22:26.780: INFO: (19) /api/v1/namespaces/proxy-2609/services/proxy-service-b8xzh:portname2/proxy/: bar (200; 10.976065ms)
Feb 17 18:22:26.781: INFO: (19) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname2/proxy/: tls qux (200; 11.701433ms)
Feb 17 18:22:26.781: INFO: (19) /api/v1/namespaces/proxy-2609/services/https:proxy-service-b8xzh:tlsportname1/proxy/: tls baz (200; 12.406464ms)
Feb 17 18:22:26.782: INFO: (19) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname2/proxy/: bar (200; 12.688814ms)
Feb 17 18:22:26.782: INFO: (19) /api/v1/namespaces/proxy-2609/services/http:proxy-service-b8xzh:portname1/proxy/: foo (200; 13.259744ms)
STEP: deleting ReplicationController proxy-service-b8xzh in namespace proxy-2609, will wait for the garbage collector to delete the pods
Feb 17 18:22:26.851: INFO: Deleting ReplicationController proxy-service-b8xzh took: 14.68304ms
Feb 17 18:22:26.952: INFO: Terminating ReplicationController proxy-service-b8xzh pods took: 100.262058ms
[AfterEach] version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:22:28.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2609" for this suite.

• [SLOW TEST:6.339 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":278,"completed":93,"skipped":1479,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:22:28.567: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9464
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:22:28.725: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25f1df9b-3659-4367-aafc-b97380b6f69b" in namespace "projected-9464" to be "success or failure"
Feb 17 18:22:28.729: INFO: Pod "downwardapi-volume-25f1df9b-3659-4367-aafc-b97380b6f69b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.523499ms
Feb 17 18:22:30.733: INFO: Pod "downwardapi-volume-25f1df9b-3659-4367-aafc-b97380b6f69b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008101029s
STEP: Saw pod success
Feb 17 18:22:30.733: INFO: Pod "downwardapi-volume-25f1df9b-3659-4367-aafc-b97380b6f69b" satisfied condition "success or failure"
Feb 17 18:22:30.738: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downwardapi-volume-25f1df9b-3659-4367-aafc-b97380b6f69b container client-container: <nil>
STEP: delete the pod
Feb 17 18:22:30.766: INFO: Waiting for pod downwardapi-volume-25f1df9b-3659-4367-aafc-b97380b6f69b to disappear
Feb 17 18:22:30.770: INFO: Pod downwardapi-volume-25f1df9b-3659-4367-aafc-b97380b6f69b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:22:30.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9464" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":278,"completed":94,"skipped":1481,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:22:30.784: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-6d7m
STEP: Creating a pod to test atomic-volume-subpath
Feb 17 18:22:30.968: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6d7m" in namespace "subpath-9696" to be "success or failure"
Feb 17 18:22:30.972: INFO: Pod "pod-subpath-test-projected-6d7m": Phase="Pending", Reason="", readiness=false. Elapsed: 3.459941ms
Feb 17 18:22:32.981: INFO: Pod "pod-subpath-test-projected-6d7m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013074658s
Feb 17 18:22:34.986: INFO: Pod "pod-subpath-test-projected-6d7m": Phase="Running", Reason="", readiness=true. Elapsed: 4.018179989s
Feb 17 18:22:36.991: INFO: Pod "pod-subpath-test-projected-6d7m": Phase="Running", Reason="", readiness=true. Elapsed: 6.023336318s
Feb 17 18:22:38.996: INFO: Pod "pod-subpath-test-projected-6d7m": Phase="Running", Reason="", readiness=true. Elapsed: 8.027490539s
Feb 17 18:22:41.001: INFO: Pod "pod-subpath-test-projected-6d7m": Phase="Running", Reason="", readiness=true. Elapsed: 10.032609448s
Feb 17 18:22:43.006: INFO: Pod "pod-subpath-test-projected-6d7m": Phase="Running", Reason="", readiness=true. Elapsed: 12.037927928s
Feb 17 18:22:45.011: INFO: Pod "pod-subpath-test-projected-6d7m": Phase="Running", Reason="", readiness=true. Elapsed: 14.042937277s
Feb 17 18:22:47.015: INFO: Pod "pod-subpath-test-projected-6d7m": Phase="Running", Reason="", readiness=true. Elapsed: 16.046640415s
Feb 17 18:22:49.019: INFO: Pod "pod-subpath-test-projected-6d7m": Phase="Running", Reason="", readiness=true. Elapsed: 18.050455559s
Feb 17 18:22:51.024: INFO: Pod "pod-subpath-test-projected-6d7m": Phase="Running", Reason="", readiness=true. Elapsed: 20.055746753s
Feb 17 18:22:53.028: INFO: Pod "pod-subpath-test-projected-6d7m": Phase="Running", Reason="", readiness=true. Elapsed: 22.059389851s
Feb 17 18:22:55.031: INFO: Pod "pod-subpath-test-projected-6d7m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.062933112s
STEP: Saw pod success
Feb 17 18:22:55.031: INFO: Pod "pod-subpath-test-projected-6d7m" satisfied condition "success or failure"
Feb 17 18:22:55.035: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-subpath-test-projected-6d7m container test-container-subpath-projected-6d7m: <nil>
STEP: delete the pod
Feb 17 18:22:55.071: INFO: Waiting for pod pod-subpath-test-projected-6d7m to disappear
Feb 17 18:22:55.074: INFO: Pod pod-subpath-test-projected-6d7m no longer exists
STEP: Deleting pod pod-subpath-test-projected-6d7m
Feb 17 18:22:55.074: INFO: Deleting pod "pod-subpath-test-projected-6d7m" in namespace "subpath-9696"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:22:55.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9696" for this suite.

• [SLOW TEST:24.305 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":278,"completed":95,"skipped":1487,"failed":0}
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:22:55.089: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8691
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:22:55.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8691" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":278,"completed":96,"skipped":1487,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:22:55.290: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6884
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 17 18:22:55.439: INFO: PodSpec: initContainers in spec.initContainers
Feb 17 18:23:42.519: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-1793a678-36b5-4983-869f-99bc613570d0", GenerateName:"", Namespace:"init-container-6884", SelfLink:"/api/v1/namespaces/init-container-6884/pods/pod-init-1793a678-36b5-4983-869f-99bc613570d0", UID:"fc1c47d8-c88a-4732-b249-16499b6b4d95", ResourceVersion:"12040", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63717560575, loc:(*time.Location)(0x7db7bc0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"439031762"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.5.34/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-7kcv8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc003cef7c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7kcv8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7kcv8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7kcv8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00503b2a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-16-86-181.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002699140), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00503b330)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00503b350)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00503b358), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00503b35c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560575, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560575, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560575, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560575, loc:(*time.Location)(0x7db7bc0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.86.181", PodIP:"100.96.5.34", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.96.5.34"}}, StartTime:(*v1.Time)(0xc0034d9f00), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00074c7e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00074c850)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://d809c98807f01bb4fe01c020701855b8eec4468fa7f2d5b4b321de8a7f207781", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0034d9f40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0034d9f20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc00503b3df)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:23:42.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6884" for this suite.

• [SLOW TEST:47.243 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":278,"completed":97,"skipped":1491,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:23:42.533: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7637
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7637.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7637.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 18:23:46.743: INFO: DNS probes using dns-7637/dns-test-254ccebe-d1c1-42ad-bb4e-e6ffcfd6a543 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:23:46.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7637" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":278,"completed":98,"skipped":1509,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:23:46.775: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-54ea8cfe-8aab-4c2e-95a4-266d637f0684
STEP: Creating a pod to test consume secrets
Feb 17 18:23:46.942: INFO: Waiting up to 5m0s for pod "pod-secrets-f59366eb-08b4-4223-b89f-e120fbdbe76e" in namespace "secrets-8339" to be "success or failure"
Feb 17 18:23:46.949: INFO: Pod "pod-secrets-f59366eb-08b4-4223-b89f-e120fbdbe76e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.767731ms
Feb 17 18:23:48.952: INFO: Pod "pod-secrets-f59366eb-08b4-4223-b89f-e120fbdbe76e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010367005s
Feb 17 18:23:50.957: INFO: Pod "pod-secrets-f59366eb-08b4-4223-b89f-e120fbdbe76e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015270533s
STEP: Saw pod success
Feb 17 18:23:50.957: INFO: Pod "pod-secrets-f59366eb-08b4-4223-b89f-e120fbdbe76e" satisfied condition "success or failure"
Feb 17 18:23:50.960: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-secrets-f59366eb-08b4-4223-b89f-e120fbdbe76e container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 18:23:50.986: INFO: Waiting for pod pod-secrets-f59366eb-08b4-4223-b89f-e120fbdbe76e to disappear
Feb 17 18:23:50.990: INFO: Pod pod-secrets-f59366eb-08b4-4223-b89f-e120fbdbe76e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:23:50.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8339" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":99,"skipped":1543,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:23:51.002: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-2905
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:23:51.145: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2905
I0217 18:23:51.157152      26 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2905, replica count: 1
I0217 18:23:52.207663      26 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 18:23:53.208002      26 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 18:23:53.336: INFO: Created: latency-svc-n75dt
Feb 17 18:23:53.342: INFO: Got endpoints: latency-svc-n75dt [33.730004ms]
Feb 17 18:23:53.366: INFO: Created: latency-svc-rkvgc
Feb 17 18:23:53.384: INFO: Created: latency-svc-m5tm4
Feb 17 18:23:53.384: INFO: Got endpoints: latency-svc-rkvgc [42.238516ms]
Feb 17 18:23:53.400: INFO: Got endpoints: latency-svc-m5tm4 [58.03926ms]
Feb 17 18:23:53.412: INFO: Created: latency-svc-p79q6
Feb 17 18:23:53.416: INFO: Got endpoints: latency-svc-p79q6 [74.226174ms]
Feb 17 18:23:53.437: INFO: Created: latency-svc-68lcg
Feb 17 18:23:53.449: INFO: Created: latency-svc-6874x
Feb 17 18:23:53.456: INFO: Got endpoints: latency-svc-68lcg [114.224916ms]
Feb 17 18:23:53.472: INFO: Got endpoints: latency-svc-6874x [130.087338ms]
Feb 17 18:23:53.492: INFO: Created: latency-svc-v25j4
Feb 17 18:23:53.493: INFO: Got endpoints: latency-svc-v25j4 [151.050294ms]
Feb 17 18:23:53.510: INFO: Created: latency-svc-szhkx
Feb 17 18:23:53.527: INFO: Got endpoints: latency-svc-szhkx [184.835636ms]
Feb 17 18:23:53.530: INFO: Created: latency-svc-db57h
Feb 17 18:23:53.546: INFO: Got endpoints: latency-svc-db57h [203.715622ms]
Feb 17 18:23:53.548: INFO: Created: latency-svc-p99s5
Feb 17 18:23:53.556: INFO: Got endpoints: latency-svc-p99s5 [213.940636ms]
Feb 17 18:23:53.565: INFO: Created: latency-svc-br26j
Feb 17 18:23:53.580: INFO: Created: latency-svc-xp7nv
Feb 17 18:23:53.581: INFO: Got endpoints: latency-svc-br26j [239.349594ms]
Feb 17 18:23:53.602: INFO: Got endpoints: latency-svc-xp7nv [259.95252ms]
Feb 17 18:23:53.607: INFO: Created: latency-svc-z2qvw
Feb 17 18:23:53.616: INFO: Got endpoints: latency-svc-z2qvw [273.711249ms]
Feb 17 18:23:53.629: INFO: Created: latency-svc-9dkgs
Feb 17 18:23:53.643: INFO: Got endpoints: latency-svc-9dkgs [300.889334ms]
Feb 17 18:23:53.645: INFO: Created: latency-svc-hh4qg
Feb 17 18:23:53.654: INFO: Got endpoints: latency-svc-hh4qg [311.990135ms]
Feb 17 18:23:53.667: INFO: Created: latency-svc-tcsmc
Feb 17 18:23:53.670: INFO: Got endpoints: latency-svc-tcsmc [327.922938ms]
Feb 17 18:23:53.685: INFO: Created: latency-svc-fzpl7
Feb 17 18:23:53.693: INFO: Got endpoints: latency-svc-fzpl7 [309.516456ms]
Feb 17 18:23:53.710: INFO: Created: latency-svc-p8k7r
Feb 17 18:23:53.722: INFO: Got endpoints: latency-svc-p8k7r [322.077278ms]
Feb 17 18:23:53.732: INFO: Created: latency-svc-xhm2j
Feb 17 18:23:53.737: INFO: Got endpoints: latency-svc-xhm2j [320.999943ms]
Feb 17 18:23:53.751: INFO: Created: latency-svc-blp7v
Feb 17 18:23:53.757: INFO: Got endpoints: latency-svc-blp7v [301.017011ms]
Feb 17 18:23:53.771: INFO: Created: latency-svc-p6ghd
Feb 17 18:23:53.788: INFO: Got endpoints: latency-svc-p6ghd [316.06575ms]
Feb 17 18:23:53.791: INFO: Created: latency-svc-bz9cm
Feb 17 18:23:53.806: INFO: Got endpoints: latency-svc-bz9cm [312.561743ms]
Feb 17 18:23:53.814: INFO: Created: latency-svc-4m5fr
Feb 17 18:23:53.831: INFO: Got endpoints: latency-svc-4m5fr [304.252162ms]
Feb 17 18:23:53.833: INFO: Created: latency-svc-g5jnn
Feb 17 18:23:53.845: INFO: Got endpoints: latency-svc-g5jnn [299.098824ms]
Feb 17 18:23:53.858: INFO: Created: latency-svc-zx5x9
Feb 17 18:23:53.861: INFO: Got endpoints: latency-svc-zx5x9 [304.510258ms]
Feb 17 18:23:53.878: INFO: Created: latency-svc-j6zzq
Feb 17 18:23:53.896: INFO: Created: latency-svc-dxhtb
Feb 17 18:23:53.899: INFO: Got endpoints: latency-svc-j6zzq [317.263107ms]
Feb 17 18:23:53.907: INFO: Got endpoints: latency-svc-dxhtb [305.19677ms]
Feb 17 18:23:53.926: INFO: Created: latency-svc-7n2dr
Feb 17 18:23:53.949: INFO: Got endpoints: latency-svc-7n2dr [333.104547ms]
Feb 17 18:23:53.954: INFO: Created: latency-svc-95s6z
Feb 17 18:23:53.970: INFO: Created: latency-svc-4hxlg
Feb 17 18:23:53.971: INFO: Got endpoints: latency-svc-95s6z [328.094391ms]
Feb 17 18:23:53.980: INFO: Got endpoints: latency-svc-4hxlg [325.41065ms]
Feb 17 18:23:53.997: INFO: Created: latency-svc-sj975
Feb 17 18:23:54.005: INFO: Got endpoints: latency-svc-sj975 [334.654162ms]
Feb 17 18:23:54.014: INFO: Created: latency-svc-gttcn
Feb 17 18:23:54.018: INFO: Got endpoints: latency-svc-gttcn [324.929427ms]
Feb 17 18:23:54.035: INFO: Created: latency-svc-944l4
Feb 17 18:23:54.047: INFO: Got endpoints: latency-svc-944l4 [42.171834ms]
Feb 17 18:23:54.057: INFO: Created: latency-svc-lfbz8
Feb 17 18:23:54.064: INFO: Got endpoints: latency-svc-lfbz8 [341.558558ms]
Feb 17 18:23:54.078: INFO: Created: latency-svc-smjns
Feb 17 18:23:54.086: INFO: Got endpoints: latency-svc-smjns [348.664597ms]
Feb 17 18:23:54.115: INFO: Created: latency-svc-p5c8c
Feb 17 18:23:54.122: INFO: Got endpoints: latency-svc-p5c8c [333.75182ms]
Feb 17 18:23:54.126: INFO: Created: latency-svc-5kpgj
Feb 17 18:23:54.126: INFO: Got endpoints: latency-svc-5kpgj [368.656637ms]
Feb 17 18:23:54.158: INFO: Created: latency-svc-242p5
Feb 17 18:23:54.177: INFO: Got endpoints: latency-svc-242p5 [371.424379ms]
Feb 17 18:23:54.183: INFO: Created: latency-svc-l6mzt
Feb 17 18:23:54.197: INFO: Got endpoints: latency-svc-l6mzt [366.154349ms]
Feb 17 18:23:54.207: INFO: Created: latency-svc-gdbmb
Feb 17 18:23:54.234: INFO: Got endpoints: latency-svc-gdbmb [388.918929ms]
Feb 17 18:23:54.305: INFO: Created: latency-svc-msbk4
Feb 17 18:23:54.329: INFO: Got endpoints: latency-svc-msbk4 [468.120542ms]
Feb 17 18:23:54.331: INFO: Created: latency-svc-bhtth
Feb 17 18:23:54.341: INFO: Got endpoints: latency-svc-bhtth [442.19877ms]
Feb 17 18:23:54.353: INFO: Created: latency-svc-74xv8
Feb 17 18:23:54.404: INFO: Got endpoints: latency-svc-74xv8 [496.625339ms]
Feb 17 18:23:54.411: INFO: Created: latency-svc-mhgkw
Feb 17 18:23:54.430: INFO: Got endpoints: latency-svc-mhgkw [481.054423ms]
Feb 17 18:23:54.432: INFO: Created: latency-svc-7lnvb
Feb 17 18:23:54.442: INFO: Got endpoints: latency-svc-7lnvb [471.188583ms]
Feb 17 18:23:54.461: INFO: Created: latency-svc-nktpm
Feb 17 18:23:54.466: INFO: Got endpoints: latency-svc-nktpm [486.568587ms]
Feb 17 18:23:54.483: INFO: Created: latency-svc-k2gcc
Feb 17 18:23:54.505: INFO: Got endpoints: latency-svc-k2gcc [486.090345ms]
Feb 17 18:23:54.506: INFO: Created: latency-svc-6ftpf
Feb 17 18:23:54.524: INFO: Created: latency-svc-pwbhq
Feb 17 18:23:54.530: INFO: Got endpoints: latency-svc-pwbhq [466.282184ms]
Feb 17 18:23:54.530: INFO: Got endpoints: latency-svc-6ftpf [483.044339ms]
Feb 17 18:23:54.545: INFO: Created: latency-svc-m5pq4
Feb 17 18:23:54.563: INFO: Got endpoints: latency-svc-m5pq4 [477.22675ms]
Feb 17 18:23:54.567: INFO: Created: latency-svc-8ktt5
Feb 17 18:23:54.595: INFO: Got endpoints: latency-svc-8ktt5 [473.312007ms]
Feb 17 18:23:54.595: INFO: Created: latency-svc-nx8p9
Feb 17 18:23:54.606: INFO: Created: latency-svc-vjv47
Feb 17 18:23:54.606: INFO: Got endpoints: latency-svc-nx8p9 [480.564901ms]
Feb 17 18:23:54.621: INFO: Got endpoints: latency-svc-vjv47 [443.977352ms]
Feb 17 18:23:54.633: INFO: Created: latency-svc-z5hwl
Feb 17 18:23:54.642: INFO: Got endpoints: latency-svc-z5hwl [444.868987ms]
Feb 17 18:23:54.654: INFO: Created: latency-svc-q68xs
Feb 17 18:23:54.658: INFO: Got endpoints: latency-svc-q68xs [423.958522ms]
Feb 17 18:23:54.671: INFO: Created: latency-svc-gll46
Feb 17 18:23:54.688: INFO: Got endpoints: latency-svc-gll46 [358.737633ms]
Feb 17 18:23:54.718: INFO: Created: latency-svc-vwlsw
Feb 17 18:23:54.719: INFO: Created: latency-svc-nf29b
Feb 17 18:23:54.719: INFO: Got endpoints: latency-svc-vwlsw [378.468721ms]
Feb 17 18:23:54.745: INFO: Created: latency-svc-rmbsw
Feb 17 18:23:54.746: INFO: Got endpoints: latency-svc-nf29b [341.590252ms]
Feb 17 18:23:54.761: INFO: Created: latency-svc-hz8z9
Feb 17 18:23:54.777: INFO: Created: latency-svc-cjvp4
Feb 17 18:23:54.794: INFO: Created: latency-svc-jb9tm
Feb 17 18:23:54.796: INFO: Got endpoints: latency-svc-rmbsw [366.305443ms]
Feb 17 18:23:54.815: INFO: Created: latency-svc-2f4gp
Feb 17 18:23:54.835: INFO: Created: latency-svc-mgj8z
Feb 17 18:23:54.853: INFO: Got endpoints: latency-svc-hz8z9 [410.705028ms]
Feb 17 18:23:54.859: INFO: Created: latency-svc-w6m4d
Feb 17 18:23:54.876: INFO: Created: latency-svc-v9f9k
Feb 17 18:23:54.891: INFO: Created: latency-svc-hkznt
Feb 17 18:23:54.892: INFO: Got endpoints: latency-svc-cjvp4 [426.19223ms]
Feb 17 18:23:54.909: INFO: Created: latency-svc-rgzt7
Feb 17 18:23:54.932: INFO: Created: latency-svc-c644s
Feb 17 18:23:54.954: INFO: Created: latency-svc-68bxp
Feb 17 18:23:54.956: INFO: Got endpoints: latency-svc-jb9tm [451.130357ms]
Feb 17 18:23:54.974: INFO: Created: latency-svc-vzhrj
Feb 17 18:23:54.994: INFO: Created: latency-svc-gppxv
Feb 17 18:23:54.995: INFO: Got endpoints: latency-svc-2f4gp [464.934971ms]
Feb 17 18:23:55.009: INFO: Created: latency-svc-k7dtv
Feb 17 18:23:55.024: INFO: Created: latency-svc-bp6mx
Feb 17 18:23:55.042: INFO: Got endpoints: latency-svc-mgj8z [512.582336ms]
Feb 17 18:23:55.052: INFO: Created: latency-svc-ln8zw
Feb 17 18:23:55.075: INFO: Created: latency-svc-hhnlj
Feb 17 18:23:55.098: INFO: Got endpoints: latency-svc-w6m4d [534.688905ms]
Feb 17 18:23:55.100: INFO: Created: latency-svc-qh7t6
Feb 17 18:23:55.116: INFO: Created: latency-svc-kntch
Feb 17 18:23:55.132: INFO: Created: latency-svc-5qmwn
Feb 17 18:23:55.147: INFO: Got endpoints: latency-svc-v9f9k [552.135322ms]
Feb 17 18:23:55.149: INFO: Created: latency-svc-v7vqz
Feb 17 18:23:55.168: INFO: Created: latency-svc-cfpn2
Feb 17 18:23:55.194: INFO: Got endpoints: latency-svc-hkznt [587.983113ms]
Feb 17 18:23:55.214: INFO: Created: latency-svc-h6w5c
Feb 17 18:23:55.242: INFO: Got endpoints: latency-svc-rgzt7 [620.464878ms]
Feb 17 18:23:55.262: INFO: Created: latency-svc-srbwt
Feb 17 18:23:55.291: INFO: Got endpoints: latency-svc-c644s [649.080603ms]
Feb 17 18:23:55.348: INFO: Created: latency-svc-x7c27
Feb 17 18:23:55.350: INFO: Got endpoints: latency-svc-68bxp [692.554522ms]
Feb 17 18:23:55.375: INFO: Created: latency-svc-cv9tv
Feb 17 18:23:55.390: INFO: Got endpoints: latency-svc-vzhrj [702.485455ms]
Feb 17 18:23:55.412: INFO: Created: latency-svc-hpv7k
Feb 17 18:23:55.441: INFO: Got endpoints: latency-svc-gppxv [721.52764ms]
Feb 17 18:23:55.463: INFO: Created: latency-svc-ckkdh
Feb 17 18:23:55.491: INFO: Got endpoints: latency-svc-k7dtv [745.408821ms]
Feb 17 18:23:55.511: INFO: Created: latency-svc-f2b7c
Feb 17 18:23:55.549: INFO: Got endpoints: latency-svc-bp6mx [752.793447ms]
Feb 17 18:23:55.569: INFO: Created: latency-svc-gw9p2
Feb 17 18:23:55.591: INFO: Got endpoints: latency-svc-ln8zw [698.666052ms]
Feb 17 18:23:55.610: INFO: Created: latency-svc-9bmgt
Feb 17 18:23:55.649: INFO: Got endpoints: latency-svc-hhnlj [795.820439ms]
Feb 17 18:23:55.671: INFO: Created: latency-svc-2dvl5
Feb 17 18:23:55.692: INFO: Got endpoints: latency-svc-qh7t6 [735.937343ms]
Feb 17 18:23:55.717: INFO: Created: latency-svc-clbtj
Feb 17 18:23:55.740: INFO: Got endpoints: latency-svc-kntch [745.191656ms]
Feb 17 18:23:55.771: INFO: Created: latency-svc-5m2gm
Feb 17 18:23:55.791: INFO: Got endpoints: latency-svc-5qmwn [748.768713ms]
Feb 17 18:23:55.811: INFO: Created: latency-svc-k7sjz
Feb 17 18:23:55.843: INFO: Got endpoints: latency-svc-v7vqz [744.840545ms]
Feb 17 18:23:55.866: INFO: Created: latency-svc-65znw
Feb 17 18:23:55.892: INFO: Got endpoints: latency-svc-cfpn2 [744.56629ms]
Feb 17 18:23:55.912: INFO: Created: latency-svc-r9bf5
Feb 17 18:23:55.941: INFO: Got endpoints: latency-svc-h6w5c [746.98354ms]
Feb 17 18:23:55.961: INFO: Created: latency-svc-544k7
Feb 17 18:23:55.999: INFO: Got endpoints: latency-svc-srbwt [757.694554ms]
Feb 17 18:23:56.022: INFO: Created: latency-svc-c6d26
Feb 17 18:23:56.040: INFO: Got endpoints: latency-svc-x7c27 [749.096327ms]
Feb 17 18:23:56.061: INFO: Created: latency-svc-5x24q
Feb 17 18:23:56.091: INFO: Got endpoints: latency-svc-cv9tv [740.212716ms]
Feb 17 18:23:56.113: INFO: Created: latency-svc-5jk7n
Feb 17 18:23:56.148: INFO: Got endpoints: latency-svc-hpv7k [758.309158ms]
Feb 17 18:23:56.175: INFO: Created: latency-svc-cwjhh
Feb 17 18:23:56.192: INFO: Got endpoints: latency-svc-ckkdh [750.863088ms]
Feb 17 18:23:56.226: INFO: Created: latency-svc-b2lpk
Feb 17 18:23:56.241: INFO: Got endpoints: latency-svc-f2b7c [750.493127ms]
Feb 17 18:23:56.261: INFO: Created: latency-svc-xqsfl
Feb 17 18:23:56.291: INFO: Got endpoints: latency-svc-gw9p2 [741.54353ms]
Feb 17 18:23:56.313: INFO: Created: latency-svc-shjnt
Feb 17 18:23:56.340: INFO: Got endpoints: latency-svc-9bmgt [749.075273ms]
Feb 17 18:23:56.363: INFO: Created: latency-svc-96mdw
Feb 17 18:23:56.391: INFO: Got endpoints: latency-svc-2dvl5 [742.34407ms]
Feb 17 18:23:56.411: INFO: Created: latency-svc-rc95c
Feb 17 18:23:56.453: INFO: Got endpoints: latency-svc-clbtj [761.073442ms]
Feb 17 18:23:56.477: INFO: Created: latency-svc-4w5l8
Feb 17 18:23:56.491: INFO: Got endpoints: latency-svc-5m2gm [751.150832ms]
Feb 17 18:23:56.509: INFO: Created: latency-svc-mp4mv
Feb 17 18:23:56.540: INFO: Got endpoints: latency-svc-k7sjz [749.124325ms]
Feb 17 18:23:56.573: INFO: Created: latency-svc-mwhvj
Feb 17 18:23:56.592: INFO: Got endpoints: latency-svc-65znw [748.783089ms]
Feb 17 18:23:56.613: INFO: Created: latency-svc-wkx4x
Feb 17 18:23:56.641: INFO: Got endpoints: latency-svc-r9bf5 [749.036549ms]
Feb 17 18:23:56.661: INFO: Created: latency-svc-r9n69
Feb 17 18:23:56.691: INFO: Got endpoints: latency-svc-544k7 [749.216708ms]
Feb 17 18:23:56.711: INFO: Created: latency-svc-flr6p
Feb 17 18:23:56.741: INFO: Got endpoints: latency-svc-c6d26 [741.678521ms]
Feb 17 18:23:56.763: INFO: Created: latency-svc-s4pw5
Feb 17 18:23:56.798: INFO: Got endpoints: latency-svc-5x24q [757.523405ms]
Feb 17 18:23:56.818: INFO: Created: latency-svc-g5ctk
Feb 17 18:23:56.841: INFO: Got endpoints: latency-svc-5jk7n [750.561236ms]
Feb 17 18:23:56.862: INFO: Created: latency-svc-gmwqc
Feb 17 18:23:56.893: INFO: Got endpoints: latency-svc-cwjhh [744.568217ms]
Feb 17 18:23:56.924: INFO: Created: latency-svc-g2dgb
Feb 17 18:23:56.946: INFO: Got endpoints: latency-svc-b2lpk [754.530949ms]
Feb 17 18:23:56.966: INFO: Created: latency-svc-bdqwk
Feb 17 18:23:56.992: INFO: Got endpoints: latency-svc-xqsfl [750.86767ms]
Feb 17 18:23:57.024: INFO: Created: latency-svc-q96wb
Feb 17 18:23:57.042: INFO: Got endpoints: latency-svc-shjnt [750.885905ms]
Feb 17 18:23:57.063: INFO: Created: latency-svc-rjw42
Feb 17 18:23:57.093: INFO: Got endpoints: latency-svc-96mdw [752.663415ms]
Feb 17 18:23:57.113: INFO: Created: latency-svc-zwj9m
Feb 17 18:23:57.141: INFO: Got endpoints: latency-svc-rc95c [749.857909ms]
Feb 17 18:23:57.161: INFO: Created: latency-svc-lgw7r
Feb 17 18:23:57.191: INFO: Got endpoints: latency-svc-4w5l8 [738.331743ms]
Feb 17 18:23:57.213: INFO: Created: latency-svc-7xw8b
Feb 17 18:23:57.250: INFO: Got endpoints: latency-svc-mp4mv [758.612037ms]
Feb 17 18:23:57.272: INFO: Created: latency-svc-59z2z
Feb 17 18:23:57.291: INFO: Got endpoints: latency-svc-mwhvj [750.25161ms]
Feb 17 18:23:57.312: INFO: Created: latency-svc-nmbrw
Feb 17 18:23:57.342: INFO: Got endpoints: latency-svc-wkx4x [749.998715ms]
Feb 17 18:23:57.369: INFO: Created: latency-svc-bmx2l
Feb 17 18:23:57.391: INFO: Got endpoints: latency-svc-r9n69 [750.256067ms]
Feb 17 18:23:57.414: INFO: Created: latency-svc-8ndx7
Feb 17 18:23:57.445: INFO: Got endpoints: latency-svc-flr6p [754.294957ms]
Feb 17 18:23:57.475: INFO: Created: latency-svc-msm6m
Feb 17 18:23:57.490: INFO: Got endpoints: latency-svc-s4pw5 [749.154018ms]
Feb 17 18:23:57.512: INFO: Created: latency-svc-jr5qh
Feb 17 18:23:57.541: INFO: Got endpoints: latency-svc-g5ctk [742.708227ms]
Feb 17 18:23:57.560: INFO: Created: latency-svc-hq7v5
Feb 17 18:23:57.591: INFO: Got endpoints: latency-svc-gmwqc [749.57925ms]
Feb 17 18:23:57.610: INFO: Created: latency-svc-xd56b
Feb 17 18:23:57.640: INFO: Got endpoints: latency-svc-g2dgb [747.365955ms]
Feb 17 18:23:57.661: INFO: Created: latency-svc-rlb9j
Feb 17 18:23:57.701: INFO: Got endpoints: latency-svc-bdqwk [754.894956ms]
Feb 17 18:23:57.722: INFO: Created: latency-svc-qtgzf
Feb 17 18:23:57.748: INFO: Got endpoints: latency-svc-q96wb [755.147286ms]
Feb 17 18:23:57.768: INFO: Created: latency-svc-xblg7
Feb 17 18:23:57.791: INFO: Got endpoints: latency-svc-rjw42 [749.386472ms]
Feb 17 18:23:57.817: INFO: Created: latency-svc-vzgp9
Feb 17 18:23:57.848: INFO: Got endpoints: latency-svc-zwj9m [755.137799ms]
Feb 17 18:23:57.868: INFO: Created: latency-svc-6xxbr
Feb 17 18:23:57.891: INFO: Got endpoints: latency-svc-lgw7r [749.988934ms]
Feb 17 18:23:57.911: INFO: Created: latency-svc-6c7v2
Feb 17 18:23:57.941: INFO: Got endpoints: latency-svc-7xw8b [750.110415ms]
Feb 17 18:23:57.963: INFO: Created: latency-svc-zggxw
Feb 17 18:23:57.990: INFO: Got endpoints: latency-svc-59z2z [740.087562ms]
Feb 17 18:23:58.010: INFO: Created: latency-svc-8nqhl
Feb 17 18:23:58.042: INFO: Got endpoints: latency-svc-nmbrw [751.245331ms]
Feb 17 18:23:58.068: INFO: Created: latency-svc-rvftd
Feb 17 18:23:58.091: INFO: Got endpoints: latency-svc-bmx2l [749.683774ms]
Feb 17 18:23:58.112: INFO: Created: latency-svc-f2qsk
Feb 17 18:23:58.152: INFO: Got endpoints: latency-svc-8ndx7 [761.011224ms]
Feb 17 18:23:58.175: INFO: Created: latency-svc-ks7hm
Feb 17 18:23:58.191: INFO: Got endpoints: latency-svc-msm6m [746.434297ms]
Feb 17 18:23:58.212: INFO: Created: latency-svc-rq9mf
Feb 17 18:23:58.241: INFO: Got endpoints: latency-svc-jr5qh [751.093627ms]
Feb 17 18:23:58.277: INFO: Created: latency-svc-vggm4
Feb 17 18:23:58.291: INFO: Got endpoints: latency-svc-hq7v5 [749.768458ms]
Feb 17 18:23:58.311: INFO: Created: latency-svc-rwt78
Feb 17 18:23:58.347: INFO: Got endpoints: latency-svc-xd56b [756.618307ms]
Feb 17 18:23:58.367: INFO: Created: latency-svc-6kqp7
Feb 17 18:23:58.391: INFO: Got endpoints: latency-svc-rlb9j [750.912817ms]
Feb 17 18:23:58.412: INFO: Created: latency-svc-nbk2t
Feb 17 18:23:58.441: INFO: Got endpoints: latency-svc-qtgzf [739.813636ms]
Feb 17 18:23:58.460: INFO: Created: latency-svc-tc8l9
Feb 17 18:23:58.499: INFO: Got endpoints: latency-svc-xblg7 [751.750782ms]
Feb 17 18:23:58.519: INFO: Created: latency-svc-l4fgz
Feb 17 18:23:58.542: INFO: Got endpoints: latency-svc-vzgp9 [750.129702ms]
Feb 17 18:23:58.573: INFO: Created: latency-svc-4psqr
Feb 17 18:23:58.592: INFO: Got endpoints: latency-svc-6xxbr [743.948616ms]
Feb 17 18:23:58.631: INFO: Created: latency-svc-2h7fh
Feb 17 18:23:58.641: INFO: Got endpoints: latency-svc-6c7v2 [750.036368ms]
Feb 17 18:23:58.661: INFO: Created: latency-svc-cb7dq
Feb 17 18:23:58.692: INFO: Got endpoints: latency-svc-zggxw [750.519503ms]
Feb 17 18:23:58.735: INFO: Created: latency-svc-mth2s
Feb 17 18:23:58.741: INFO: Got endpoints: latency-svc-8nqhl [750.932984ms]
Feb 17 18:23:58.759: INFO: Created: latency-svc-4dcmv
Feb 17 18:23:58.791: INFO: Got endpoints: latency-svc-rvftd [748.937013ms]
Feb 17 18:23:58.811: INFO: Created: latency-svc-hkv28
Feb 17 18:23:58.842: INFO: Got endpoints: latency-svc-f2qsk [750.874477ms]
Feb 17 18:23:58.870: INFO: Created: latency-svc-w976h
Feb 17 18:23:58.891: INFO: Got endpoints: latency-svc-ks7hm [738.049201ms]
Feb 17 18:23:58.909: INFO: Created: latency-svc-4m44d
Feb 17 18:23:58.951: INFO: Got endpoints: latency-svc-rq9mf [759.502794ms]
Feb 17 18:23:58.976: INFO: Created: latency-svc-gf9rz
Feb 17 18:23:58.997: INFO: Got endpoints: latency-svc-vggm4 [755.387547ms]
Feb 17 18:23:59.018: INFO: Created: latency-svc-f97b9
Feb 17 18:23:59.045: INFO: Got endpoints: latency-svc-rwt78 [754.437242ms]
Feb 17 18:23:59.065: INFO: Created: latency-svc-ctr8r
Feb 17 18:23:59.092: INFO: Got endpoints: latency-svc-6kqp7 [744.729525ms]
Feb 17 18:23:59.114: INFO: Created: latency-svc-498w5
Feb 17 18:23:59.142: INFO: Got endpoints: latency-svc-nbk2t [750.182922ms]
Feb 17 18:23:59.182: INFO: Created: latency-svc-mxqtz
Feb 17 18:23:59.192: INFO: Got endpoints: latency-svc-tc8l9 [750.614687ms]
Feb 17 18:23:59.211: INFO: Created: latency-svc-tdjwt
Feb 17 18:23:59.242: INFO: Got endpoints: latency-svc-l4fgz [742.224288ms]
Feb 17 18:23:59.263: INFO: Created: latency-svc-xzwnk
Feb 17 18:23:59.294: INFO: Got endpoints: latency-svc-4psqr [752.148926ms]
Feb 17 18:23:59.320: INFO: Created: latency-svc-nvl4s
Feb 17 18:23:59.341: INFO: Got endpoints: latency-svc-2h7fh [749.354715ms]
Feb 17 18:23:59.363: INFO: Created: latency-svc-bk5x6
Feb 17 18:23:59.392: INFO: Got endpoints: latency-svc-cb7dq [750.925175ms]
Feb 17 18:23:59.430: INFO: Created: latency-svc-vmj5j
Feb 17 18:23:59.442: INFO: Got endpoints: latency-svc-mth2s [750.166913ms]
Feb 17 18:23:59.465: INFO: Created: latency-svc-5drgb
Feb 17 18:23:59.492: INFO: Got endpoints: latency-svc-4dcmv [750.338076ms]
Feb 17 18:23:59.516: INFO: Created: latency-svc-zdrkf
Feb 17 18:23:59.541: INFO: Got endpoints: latency-svc-hkv28 [749.563079ms]
Feb 17 18:23:59.561: INFO: Created: latency-svc-f5mjf
Feb 17 18:23:59.592: INFO: Got endpoints: latency-svc-w976h [749.670248ms]
Feb 17 18:23:59.614: INFO: Created: latency-svc-q28l6
Feb 17 18:23:59.641: INFO: Got endpoints: latency-svc-4m44d [750.841836ms]
Feb 17 18:23:59.671: INFO: Created: latency-svc-9gfhp
Feb 17 18:23:59.691: INFO: Got endpoints: latency-svc-gf9rz [739.743828ms]
Feb 17 18:23:59.711: INFO: Created: latency-svc-n666v
Feb 17 18:23:59.753: INFO: Got endpoints: latency-svc-f97b9 [756.163976ms]
Feb 17 18:23:59.784: INFO: Created: latency-svc-wrq5s
Feb 17 18:23:59.793: INFO: Got endpoints: latency-svc-ctr8r [748.267872ms]
Feb 17 18:23:59.814: INFO: Created: latency-svc-nmwgh
Feb 17 18:23:59.841: INFO: Got endpoints: latency-svc-498w5 [748.932777ms]
Feb 17 18:23:59.872: INFO: Created: latency-svc-jft5g
Feb 17 18:23:59.892: INFO: Got endpoints: latency-svc-mxqtz [750.023006ms]
Feb 17 18:23:59.914: INFO: Created: latency-svc-5bvpd
Feb 17 18:23:59.942: INFO: Got endpoints: latency-svc-tdjwt [749.831129ms]
Feb 17 18:23:59.970: INFO: Created: latency-svc-z5c8v
Feb 17 18:23:59.991: INFO: Got endpoints: latency-svc-xzwnk [749.857104ms]
Feb 17 18:24:00.024: INFO: Created: latency-svc-b2q5b
Feb 17 18:24:00.041: INFO: Got endpoints: latency-svc-nvl4s [747.035324ms]
Feb 17 18:24:00.062: INFO: Created: latency-svc-q9m67
Feb 17 18:24:00.100: INFO: Got endpoints: latency-svc-bk5x6 [758.084325ms]
Feb 17 18:24:00.120: INFO: Created: latency-svc-27prr
Feb 17 18:24:00.144: INFO: Got endpoints: latency-svc-vmj5j [751.535411ms]
Feb 17 18:24:00.164: INFO: Created: latency-svc-699mt
Feb 17 18:24:00.193: INFO: Got endpoints: latency-svc-5drgb [751.405937ms]
Feb 17 18:24:00.223: INFO: Created: latency-svc-5cz7p
Feb 17 18:24:00.242: INFO: Got endpoints: latency-svc-zdrkf [750.277993ms]
Feb 17 18:24:00.261: INFO: Created: latency-svc-tzqnk
Feb 17 18:24:00.297: INFO: Got endpoints: latency-svc-f5mjf [755.944745ms]
Feb 17 18:24:00.319: INFO: Created: latency-svc-w9jgp
Feb 17 18:24:00.341: INFO: Got endpoints: latency-svc-q28l6 [748.789273ms]
Feb 17 18:24:00.385: INFO: Created: latency-svc-9fpmh
Feb 17 18:24:00.392: INFO: Got endpoints: latency-svc-9gfhp [750.607222ms]
Feb 17 18:24:00.412: INFO: Created: latency-svc-rvxhn
Feb 17 18:24:00.444: INFO: Got endpoints: latency-svc-n666v [752.782535ms]
Feb 17 18:24:00.464: INFO: Created: latency-svc-lp9g5
Feb 17 18:24:00.491: INFO: Got endpoints: latency-svc-wrq5s [737.731659ms]
Feb 17 18:24:00.525: INFO: Created: latency-svc-59rfh
Feb 17 18:24:00.541: INFO: Got endpoints: latency-svc-nmwgh [748.086089ms]
Feb 17 18:24:00.560: INFO: Created: latency-svc-t596j
Feb 17 18:24:00.593: INFO: Got endpoints: latency-svc-jft5g [751.486441ms]
Feb 17 18:24:00.613: INFO: Created: latency-svc-hb6d5
Feb 17 18:24:00.641: INFO: Got endpoints: latency-svc-5bvpd [748.90014ms]
Feb 17 18:24:00.674: INFO: Created: latency-svc-9nsn6
Feb 17 18:24:00.692: INFO: Got endpoints: latency-svc-z5c8v [750.494504ms]
Feb 17 18:24:00.712: INFO: Created: latency-svc-pn5h2
Feb 17 18:24:00.742: INFO: Got endpoints: latency-svc-b2q5b [750.577175ms]
Feb 17 18:24:00.763: INFO: Created: latency-svc-n6snj
Feb 17 18:24:00.792: INFO: Got endpoints: latency-svc-q9m67 [751.009757ms]
Feb 17 18:24:00.814: INFO: Created: latency-svc-smhw7
Feb 17 18:24:00.842: INFO: Got endpoints: latency-svc-27prr [742.740904ms]
Feb 17 18:24:00.863: INFO: Created: latency-svc-7n5zk
Feb 17 18:24:00.901: INFO: Got endpoints: latency-svc-699mt [757.21452ms]
Feb 17 18:24:00.922: INFO: Created: latency-svc-76zjh
Feb 17 18:24:00.941: INFO: Got endpoints: latency-svc-5cz7p [747.273597ms]
Feb 17 18:24:00.965: INFO: Created: latency-svc-9g6rp
Feb 17 18:24:00.992: INFO: Got endpoints: latency-svc-tzqnk [750.385518ms]
Feb 17 18:24:01.012: INFO: Created: latency-svc-2phpl
Feb 17 18:24:01.042: INFO: Got endpoints: latency-svc-w9jgp [745.376393ms]
Feb 17 18:24:01.073: INFO: Created: latency-svc-2229b
Feb 17 18:24:01.094: INFO: Got endpoints: latency-svc-9fpmh [753.007016ms]
Feb 17 18:24:01.141: INFO: Created: latency-svc-gjqwf
Feb 17 18:24:01.142: INFO: Got endpoints: latency-svc-rvxhn [750.111963ms]
Feb 17 18:24:01.166: INFO: Created: latency-svc-6cw7f
Feb 17 18:24:01.192: INFO: Got endpoints: latency-svc-lp9g5 [748.235237ms]
Feb 17 18:24:01.262: INFO: Got endpoints: latency-svc-59rfh [771.094367ms]
Feb 17 18:24:01.294: INFO: Got endpoints: latency-svc-t596j [752.44312ms]
Feb 17 18:24:01.346: INFO: Got endpoints: latency-svc-hb6d5 [753.344652ms]
Feb 17 18:24:01.394: INFO: Got endpoints: latency-svc-9nsn6 [752.980263ms]
Feb 17 18:24:01.442: INFO: Got endpoints: latency-svc-pn5h2 [749.710791ms]
Feb 17 18:24:01.495: INFO: Got endpoints: latency-svc-n6snj [753.057985ms]
Feb 17 18:24:01.542: INFO: Got endpoints: latency-svc-smhw7 [749.924855ms]
Feb 17 18:24:01.596: INFO: Got endpoints: latency-svc-7n5zk [753.847671ms]
Feb 17 18:24:01.641: INFO: Got endpoints: latency-svc-76zjh [740.365859ms]
Feb 17 18:24:01.692: INFO: Got endpoints: latency-svc-9g6rp [750.743701ms]
Feb 17 18:24:01.743: INFO: Got endpoints: latency-svc-2phpl [750.860443ms]
Feb 17 18:24:01.803: INFO: Got endpoints: latency-svc-2229b [761.179944ms]
Feb 17 18:24:01.846: INFO: Got endpoints: latency-svc-gjqwf [752.670932ms]
Feb 17 18:24:01.894: INFO: Got endpoints: latency-svc-6cw7f [751.838225ms]
Feb 17 18:24:01.894: INFO: Latencies: [42.171834ms 42.238516ms 58.03926ms 74.226174ms 114.224916ms 130.087338ms 151.050294ms 184.835636ms 203.715622ms 213.940636ms 239.349594ms 259.95252ms 273.711249ms 299.098824ms 300.889334ms 301.017011ms 304.252162ms 304.510258ms 305.19677ms 309.516456ms 311.990135ms 312.561743ms 316.06575ms 317.263107ms 320.999943ms 322.077278ms 324.929427ms 325.41065ms 327.922938ms 328.094391ms 333.104547ms 333.75182ms 334.654162ms 341.558558ms 341.590252ms 348.664597ms 358.737633ms 366.154349ms 366.305443ms 368.656637ms 371.424379ms 378.468721ms 388.918929ms 410.705028ms 423.958522ms 426.19223ms 442.19877ms 443.977352ms 444.868987ms 451.130357ms 464.934971ms 466.282184ms 468.120542ms 471.188583ms 473.312007ms 477.22675ms 480.564901ms 481.054423ms 483.044339ms 486.090345ms 486.568587ms 496.625339ms 512.582336ms 534.688905ms 552.135322ms 587.983113ms 620.464878ms 649.080603ms 692.554522ms 698.666052ms 702.485455ms 721.52764ms 735.937343ms 737.731659ms 738.049201ms 738.331743ms 739.743828ms 739.813636ms 740.087562ms 740.212716ms 740.365859ms 741.54353ms 741.678521ms 742.224288ms 742.34407ms 742.708227ms 742.740904ms 743.948616ms 744.56629ms 744.568217ms 744.729525ms 744.840545ms 745.191656ms 745.376393ms 745.408821ms 746.434297ms 746.98354ms 747.035324ms 747.273597ms 747.365955ms 748.086089ms 748.235237ms 748.267872ms 748.768713ms 748.783089ms 748.789273ms 748.90014ms 748.932777ms 748.937013ms 749.036549ms 749.075273ms 749.096327ms 749.124325ms 749.154018ms 749.216708ms 749.354715ms 749.386472ms 749.563079ms 749.57925ms 749.670248ms 749.683774ms 749.710791ms 749.768458ms 749.831129ms 749.857104ms 749.857909ms 749.924855ms 749.988934ms 749.998715ms 750.023006ms 750.036368ms 750.110415ms 750.111963ms 750.129702ms 750.166913ms 750.182922ms 750.25161ms 750.256067ms 750.277993ms 750.338076ms 750.385518ms 750.493127ms 750.494504ms 750.519503ms 750.561236ms 750.577175ms 750.607222ms 750.614687ms 750.743701ms 750.841836ms 750.860443ms 750.863088ms 750.86767ms 750.874477ms 750.885905ms 750.912817ms 750.925175ms 750.932984ms 751.009757ms 751.093627ms 751.150832ms 751.245331ms 751.405937ms 751.486441ms 751.535411ms 751.750782ms 751.838225ms 752.148926ms 752.44312ms 752.663415ms 752.670932ms 752.782535ms 752.793447ms 752.980263ms 753.007016ms 753.057985ms 753.344652ms 753.847671ms 754.294957ms 754.437242ms 754.530949ms 754.894956ms 755.137799ms 755.147286ms 755.387547ms 755.944745ms 756.163976ms 756.618307ms 757.21452ms 757.523405ms 757.694554ms 758.084325ms 758.309158ms 758.612037ms 759.502794ms 761.011224ms 761.073442ms 761.179944ms 771.094367ms 795.820439ms]
Feb 17 18:24:01.894: INFO: 50 %ile: 748.086089ms
Feb 17 18:24:01.894: INFO: 90 %ile: 754.530949ms
Feb 17 18:24:01.894: INFO: 99 %ile: 771.094367ms
Feb 17 18:24:01.894: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:24:01.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2905" for this suite.

• [SLOW TEST:10.912 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":278,"completed":100,"skipped":1551,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:24:01.914: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5942
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:24:02.063: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 17 18:24:02.076: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 17 18:24:07.097: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 17 18:24:07.097: INFO: Creating deployment "test-rolling-update-deployment"
Feb 17 18:24:07.118: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 17 18:24:07.133: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 17 18:24:09.141: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 17 18:24:09.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560647, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560647, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560647, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560647, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:24:11.152: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 17 18:24:11.183: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5942 /apis/apps/v1/namespaces/deployment-5942/deployments/test-rolling-update-deployment 5bb1acf8-a586-4955-8d5e-7eee286e97b4 13036 1 2020-02-17 18:24:07 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004ede678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-17 18:24:07 +0000 UTC,LastTransitionTime:2020-02-17 18:24:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-02-17 18:24:09 +0000 UTC,LastTransitionTime:2020-02-17 18:24:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 17 18:24:11.186: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-5942 /apis/apps/v1/namespaces/deployment-5942/replicasets/test-rolling-update-deployment-67cf4f6444 c221b890-4888-4da3-97b5-19021720a04b 13019 1 2020-02-17 18:24:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5bb1acf8-a586-4955-8d5e-7eee286e97b4 0xc004edeb47 0xc004edeb48}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004edebb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 17 18:24:11.186: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 17 18:24:11.187: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5942 /apis/apps/v1/namespaces/deployment-5942/replicasets/test-rolling-update-controller c60233a3-f0c9-4fff-8e8d-da0cc976be55 13034 2 2020-02-17 18:24:02 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5bb1acf8-a586-4955-8d5e-7eee286e97b4 0xc004edea77 0xc004edea78}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004edead8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 18:24:11.193: INFO: Pod "test-rolling-update-deployment-67cf4f6444-t4s86" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-t4s86 test-rolling-update-deployment-67cf4f6444- deployment-5942 /api/v1/namespaces/deployment-5942/pods/test-rolling-update-deployment-67cf4f6444-t4s86 2e36f70f-a94f-4bde-8cb6-29e42d11de10 13018 0 2020-02-17 18:24:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[cni.projectcalico.org/podIP:100.96.5.35/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 c221b890-4888-4da3-97b5-19021720a04b 0xc004edf047 0xc004edf048}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-74sr6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-74sr6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-74sr6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-86-181.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:24:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:24:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:24:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 18:24:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.86.181,PodIP:100.96.5.35,StartTime:2020-02-17 18:24:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 18:24:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://7b8627b7bb886b075edbfd0876f05ac7992415982aa4ae3e397e9537c60c5ff2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.5.35,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:24:11.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5942" for this suite.

• [SLOW TEST:9.292 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":278,"completed":101,"skipped":1571,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:24:11.207: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-287fb761-a3ec-47b3-a4a7-a8b9c79c78f8 in namespace container-probe-6643
Feb 17 18:24:13.420: INFO: Started pod liveness-287fb761-a3ec-47b3-a4a7-a8b9c79c78f8 in namespace container-probe-6643
STEP: checking the pod's current state and verifying that restartCount is present
Feb 17 18:24:13.424: INFO: Initial restart count of pod liveness-287fb761-a3ec-47b3-a4a7-a8b9c79c78f8 is 0
Feb 17 18:24:29.481: INFO: Restart count of pod container-probe-6643/liveness-287fb761-a3ec-47b3-a4a7-a8b9c79c78f8 is now 1 (16.056496311s elapsed)
Feb 17 18:24:49.529: INFO: Restart count of pod container-probe-6643/liveness-287fb761-a3ec-47b3-a4a7-a8b9c79c78f8 is now 2 (36.104889385s elapsed)
Feb 17 18:25:09.580: INFO: Restart count of pod container-probe-6643/liveness-287fb761-a3ec-47b3-a4a7-a8b9c79c78f8 is now 3 (56.155768617s elapsed)
Feb 17 18:25:29.629: INFO: Restart count of pod container-probe-6643/liveness-287fb761-a3ec-47b3-a4a7-a8b9c79c78f8 is now 4 (1m16.20511279s elapsed)
Feb 17 18:26:33.774: INFO: Restart count of pod container-probe-6643/liveness-287fb761-a3ec-47b3-a4a7-a8b9c79c78f8 is now 5 (2m20.349554921s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:26:33.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6643" for this suite.

• [SLOW TEST:142.598 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":278,"completed":102,"skipped":1586,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:26:33.805: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4315
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 17 18:26:33.957: INFO: Waiting up to 5m0s for pod "downward-api-fbc7a699-9237-4f0e-9d9d-ee9fdb91fcec" in namespace "downward-api-4315" to be "success or failure"
Feb 17 18:26:33.964: INFO: Pod "downward-api-fbc7a699-9237-4f0e-9d9d-ee9fdb91fcec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.245562ms
Feb 17 18:26:35.968: INFO: Pod "downward-api-fbc7a699-9237-4f0e-9d9d-ee9fdb91fcec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011070742s
Feb 17 18:26:37.971: INFO: Pod "downward-api-fbc7a699-9237-4f0e-9d9d-ee9fdb91fcec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014451906s
STEP: Saw pod success
Feb 17 18:26:37.971: INFO: Pod "downward-api-fbc7a699-9237-4f0e-9d9d-ee9fdb91fcec" satisfied condition "success or failure"
Feb 17 18:26:37.976: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downward-api-fbc7a699-9237-4f0e-9d9d-ee9fdb91fcec container dapi-container: <nil>
STEP: delete the pod
Feb 17 18:26:38.012: INFO: Waiting for pod downward-api-fbc7a699-9237-4f0e-9d9d-ee9fdb91fcec to disappear
Feb 17 18:26:38.015: INFO: Pod downward-api-fbc7a699-9237-4f0e-9d9d-ee9fdb91fcec no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:26:38.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4315" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":278,"completed":103,"skipped":1657,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:26:38.026: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3592
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:26:38.186: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f9ae88e-9a7e-4594-9d22-b1b7ad0574ec" in namespace "projected-3592" to be "success or failure"
Feb 17 18:26:38.193: INFO: Pod "downwardapi-volume-1f9ae88e-9a7e-4594-9d22-b1b7ad0574ec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.219313ms
Feb 17 18:26:40.197: INFO: Pod "downwardapi-volume-1f9ae88e-9a7e-4594-9d22-b1b7ad0574ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010763612s
Feb 17 18:26:42.200: INFO: Pod "downwardapi-volume-1f9ae88e-9a7e-4594-9d22-b1b7ad0574ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01436837s
STEP: Saw pod success
Feb 17 18:26:42.200: INFO: Pod "downwardapi-volume-1f9ae88e-9a7e-4594-9d22-b1b7ad0574ec" satisfied condition "success or failure"
Feb 17 18:26:42.205: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downwardapi-volume-1f9ae88e-9a7e-4594-9d22-b1b7ad0574ec container client-container: <nil>
STEP: delete the pod
Feb 17 18:26:42.230: INFO: Waiting for pod downwardapi-volume-1f9ae88e-9a7e-4594-9d22-b1b7ad0574ec to disappear
Feb 17 18:26:42.234: INFO: Pod downwardapi-volume-1f9ae88e-9a7e-4594-9d22-b1b7ad0574ec no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:26:42.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3592" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":278,"completed":104,"skipped":1672,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:26:42.258: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1177
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 17 18:26:46.951: INFO: Successfully updated pod "annotationupdate8a17dfd9-9c80-431c-b58e-eb8e52e8ac0f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:26:48.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1177" for this suite.

• [SLOW TEST:6.733 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":278,"completed":105,"skipped":1674,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:26:48.991: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-8162
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Feb 17 18:26:49.137: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Feb 17 18:26:49.694: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb 17 18:26:51.756: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:26:53.761: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:26:55.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:26:57.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:26:59.761: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:27:01.763: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560809, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:27:05.926: INFO: Waited 2.156496036s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:27:06.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8162" for this suite.

• [SLOW TEST:17.549 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":278,"completed":106,"skipped":1701,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:27:06.540: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-b50f3887-404c-4d59-b8f4-a5352c47df6d in namespace container-probe-7208
Feb 17 18:27:10.818: INFO: Started pod liveness-b50f3887-404c-4d59-b8f4-a5352c47df6d in namespace container-probe-7208
STEP: checking the pod's current state and verifying that restartCount is present
Feb 17 18:27:10.822: INFO: Initial restart count of pod liveness-b50f3887-404c-4d59-b8f4-a5352c47df6d is 0
Feb 17 18:27:32.878: INFO: Restart count of pod container-probe-7208/liveness-b50f3887-404c-4d59-b8f4-a5352c47df6d is now 1 (22.055930867s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:27:32.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7208" for this suite.

• [SLOW TEST:26.371 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":278,"completed":107,"skipped":1736,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:27:32.911: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1214
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-1214
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 17 18:27:33.061: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 17 18:27:57.185: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.96 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1214 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:27:57.185: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:27:58.314: INFO: Found all expected endpoints: [netserver-0]
Feb 17 18:27:58.320: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.0.22 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1214 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:27:58.320: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:27:59.438: INFO: Found all expected endpoints: [netserver-1]
Feb 17 18:27:59.442: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.5.36 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1214 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:27:59.442: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:28:00.575: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:28:00.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1214" for this suite.

• [SLOW TEST:27.677 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":108,"skipped":1756,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:28:00.589: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3251
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3251.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3251.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3251.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3251.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 18:28:12.780: INFO: DNS probes using dns-test-26bff9de-733f-40bf-9c92-c349ebad79e5 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3251.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3251.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3251.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3251.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 18:28:16.846: INFO: DNS probes using dns-test-db462c83-e7f2-471c-955a-34e5890c7d2c succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3251.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3251.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3251.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3251.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 18:28:20.933: INFO: DNS probes using dns-test-1a0ef7c7-e2d8-4657-9491-91555b71a8f3 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:28:20.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3251" for this suite.

• [SLOW TEST:20.403 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":278,"completed":109,"skipped":1763,"failed":0}
S
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:28:20.992: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Feb 17 18:28:21.146: INFO: Created pod &Pod{ObjectMeta:{dns-4333  dns-4333 /api/v1/namespaces/dns-4333/pods/dns-4333 32a330e4-9698-4834-acfe-da332dca5411 14808 0 2020-02-17 18:28:21 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pfcl2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pfcl2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pfcl2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Feb 17 18:28:23.157: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4333 PodName:dns-4333 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:28:23.157: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Verifying customized DNS server is configured on pod...
Feb 17 18:28:23.293: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4333 PodName:dns-4333 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:28:23.293: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:28:23.439: INFO: Deleting pod dns-4333...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:28:23.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4333" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":278,"completed":110,"skipped":1764,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:28:23.469: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-5be69fd5-708a-49dc-8814-d82fa14e1eb8 in namespace container-probe-6917
Feb 17 18:28:25.637: INFO: Started pod busybox-5be69fd5-708a-49dc-8814-d82fa14e1eb8 in namespace container-probe-6917
STEP: checking the pod's current state and verifying that restartCount is present
Feb 17 18:28:25.640: INFO: Initial restart count of pod busybox-5be69fd5-708a-49dc-8814-d82fa14e1eb8 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:32:26.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6917" for this suite.

• [SLOW TEST:242.787 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":278,"completed":111,"skipped":1769,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:32:26.257: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-899
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Feb 17 18:32:26.406: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:32:47.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-899" for this suite.

• [SLOW TEST:21.512 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":278,"completed":112,"skipped":1769,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:32:47.769: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8054
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 17 18:32:47.935: INFO: Waiting up to 5m0s for pod "downward-api-97818fdb-1899-494a-9e21-437e898734dc" in namespace "downward-api-8054" to be "success or failure"
Feb 17 18:32:47.938: INFO: Pod "downward-api-97818fdb-1899-494a-9e21-437e898734dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.611226ms
Feb 17 18:32:49.943: INFO: Pod "downward-api-97818fdb-1899-494a-9e21-437e898734dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007416694s
STEP: Saw pod success
Feb 17 18:32:49.943: INFO: Pod "downward-api-97818fdb-1899-494a-9e21-437e898734dc" satisfied condition "success or failure"
Feb 17 18:32:49.948: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downward-api-97818fdb-1899-494a-9e21-437e898734dc container dapi-container: <nil>
STEP: delete the pod
Feb 17 18:32:49.983: INFO: Waiting for pod downward-api-97818fdb-1899-494a-9e21-437e898734dc to disappear
Feb 17 18:32:49.988: INFO: Pod downward-api-97818fdb-1899-494a-9e21-437e898734dc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:32:49.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8054" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":278,"completed":113,"skipped":1777,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:32:50.001: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6265
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:32:50.167: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2e82030d-34df-459c-a68b-d13a78b10804" in namespace "projected-6265" to be "success or failure"
Feb 17 18:32:50.172: INFO: Pod "downwardapi-volume-2e82030d-34df-459c-a68b-d13a78b10804": Phase="Pending", Reason="", readiness=false. Elapsed: 4.832609ms
Feb 17 18:32:52.176: INFO: Pod "downwardapi-volume-2e82030d-34df-459c-a68b-d13a78b10804": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008154085s
Feb 17 18:32:54.179: INFO: Pod "downwardapi-volume-2e82030d-34df-459c-a68b-d13a78b10804": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011189107s
STEP: Saw pod success
Feb 17 18:32:54.179: INFO: Pod "downwardapi-volume-2e82030d-34df-459c-a68b-d13a78b10804" satisfied condition "success or failure"
Feb 17 18:32:54.183: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downwardapi-volume-2e82030d-34df-459c-a68b-d13a78b10804 container client-container: <nil>
STEP: delete the pod
Feb 17 18:32:54.208: INFO: Waiting for pod downwardapi-volume-2e82030d-34df-459c-a68b-d13a78b10804 to disappear
Feb 17 18:32:54.211: INFO: Pod downwardapi-volume-2e82030d-34df-459c-a68b-d13a78b10804 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:32:54.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6265" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":114,"skipped":1796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:32:54.224: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9991
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-9c5193ef-de07-4a9b-b8f4-5abd49316fe5
STEP: Creating secret with name s-test-opt-upd-3d97e466-ce85-47b0-a852-b97af7db2e48
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9c5193ef-de07-4a9b-b8f4-5abd49316fe5
STEP: Updating secret s-test-opt-upd-3d97e466-ce85-47b0-a852-b97af7db2e48
STEP: Creating secret with name s-test-opt-create-15281341-10cf-4e5e-b9f7-8f38d17ab0b3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:33:00.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9991" for this suite.

• [SLOW TEST:6.302 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":115,"skipped":1823,"failed":0}
SSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:33:00.526: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9554
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:33:00.678: INFO: Waiting up to 5m0s for pod "busybox-user-65534-faabfe24-6a3f-47b7-8e13-b345f4ccc1da" in namespace "security-context-test-9554" to be "success or failure"
Feb 17 18:33:00.681: INFO: Pod "busybox-user-65534-faabfe24-6a3f-47b7-8e13-b345f4ccc1da": Phase="Pending", Reason="", readiness=false. Elapsed: 3.032164ms
Feb 17 18:33:02.685: INFO: Pod "busybox-user-65534-faabfe24-6a3f-47b7-8e13-b345f4ccc1da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006399322s
Feb 17 18:33:02.685: INFO: Pod "busybox-user-65534-faabfe24-6a3f-47b7-8e13-b345f4ccc1da" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:33:02.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9554" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":116,"skipped":1828,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:33:02.700: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:33:03.282: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:33:06.315: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
Feb 17 18:33:06.350: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook
Feb 17 18:33:07.491: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:33:07.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2773" for this suite.
STEP: Destroying namespace "webhook-2773-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":278,"completed":117,"skipped":1854,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:33:07.591: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8249
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Feb 17 18:33:07.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=kubectl-8249 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 17 18:33:10.880: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 17 18:33:10.880: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:33:12.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8249" for this suite.

• [SLOW TEST:5.320 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1944
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":278,"completed":118,"skipped":1868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:33:12.911: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5955
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-eb28df55-a295-4fa4-ab09-ecb82492f687
STEP: Creating a pod to test consume secrets
Feb 17 18:33:13.104: INFO: Waiting up to 5m0s for pod "pod-secrets-11abe85b-a1e7-41e8-bb2d-8d645e0d0fd4" in namespace "secrets-5955" to be "success or failure"
Feb 17 18:33:13.107: INFO: Pod "pod-secrets-11abe85b-a1e7-41e8-bb2d-8d645e0d0fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.052153ms
Feb 17 18:33:15.110: INFO: Pod "pod-secrets-11abe85b-a1e7-41e8-bb2d-8d645e0d0fd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006245751s
STEP: Saw pod success
Feb 17 18:33:15.110: INFO: Pod "pod-secrets-11abe85b-a1e7-41e8-bb2d-8d645e0d0fd4" satisfied condition "success or failure"
Feb 17 18:33:15.114: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-secrets-11abe85b-a1e7-41e8-bb2d-8d645e0d0fd4 container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 18:33:15.141: INFO: Waiting for pod pod-secrets-11abe85b-a1e7-41e8-bb2d-8d645e0d0fd4 to disappear
Feb 17 18:33:15.143: INFO: Pod pod-secrets-11abe85b-a1e7-41e8-bb2d-8d645e0d0fd4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:33:15.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5955" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":119,"skipped":1894,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:33:15.158: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1030
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Feb 17 18:33:15.316: INFO: Waiting up to 5m0s for pod "client-containers-8df278df-7f15-4233-bac3-1cb531ec6ca8" in namespace "containers-1030" to be "success or failure"
Feb 17 18:33:15.324: INFO: Pod "client-containers-8df278df-7f15-4233-bac3-1cb531ec6ca8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.936259ms
Feb 17 18:33:17.329: INFO: Pod "client-containers-8df278df-7f15-4233-bac3-1cb531ec6ca8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013031935s
STEP: Saw pod success
Feb 17 18:33:17.329: INFO: Pod "client-containers-8df278df-7f15-4233-bac3-1cb531ec6ca8" satisfied condition "success or failure"
Feb 17 18:33:17.334: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod client-containers-8df278df-7f15-4233-bac3-1cb531ec6ca8 container test-container: <nil>
STEP: delete the pod
Feb 17 18:33:17.360: INFO: Waiting for pod client-containers-8df278df-7f15-4233-bac3-1cb531ec6ca8 to disappear
Feb 17 18:33:17.363: INFO: Pod client-containers-8df278df-7f15-4233-bac3-1cb531ec6ca8 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:33:17.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1030" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":278,"completed":120,"skipped":1903,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:33:17.377: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4955
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:33:21.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4955" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":278,"completed":121,"skipped":1920,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:33:21.580: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8402
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:33:22.361: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:33:24.373: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561202, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561202, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561202, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561202, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:33:27.409: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
Feb 17 18:33:28.458: INFO: Waiting for webhook configuration to be ready...
Feb 17 18:33:29.578: INFO: Waiting for webhook configuration to be ready...
Feb 17 18:33:30.696: INFO: Waiting for webhook configuration to be ready...
Feb 17 18:33:31.785: INFO: Waiting for webhook configuration to be ready...
Feb 17 18:33:32.908: INFO: Waiting for webhook configuration to be ready...
Feb 17 18:33:33.995: INFO: Waiting for webhook configuration to be ready...
Feb 17 18:33:35.083: INFO: Waiting for webhook configuration to be ready...
Feb 17 18:33:36.207: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:33:39.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8402" for this suite.
STEP: Destroying namespace "webhook-8402-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.853 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":278,"completed":122,"skipped":1925,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:33:39.433: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4096
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:33:39.644: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5453f644-f403-4e3e-8dd7-09a01c40ec72" in namespace "downward-api-4096" to be "success or failure"
Feb 17 18:33:39.650: INFO: Pod "downwardapi-volume-5453f644-f403-4e3e-8dd7-09a01c40ec72": Phase="Pending", Reason="", readiness=false. Elapsed: 5.633214ms
Feb 17 18:33:41.653: INFO: Pod "downwardapi-volume-5453f644-f403-4e3e-8dd7-09a01c40ec72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008940443s
Feb 17 18:33:43.658: INFO: Pod "downwardapi-volume-5453f644-f403-4e3e-8dd7-09a01c40ec72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014373893s
STEP: Saw pod success
Feb 17 18:33:43.659: INFO: Pod "downwardapi-volume-5453f644-f403-4e3e-8dd7-09a01c40ec72" satisfied condition "success or failure"
Feb 17 18:33:43.663: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downwardapi-volume-5453f644-f403-4e3e-8dd7-09a01c40ec72 container client-container: <nil>
STEP: delete the pod
Feb 17 18:33:43.699: INFO: Waiting for pod downwardapi-volume-5453f644-f403-4e3e-8dd7-09a01c40ec72 to disappear
Feb 17 18:33:43.702: INFO: Pod downwardapi-volume-5453f644-f403-4e3e-8dd7-09a01c40ec72 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:33:43.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4096" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":278,"completed":123,"skipped":1952,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:33:43.715: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1375
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Feb 17 18:34:23.900: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0217 18:34:23.900328      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:34:23.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1375" for this suite.

• [SLOW TEST:40.198 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":278,"completed":124,"skipped":1956,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:34:23.912: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-414
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Feb 17 18:34:24.071: INFO: Waiting up to 5m0s for pod "var-expansion-32ac40f7-c8b7-4aae-9a3d-4fd3bd26ac83" in namespace "var-expansion-414" to be "success or failure"
Feb 17 18:34:24.074: INFO: Pod "var-expansion-32ac40f7-c8b7-4aae-9a3d-4fd3bd26ac83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.639138ms
Feb 17 18:34:26.077: INFO: Pod "var-expansion-32ac40f7-c8b7-4aae-9a3d-4fd3bd26ac83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006238498s
Feb 17 18:34:28.082: INFO: Pod "var-expansion-32ac40f7-c8b7-4aae-9a3d-4fd3bd26ac83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011354372s
STEP: Saw pod success
Feb 17 18:34:28.082: INFO: Pod "var-expansion-32ac40f7-c8b7-4aae-9a3d-4fd3bd26ac83" satisfied condition "success or failure"
Feb 17 18:34:28.085: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod var-expansion-32ac40f7-c8b7-4aae-9a3d-4fd3bd26ac83 container dapi-container: <nil>
STEP: delete the pod
Feb 17 18:34:28.109: INFO: Waiting for pod var-expansion-32ac40f7-c8b7-4aae-9a3d-4fd3bd26ac83 to disappear
Feb 17 18:34:28.114: INFO: Pod var-expansion-32ac40f7-c8b7-4aae-9a3d-4fd3bd26ac83 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:34:28.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-414" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":278,"completed":125,"skipped":1981,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:34:28.128: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8877
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1733
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 18:34:28.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-8877'
Feb 17 18:34:28.357: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 17 18:34:28.357: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1738
Feb 17 18:34:30.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete deployment e2e-test-httpd-deployment --namespace=kubectl-8877'
Feb 17 18:34:30.457: INFO: stderr: ""
Feb 17 18:34:30.457: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:34:30.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8877" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":278,"completed":126,"skipped":2010,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:34:30.468: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-610
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:34:30.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 version'
Feb 17 18:34:30.685: INFO: stderr: ""
Feb 17 18:34:30.685: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.3\", GitCommit:\"06ad960bfd03b39c8310aaf92d1e7c12ce618213\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T18:14:22Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.3\", GitCommit:\"06ad960bfd03b39c8310aaf92d1e7c12ce618213\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T18:07:13Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:34:30.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-610" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":278,"completed":127,"skipped":2012,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:34:30.698: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1469
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 17 18:34:30.857: INFO: Waiting up to 5m0s for pod "pod-a64e8833-35b9-44d9-99f3-57b1fee399a6" in namespace "emptydir-1469" to be "success or failure"
Feb 17 18:34:30.863: INFO: Pod "pod-a64e8833-35b9-44d9-99f3-57b1fee399a6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.944287ms
Feb 17 18:34:32.866: INFO: Pod "pod-a64e8833-35b9-44d9-99f3-57b1fee399a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009416593s
Feb 17 18:34:34.870: INFO: Pod "pod-a64e8833-35b9-44d9-99f3-57b1fee399a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012970382s
STEP: Saw pod success
Feb 17 18:34:34.870: INFO: Pod "pod-a64e8833-35b9-44d9-99f3-57b1fee399a6" satisfied condition "success or failure"
Feb 17 18:34:34.874: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-a64e8833-35b9-44d9-99f3-57b1fee399a6 container test-container: <nil>
STEP: delete the pod
Feb 17 18:34:34.900: INFO: Waiting for pod pod-a64e8833-35b9-44d9-99f3-57b1fee399a6 to disappear
Feb 17 18:34:34.902: INFO: Pod pod-a64e8833-35b9-44d9-99f3-57b1fee399a6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:34:34.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1469" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":128,"skipped":2028,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:34:34.915: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7841
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:34:35.071: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Feb 17 18:34:43.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-7841 create -f -'
Feb 17 18:34:44.069: INFO: stderr: ""
Feb 17 18:34:44.069: INFO: stdout: "e2e-test-crd-publish-openapi-5428-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 17 18:34:44.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-7841 delete e2e-test-crd-publish-openapi-5428-crds test-foo'
Feb 17 18:34:44.184: INFO: stderr: ""
Feb 17 18:34:44.184: INFO: stdout: "e2e-test-crd-publish-openapi-5428-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 17 18:34:44.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-7841 apply -f -'
Feb 17 18:34:44.329: INFO: stderr: ""
Feb 17 18:34:44.329: INFO: stdout: "e2e-test-crd-publish-openapi-5428-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 17 18:34:44.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-7841 delete e2e-test-crd-publish-openapi-5428-crds test-foo'
Feb 17 18:34:44.417: INFO: stderr: ""
Feb 17 18:34:44.417: INFO: stdout: "e2e-test-crd-publish-openapi-5428-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Feb 17 18:34:44.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-7841 create -f -'
Feb 17 18:34:44.550: INFO: rc: 1
Feb 17 18:34:44.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-7841 apply -f -'
Feb 17 18:34:44.744: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Feb 17 18:34:44.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-7841 create -f -'
Feb 17 18:34:44.930: INFO: rc: 1
Feb 17 18:34:44.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-7841 apply -f -'
Feb 17 18:34:45.060: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Feb 17 18:34:45.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 explain e2e-test-crd-publish-openapi-5428-crds'
Feb 17 18:34:45.269: INFO: stderr: ""
Feb 17 18:34:45.269: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5428-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Feb 17 18:34:45.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 explain e2e-test-crd-publish-openapi-5428-crds.metadata'
Feb 17 18:34:45.472: INFO: stderr: ""
Feb 17 18:34:45.472: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5428-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 17 18:34:45.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 explain e2e-test-crd-publish-openapi-5428-crds.spec'
Feb 17 18:34:45.626: INFO: stderr: ""
Feb 17 18:34:45.626: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5428-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 17 18:34:45.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 explain e2e-test-crd-publish-openapi-5428-crds.spec.bars'
Feb 17 18:34:45.769: INFO: stderr: ""
Feb 17 18:34:45.769: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5428-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Feb 17 18:34:45.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 explain e2e-test-crd-publish-openapi-5428-crds.spec.bars2'
Feb 17 18:34:45.902: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:34:49.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7841" for this suite.

• [SLOW TEST:14.487 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":278,"completed":129,"skipped":2033,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:34:49.402: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6155
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-e7a67558-4c62-46a6-bafa-6f1069b419e4 in namespace container-probe-6155
Feb 17 18:34:51.585: INFO: Started pod busybox-e7a67558-4c62-46a6-bafa-6f1069b419e4 in namespace container-probe-6155
STEP: checking the pod's current state and verifying that restartCount is present
Feb 17 18:34:51.590: INFO: Initial restart count of pod busybox-e7a67558-4c62-46a6-bafa-6f1069b419e4 is 0
Feb 17 18:35:37.727: INFO: Restart count of pod container-probe-6155/busybox-e7a67558-4c62-46a6-bafa-6f1069b419e4 is now 1 (46.137784498s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:35:37.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6155" for this suite.

• [SLOW TEST:48.359 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":278,"completed":130,"skipped":2039,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:35:37.761: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7372
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Feb 17 18:35:37.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 create -f - --namespace=kubectl-7372'
Feb 17 18:35:38.128: INFO: stderr: ""
Feb 17 18:35:38.128: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 18:35:38.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7372'
Feb 17 18:35:38.193: INFO: stderr: ""
Feb 17 18:35:38.193: INFO: stdout: "update-demo-nautilus-72rtg update-demo-nautilus-qjnp5 "
Feb 17 18:35:38.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-72rtg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7372'
Feb 17 18:35:38.255: INFO: stderr: ""
Feb 17 18:35:38.255: INFO: stdout: ""
Feb 17 18:35:38.255: INFO: update-demo-nautilus-72rtg is created but not running
Feb 17 18:35:43.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7372'
Feb 17 18:35:43.320: INFO: stderr: ""
Feb 17 18:35:43.320: INFO: stdout: "update-demo-nautilus-72rtg update-demo-nautilus-qjnp5 "
Feb 17 18:35:43.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-72rtg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7372'
Feb 17 18:35:43.383: INFO: stderr: ""
Feb 17 18:35:43.383: INFO: stdout: "true"
Feb 17 18:35:43.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-72rtg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7372'
Feb 17 18:35:43.449: INFO: stderr: ""
Feb 17 18:35:43.449: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 18:35:43.449: INFO: validating pod update-demo-nautilus-72rtg
Feb 17 18:35:43.457: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 18:35:43.457: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 18:35:43.457: INFO: update-demo-nautilus-72rtg is verified up and running
Feb 17 18:35:43.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-qjnp5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7372'
Feb 17 18:35:43.524: INFO: stderr: ""
Feb 17 18:35:43.524: INFO: stdout: "true"
Feb 17 18:35:43.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-qjnp5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7372'
Feb 17 18:35:43.588: INFO: stderr: ""
Feb 17 18:35:43.588: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 18:35:43.588: INFO: validating pod update-demo-nautilus-qjnp5
Feb 17 18:35:43.594: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 18:35:43.594: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 18:35:43.595: INFO: update-demo-nautilus-qjnp5 is verified up and running
STEP: scaling down the replication controller
Feb 17 18:35:43.596: INFO: scanned /root for discovery docs: <nil>
Feb 17 18:35:43.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7372'
Feb 17 18:35:44.695: INFO: stderr: ""
Feb 17 18:35:44.695: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 18:35:44.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7372'
Feb 17 18:35:44.762: INFO: stderr: ""
Feb 17 18:35:44.762: INFO: stdout: "update-demo-nautilus-72rtg update-demo-nautilus-qjnp5 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 17 18:35:49.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7372'
Feb 17 18:35:49.828: INFO: stderr: ""
Feb 17 18:35:49.828: INFO: stdout: "update-demo-nautilus-qjnp5 "
Feb 17 18:35:49.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-qjnp5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7372'
Feb 17 18:35:49.892: INFO: stderr: ""
Feb 17 18:35:49.892: INFO: stdout: "true"
Feb 17 18:35:49.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-qjnp5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7372'
Feb 17 18:35:49.956: INFO: stderr: ""
Feb 17 18:35:49.956: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 18:35:49.956: INFO: validating pod update-demo-nautilus-qjnp5
Feb 17 18:35:49.962: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 18:35:49.962: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 18:35:49.962: INFO: update-demo-nautilus-qjnp5 is verified up and running
STEP: scaling up the replication controller
Feb 17 18:35:49.964: INFO: scanned /root for discovery docs: <nil>
Feb 17 18:35:49.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7372'
Feb 17 18:35:51.051: INFO: stderr: ""
Feb 17 18:35:51.051: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 18:35:51.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7372'
Feb 17 18:35:51.121: INFO: stderr: ""
Feb 17 18:35:51.121: INFO: stdout: "update-demo-nautilus-fk9ld update-demo-nautilus-qjnp5 "
Feb 17 18:35:51.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-fk9ld -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7372'
Feb 17 18:35:51.185: INFO: stderr: ""
Feb 17 18:35:51.185: INFO: stdout: ""
Feb 17 18:35:51.185: INFO: update-demo-nautilus-fk9ld is created but not running
Feb 17 18:35:56.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7372'
Feb 17 18:35:56.251: INFO: stderr: ""
Feb 17 18:35:56.251: INFO: stdout: "update-demo-nautilus-fk9ld update-demo-nautilus-qjnp5 "
Feb 17 18:35:56.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-fk9ld -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7372'
Feb 17 18:35:56.313: INFO: stderr: ""
Feb 17 18:35:56.313: INFO: stdout: "true"
Feb 17 18:35:56.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-fk9ld -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7372'
Feb 17 18:35:56.377: INFO: stderr: ""
Feb 17 18:35:56.378: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 18:35:56.378: INFO: validating pod update-demo-nautilus-fk9ld
Feb 17 18:35:56.384: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 18:35:56.384: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 18:35:56.384: INFO: update-demo-nautilus-fk9ld is verified up and running
Feb 17 18:35:56.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-qjnp5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7372'
Feb 17 18:35:56.448: INFO: stderr: ""
Feb 17 18:35:56.448: INFO: stdout: "true"
Feb 17 18:35:56.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-qjnp5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7372'
Feb 17 18:35:56.510: INFO: stderr: ""
Feb 17 18:35:56.510: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 18:35:56.510: INFO: validating pod update-demo-nautilus-qjnp5
Feb 17 18:35:56.515: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 18:35:56.515: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 18:35:56.515: INFO: update-demo-nautilus-qjnp5 is verified up and running
STEP: using delete to clean up resources
Feb 17 18:35:56.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete --grace-period=0 --force -f - --namespace=kubectl-7372'
Feb 17 18:35:56.587: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 18:35:56.587: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 17 18:35:56.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7372'
Feb 17 18:35:56.652: INFO: stderr: "No resources found in kubectl-7372 namespace.\n"
Feb 17 18:35:56.652: INFO: stdout: ""
Feb 17 18:35:56.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -l name=update-demo --namespace=kubectl-7372 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 17 18:35:56.717: INFO: stderr: ""
Feb 17 18:35:56.717: INFO: stdout: "update-demo-nautilus-fk9ld\nupdate-demo-nautilus-qjnp5\n"
Feb 17 18:35:57.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7372'
Feb 17 18:35:57.287: INFO: stderr: "No resources found in kubectl-7372 namespace.\n"
Feb 17 18:35:57.287: INFO: stdout: ""
Feb 17 18:35:57.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -l name=update-demo --namespace=kubectl-7372 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 17 18:35:57.350: INFO: stderr: ""
Feb 17 18:35:57.350: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:35:57.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7372" for this suite.

• [SLOW TEST:19.604 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":278,"completed":131,"skipped":2039,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:35:57.366: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:35:58.023: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:36:00.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561358, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561358, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561358, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561358, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:36:03.069: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:36:05.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8433" for this suite.
STEP: Destroying namespace "webhook-8433-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.924 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":278,"completed":132,"skipped":2054,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:36:05.290: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1520
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1520
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1520
STEP: creating replication controller externalsvc in namespace services-1520
I0217 18:36:05.503480      26 runners.go:189] Created replication controller with name: externalsvc, namespace: services-1520, replica count: 2
I0217 18:36:08.554015      26 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Feb 17 18:36:08.592: INFO: Creating new exec pod
Feb 17 18:36:12.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-1520 execpodpn67h -- /bin/sh -x -c nslookup nodeport-service'
Feb 17 18:36:13.817: INFO: stderr: "+ nslookup nodeport-service\n"
Feb 17 18:36:13.817: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-1520.svc.cluster.local\tcanonical name = externalsvc.services-1520.svc.cluster.local.\nName:\texternalsvc.services-1520.svc.cluster.local\nAddress: 100.64.208.215\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1520, will wait for the garbage collector to delete the pods
Feb 17 18:36:13.883: INFO: Deleting ReplicationController externalsvc took: 11.325167ms
Feb 17 18:36:13.984: INFO: Terminating ReplicationController externalsvc pods took: 100.243591ms
Feb 17 18:36:27.829: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:36:27.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1520" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:22.573 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":278,"completed":133,"skipped":2065,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:36:27.863: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2946
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-9d9097d9-8763-45ae-80a0-89516a4e7dd0
STEP: Creating a pod to test consume configMaps
Feb 17 18:36:28.031: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9a04ddb2-78b2-4cd0-9436-5824cfad9f54" in namespace "projected-2946" to be "success or failure"
Feb 17 18:36:28.037: INFO: Pod "pod-projected-configmaps-9a04ddb2-78b2-4cd0-9436-5824cfad9f54": Phase="Pending", Reason="", readiness=false. Elapsed: 5.97856ms
Feb 17 18:36:30.043: INFO: Pod "pod-projected-configmaps-9a04ddb2-78b2-4cd0-9436-5824cfad9f54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011352141s
Feb 17 18:36:32.048: INFO: Pod "pod-projected-configmaps-9a04ddb2-78b2-4cd0-9436-5824cfad9f54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017120572s
STEP: Saw pod success
Feb 17 18:36:32.048: INFO: Pod "pod-projected-configmaps-9a04ddb2-78b2-4cd0-9436-5824cfad9f54" satisfied condition "success or failure"
Feb 17 18:36:32.052: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod pod-projected-configmaps-9a04ddb2-78b2-4cd0-9436-5824cfad9f54 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:36:32.082: INFO: Waiting for pod pod-projected-configmaps-9a04ddb2-78b2-4cd0-9436-5824cfad9f54 to disappear
Feb 17 18:36:32.087: INFO: Pod pod-projected-configmaps-9a04ddb2-78b2-4cd0-9436-5824cfad9f54 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:36:32.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2946" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":278,"completed":134,"skipped":2112,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:36:32.102: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-8046
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:36:32.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8046" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":278,"completed":135,"skipped":2117,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:36:32.270: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-585
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-6e55af35-2807-4ccd-82b0-b6ccbdd2f291
STEP: Creating a pod to test consume configMaps
Feb 17 18:36:32.443: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-edc0aa89-d5ec-4e81-99cb-713d39fead91" in namespace "projected-585" to be "success or failure"
Feb 17 18:36:32.456: INFO: Pod "pod-projected-configmaps-edc0aa89-d5ec-4e81-99cb-713d39fead91": Phase="Pending", Reason="", readiness=false. Elapsed: 12.836616ms
Feb 17 18:36:34.461: INFO: Pod "pod-projected-configmaps-edc0aa89-d5ec-4e81-99cb-713d39fead91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017658463s
STEP: Saw pod success
Feb 17 18:36:34.461: INFO: Pod "pod-projected-configmaps-edc0aa89-d5ec-4e81-99cb-713d39fead91" satisfied condition "success or failure"
Feb 17 18:36:34.465: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod pod-projected-configmaps-edc0aa89-d5ec-4e81-99cb-713d39fead91 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:36:34.492: INFO: Waiting for pod pod-projected-configmaps-edc0aa89-d5ec-4e81-99cb-713d39fead91 to disappear
Feb 17 18:36:34.495: INFO: Pod pod-projected-configmaps-edc0aa89-d5ec-4e81-99cb-713d39fead91 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:36:34.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-585" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":278,"completed":136,"skipped":2126,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:36:34.509: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7561
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 17 18:36:34.667: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:36:38.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7561" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":278,"completed":137,"skipped":2155,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:36:38.936: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2281
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-lpsv
STEP: Creating a pod to test atomic-volume-subpath
Feb 17 18:36:39.103: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lpsv" in namespace "subpath-2281" to be "success or failure"
Feb 17 18:36:39.112: INFO: Pod "pod-subpath-test-configmap-lpsv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.470563ms
Feb 17 18:36:41.117: INFO: Pod "pod-subpath-test-configmap-lpsv": Phase="Running", Reason="", readiness=true. Elapsed: 2.013855983s
Feb 17 18:36:43.123: INFO: Pod "pod-subpath-test-configmap-lpsv": Phase="Running", Reason="", readiness=true. Elapsed: 4.019114364s
Feb 17 18:36:45.127: INFO: Pod "pod-subpath-test-configmap-lpsv": Phase="Running", Reason="", readiness=true. Elapsed: 6.02385407s
Feb 17 18:36:47.134: INFO: Pod "pod-subpath-test-configmap-lpsv": Phase="Running", Reason="", readiness=true. Elapsed: 8.030866571s
Feb 17 18:36:49.139: INFO: Pod "pod-subpath-test-configmap-lpsv": Phase="Running", Reason="", readiness=true. Elapsed: 10.035709041s
Feb 17 18:36:51.144: INFO: Pod "pod-subpath-test-configmap-lpsv": Phase="Running", Reason="", readiness=true. Elapsed: 12.040564022s
Feb 17 18:36:53.149: INFO: Pod "pod-subpath-test-configmap-lpsv": Phase="Running", Reason="", readiness=true. Elapsed: 14.045705293s
Feb 17 18:36:55.154: INFO: Pod "pod-subpath-test-configmap-lpsv": Phase="Running", Reason="", readiness=true. Elapsed: 16.050757439s
Feb 17 18:36:57.159: INFO: Pod "pod-subpath-test-configmap-lpsv": Phase="Running", Reason="", readiness=true. Elapsed: 18.055743012s
Feb 17 18:36:59.165: INFO: Pod "pod-subpath-test-configmap-lpsv": Phase="Running", Reason="", readiness=true. Elapsed: 20.06162598s
Feb 17 18:37:01.171: INFO: Pod "pod-subpath-test-configmap-lpsv": Phase="Running", Reason="", readiness=true. Elapsed: 22.067258059s
Feb 17 18:37:03.176: INFO: Pod "pod-subpath-test-configmap-lpsv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.072942726s
STEP: Saw pod success
Feb 17 18:37:03.176: INFO: Pod "pod-subpath-test-configmap-lpsv" satisfied condition "success or failure"
Feb 17 18:37:03.180: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-subpath-test-configmap-lpsv container test-container-subpath-configmap-lpsv: <nil>
STEP: delete the pod
Feb 17 18:37:03.208: INFO: Waiting for pod pod-subpath-test-configmap-lpsv to disappear
Feb 17 18:37:03.211: INFO: Pod pod-subpath-test-configmap-lpsv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lpsv
Feb 17 18:37:03.211: INFO: Deleting pod "pod-subpath-test-configmap-lpsv" in namespace "subpath-2281"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:37:03.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2281" for this suite.

• [SLOW TEST:24.293 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":278,"completed":138,"skipped":2156,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:37:03.229: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1847
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:37:03.393: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:37:09.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1847" for this suite.

• [SLOW TEST:6.554 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":278,"completed":139,"skipped":2158,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:37:09.783: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Feb 17 18:37:10.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 api-versions'
Feb 17 18:37:10.155: INFO: stderr: ""
Feb 17 18:37:10.155: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncore.kublr.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:37:10.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-188" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":278,"completed":140,"skipped":2167,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:37:10.170: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4923
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-35970a81-c46a-45eb-a081-9eeddf4d63eb
STEP: Creating configMap with name cm-test-opt-upd-1fe47539-3164-44d1-b73d-cddfa32ac3a2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-35970a81-c46a-45eb-a081-9eeddf4d63eb
STEP: Updating configmap cm-test-opt-upd-1fe47539-3164-44d1-b73d-cddfa32ac3a2
STEP: Creating configMap with name cm-test-opt-create-d55be2d9-fd10-4c86-9018-dec671b66cd6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:37:16.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4923" for this suite.

• [SLOW TEST:6.314 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":278,"completed":141,"skipped":2197,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:37:16.484: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3727
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 17 18:37:24.694: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 18:37:24.699: INFO: Pod pod-with-prestop-http-hook still exists
Feb 17 18:37:26.699: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 18:37:26.704: INFO: Pod pod-with-prestop-http-hook still exists
Feb 17 18:37:28.699: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 18:37:28.705: INFO: Pod pod-with-prestop-http-hook still exists
Feb 17 18:37:30.699: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 18:37:30.705: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:37:30.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3727" for this suite.

• [SLOW TEST:14.247 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":278,"completed":142,"skipped":2216,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:37:30.731: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 17 18:37:35.946: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:37:36.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9506" for this suite.

• [SLOW TEST:6.253 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":278,"completed":143,"skipped":2218,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:37:36.984: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4970
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-b56354b5-0aee-4459-b3e6-ccbb9864095b
STEP: Creating a pod to test consume secrets
Feb 17 18:37:37.160: INFO: Waiting up to 5m0s for pod "pod-secrets-aee91e1a-9621-453c-874c-6dbed3205b58" in namespace "secrets-4970" to be "success or failure"
Feb 17 18:37:37.169: INFO: Pod "pod-secrets-aee91e1a-9621-453c-874c-6dbed3205b58": Phase="Pending", Reason="", readiness=false. Elapsed: 8.540102ms
Feb 17 18:37:39.174: INFO: Pod "pod-secrets-aee91e1a-9621-453c-874c-6dbed3205b58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014240214s
Feb 17 18:37:41.181: INFO: Pod "pod-secrets-aee91e1a-9621-453c-874c-6dbed3205b58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021188726s
STEP: Saw pod success
Feb 17 18:37:41.181: INFO: Pod "pod-secrets-aee91e1a-9621-453c-874c-6dbed3205b58" satisfied condition "success or failure"
Feb 17 18:37:41.185: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod pod-secrets-aee91e1a-9621-453c-874c-6dbed3205b58 container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 18:37:41.221: INFO: Waiting for pod pod-secrets-aee91e1a-9621-453c-874c-6dbed3205b58 to disappear
Feb 17 18:37:41.225: INFO: Pod pod-secrets-aee91e1a-9621-453c-874c-6dbed3205b58 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:37:41.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4970" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":144,"skipped":2223,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:37:41.241: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3597
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 17 18:37:41.419: INFO: Waiting up to 5m0s for pod "pod-42ce30b8-c8b0-42dc-bc1c-7e9afc38be37" in namespace "emptydir-3597" to be "success or failure"
Feb 17 18:37:41.429: INFO: Pod "pod-42ce30b8-c8b0-42dc-bc1c-7e9afc38be37": Phase="Pending", Reason="", readiness=false. Elapsed: 10.083667ms
Feb 17 18:37:43.435: INFO: Pod "pod-42ce30b8-c8b0-42dc-bc1c-7e9afc38be37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015930967s
STEP: Saw pod success
Feb 17 18:37:43.435: INFO: Pod "pod-42ce30b8-c8b0-42dc-bc1c-7e9afc38be37" satisfied condition "success or failure"
Feb 17 18:37:43.439: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod pod-42ce30b8-c8b0-42dc-bc1c-7e9afc38be37 container test-container: <nil>
STEP: delete the pod
Feb 17 18:37:43.467: INFO: Waiting for pod pod-42ce30b8-c8b0-42dc-bc1c-7e9afc38be37 to disappear
Feb 17 18:37:43.471: INFO: Pod pod-42ce30b8-c8b0-42dc-bc1c-7e9afc38be37 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:37:43.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3597" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":145,"skipped":2223,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:37:43.485: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1413
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 17 18:37:43.655: INFO: Waiting up to 5m0s for pod "pod-c6de1a37-b9e9-4ade-aff1-5e2afa655948" in namespace "emptydir-1413" to be "success or failure"
Feb 17 18:37:43.660: INFO: Pod "pod-c6de1a37-b9e9-4ade-aff1-5e2afa655948": Phase="Pending", Reason="", readiness=false. Elapsed: 4.881674ms
Feb 17 18:37:45.666: INFO: Pod "pod-c6de1a37-b9e9-4ade-aff1-5e2afa655948": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0100444s
STEP: Saw pod success
Feb 17 18:37:45.666: INFO: Pod "pod-c6de1a37-b9e9-4ade-aff1-5e2afa655948" satisfied condition "success or failure"
Feb 17 18:37:45.669: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-c6de1a37-b9e9-4ade-aff1-5e2afa655948 container test-container: <nil>
STEP: delete the pod
Feb 17 18:37:45.692: INFO: Waiting for pod pod-c6de1a37-b9e9-4ade-aff1-5e2afa655948 to disappear
Feb 17 18:37:45.696: INFO: Pod pod-c6de1a37-b9e9-4ade-aff1-5e2afa655948 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:37:45.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1413" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":146,"skipped":2232,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:37:45.711: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8284
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 17 18:37:50.408: INFO: Successfully updated pod "pod-update-3d4c51b1-a086-450b-bb4f-d19bfd10a420"
STEP: verifying the updated pod is in kubernetes
Feb 17 18:37:50.418: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:37:50.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8284" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":278,"completed":147,"skipped":2247,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:37:50.432: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1959
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1861
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 18:37:50.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1959'
Feb 17 18:37:50.672: INFO: stderr: ""
Feb 17 18:37:50.672: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1866
Feb 17 18:37:50.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete pods e2e-test-httpd-pod --namespace=kubectl-1959'
Feb 17 18:37:59.961: INFO: stderr: ""
Feb 17 18:37:59.961: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:37:59.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1959" for this suite.

• [SLOW TEST:9.543 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1857
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":278,"completed":148,"skipped":2282,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:37:59.976: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7888
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-7888
STEP: creating replication controller nodeport-test in namespace services-7888
I0217 18:38:00.157109      26 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-7888, replica count: 2
Feb 17 18:38:03.207: INFO: Creating new exec pod
I0217 18:38:03.207559      26 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 18:38:06.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-7888 execpodnfsvn -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Feb 17 18:38:09.449: INFO: rc: 1
Feb 17 18:38:09.449: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-7888 execpodnfsvn -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 nodeport-test 80
nc: connect to nodeport-test port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Feb 17 18:38:10.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-7888 execpodnfsvn -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Feb 17 18:38:10.649: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 17 18:38:10.649: INFO: stdout: ""
Feb 17 18:38:10.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-7888 execpodnfsvn -- /bin/sh -x -c nc -zv -t -w 2 100.65.124.139 80'
Feb 17 18:38:10.858: INFO: stderr: "+ nc -zv -t -w 2 100.65.124.139 80\nConnection to 100.65.124.139 80 port [tcp/http] succeeded!\n"
Feb 17 18:38:10.858: INFO: stdout: ""
Feb 17 18:38:10.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-7888 execpodnfsvn -- /bin/sh -x -c nc -zv -t -w 2 172.16.71.125 31794'
Feb 17 18:38:11.062: INFO: stderr: "+ nc -zv -t -w 2 172.16.71.125 31794\nConnection to 172.16.71.125 31794 port [tcp/31794] succeeded!\n"
Feb 17 18:38:11.062: INFO: stdout: ""
Feb 17 18:38:11.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-7888 execpodnfsvn -- /bin/sh -x -c nc -zv -t -w 2 172.16.86.181 31794'
Feb 17 18:38:11.267: INFO: stderr: "+ nc -zv -t -w 2 172.16.86.181 31794\nConnection to 172.16.86.181 31794 port [tcp/31794] succeeded!\n"
Feb 17 18:38:11.267: INFO: stdout: ""
Feb 17 18:38:11.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-7888 execpodnfsvn -- /bin/sh -x -c nc -zv -t -w 2 3.228.8.24 31794'
Feb 17 18:38:11.458: INFO: stderr: "+ nc -zv -t -w 2 3.228.8.24 31794\nConnection to 3.228.8.24 31794 port [tcp/31794] succeeded!\n"
Feb 17 18:38:11.458: INFO: stdout: ""
Feb 17 18:38:11.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-7888 execpodnfsvn -- /bin/sh -x -c nc -zv -t -w 2 54.157.144.225 31794'
Feb 17 18:38:11.651: INFO: stderr: "+ nc -zv -t -w 2 54.157.144.225 31794\nConnection to 54.157.144.225 31794 port [tcp/31794] succeeded!\n"
Feb 17 18:38:11.651: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:38:11.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7888" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:11.695 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":278,"completed":149,"skipped":2283,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:38:11.671: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-208
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 17 18:38:11.844: INFO: Waiting up to 5m0s for pod "pod-e70235bc-6fa4-4352-a862-499a4bd0312b" in namespace "emptydir-208" to be "success or failure"
Feb 17 18:38:11.848: INFO: Pod "pod-e70235bc-6fa4-4352-a862-499a4bd0312b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.811408ms
Feb 17 18:38:13.854: INFO: Pod "pod-e70235bc-6fa4-4352-a862-499a4bd0312b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010418823s
Feb 17 18:38:15.860: INFO: Pod "pod-e70235bc-6fa4-4352-a862-499a4bd0312b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015823777s
STEP: Saw pod success
Feb 17 18:38:15.860: INFO: Pod "pod-e70235bc-6fa4-4352-a862-499a4bd0312b" satisfied condition "success or failure"
Feb 17 18:38:15.864: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod pod-e70235bc-6fa4-4352-a862-499a4bd0312b container test-container: <nil>
STEP: delete the pod
Feb 17 18:38:15.891: INFO: Waiting for pod pod-e70235bc-6fa4-4352-a862-499a4bd0312b to disappear
Feb 17 18:38:15.898: INFO: Pod pod-e70235bc-6fa4-4352-a862-499a4bd0312b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:38:15.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-208" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":150,"skipped":2296,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:38:15.913: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5920
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 17 18:38:16.180: INFO: Waiting up to 5m0s for pod "pod-b9ff4895-b56f-4177-acd1-e89c28c1bb96" in namespace "emptydir-5920" to be "success or failure"
Feb 17 18:38:16.184: INFO: Pod "pod-b9ff4895-b56f-4177-acd1-e89c28c1bb96": Phase="Pending", Reason="", readiness=false. Elapsed: 3.486288ms
Feb 17 18:38:18.189: INFO: Pod "pod-b9ff4895-b56f-4177-acd1-e89c28c1bb96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009097564s
Feb 17 18:38:20.194: INFO: Pod "pod-b9ff4895-b56f-4177-acd1-e89c28c1bb96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014046441s
STEP: Saw pod success
Feb 17 18:38:20.194: INFO: Pod "pod-b9ff4895-b56f-4177-acd1-e89c28c1bb96" satisfied condition "success or failure"
Feb 17 18:38:20.198: INFO: Trying to get logs from node ip-172-16-71-125.ec2.internal pod pod-b9ff4895-b56f-4177-acd1-e89c28c1bb96 container test-container: <nil>
STEP: delete the pod
Feb 17 18:38:20.236: INFO: Waiting for pod pod-b9ff4895-b56f-4177-acd1-e89c28c1bb96 to disappear
Feb 17 18:38:20.250: INFO: Pod pod-b9ff4895-b56f-4177-acd1-e89c28c1bb96 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:38:20.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5920" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":151,"skipped":2309,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:38:20.265: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9757
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:38:31.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9757" for this suite.

• [SLOW TEST:11.220 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":278,"completed":152,"skipped":2346,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:38:31.485: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4511
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 17 18:38:37.687: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 18:38:37.690: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 18:38:39.690: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 18:38:39.699: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 18:38:41.690: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 18:38:41.697: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 18:38:43.690: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 18:38:43.695: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 18:38:45.690: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 18:38:45.696: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 18:38:47.690: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 18:38:47.699: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:38:47.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4511" for this suite.

• [SLOW TEST:16.239 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":278,"completed":153,"skipped":2380,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:38:47.725: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3868
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-bfc78e81-84ed-481d-9888-e457989c888f
STEP: Creating a pod to test consume configMaps
Feb 17 18:38:47.891: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ab9fa0f-c261-4a00-99d9-767928573f00" in namespace "configmap-3868" to be "success or failure"
Feb 17 18:38:47.899: INFO: Pod "pod-configmaps-2ab9fa0f-c261-4a00-99d9-767928573f00": Phase="Pending", Reason="", readiness=false. Elapsed: 8.139557ms
Feb 17 18:38:49.904: INFO: Pod "pod-configmaps-2ab9fa0f-c261-4a00-99d9-767928573f00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013267902s
Feb 17 18:38:51.910: INFO: Pod "pod-configmaps-2ab9fa0f-c261-4a00-99d9-767928573f00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019287886s
STEP: Saw pod success
Feb 17 18:38:51.910: INFO: Pod "pod-configmaps-2ab9fa0f-c261-4a00-99d9-767928573f00" satisfied condition "success or failure"
Feb 17 18:38:51.914: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-configmaps-2ab9fa0f-c261-4a00-99d9-767928573f00 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:38:51.942: INFO: Waiting for pod pod-configmaps-2ab9fa0f-c261-4a00-99d9-767928573f00 to disappear
Feb 17 18:38:51.946: INFO: Pod pod-configmaps-2ab9fa0f-c261-4a00-99d9-767928573f00 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:38:51.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3868" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":278,"completed":154,"skipped":2381,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:38:51.963: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8140
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 17 18:38:56.666: INFO: Successfully updated pod "labelsupdate3595c78c-1462-4ead-aa01-f40f5fc9079f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:38:58.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8140" for this suite.

• [SLOW TEST:6.749 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":278,"completed":155,"skipped":2437,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:38:58.712: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-952
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0217 18:39:04.907633      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 17 18:39:04.907: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:39:04.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-952" for this suite.

• [SLOW TEST:6.211 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":278,"completed":156,"skipped":2441,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:39:04.922: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2511
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 17 18:39:05.098: INFO: Waiting up to 5m0s for pod "downward-api-71fc30e5-ec40-4479-b81d-d06c41a0de08" in namespace "downward-api-2511" to be "success or failure"
Feb 17 18:39:05.103: INFO: Pod "downward-api-71fc30e5-ec40-4479-b81d-d06c41a0de08": Phase="Pending", Reason="", readiness=false. Elapsed: 5.353039ms
Feb 17 18:39:07.108: INFO: Pod "downward-api-71fc30e5-ec40-4479-b81d-d06c41a0de08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01007222s
Feb 17 18:39:09.116: INFO: Pod "downward-api-71fc30e5-ec40-4479-b81d-d06c41a0de08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017906738s
STEP: Saw pod success
Feb 17 18:39:09.116: INFO: Pod "downward-api-71fc30e5-ec40-4479-b81d-d06c41a0de08" satisfied condition "success or failure"
Feb 17 18:39:09.120: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod downward-api-71fc30e5-ec40-4479-b81d-d06c41a0de08 container dapi-container: <nil>
STEP: delete the pod
Feb 17 18:39:09.147: INFO: Waiting for pod downward-api-71fc30e5-ec40-4479-b81d-d06c41a0de08 to disappear
Feb 17 18:39:09.152: INFO: Pod downward-api-71fc30e5-ec40-4479-b81d-d06c41a0de08 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:39:09.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2511" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":278,"completed":157,"skipped":2461,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:39:09.173: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4330
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:39:09.333: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 17 18:39:17.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-4330 create -f -'
Feb 17 18:39:18.308: INFO: stderr: ""
Feb 17 18:39:18.308: INFO: stdout: "e2e-test-crd-publish-openapi-675-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 17 18:39:18.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-4330 delete e2e-test-crd-publish-openapi-675-crds test-cr'
Feb 17 18:39:18.417: INFO: stderr: ""
Feb 17 18:39:18.417: INFO: stdout: "e2e-test-crd-publish-openapi-675-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 17 18:39:18.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-4330 apply -f -'
Feb 17 18:39:18.667: INFO: stderr: ""
Feb 17 18:39:18.667: INFO: stdout: "e2e-test-crd-publish-openapi-675-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 17 18:39:18.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-4330 delete e2e-test-crd-publish-openapi-675-crds test-cr'
Feb 17 18:39:18.745: INFO: stderr: ""
Feb 17 18:39:18.745: INFO: stdout: "e2e-test-crd-publish-openapi-675-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 17 18:39:18.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 explain e2e-test-crd-publish-openapi-675-crds'
Feb 17 18:39:18.940: INFO: stderr: ""
Feb 17 18:39:18.940: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-675-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:39:22.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4330" for this suite.

• [SLOW TEST:13.252 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":278,"completed":158,"skipped":2466,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:39:22.425: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9341
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:39:22.579: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 17 18:39:30.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-9341 create -f -'
Feb 17 18:39:31.601: INFO: stderr: ""
Feb 17 18:39:31.601: INFO: stdout: "e2e-test-crd-publish-openapi-2405-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 17 18:39:31.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-9341 delete e2e-test-crd-publish-openapi-2405-crds test-cr'
Feb 17 18:39:31.718: INFO: stderr: ""
Feb 17 18:39:31.718: INFO: stdout: "e2e-test-crd-publish-openapi-2405-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 17 18:39:31.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-9341 apply -f -'
Feb 17 18:39:31.930: INFO: stderr: ""
Feb 17 18:39:31.930: INFO: stdout: "e2e-test-crd-publish-openapi-2405-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 17 18:39:31.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-9341 delete e2e-test-crd-publish-openapi-2405-crds test-cr'
Feb 17 18:39:32.005: INFO: stderr: ""
Feb 17 18:39:32.005: INFO: stdout: "e2e-test-crd-publish-openapi-2405-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 17 18:39:32.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 explain e2e-test-crd-publish-openapi-2405-crds'
Feb 17 18:39:32.198: INFO: stderr: ""
Feb 17 18:39:32.198: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2405-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:39:35.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9341" for this suite.

• [SLOW TEST:13.281 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":278,"completed":159,"skipped":2469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:39:35.707: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-3974/configmap-test-17ef508d-a335-41c9-863e-91cc8b7cd4e0
STEP: Creating a pod to test consume configMaps
Feb 17 18:39:35.876: INFO: Waiting up to 5m0s for pod "pod-configmaps-1f0f5dc6-ba1a-4b82-b6bc-d78b874d6614" in namespace "configmap-3974" to be "success or failure"
Feb 17 18:39:35.880: INFO: Pod "pod-configmaps-1f0f5dc6-ba1a-4b82-b6bc-d78b874d6614": Phase="Pending", Reason="", readiness=false. Elapsed: 3.800187ms
Feb 17 18:39:37.885: INFO: Pod "pod-configmaps-1f0f5dc6-ba1a-4b82-b6bc-d78b874d6614": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008926644s
Feb 17 18:39:39.890: INFO: Pod "pod-configmaps-1f0f5dc6-ba1a-4b82-b6bc-d78b874d6614": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013879154s
STEP: Saw pod success
Feb 17 18:39:39.890: INFO: Pod "pod-configmaps-1f0f5dc6-ba1a-4b82-b6bc-d78b874d6614" satisfied condition "success or failure"
Feb 17 18:39:39.893: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-configmaps-1f0f5dc6-ba1a-4b82-b6bc-d78b874d6614 container env-test: <nil>
STEP: delete the pod
Feb 17 18:39:39.928: INFO: Waiting for pod pod-configmaps-1f0f5dc6-ba1a-4b82-b6bc-d78b874d6614 to disappear
Feb 17 18:39:39.932: INFO: Pod pod-configmaps-1f0f5dc6-ba1a-4b82-b6bc-d78b874d6614 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:39:39.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3974" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":278,"completed":160,"skipped":2578,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:39:39.945: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-4144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:39:40.666: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:39:43.698: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:39:43.703: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:39:50.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4144" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:11.097 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":278,"completed":161,"skipped":2607,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:39:51.043: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6857
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:39:55.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6857" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":278,"completed":162,"skipped":2627,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:39:55.169: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:39:57.371: INFO: Waiting up to 5m0s for pod "client-envvars-dfd2f23d-ad01-4d49-a495-943de5754db6" in namespace "pods-3690" to be "success or failure"
Feb 17 18:39:57.376: INFO: Pod "client-envvars-dfd2f23d-ad01-4d49-a495-943de5754db6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.317349ms
Feb 17 18:39:59.380: INFO: Pod "client-envvars-dfd2f23d-ad01-4d49-a495-943de5754db6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008991486s
STEP: Saw pod success
Feb 17 18:39:59.380: INFO: Pod "client-envvars-dfd2f23d-ad01-4d49-a495-943de5754db6" satisfied condition "success or failure"
Feb 17 18:39:59.384: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod client-envvars-dfd2f23d-ad01-4d49-a495-943de5754db6 container env3cont: <nil>
STEP: delete the pod
Feb 17 18:39:59.419: INFO: Waiting for pod client-envvars-dfd2f23d-ad01-4d49-a495-943de5754db6 to disappear
Feb 17 18:39:59.423: INFO: Pod client-envvars-dfd2f23d-ad01-4d49-a495-943de5754db6 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:39:59.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3690" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":278,"completed":163,"skipped":2643,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:39:59.437: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4900
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 17 18:40:04.151: INFO: Successfully updated pod "labelsupdated9510595-089c-42dc-957e-630a5685018f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:40:06.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4900" for this suite.

• [SLOW TEST:6.757 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":278,"completed":164,"skipped":2664,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:40:06.195: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5737
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:40:06.764: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:40:08.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561606, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561606, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561606, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561606, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:40:11.803: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:40:11.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5737" for this suite.
STEP: Destroying namespace "webhook-5737-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.714 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":278,"completed":165,"skipped":2672,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:40:11.909: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2681
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2681
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2681
STEP: creating replication controller externalsvc in namespace services-2681
I0217 18:40:12.125827      26 runners.go:189] Created replication controller with name: externalsvc, namespace: services-2681, replica count: 2
I0217 18:40:15.176205      26 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Feb 17 18:40:15.207: INFO: Creating new exec pod
Feb 17 18:40:17.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=services-2681 execpodkcwfp -- /bin/sh -x -c nslookup clusterip-service'
Feb 17 18:40:17.430: INFO: stderr: "+ nslookup clusterip-service\n"
Feb 17 18:40:17.430: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-2681.svc.cluster.local\tcanonical name = externalsvc.services-2681.svc.cluster.local.\nName:\texternalsvc.services-2681.svc.cluster.local\nAddress: 100.71.91.38\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2681, will wait for the garbage collector to delete the pods
Feb 17 18:40:17.495: INFO: Deleting ReplicationController externalsvc took: 10.661712ms
Feb 17 18:40:17.995: INFO: Terminating ReplicationController externalsvc pods took: 500.306875ms
Feb 17 18:40:22.326: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:40:22.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2681" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:10.452 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":278,"completed":166,"skipped":2691,"failed":0}
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:40:22.361: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4294
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:40:38.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4294" for this suite.

• [SLOW TEST:16.282 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":278,"completed":167,"skipped":2691,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:40:38.643: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7630
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-7630
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 17 18:40:38.794: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 17 18:41:02.933: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.148:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7630 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:41:02.933: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:41:03.050: INFO: Found all expected endpoints: [netserver-0]
Feb 17 18:41:03.054: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.31:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7630 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:41:03.054: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:41:03.186: INFO: Found all expected endpoints: [netserver-1]
Feb 17 18:41:03.192: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.5.61:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7630 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:41:03.192: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:41:03.336: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:41:03.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7630" for this suite.

• [SLOW TEST:24.709 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":168,"skipped":2715,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:41:03.352: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5044
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:41:03.511: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:41:09.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5044" for this suite.

• [SLOW TEST:6.235 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":278,"completed":169,"skipped":2723,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:41:09.587: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8691
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-8691
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8691 to expose endpoints map[]
Feb 17 18:41:09.954: INFO: Get endpoints failed (4.146792ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Feb 17 18:41:10.964: INFO: successfully validated that service multi-endpoint-test in namespace services-8691 exposes endpoints map[] (1.01474181s elapsed)
STEP: Creating pod pod1 in namespace services-8691
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8691 to expose endpoints map[pod1:[100]]
Feb 17 18:41:14.016: INFO: successfully validated that service multi-endpoint-test in namespace services-8691 exposes endpoints map[pod1:[100]] (3.03440903s elapsed)
STEP: Creating pod pod2 in namespace services-8691
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8691 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 17 18:41:17.083: INFO: successfully validated that service multi-endpoint-test in namespace services-8691 exposes endpoints map[pod1:[100] pod2:[101]] (3.060045822s elapsed)
STEP: Deleting pod pod1 in namespace services-8691
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8691 to expose endpoints map[pod2:[101]]
Feb 17 18:41:17.109: INFO: successfully validated that service multi-endpoint-test in namespace services-8691 exposes endpoints map[pod2:[101]] (14.903843ms elapsed)
STEP: Deleting pod pod2 in namespace services-8691
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8691 to expose endpoints map[]
Feb 17 18:41:18.131: INFO: successfully validated that service multi-endpoint-test in namespace services-8691 exposes endpoints map[] (1.009339132s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:41:18.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8691" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

• [SLOW TEST:8.595 seconds]
[sig-network] Services
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":278,"completed":170,"skipped":2727,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:41:18.182: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5633
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-820f07ac-1353-40e6-9885-446ebae98a69
STEP: Creating a pod to test consume secrets
Feb 17 18:41:18.360: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e70763f4-c1b0-4e31-9050-65d91b27fe35" in namespace "projected-5633" to be "success or failure"
Feb 17 18:41:18.367: INFO: Pod "pod-projected-secrets-e70763f4-c1b0-4e31-9050-65d91b27fe35": Phase="Pending", Reason="", readiness=false. Elapsed: 7.141187ms
Feb 17 18:41:20.372: INFO: Pod "pod-projected-secrets-e70763f4-c1b0-4e31-9050-65d91b27fe35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011552814s
STEP: Saw pod success
Feb 17 18:41:20.372: INFO: Pod "pod-projected-secrets-e70763f4-c1b0-4e31-9050-65d91b27fe35" satisfied condition "success or failure"
Feb 17 18:41:20.382: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-projected-secrets-e70763f4-c1b0-4e31-9050-65d91b27fe35 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 17 18:41:20.417: INFO: Waiting for pod pod-projected-secrets-e70763f4-c1b0-4e31-9050-65d91b27fe35 to disappear
Feb 17 18:41:20.424: INFO: Pod pod-projected-secrets-e70763f4-c1b0-4e31-9050-65d91b27fe35 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:41:20.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5633" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":171,"skipped":2740,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:41:20.447: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6434
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:41:20.621: INFO: Waiting up to 5m0s for pod "downwardapi-volume-665fb542-8ee4-4571-8b82-535d7f1c3e22" in namespace "projected-6434" to be "success or failure"
Feb 17 18:41:20.625: INFO: Pod "downwardapi-volume-665fb542-8ee4-4571-8b82-535d7f1c3e22": Phase="Pending", Reason="", readiness=false. Elapsed: 3.716382ms
Feb 17 18:41:22.630: INFO: Pod "downwardapi-volume-665fb542-8ee4-4571-8b82-535d7f1c3e22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009207509s
STEP: Saw pod success
Feb 17 18:41:22.630: INFO: Pod "downwardapi-volume-665fb542-8ee4-4571-8b82-535d7f1c3e22" satisfied condition "success or failure"
Feb 17 18:41:22.634: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod downwardapi-volume-665fb542-8ee4-4571-8b82-535d7f1c3e22 container client-container: <nil>
STEP: delete the pod
Feb 17 18:41:22.662: INFO: Waiting for pod downwardapi-volume-665fb542-8ee4-4571-8b82-535d7f1c3e22 to disappear
Feb 17 18:41:22.666: INFO: Pod downwardapi-volume-665fb542-8ee4-4571-8b82-535d7f1c3e22 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:41:22.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6434" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":172,"skipped":2757,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:41:22.680: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2380
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
Feb 17 18:41:23.893: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0217 18:41:23.893461      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:41:23.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2380" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":278,"completed":173,"skipped":2788,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:41:23.908: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5060
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:41:26.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5060" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":174,"skipped":2803,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:41:26.109: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3335
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-3335
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Feb 17 18:41:26.289: INFO: Found 0 stateful pods, waiting for 3
Feb 17 18:41:36.295: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 18:41:36.295: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 18:41:36.295: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 18:41:36.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-3335 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 18:41:36.506: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 18:41:36.506: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 18:41:36.506: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 17 18:41:46.543: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 17 18:41:56.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-3335 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:41:56.763: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 18:41:56.763: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 18:41:56.763: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Feb 17 18:42:26.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-3335 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 18:42:26.994: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 18:42:26.994: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 18:42:26.994: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 18:42:37.035: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 17 18:42:47.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-3335 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:42:47.260: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 18:42:47.260: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 18:42:47.260: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 18:42:57.287: INFO: Waiting for StatefulSet statefulset-3335/ss2 to complete update
Feb 17 18:42:57.287: INFO: Waiting for Pod statefulset-3335/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 18:42:57.287: INFO: Waiting for Pod statefulset-3335/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 18:42:57.287: INFO: Waiting for Pod statefulset-3335/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 18:43:07.298: INFO: Waiting for StatefulSet statefulset-3335/ss2 to complete update
Feb 17 18:43:07.298: INFO: Waiting for Pod statefulset-3335/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 18:43:17.297: INFO: Waiting for StatefulSet statefulset-3335/ss2 to complete update
Feb 17 18:43:17.297: INFO: Waiting for Pod statefulset-3335/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 17 18:43:27.297: INFO: Deleting all statefulset in ns statefulset-3335
Feb 17 18:43:27.301: INFO: Scaling statefulset ss2 to 0
Feb 17 18:43:47.321: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 18:43:47.324: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:43:47.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3335" for this suite.

• [SLOW TEST:141.249 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":278,"completed":175,"skipped":2824,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:43:47.358: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6154
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0217 18:44:18.060816      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 17 18:44:18.060: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:44:18.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6154" for this suite.

• [SLOW TEST:30.716 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":278,"completed":176,"skipped":2861,"failed":0}
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:44:18.074: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-4113
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:44:18.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-4113" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":278,"completed":177,"skipped":2861,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:44:18.303: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3263
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:44:34.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3263" for this suite.

• [SLOW TEST:16.300 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":278,"completed":178,"skipped":2871,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:44:34.604: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4161
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:44:34.767: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b41a271-bcd9-49a8-bd51-5127ca8e7a7f" in namespace "downward-api-4161" to be "success or failure"
Feb 17 18:44:34.776: INFO: Pod "downwardapi-volume-0b41a271-bcd9-49a8-bd51-5127ca8e7a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.636069ms
Feb 17 18:44:36.781: INFO: Pod "downwardapi-volume-0b41a271-bcd9-49a8-bd51-5127ca8e7a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014342793s
Feb 17 18:44:38.787: INFO: Pod "downwardapi-volume-0b41a271-bcd9-49a8-bd51-5127ca8e7a7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02062782s
STEP: Saw pod success
Feb 17 18:44:38.787: INFO: Pod "downwardapi-volume-0b41a271-bcd9-49a8-bd51-5127ca8e7a7f" satisfied condition "success or failure"
Feb 17 18:44:38.791: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downwardapi-volume-0b41a271-bcd9-49a8-bd51-5127ca8e7a7f container client-container: <nil>
STEP: delete the pod
Feb 17 18:44:38.829: INFO: Waiting for pod downwardapi-volume-0b41a271-bcd9-49a8-bd51-5127ca8e7a7f to disappear
Feb 17 18:44:38.832: INFO: Pod downwardapi-volume-0b41a271-bcd9-49a8-bd51-5127ca8e7a7f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:44:38.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4161" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":278,"completed":179,"skipped":2876,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:44:38.846: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-919
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-e4c9287d-bed3-4164-8589-1e98499a8890
STEP: Creating a pod to test consume secrets
Feb 17 18:44:39.013: INFO: Waiting up to 5m0s for pod "pod-secrets-dffd1feb-ea27-41bd-a16b-0ff5404443a1" in namespace "secrets-919" to be "success or failure"
Feb 17 18:44:39.017: INFO: Pod "pod-secrets-dffd1feb-ea27-41bd-a16b-0ff5404443a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259165ms
Feb 17 18:44:41.024: INFO: Pod "pod-secrets-dffd1feb-ea27-41bd-a16b-0ff5404443a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011242093s
STEP: Saw pod success
Feb 17 18:44:41.024: INFO: Pod "pod-secrets-dffd1feb-ea27-41bd-a16b-0ff5404443a1" satisfied condition "success or failure"
Feb 17 18:44:41.028: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-secrets-dffd1feb-ea27-41bd-a16b-0ff5404443a1 container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 18:44:41.056: INFO: Waiting for pod pod-secrets-dffd1feb-ea27-41bd-a16b-0ff5404443a1 to disappear
Feb 17 18:44:41.060: INFO: Pod pod-secrets-dffd1feb-ea27-41bd-a16b-0ff5404443a1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:44:41.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-919" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":180,"skipped":2886,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:44:41.083: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6573
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:44:41.847: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Feb 17 18:44:43.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561881, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561881, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561881, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561881, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:44:46.884: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:44:48.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6573" for this suite.
STEP: Destroying namespace "webhook-6573-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.166 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":278,"completed":181,"skipped":2980,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:44:48.249: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9805
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:44:56.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9805" for this suite.

• [SLOW TEST:8.180 seconds]
[sig-apps] Job
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":278,"completed":182,"skipped":2989,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:44:56.429: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6549
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-715d6708-22f7-4a34-8932-e6a67b59a495
STEP: Creating a pod to test consume secrets
Feb 17 18:44:56.608: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-190037e0-3c76-440f-82f0-a1beda08e46a" in namespace "projected-6549" to be "success or failure"
Feb 17 18:44:56.616: INFO: Pod "pod-projected-secrets-190037e0-3c76-440f-82f0-a1beda08e46a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.950502ms
Feb 17 18:44:58.620: INFO: Pod "pod-projected-secrets-190037e0-3c76-440f-82f0-a1beda08e46a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012404576s
STEP: Saw pod success
Feb 17 18:44:58.620: INFO: Pod "pod-projected-secrets-190037e0-3c76-440f-82f0-a1beda08e46a" satisfied condition "success or failure"
Feb 17 18:44:58.624: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-projected-secrets-190037e0-3c76-440f-82f0-a1beda08e46a container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 17 18:44:58.651: INFO: Waiting for pod pod-projected-secrets-190037e0-3c76-440f-82f0-a1beda08e46a to disappear
Feb 17 18:44:58.654: INFO: Pod pod-projected-secrets-190037e0-3c76-440f-82f0-a1beda08e46a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:44:58.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6549" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":278,"completed":183,"skipped":2991,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:44:58.668: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8565
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:45:09.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8565" for this suite.

• [SLOW TEST:11.256 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":278,"completed":184,"skipped":2997,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:45:09.923: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3410
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Feb 17 18:45:10.085: INFO: Waiting up to 5m0s for pod "var-expansion-51ed6db8-d536-40dc-98d9-12289142a50c" in namespace "var-expansion-3410" to be "success or failure"
Feb 17 18:45:10.094: INFO: Pod "var-expansion-51ed6db8-d536-40dc-98d9-12289142a50c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.087353ms
Feb 17 18:45:12.100: INFO: Pod "var-expansion-51ed6db8-d536-40dc-98d9-12289142a50c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01458329s
Feb 17 18:45:14.105: INFO: Pod "var-expansion-51ed6db8-d536-40dc-98d9-12289142a50c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020073238s
STEP: Saw pod success
Feb 17 18:45:14.105: INFO: Pod "var-expansion-51ed6db8-d536-40dc-98d9-12289142a50c" satisfied condition "success or failure"
Feb 17 18:45:14.109: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod var-expansion-51ed6db8-d536-40dc-98d9-12289142a50c container dapi-container: <nil>
STEP: delete the pod
Feb 17 18:45:14.136: INFO: Waiting for pod var-expansion-51ed6db8-d536-40dc-98d9-12289142a50c to disappear
Feb 17 18:45:14.139: INFO: Pod var-expansion-51ed6db8-d536-40dc-98d9-12289142a50c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:45:14.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3410" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":278,"completed":185,"skipped":3002,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:45:14.153: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 17 18:45:14.330: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4974 /api/v1/namespaces/watch-4974/configmaps/e2e-watch-test-watch-closed f4096062-fe63-4aba-9d0e-6af18c35e492 22094 0 2020-02-17 18:45:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 17 18:45:14.330: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4974 /api/v1/namespaces/watch-4974/configmaps/e2e-watch-test-watch-closed f4096062-fe63-4aba-9d0e-6af18c35e492 22095 0 2020-02-17 18:45:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 17 18:45:14.353: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4974 /api/v1/namespaces/watch-4974/configmaps/e2e-watch-test-watch-closed f4096062-fe63-4aba-9d0e-6af18c35e492 22096 0 2020-02-17 18:45:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 17 18:45:14.353: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4974 /api/v1/namespaces/watch-4974/configmaps/e2e-watch-test-watch-closed f4096062-fe63-4aba-9d0e-6af18c35e492 22097 0 2020-02-17 18:45:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:45:14.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4974" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":278,"completed":186,"skipped":3006,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:45:14.367: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 17 18:45:17.557: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:45:17.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6504" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":278,"completed":187,"skipped":3015,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:45:17.599: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4830
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:45:17.761: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21491919-71ba-4bd0-ac7e-fbb6e8a04ab7" in namespace "projected-4830" to be "success or failure"
Feb 17 18:45:17.766: INFO: Pod "downwardapi-volume-21491919-71ba-4bd0-ac7e-fbb6e8a04ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.927163ms
Feb 17 18:45:19.770: INFO: Pod "downwardapi-volume-21491919-71ba-4bd0-ac7e-fbb6e8a04ab7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009592444s
Feb 17 18:45:21.776: INFO: Pod "downwardapi-volume-21491919-71ba-4bd0-ac7e-fbb6e8a04ab7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01534192s
STEP: Saw pod success
Feb 17 18:45:21.776: INFO: Pod "downwardapi-volume-21491919-71ba-4bd0-ac7e-fbb6e8a04ab7" satisfied condition "success or failure"
Feb 17 18:45:21.780: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downwardapi-volume-21491919-71ba-4bd0-ac7e-fbb6e8a04ab7 container client-container: <nil>
STEP: delete the pod
Feb 17 18:45:21.808: INFO: Waiting for pod downwardapi-volume-21491919-71ba-4bd0-ac7e-fbb6e8a04ab7 to disappear
Feb 17 18:45:21.813: INFO: Pod downwardapi-volume-21491919-71ba-4bd0-ac7e-fbb6e8a04ab7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:45:21.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4830" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":278,"completed":188,"skipped":3030,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:45:21.826: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9911
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Feb 17 18:45:22.000: INFO: Waiting up to 5m0s for pod "client-containers-354c4b64-22bf-482b-8595-448e9dcf6c9c" in namespace "containers-9911" to be "success or failure"
Feb 17 18:45:22.010: INFO: Pod "client-containers-354c4b64-22bf-482b-8595-448e9dcf6c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.462945ms
Feb 17 18:45:24.018: INFO: Pod "client-containers-354c4b64-22bf-482b-8595-448e9dcf6c9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01787444s
Feb 17 18:45:26.023: INFO: Pod "client-containers-354c4b64-22bf-482b-8595-448e9dcf6c9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022757269s
STEP: Saw pod success
Feb 17 18:45:26.023: INFO: Pod "client-containers-354c4b64-22bf-482b-8595-448e9dcf6c9c" satisfied condition "success or failure"
Feb 17 18:45:26.028: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod client-containers-354c4b64-22bf-482b-8595-448e9dcf6c9c container test-container: <nil>
STEP: delete the pod
Feb 17 18:45:26.051: INFO: Waiting for pod client-containers-354c4b64-22bf-482b-8595-448e9dcf6c9c to disappear
Feb 17 18:45:26.056: INFO: Pod client-containers-354c4b64-22bf-482b-8595-448e9dcf6c9c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:45:26.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9911" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":278,"completed":189,"skipped":3069,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:45:26.070: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2659
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:45:26.265: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"29d9bed5-3071-4796-a615-bc592f8757ab", Controller:(*bool)(0xc002ffa076), BlockOwnerDeletion:(*bool)(0xc002ffa077)}}
Feb 17 18:45:26.276: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"671821ed-977f-4df9-b110-b4fe6bfe48b7", Controller:(*bool)(0xc002cf08ae), BlockOwnerDeletion:(*bool)(0xc002cf08af)}}
Feb 17 18:45:26.297: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"9c628ad3-5db6-419a-a3e3-620b453107f4", Controller:(*bool)(0xc002116a6e), BlockOwnerDeletion:(*bool)(0xc002116a6f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:45:31.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2659" for this suite.

• [SLOW TEST:5.284 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":278,"completed":190,"skipped":3072,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:45:31.354: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-6543
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Feb 17 18:45:31.520: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6543" to be "success or failure"
Feb 17 18:45:31.525: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.518865ms
Feb 17 18:45:33.530: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009511535s
Feb 17 18:45:35.536: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015542838s
STEP: Saw pod success
Feb 17 18:45:35.536: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 17 18:45:35.540: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 17 18:45:35.568: INFO: Waiting for pod pod-host-path-test to disappear
Feb 17 18:45:35.572: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:45:35.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6543" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":191,"skipped":3085,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:45:35.586: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5752
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Feb 17 18:45:35.737: INFO: namespace kubectl-5752
Feb 17 18:45:35.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 create -f - --namespace=kubectl-5752'
Feb 17 18:45:35.947: INFO: stderr: ""
Feb 17 18:45:35.947: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Feb 17 18:45:36.952: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 18:45:36.952: INFO: Found 0 / 1
Feb 17 18:45:37.955: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 18:45:37.955: INFO: Found 1 / 1
Feb 17 18:45:37.955: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 17 18:45:37.959: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 18:45:37.959: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 17 18:45:37.959: INFO: wait on agnhost-master startup in kubectl-5752 
Feb 17 18:45:37.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 logs agnhost-master-qpkww agnhost-master --namespace=kubectl-5752'
Feb 17 18:45:38.044: INFO: stderr: ""
Feb 17 18:45:38.044: INFO: stdout: "Paused\n"
STEP: exposing RC
Feb 17 18:45:38.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-5752'
Feb 17 18:45:38.143: INFO: stderr: ""
Feb 17 18:45:38.143: INFO: stdout: "service/rm2 exposed\n"
Feb 17 18:45:38.149: INFO: Service rm2 in namespace kubectl-5752 found.
STEP: exposing service
Feb 17 18:45:40.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-5752'
Feb 17 18:45:40.260: INFO: stderr: ""
Feb 17 18:45:40.260: INFO: stdout: "service/rm3 exposed\n"
Feb 17 18:45:40.266: INFO: Service rm3 in namespace kubectl-5752 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:45:42.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5752" for this suite.

• [SLOW TEST:6.702 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1295
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":278,"completed":192,"skipped":3120,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:45:42.289: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1750
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 17 18:45:42.456: INFO: Waiting up to 5m0s for pod "pod-40a12d97-47ce-4add-bf54-67f694d4c4ac" in namespace "emptydir-1750" to be "success or failure"
Feb 17 18:45:42.461: INFO: Pod "pod-40a12d97-47ce-4add-bf54-67f694d4c4ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.878233ms
Feb 17 18:45:44.466: INFO: Pod "pod-40a12d97-47ce-4add-bf54-67f694d4c4ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009670685s
STEP: Saw pod success
Feb 17 18:45:44.466: INFO: Pod "pod-40a12d97-47ce-4add-bf54-67f694d4c4ac" satisfied condition "success or failure"
Feb 17 18:45:44.469: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod pod-40a12d97-47ce-4add-bf54-67f694d4c4ac container test-container: <nil>
STEP: delete the pod
Feb 17 18:45:44.524: INFO: Waiting for pod pod-40a12d97-47ce-4add-bf54-67f694d4c4ac to disappear
Feb 17 18:45:44.528: INFO: Pod pod-40a12d97-47ce-4add-bf54-67f694d4c4ac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:45:44.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1750" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":193,"skipped":3121,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:45:44.543: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3693
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3693.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3693.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3693.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3693.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3693.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3693.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 18:45:48.777: INFO: DNS probes using dns-3693/dns-test-f1a0187b-3530-4682-9270-4557d6bcf897 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:45:48.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3693" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":278,"completed":194,"skipped":3123,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:45:48.848: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6560
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7693
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6160
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:45:55.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6560" for this suite.
STEP: Destroying namespace "nsdeletetest-7693" for this suite.
Feb 17 18:45:55.358: INFO: Namespace nsdeletetest-7693 was already deleted
STEP: Destroying namespace "nsdeletetest-6160" for this suite.

• [SLOW TEST:6.517 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":278,"completed":195,"skipped":3129,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:45:55.366: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9415
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1464
STEP: creating an pod
Feb 17 18:45:55.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-9415 -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 17 18:45:55.594: INFO: stderr: ""
Feb 17 18:45:55.594: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Feb 17 18:45:55.594: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 17 18:45:55.594: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9415" to be "running and ready, or succeeded"
Feb 17 18:45:55.602: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.838912ms
Feb 17 18:45:57.608: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013887755s
Feb 17 18:45:59.613: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.018958364s
Feb 17 18:45:59.613: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 17 18:45:59.613: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Feb 17 18:45:59.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 logs logs-generator logs-generator --namespace=kubectl-9415'
Feb 17 18:45:59.690: INFO: stderr: ""
Feb 17 18:45:59.690: INFO: stdout: "I0217 18:45:57.077876       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/br5 281\nI0217 18:45:57.278049       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/f8v5 442\nI0217 18:45:57.478087       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/rvjb 366\nI0217 18:45:57.678006       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/6s2 590\nI0217 18:45:57.878103       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/sw2 326\nI0217 18:45:58.078099       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/xsfq 295\nI0217 18:45:58.278070       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/t2q 396\nI0217 18:45:58.478066       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/ckh 423\nI0217 18:45:58.678075       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/4xj 392\nI0217 18:45:58.878088       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/sv7 543\nI0217 18:45:59.077996       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/s4xq 434\nI0217 18:45:59.278119       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/89vd 517\nI0217 18:45:59.478069       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/5mf5 355\nI0217 18:45:59.678059       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/wgcl 206\n"
STEP: limiting log lines
Feb 17 18:45:59.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 logs logs-generator logs-generator --namespace=kubectl-9415 --tail=1'
Feb 17 18:45:59.767: INFO: stderr: ""
Feb 17 18:45:59.767: INFO: stdout: "I0217 18:45:59.678059       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/wgcl 206\n"
Feb 17 18:45:59.767: INFO: got output "I0217 18:45:59.678059       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/wgcl 206\n"
STEP: limiting log bytes
Feb 17 18:45:59.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 logs logs-generator logs-generator --namespace=kubectl-9415 --limit-bytes=1'
Feb 17 18:45:59.844: INFO: stderr: ""
Feb 17 18:45:59.844: INFO: stdout: "I"
Feb 17 18:45:59.844: INFO: got output "I"
STEP: exposing timestamps
Feb 17 18:45:59.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 logs logs-generator logs-generator --namespace=kubectl-9415 --tail=1 --timestamps'
Feb 17 18:45:59.931: INFO: stderr: ""
Feb 17 18:45:59.931: INFO: stdout: "2020-02-17T18:45:59.878212084Z I0217 18:45:59.878099       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/j2dm 593\n"
Feb 17 18:45:59.931: INFO: got output "2020-02-17T18:45:59.878212084Z I0217 18:45:59.878099       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/j2dm 593\n"
STEP: restricting to a time range
Feb 17 18:46:02.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 logs logs-generator logs-generator --namespace=kubectl-9415 --since=1s'
Feb 17 18:46:02.512: INFO: stderr: ""
Feb 17 18:46:02.512: INFO: stdout: "I0217 18:46:01.680543       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/lnz 493\nI0217 18:46:01.878106       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/tl8s 282\nI0217 18:46:02.078070       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/dzm 480\nI0217 18:46:02.278124       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/ztbv 549\nI0217 18:46:02.478069       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/78l8 385\n"
Feb 17 18:46:02.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 logs logs-generator logs-generator --namespace=kubectl-9415 --since=24h'
Feb 17 18:46:02.591: INFO: stderr: ""
Feb 17 18:46:02.591: INFO: stdout: "I0217 18:45:57.077876       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/br5 281\nI0217 18:45:57.278049       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/f8v5 442\nI0217 18:45:57.478087       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/rvjb 366\nI0217 18:45:57.678006       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/6s2 590\nI0217 18:45:57.878103       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/sw2 326\nI0217 18:45:58.078099       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/xsfq 295\nI0217 18:45:58.278070       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/t2q 396\nI0217 18:45:58.478066       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/ckh 423\nI0217 18:45:58.678075       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/4xj 392\nI0217 18:45:58.878088       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/sv7 543\nI0217 18:45:59.077996       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/s4xq 434\nI0217 18:45:59.278119       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/89vd 517\nI0217 18:45:59.478069       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/5mf5 355\nI0217 18:45:59.678059       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/wgcl 206\nI0217 18:45:59.878099       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/j2dm 593\nI0217 18:46:00.078096       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/jmxc 407\nI0217 18:46:00.278126       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/246 457\nI0217 18:46:00.478077       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/m2x 543\nI0217 18:46:00.678065       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/cjp2 459\nI0217 18:46:00.878100       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/ktt 341\nI0217 18:46:01.078104       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/4692 494\nI0217 18:46:01.278059       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/7r4d 429\nI0217 18:46:01.478078       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/55x 318\nI0217 18:46:01.680543       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/lnz 493\nI0217 18:46:01.878106       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/tl8s 282\nI0217 18:46:02.078070       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/dzm 480\nI0217 18:46:02.278124       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/ztbv 549\nI0217 18:46:02.478069       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/78l8 385\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1470
Feb 17 18:46:02.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete pod logs-generator --namespace=kubectl-9415'
Feb 17 18:46:07.698: INFO: stderr: ""
Feb 17 18:46:07.698: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:46:07.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9415" for this suite.

• [SLOW TEST:12.348 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":278,"completed":196,"skipped":3140,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:46:07.714: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
STEP: creating the pod
Feb 17 18:46:07.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 create -f - --namespace=kubectl-3166'
Feb 17 18:46:08.017: INFO: stderr: ""
Feb 17 18:46:08.017: INFO: stdout: "pod/pause created\n"
Feb 17 18:46:08.017: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 17 18:46:08.017: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3166" to be "running and ready"
Feb 17 18:46:08.024: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.935107ms
Feb 17 18:46:10.029: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.011701052s
Feb 17 18:46:10.029: INFO: Pod "pause" satisfied condition "running and ready"
Feb 17 18:46:10.029: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 17 18:46:10.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 label pods pause testing-label=testing-label-value --namespace=kubectl-3166'
Feb 17 18:46:10.102: INFO: stderr: ""
Feb 17 18:46:10.102: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 17 18:46:10.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pod pause -L testing-label --namespace=kubectl-3166'
Feb 17 18:46:10.165: INFO: stderr: ""
Feb 17 18:46:10.165: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 17 18:46:10.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 label pods pause testing-label- --namespace=kubectl-3166'
Feb 17 18:46:10.246: INFO: stderr: ""
Feb 17 18:46:10.246: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 17 18:46:10.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pod pause -L testing-label --namespace=kubectl-3166'
Feb 17 18:46:10.311: INFO: stderr: ""
Feb 17 18:46:10.311: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
STEP: using delete to clean up resources
Feb 17 18:46:10.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete --grace-period=0 --force -f - --namespace=kubectl-3166'
Feb 17 18:46:10.385: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 18:46:10.385: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 17 18:46:10.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get rc,svc -l name=pause --no-headers --namespace=kubectl-3166'
Feb 17 18:46:10.456: INFO: stderr: "No resources found in kubectl-3166 namespace.\n"
Feb 17 18:46:10.457: INFO: stdout: ""
Feb 17 18:46:10.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -l name=pause --namespace=kubectl-3166 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 17 18:46:10.520: INFO: stderr: ""
Feb 17 18:46:10.520: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:46:10.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3166" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":278,"completed":197,"skipped":3153,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:46:10.536: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2581
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:46:10.700: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e8edaa0a-72cf-4357-89a9-1afc981af642" in namespace "downward-api-2581" to be "success or failure"
Feb 17 18:46:10.704: INFO: Pod "downwardapi-volume-e8edaa0a-72cf-4357-89a9-1afc981af642": Phase="Pending", Reason="", readiness=false. Elapsed: 3.679742ms
Feb 17 18:46:12.709: INFO: Pod "downwardapi-volume-e8edaa0a-72cf-4357-89a9-1afc981af642": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009019803s
Feb 17 18:46:14.715: INFO: Pod "downwardapi-volume-e8edaa0a-72cf-4357-89a9-1afc981af642": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014860229s
STEP: Saw pod success
Feb 17 18:46:14.715: INFO: Pod "downwardapi-volume-e8edaa0a-72cf-4357-89a9-1afc981af642" satisfied condition "success or failure"
Feb 17 18:46:14.720: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downwardapi-volume-e8edaa0a-72cf-4357-89a9-1afc981af642 container client-container: <nil>
STEP: delete the pod
Feb 17 18:46:14.747: INFO: Waiting for pod downwardapi-volume-e8edaa0a-72cf-4357-89a9-1afc981af642 to disappear
Feb 17 18:46:14.750: INFO: Pod downwardapi-volume-e8edaa0a-72cf-4357-89a9-1afc981af642 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:46:14.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2581" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":198,"skipped":3160,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:46:14.766: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1257
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 17 18:46:14.918: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 17 18:46:14.935: INFO: Waiting for terminating namespaces to be deleted...
Feb 17 18:46:14.939: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-58-212.ec2.internal before test
Feb 17 18:46:14.947: INFO: kublr-node-name-reporter-2e4a883ef03ca181a5002964babb2c122cef2ef3f405e26061a11af75a5d837f-ip-172-16-58-212.ec2.internal from kube-system started at 2020-02-17 17:52:08 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.947: INFO: 	Container main ready: true, restart count 0
Feb 17 18:46:14.947: INFO: coredns-6c578c8bf7-88w67 from kube-system started at 2020-02-17 17:53:27 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.947: INFO: 	Container coredns ready: true, restart count 0
Feb 17 18:46:14.947: INFO: k8s-api-haproxy-b9ba254b202c3ae80f0c096eb09e3391b752ed191356ffd2fb26712eff44ce1b-ip-172-16-58-212.ec2.internal from kube-system started at 2020-02-17 17:52:08 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.947: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Feb 17 18:46:14.947: INFO: kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-58-212.ec2.internal from kube-system started at 2020-02-17 17:52:08 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.947: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:46:14.947: INFO: canal-v8cfc from kube-system started at 2020-02-17 17:53:10 +0000 UTC (4 container statuses recorded)
Feb 17 18:46:14.947: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:46:14.947: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 17 18:46:14.947: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Feb 17 18:46:14.947: INFO: 	Container update-network-condition ready: true, restart count 0
Feb 17 18:46:14.947: INFO: node-local-dns-rvfkr from kube-system started at 2020-02-17 17:53:20 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.947: INFO: 	Container node-cache ready: true, restart count 0
Feb 17 18:46:14.947: INFO: sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-zlh59 from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:46:14.947: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:46:14.947: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:46:14.947: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-71-125.ec2.internal before test
Feb 17 18:46:14.965: INFO: kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-71-125.ec2.internal from kube-system started at 2020-02-17 17:51:59 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.965: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:46:14.965: INFO: kublr-node-name-reporter-3cd39b06f3a526166a2bdbb7d0aa396f06e89576f16de214708f3a78d146fdd2-ip-172-16-71-125.ec2.internal from kube-system started at 2020-02-17 17:51:59 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.965: INFO: 	Container main ready: true, restart count 0
Feb 17 18:46:14.965: INFO: k8s-api-haproxy-92dea82227153ce8905e2adce84945a2a46a1ff902bbfb5846f597d0315ec0e1-ip-172-16-71-125.ec2.internal from kube-system started at 2020-02-17 17:51:59 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.965: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Feb 17 18:46:14.965: INFO: node-local-dns-b4j9f from kube-system started at 2020-02-17 17:53:19 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.965: INFO: 	Container node-cache ready: true, restart count 0
Feb 17 18:46:14.965: INFO: sonobuoy from sonobuoy started at 2020-02-17 18:03:16 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.965: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 17 18:46:14.965: INFO: canal-9jpnm from kube-system started at 2020-02-17 17:53:09 +0000 UTC (4 container statuses recorded)
Feb 17 18:46:14.965: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:46:14.965: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 17 18:46:14.965: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Feb 17 18:46:14.965: INFO: 	Container update-network-condition ready: true, restart count 0
Feb 17 18:46:14.965: INFO: kublr-system-shell-5b9bd5c865-lmlsp from kube-system started at 2020-02-17 17:54:02 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.965: INFO: 	Container shell ready: true, restart count 0
Feb 17 18:46:14.965: INFO: sonobuoy-e2e-job-20e37349fda64c29 from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:46:14.965: INFO: 	Container e2e ready: true, restart count 0
Feb 17 18:46:14.965: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:46:14.965: INFO: sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-d99zx from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:46:14.965: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:46:14.965: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:46:14.965: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-86-181.ec2.internal before test
Feb 17 18:46:14.972: INFO: kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-86-181.ec2.internal from kube-system started at 2020-02-17 17:51:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.972: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:46:14.972: INFO: kublr-node-name-reporter-d9895b0065d8993619f85e1df198fa878363f17b2ecacbc683c208bcec5934f7-ip-172-16-86-181.ec2.internal from kube-system started at 2020-02-17 17:51:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.972: INFO: 	Container main ready: true, restart count 0
Feb 17 18:46:14.972: INFO: coredns-6c578c8bf7-bwb2d from kube-system started at 2020-02-17 17:53:34 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.972: INFO: 	Container coredns ready: true, restart count 0
Feb 17 18:46:14.972: INFO: metrics-server-v0.3.6-7b66f9c8dc-n74zk from kube-system started at 2020-02-17 17:53:36 +0000 UTC (2 container statuses recorded)
Feb 17 18:46:14.972: INFO: 	Container metrics-server ready: true, restart count 0
Feb 17 18:46:14.972: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 17 18:46:14.972: INFO: k8s-api-haproxy-b3a73b75b08bcd3ee17799fee24d51365c32935f7350cdcde3e2adb2935e8f65-ip-172-16-86-181.ec2.internal from kube-system started at 2020-02-17 17:51:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.972: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Feb 17 18:46:14.972: INFO: canal-ng4fk from kube-system started at 2020-02-17 17:53:14 +0000 UTC (4 container statuses recorded)
Feb 17 18:46:14.972: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:46:14.972: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 17 18:46:14.972: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Feb 17 18:46:14.972: INFO: 	Container update-network-condition ready: true, restart count 0
Feb 17 18:46:14.972: INFO: node-local-dns-7prxg from kube-system started at 2020-02-17 17:53:34 +0000 UTC (1 container statuses recorded)
Feb 17 18:46:14.972: INFO: 	Container node-cache ready: true, restart count 0
Feb 17 18:46:14.972: INFO: sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-q7qcp from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:46:14.972: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:46:14.972: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f444f1f20d0794], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:46:16.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1257" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":278,"completed":199,"skipped":3166,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:46:16.033: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Feb 17 18:46:16.189: INFO: Waiting up to 5m0s for pod "downward-api-4c416240-7607-4356-950d-c9226cb9c3f9" in namespace "downward-api-7259" to be "success or failure"
Feb 17 18:46:16.195: INFO: Pod "downward-api-4c416240-7607-4356-950d-c9226cb9c3f9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016436ms
Feb 17 18:46:18.202: INFO: Pod "downward-api-4c416240-7607-4356-950d-c9226cb9c3f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012846323s
Feb 17 18:46:20.207: INFO: Pod "downward-api-4c416240-7607-4356-950d-c9226cb9c3f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018016267s
STEP: Saw pod success
Feb 17 18:46:20.207: INFO: Pod "downward-api-4c416240-7607-4356-950d-c9226cb9c3f9" satisfied condition "success or failure"
Feb 17 18:46:20.211: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downward-api-4c416240-7607-4356-950d-c9226cb9c3f9 container dapi-container: <nil>
STEP: delete the pod
Feb 17 18:46:20.247: INFO: Waiting for pod downward-api-4c416240-7607-4356-950d-c9226cb9c3f9 to disappear
Feb 17 18:46:20.250: INFO: Pod downward-api-4c416240-7607-4356-950d-c9226cb9c3f9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:46:20.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7259" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":278,"completed":200,"skipped":3237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:46:20.264: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4393
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Feb 17 18:46:20.429: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:46:23.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4393" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":278,"completed":201,"skipped":3267,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:46:23.495: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-34
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Feb 17 18:46:23.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 cluster-info'
Feb 17 18:46:23.721: INFO: stderr: ""
Feb 17 18:46:23.721: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mmetrics-server\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:46:23.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-34" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":278,"completed":202,"skipped":3286,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:46:23.736: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-70
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-70
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-70
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-70
Feb 17 18:46:23.895: INFO: Found 0 stateful pods, waiting for 1
Feb 17 18:46:33.901: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 17 18:46:33.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 18:46:34.100: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 18:46:34.100: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 18:46:34.100: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 18:46:34.105: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 17 18:46:44.110: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 18:46:44.110: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 18:46:44.128: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Feb 17 18:46:44.128: INFO: ss-0  ip-172-16-58-212.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:34 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  }]
Feb 17 18:46:44.128: INFO: 
Feb 17 18:46:44.128: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 17 18:46:45.137: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994931233s
Feb 17 18:46:46.142: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985650541s
Feb 17 18:46:47.149: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980311096s
Feb 17 18:46:48.154: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974208574s
Feb 17 18:46:49.158: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969234215s
Feb 17 18:46:50.165: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964435247s
Feb 17 18:46:51.170: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.95829407s
Feb 17 18:46:52.175: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.95301629s
Feb 17 18:46:53.181: INFO: Verifying statefulset ss doesn't scale past 3 for another 948.146976ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-70
Feb 17 18:46:54.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:46:54.394: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 18:46:54.394: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 18:46:54.394: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 18:46:54.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:46:54.595: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 17 18:46:54.595: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 18:46:54.595: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 18:46:54.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:46:54.801: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 17 18:46:54.801: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 18:46:54.801: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 18:46:54.806: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 18:46:54.806: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 18:46:54.806: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 17 18:46:54.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 18:46:55.013: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 18:46:55.013: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 18:46:55.013: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 18:46:55.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 18:46:55.216: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 18:46:55.216: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 18:46:55.216: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 18:46:55.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 18:46:55.433: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 18:46:55.433: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 18:46:55.433: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 18:46:55.433: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 18:46:55.439: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 17 18:47:05.449: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 18:47:05.449: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 18:47:05.449: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 18:47:05.465: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Feb 17 18:47:05.465: INFO: ss-0  ip-172-16-58-212.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  }]
Feb 17 18:47:05.465: INFO: ss-1  ip-172-16-86-181.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  }]
Feb 17 18:47:05.465: INFO: ss-2  ip-172-16-71-125.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  }]
Feb 17 18:47:05.465: INFO: 
Feb 17 18:47:05.465: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 17 18:47:06.469: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Feb 17 18:47:06.469: INFO: ss-0  ip-172-16-58-212.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  }]
Feb 17 18:47:06.470: INFO: ss-1  ip-172-16-86-181.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  }]
Feb 17 18:47:06.470: INFO: ss-2  ip-172-16-71-125.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  }]
Feb 17 18:47:06.470: INFO: 
Feb 17 18:47:06.470: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 17 18:47:07.481: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Feb 17 18:47:07.481: INFO: ss-0  ip-172-16-58-212.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  }]
Feb 17 18:47:07.481: INFO: ss-1  ip-172-16-86-181.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  }]
Feb 17 18:47:07.481: INFO: ss-2  ip-172-16-71-125.ec2.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  }]
Feb 17 18:47:07.481: INFO: 
Feb 17 18:47:07.481: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 17 18:47:08.486: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Feb 17 18:47:08.486: INFO: ss-0  ip-172-16-58-212.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  }]
Feb 17 18:47:08.486: INFO: ss-1  ip-172-16-86-181.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  }]
Feb 17 18:47:08.486: INFO: 
Feb 17 18:47:08.486: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 17 18:47:09.491: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Feb 17 18:47:09.491: INFO: ss-0  ip-172-16-58-212.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  }]
Feb 17 18:47:09.491: INFO: ss-1  ip-172-16-86-181.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:44 +0000 UTC  }]
Feb 17 18:47:09.491: INFO: 
Feb 17 18:47:09.491: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 17 18:47:10.499: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Feb 17 18:47:10.499: INFO: ss-0  ip-172-16-58-212.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  }]
Feb 17 18:47:10.499: INFO: 
Feb 17 18:47:10.499: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 17 18:47:11.504: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Feb 17 18:47:11.504: INFO: ss-0  ip-172-16-58-212.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  }]
Feb 17 18:47:11.504: INFO: 
Feb 17 18:47:11.504: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 17 18:47:12.509: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Feb 17 18:47:12.509: INFO: ss-0  ip-172-16-58-212.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  }]
Feb 17 18:47:12.509: INFO: 
Feb 17 18:47:12.509: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 17 18:47:13.516: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Feb 17 18:47:13.516: INFO: ss-0  ip-172-16-58-212.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  }]
Feb 17 18:47:13.516: INFO: 
Feb 17 18:47:13.516: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 17 18:47:14.521: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Feb 17 18:47:14.521: INFO: ss-0  ip-172-16-58-212.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 18:46:23 +0000 UTC  }]
Feb 17 18:47:14.521: INFO: 
Feb 17 18:47:14.521: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-70
Feb 17 18:47:15.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:47:15.629: INFO: rc: 1
Feb 17 18:47:15.629: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Feb 17 18:47:25.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:47:25.698: INFO: rc: 1
Feb 17 18:47:25.698: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:47:35.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:47:35.766: INFO: rc: 1
Feb 17 18:47:35.766: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:47:45.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:47:45.842: INFO: rc: 1
Feb 17 18:47:45.842: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:47:55.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:47:55.911: INFO: rc: 1
Feb 17 18:47:55.911: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:48:05.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:48:05.986: INFO: rc: 1
Feb 17 18:48:05.986: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:48:15.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:48:16.056: INFO: rc: 1
Feb 17 18:48:16.056: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:48:26.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:48:26.125: INFO: rc: 1
Feb 17 18:48:26.125: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:48:36.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:48:36.196: INFO: rc: 1
Feb 17 18:48:36.196: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:48:46.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:48:46.265: INFO: rc: 1
Feb 17 18:48:46.265: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:48:56.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:48:56.333: INFO: rc: 1
Feb 17 18:48:56.333: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:49:06.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:49:06.406: INFO: rc: 1
Feb 17 18:49:06.406: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:49:16.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:49:16.474: INFO: rc: 1
Feb 17 18:49:16.474: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:49:26.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:49:26.549: INFO: rc: 1
Feb 17 18:49:26.549: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:49:36.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:49:37.360: INFO: rc: 1
Feb 17 18:49:37.360: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:49:47.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:49:47.428: INFO: rc: 1
Feb 17 18:49:47.428: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:49:57.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:49:57.501: INFO: rc: 1
Feb 17 18:49:57.501: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:50:07.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:50:07.570: INFO: rc: 1
Feb 17 18:50:07.570: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:50:17.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:50:17.639: INFO: rc: 1
Feb 17 18:50:17.639: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:50:27.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:50:27.708: INFO: rc: 1
Feb 17 18:50:27.708: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:50:37.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:50:37.776: INFO: rc: 1
Feb 17 18:50:37.776: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:50:47.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:50:47.845: INFO: rc: 1
Feb 17 18:50:47.845: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:50:57.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:50:57.915: INFO: rc: 1
Feb 17 18:50:57.915: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:51:07.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:51:07.981: INFO: rc: 1
Feb 17 18:51:07.981: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:51:17.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:51:18.051: INFO: rc: 1
Feb 17 18:51:18.051: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:51:28.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:51:28.120: INFO: rc: 1
Feb 17 18:51:28.120: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:51:38.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:51:38.188: INFO: rc: 1
Feb 17 18:51:38.188: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:51:48.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:51:48.259: INFO: rc: 1
Feb 17 18:51:48.259: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:51:58.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:51:58.327: INFO: rc: 1
Feb 17 18:51:58.327: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:52:08.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:52:08.395: INFO: rc: 1
Feb 17 18:52:08.395: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Feb 17 18:52:18.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 exec --namespace=statefulset-70 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 18:52:18.466: INFO: rc: 1
Feb 17 18:52:18.466: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Feb 17 18:52:18.466: INFO: Scaling statefulset ss to 0
Feb 17 18:52:18.481: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 17 18:52:18.485: INFO: Deleting all statefulset in ns statefulset-70
Feb 17 18:52:18.489: INFO: Scaling statefulset ss to 0
Feb 17 18:52:18.501: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 18:52:18.504: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:52:18.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-70" for this suite.

• [SLOW TEST:354.812 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":278,"completed":203,"skipped":3307,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:52:18.548: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5841
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:52:19.044: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:52:21.058: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562339, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562339, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562339, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562339, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:52:24.084: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:52:25.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5841" for this suite.
STEP: Destroying namespace "webhook-5841-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.693 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":278,"completed":204,"skipped":3333,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:52:25.241: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-9335fdb5-928d-4f0f-b343-53cb3e91174f
STEP: Creating a pod to test consume configMaps
Feb 17 18:52:25.407: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fa85e9a8-c33e-470f-88a3-9e4c540a058d" in namespace "projected-9584" to be "success or failure"
Feb 17 18:52:25.414: INFO: Pod "pod-projected-configmaps-fa85e9a8-c33e-470f-88a3-9e4c540a058d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.758153ms
Feb 17 18:52:27.418: INFO: Pod "pod-projected-configmaps-fa85e9a8-c33e-470f-88a3-9e4c540a058d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011334434s
Feb 17 18:52:29.423: INFO: Pod "pod-projected-configmaps-fa85e9a8-c33e-470f-88a3-9e4c540a058d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016579425s
STEP: Saw pod success
Feb 17 18:52:29.423: INFO: Pod "pod-projected-configmaps-fa85e9a8-c33e-470f-88a3-9e4c540a058d" satisfied condition "success or failure"
Feb 17 18:52:29.428: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-projected-configmaps-fa85e9a8-c33e-470f-88a3-9e4c540a058d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:52:29.465: INFO: Waiting for pod pod-projected-configmaps-fa85e9a8-c33e-470f-88a3-9e4c540a058d to disappear
Feb 17 18:52:29.470: INFO: Pod pod-projected-configmaps-fa85e9a8-c33e-470f-88a3-9e4c540a058d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:52:29.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9584" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":205,"skipped":3359,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:52:29.484: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 17 18:52:29.677: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:29.677: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:29.677: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:29.680: INFO: Number of nodes with available pods: 0
Feb 17 18:52:29.680: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:52:30.689: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:30.689: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:30.689: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:30.693: INFO: Number of nodes with available pods: 0
Feb 17 18:52:30.693: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:52:31.687: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:31.687: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:31.687: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:31.691: INFO: Number of nodes with available pods: 1
Feb 17 18:52:31.691: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:52:32.688: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:32.688: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:32.688: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:32.693: INFO: Number of nodes with available pods: 3
Feb 17 18:52:32.693: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 17 18:52:32.721: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:32.721: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:32.721: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:32.726: INFO: Number of nodes with available pods: 2
Feb 17 18:52:32.726: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:52:33.733: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:33.733: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:33.733: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:33.737: INFO: Number of nodes with available pods: 2
Feb 17 18:52:33.737: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:52:34.733: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:34.733: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:34.733: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:34.738: INFO: Number of nodes with available pods: 2
Feb 17 18:52:34.738: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 18:52:35.733: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:35.733: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:35.733: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 18:52:35.737: INFO: Number of nodes with available pods: 3
Feb 17 18:52:35.737: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3141, will wait for the garbage collector to delete the pods
Feb 17 18:52:35.810: INFO: Deleting DaemonSet.extensions daemon-set took: 11.16853ms
Feb 17 18:52:35.910: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.260284ms
Feb 17 18:52:47.715: INFO: Number of nodes with available pods: 0
Feb 17 18:52:47.715: INFO: Number of running nodes: 0, number of available pods: 0
Feb 17 18:52:47.719: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3141/daemonsets","resourceVersion":"24487"},"items":null}

Feb 17 18:52:47.729: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3141/pods","resourceVersion":"24487"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:52:47.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3141" for this suite.

• [SLOW TEST:18.277 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":278,"completed":206,"skipped":3360,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:52:47.763: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-793
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-632bf33e-85e6-4d6d-b48e-6bd67667d853
STEP: Creating a pod to test consume configMaps
Feb 17 18:52:47.927: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1c36f8b5-67a9-42ef-bb02-67cbc44cabf6" in namespace "projected-793" to be "success or failure"
Feb 17 18:52:47.934: INFO: Pod "pod-projected-configmaps-1c36f8b5-67a9-42ef-bb02-67cbc44cabf6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.884587ms
Feb 17 18:52:49.939: INFO: Pod "pod-projected-configmaps-1c36f8b5-67a9-42ef-bb02-67cbc44cabf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011940627s
Feb 17 18:52:51.945: INFO: Pod "pod-projected-configmaps-1c36f8b5-67a9-42ef-bb02-67cbc44cabf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017091742s
STEP: Saw pod success
Feb 17 18:52:51.945: INFO: Pod "pod-projected-configmaps-1c36f8b5-67a9-42ef-bb02-67cbc44cabf6" satisfied condition "success or failure"
Feb 17 18:52:51.950: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-projected-configmaps-1c36f8b5-67a9-42ef-bb02-67cbc44cabf6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:52:51.975: INFO: Waiting for pod pod-projected-configmaps-1c36f8b5-67a9-42ef-bb02-67cbc44cabf6 to disappear
Feb 17 18:52:51.979: INFO: Pod pod-projected-configmaps-1c36f8b5-67a9-42ef-bb02-67cbc44cabf6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:52:51.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-793" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":278,"completed":207,"skipped":3373,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:52:51.994: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3778
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-3778
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 17 18:52:52.146: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 17 18:53:14.259: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.184:8080/dial?request=hostname&protocol=udp&host=100.96.1.183&port=8081&tries=1'] Namespace:pod-network-test-3778 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:53:14.259: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:53:14.393: INFO: Waiting for responses: map[]
Feb 17 18:53:14.398: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.184:8080/dial?request=hostname&protocol=udp&host=100.96.0.37&port=8081&tries=1'] Namespace:pod-network-test-3778 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:53:14.398: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:53:14.533: INFO: Waiting for responses: map[]
Feb 17 18:53:14.537: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.184:8080/dial?request=hostname&protocol=udp&host=100.96.5.74&port=8081&tries=1'] Namespace:pod-network-test-3778 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:53:14.537: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:53:14.664: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:53:14.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3778" for this suite.

• [SLOW TEST:22.688 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":208,"skipped":3374,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:53:14.682: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9370
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 17 18:53:14.830: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 17 18:53:14.847: INFO: Waiting for terminating namespaces to be deleted...
Feb 17 18:53:14.851: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-58-212.ec2.internal before test
Feb 17 18:53:14.859: INFO: node-local-dns-rvfkr from kube-system started at 2020-02-17 17:53:20 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.859: INFO: 	Container node-cache ready: true, restart count 0
Feb 17 18:53:14.859: INFO: sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-zlh59 from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:53:14.859: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:53:14.859: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:53:14.859: INFO: test-container-pod from pod-network-test-3778 started at 2020-02-17 18:53:10 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.859: INFO: 	Container webserver ready: true, restart count 0
Feb 17 18:53:14.859: INFO: host-test-container-pod from pod-network-test-3778 started at 2020-02-17 18:53:10 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.859: INFO: 	Container agnhost ready: true, restart count 0
Feb 17 18:53:14.859: INFO: k8s-api-haproxy-b9ba254b202c3ae80f0c096eb09e3391b752ed191356ffd2fb26712eff44ce1b-ip-172-16-58-212.ec2.internal from kube-system started at 2020-02-17 17:52:08 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.859: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Feb 17 18:53:14.859: INFO: kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-58-212.ec2.internal from kube-system started at 2020-02-17 17:52:08 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.859: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:53:14.859: INFO: canal-v8cfc from kube-system started at 2020-02-17 17:53:10 +0000 UTC (4 container statuses recorded)
Feb 17 18:53:14.859: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:53:14.859: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 17 18:53:14.859: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Feb 17 18:53:14.859: INFO: 	Container update-network-condition ready: true, restart count 0
Feb 17 18:53:14.859: INFO: kublr-node-name-reporter-2e4a883ef03ca181a5002964babb2c122cef2ef3f405e26061a11af75a5d837f-ip-172-16-58-212.ec2.internal from kube-system started at 2020-02-17 17:52:08 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.859: INFO: 	Container main ready: true, restart count 0
Feb 17 18:53:14.859: INFO: coredns-6c578c8bf7-88w67 from kube-system started at 2020-02-17 17:53:27 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.859: INFO: 	Container coredns ready: true, restart count 0
Feb 17 18:53:14.859: INFO: netserver-0 from pod-network-test-3778 started at 2020-02-17 18:52:52 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.859: INFO: 	Container webserver ready: true, restart count 0
Feb 17 18:53:14.859: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-71-125.ec2.internal before test
Feb 17 18:53:14.877: INFO: sonobuoy from sonobuoy started at 2020-02-17 18:03:16 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.877: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 17 18:53:14.877: INFO: kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-71-125.ec2.internal from kube-system started at 2020-02-17 17:51:59 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.877: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:53:14.877: INFO: kublr-node-name-reporter-3cd39b06f3a526166a2bdbb7d0aa396f06e89576f16de214708f3a78d146fdd2-ip-172-16-71-125.ec2.internal from kube-system started at 2020-02-17 17:51:59 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.877: INFO: 	Container main ready: true, restart count 0
Feb 17 18:53:14.877: INFO: k8s-api-haproxy-92dea82227153ce8905e2adce84945a2a46a1ff902bbfb5846f597d0315ec0e1-ip-172-16-71-125.ec2.internal from kube-system started at 2020-02-17 17:51:59 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.877: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Feb 17 18:53:14.877: INFO: node-local-dns-b4j9f from kube-system started at 2020-02-17 17:53:19 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.877: INFO: 	Container node-cache ready: true, restart count 0
Feb 17 18:53:14.877: INFO: netserver-1 from pod-network-test-3778 started at 2020-02-17 18:52:52 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.877: INFO: 	Container webserver ready: true, restart count 0
Feb 17 18:53:14.877: INFO: canal-9jpnm from kube-system started at 2020-02-17 17:53:09 +0000 UTC (4 container statuses recorded)
Feb 17 18:53:14.877: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:53:14.877: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 17 18:53:14.877: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Feb 17 18:53:14.877: INFO: 	Container update-network-condition ready: true, restart count 0
Feb 17 18:53:14.877: INFO: kublr-system-shell-5b9bd5c865-lmlsp from kube-system started at 2020-02-17 17:54:02 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.877: INFO: 	Container shell ready: true, restart count 0
Feb 17 18:53:14.877: INFO: sonobuoy-e2e-job-20e37349fda64c29 from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:53:14.877: INFO: 	Container e2e ready: true, restart count 0
Feb 17 18:53:14.877: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:53:14.877: INFO: sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-d99zx from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:53:14.877: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:53:14.877: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:53:14.877: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-86-181.ec2.internal before test
Feb 17 18:53:14.894: INFO: k8s-api-haproxy-b3a73b75b08bcd3ee17799fee24d51365c32935f7350cdcde3e2adb2935e8f65-ip-172-16-86-181.ec2.internal from kube-system started at 2020-02-17 17:51:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.894: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Feb 17 18:53:14.894: INFO: canal-ng4fk from kube-system started at 2020-02-17 17:53:14 +0000 UTC (4 container statuses recorded)
Feb 17 18:53:14.894: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:53:14.894: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 17 18:53:14.894: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Feb 17 18:53:14.894: INFO: 	Container update-network-condition ready: true, restart count 0
Feb 17 18:53:14.894: INFO: node-local-dns-7prxg from kube-system started at 2020-02-17 17:53:34 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.894: INFO: 	Container node-cache ready: true, restart count 0
Feb 17 18:53:14.894: INFO: sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-q7qcp from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:53:14.894: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:53:14.894: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:53:14.894: INFO: kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-86-181.ec2.internal from kube-system started at 2020-02-17 17:51:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.894: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:53:14.894: INFO: kublr-node-name-reporter-d9895b0065d8993619f85e1df198fa878363f17b2ecacbc683c208bcec5934f7-ip-172-16-86-181.ec2.internal from kube-system started at 2020-02-17 17:51:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.894: INFO: 	Container main ready: true, restart count 0
Feb 17 18:53:14.894: INFO: coredns-6c578c8bf7-bwb2d from kube-system started at 2020-02-17 17:53:34 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.894: INFO: 	Container coredns ready: true, restart count 0
Feb 17 18:53:14.894: INFO: metrics-server-v0.3.6-7b66f9c8dc-n74zk from kube-system started at 2020-02-17 17:53:36 +0000 UTC (2 container statuses recorded)
Feb 17 18:53:14.894: INFO: 	Container metrics-server ready: true, restart count 0
Feb 17 18:53:14.894: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 17 18:53:14.894: INFO: netserver-2 from pod-network-test-3778 started at 2020-02-17 18:52:52 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:14.894: INFO: 	Container webserver ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c5dd9979-4be8-43b3-bbfa-db7ad9f7ef4b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c5dd9979-4be8-43b3-bbfa-db7ad9f7ef4b off the node ip-172-16-86-181.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c5dd9979-4be8-43b3-bbfa-db7ad9f7ef4b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:53:20.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9370" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:6.328 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":278,"completed":209,"skipped":3385,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:53:21.011: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2536
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 17 18:53:21.171: INFO: Waiting up to 5m0s for pod "pod-8fb03751-6379-4693-8a3d-aadc6053ccfe" in namespace "emptydir-2536" to be "success or failure"
Feb 17 18:53:21.175: INFO: Pod "pod-8fb03751-6379-4693-8a3d-aadc6053ccfe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.520824ms
Feb 17 18:53:23.180: INFO: Pod "pod-8fb03751-6379-4693-8a3d-aadc6053ccfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008664521s
Feb 17 18:53:25.184: INFO: Pod "pod-8fb03751-6379-4693-8a3d-aadc6053ccfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012731249s
STEP: Saw pod success
Feb 17 18:53:25.184: INFO: Pod "pod-8fb03751-6379-4693-8a3d-aadc6053ccfe" satisfied condition "success or failure"
Feb 17 18:53:25.188: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-8fb03751-6379-4693-8a3d-aadc6053ccfe container test-container: <nil>
STEP: delete the pod
Feb 17 18:53:25.216: INFO: Waiting for pod pod-8fb03751-6379-4693-8a3d-aadc6053ccfe to disappear
Feb 17 18:53:25.223: INFO: Pod pod-8fb03751-6379-4693-8a3d-aadc6053ccfe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:53:25.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2536" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":210,"skipped":3404,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:53:25.243: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2076
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:53:25.409: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-06c0ced0-c5fa-4ac2-a988-a11c1e70adba" in namespace "security-context-test-2076" to be "success or failure"
Feb 17 18:53:25.412: INFO: Pod "busybox-readonly-false-06c0ced0-c5fa-4ac2-a988-a11c1e70adba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.848211ms
Feb 17 18:53:27.417: INFO: Pod "busybox-readonly-false-06c0ced0-c5fa-4ac2-a988-a11c1e70adba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008562474s
Feb 17 18:53:29.422: INFO: Pod "busybox-readonly-false-06c0ced0-c5fa-4ac2-a988-a11c1e70adba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013388303s
Feb 17 18:53:29.422: INFO: Pod "busybox-readonly-false-06c0ced0-c5fa-4ac2-a988-a11c1e70adba" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:53:29.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2076" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":278,"completed":211,"skipped":3424,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:53:29.438: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9127
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 17 18:53:29.590: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 17 18:53:29.609: INFO: Waiting for terminating namespaces to be deleted...
Feb 17 18:53:29.613: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-58-212.ec2.internal before test
Feb 17 18:53:29.620: INFO: coredns-6c578c8bf7-88w67 from kube-system started at 2020-02-17 17:53:27 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.620: INFO: 	Container coredns ready: true, restart count 0
Feb 17 18:53:29.620: INFO: kublr-node-name-reporter-2e4a883ef03ca181a5002964babb2c122cef2ef3f405e26061a11af75a5d837f-ip-172-16-58-212.ec2.internal from kube-system started at 2020-02-17 17:52:08 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.620: INFO: 	Container main ready: true, restart count 0
Feb 17 18:53:29.620: INFO: kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-58-212.ec2.internal from kube-system started at 2020-02-17 17:52:08 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.620: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:53:29.620: INFO: canal-v8cfc from kube-system started at 2020-02-17 17:53:10 +0000 UTC (4 container statuses recorded)
Feb 17 18:53:29.620: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:53:29.620: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 17 18:53:29.620: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Feb 17 18:53:29.620: INFO: 	Container update-network-condition ready: true, restart count 0
Feb 17 18:53:29.620: INFO: node-local-dns-rvfkr from kube-system started at 2020-02-17 17:53:20 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.620: INFO: 	Container node-cache ready: true, restart count 0
Feb 17 18:53:29.620: INFO: sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-zlh59 from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:53:29.620: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:53:29.620: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:53:29.620: INFO: busybox-readonly-false-06c0ced0-c5fa-4ac2-a988-a11c1e70adba from security-context-test-2076 started at 2020-02-17 18:53:25 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.620: INFO: 	Container busybox-readonly-false-06c0ced0-c5fa-4ac2-a988-a11c1e70adba ready: false, restart count 0
Feb 17 18:53:29.620: INFO: k8s-api-haproxy-b9ba254b202c3ae80f0c096eb09e3391b752ed191356ffd2fb26712eff44ce1b-ip-172-16-58-212.ec2.internal from kube-system started at 2020-02-17 17:52:08 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.620: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Feb 17 18:53:29.620: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-71-125.ec2.internal before test
Feb 17 18:53:29.628: INFO: canal-9jpnm from kube-system started at 2020-02-17 17:53:09 +0000 UTC (4 container statuses recorded)
Feb 17 18:53:29.628: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:53:29.628: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 17 18:53:29.628: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Feb 17 18:53:29.628: INFO: 	Container update-network-condition ready: true, restart count 0
Feb 17 18:53:29.628: INFO: kublr-system-shell-5b9bd5c865-lmlsp from kube-system started at 2020-02-17 17:54:02 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.628: INFO: 	Container shell ready: true, restart count 0
Feb 17 18:53:29.628: INFO: sonobuoy-e2e-job-20e37349fda64c29 from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:53:29.628: INFO: 	Container e2e ready: true, restart count 0
Feb 17 18:53:29.628: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:53:29.628: INFO: sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-d99zx from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:53:29.628: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:53:29.628: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:53:29.628: INFO: kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-71-125.ec2.internal from kube-system started at 2020-02-17 17:51:59 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.628: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:53:29.628: INFO: kublr-node-name-reporter-3cd39b06f3a526166a2bdbb7d0aa396f06e89576f16de214708f3a78d146fdd2-ip-172-16-71-125.ec2.internal from kube-system started at 2020-02-17 17:51:59 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.628: INFO: 	Container main ready: true, restart count 0
Feb 17 18:53:29.628: INFO: k8s-api-haproxy-92dea82227153ce8905e2adce84945a2a46a1ff902bbfb5846f597d0315ec0e1-ip-172-16-71-125.ec2.internal from kube-system started at 2020-02-17 17:51:59 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.628: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Feb 17 18:53:29.628: INFO: node-local-dns-b4j9f from kube-system started at 2020-02-17 17:53:19 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.628: INFO: 	Container node-cache ready: true, restart count 0
Feb 17 18:53:29.628: INFO: sonobuoy from sonobuoy started at 2020-02-17 18:03:16 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.628: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 17 18:53:29.628: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-86-181.ec2.internal before test
Feb 17 18:53:29.636: INFO: metrics-server-v0.3.6-7b66f9c8dc-n74zk from kube-system started at 2020-02-17 17:53:36 +0000 UTC (2 container statuses recorded)
Feb 17 18:53:29.636: INFO: 	Container metrics-server ready: true, restart count 0
Feb 17 18:53:29.636: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 17 18:53:29.636: INFO: kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-86-181.ec2.internal from kube-system started at 2020-02-17 17:51:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.636: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:53:29.636: INFO: kublr-node-name-reporter-d9895b0065d8993619f85e1df198fa878363f17b2ecacbc683c208bcec5934f7-ip-172-16-86-181.ec2.internal from kube-system started at 2020-02-17 17:51:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.636: INFO: 	Container main ready: true, restart count 0
Feb 17 18:53:29.636: INFO: coredns-6c578c8bf7-bwb2d from kube-system started at 2020-02-17 17:53:34 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.636: INFO: 	Container coredns ready: true, restart count 0
Feb 17 18:53:29.636: INFO: sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-q7qcp from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 18:53:29.636: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:53:29.636: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:53:29.636: INFO: k8s-api-haproxy-b3a73b75b08bcd3ee17799fee24d51365c32935f7350cdcde3e2adb2935e8f65-ip-172-16-86-181.ec2.internal from kube-system started at 2020-02-17 17:51:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.636: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Feb 17 18:53:29.636: INFO: canal-ng4fk from kube-system started at 2020-02-17 17:53:14 +0000 UTC (4 container statuses recorded)
Feb 17 18:53:29.636: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:53:29.636: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 17 18:53:29.636: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Feb 17 18:53:29.636: INFO: 	Container update-network-condition ready: true, restart count 0
Feb 17 18:53:29.636: INFO: node-local-dns-7prxg from kube-system started at 2020-02-17 17:53:34 +0000 UTC (1 container statuses recorded)
Feb 17 18:53:29.636: INFO: 	Container node-cache ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c01b4aaa-98e8-4422-82d5-02d86bc32de3 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-c01b4aaa-98e8-4422-82d5-02d86bc32de3 off the node ip-172-16-58-212.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c01b4aaa-98e8-4422-82d5-02d86bc32de3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:58:35.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9127" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:306.331 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":278,"completed":212,"skipped":3442,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:58:35.769: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-b0dd3985-16a0-413f-b4f8-3db4f1e0281c
STEP: Creating a pod to test consume secrets
Feb 17 18:58:35.945: INFO: Waiting up to 5m0s for pod "pod-secrets-6d873100-3689-43b8-b960-47a1401ea494" in namespace "secrets-9575" to be "success or failure"
Feb 17 18:58:35.963: INFO: Pod "pod-secrets-6d873100-3689-43b8-b960-47a1401ea494": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012383ms
Feb 17 18:58:37.967: INFO: Pod "pod-secrets-6d873100-3689-43b8-b960-47a1401ea494": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02224225s
STEP: Saw pod success
Feb 17 18:58:37.967: INFO: Pod "pod-secrets-6d873100-3689-43b8-b960-47a1401ea494" satisfied condition "success or failure"
Feb 17 18:58:37.971: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-secrets-6d873100-3689-43b8-b960-47a1401ea494 container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 18:58:38.005: INFO: Waiting for pod pod-secrets-6d873100-3689-43b8-b960-47a1401ea494 to disappear
Feb 17 18:58:38.009: INFO: Pod pod-secrets-6d873100-3689-43b8-b960-47a1401ea494 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:58:38.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9575" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":213,"skipped":3464,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:58:38.026: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Feb 17 18:58:38.201: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-713147157 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:58:38.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2144" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":278,"completed":214,"skipped":3473,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:58:38.271: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1151
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-6e66688d-3a35-4677-bf83-91686728f6ed
STEP: Creating a pod to test consume configMaps
Feb 17 18:58:38.439: INFO: Waiting up to 5m0s for pod "pod-configmaps-399ae719-8542-4c5e-83da-31ddcb6ac060" in namespace "configmap-1151" to be "success or failure"
Feb 17 18:58:38.443: INFO: Pod "pod-configmaps-399ae719-8542-4c5e-83da-31ddcb6ac060": Phase="Pending", Reason="", readiness=false. Elapsed: 4.577024ms
Feb 17 18:58:40.449: INFO: Pod "pod-configmaps-399ae719-8542-4c5e-83da-31ddcb6ac060": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010369161s
Feb 17 18:58:42.455: INFO: Pod "pod-configmaps-399ae719-8542-4c5e-83da-31ddcb6ac060": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016362543s
STEP: Saw pod success
Feb 17 18:58:42.455: INFO: Pod "pod-configmaps-399ae719-8542-4c5e-83da-31ddcb6ac060" satisfied condition "success or failure"
Feb 17 18:58:42.461: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod pod-configmaps-399ae719-8542-4c5e-83da-31ddcb6ac060 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:58:42.495: INFO: Waiting for pod pod-configmaps-399ae719-8542-4c5e-83da-31ddcb6ac060 to disappear
Feb 17 18:58:42.500: INFO: Pod pod-configmaps-399ae719-8542-4c5e-83da-31ddcb6ac060 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:58:42.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1151" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":215,"skipped":3483,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:58:42.515: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4944
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:58:43.671: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:58:45.690: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562723, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562723, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562723, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562723, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:58:48.715: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:58:48.720: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-586-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:58:55.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4944" for this suite.
STEP: Destroying namespace "webhook-4944-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.444 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":278,"completed":216,"skipped":3511,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:58:55.958: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9908
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-9908/configmap-test-55de9426-2b17-48b4-ac55-b2c7d29184cc
STEP: Creating a pod to test consume configMaps
Feb 17 18:58:56.136: INFO: Waiting up to 5m0s for pod "pod-configmaps-f2438bfb-5455-48ea-9966-f0cb0edffde4" in namespace "configmap-9908" to be "success or failure"
Feb 17 18:58:56.141: INFO: Pod "pod-configmaps-f2438bfb-5455-48ea-9966-f0cb0edffde4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.679414ms
Feb 17 18:58:58.146: INFO: Pod "pod-configmaps-f2438bfb-5455-48ea-9966-f0cb0edffde4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0097117s
STEP: Saw pod success
Feb 17 18:58:58.146: INFO: Pod "pod-configmaps-f2438bfb-5455-48ea-9966-f0cb0edffde4" satisfied condition "success or failure"
Feb 17 18:58:58.151: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-configmaps-f2438bfb-5455-48ea-9966-f0cb0edffde4 container env-test: <nil>
STEP: delete the pod
Feb 17 18:58:58.173: INFO: Waiting for pod pod-configmaps-f2438bfb-5455-48ea-9966-f0cb0edffde4 to disappear
Feb 17 18:58:58.177: INFO: Pod pod-configmaps-f2438bfb-5455-48ea-9966-f0cb0edffde4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:58:58.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9908" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":278,"completed":217,"skipped":3514,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:58:58.191: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9812
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Feb 17 18:59:02.374: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9812 PodName:pod-sharedvolume-5adb7189-cc76-440b-b7a3-2c28aa80e00e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:59:02.374: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 18:59:02.501: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:59:02.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9812" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":278,"completed":218,"skipped":3522,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:59:02.516: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3650
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1632
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 18:59:02.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-3650'
Feb 17 18:59:02.743: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 17 18:59:02.743: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Feb 17 18:59:02.762: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-98qbx]
Feb 17 18:59:02.763: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-98qbx" in namespace "kubectl-3650" to be "running and ready"
Feb 17 18:59:02.766: INFO: Pod "e2e-test-httpd-rc-98qbx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.830955ms
Feb 17 18:59:04.771: INFO: Pod "e2e-test-httpd-rc-98qbx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008387091s
Feb 17 18:59:06.776: INFO: Pod "e2e-test-httpd-rc-98qbx": Phase="Running", Reason="", readiness=true. Elapsed: 4.013399268s
Feb 17 18:59:06.776: INFO: Pod "e2e-test-httpd-rc-98qbx" satisfied condition "running and ready"
Feb 17 18:59:06.776: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-98qbx]
Feb 17 18:59:06.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 logs rc/e2e-test-httpd-rc --namespace=kubectl-3650'
Feb 17 18:59:06.875: INFO: stderr: ""
Feb 17 18:59:06.875: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 100.96.5.79. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 100.96.5.79. Set the 'ServerName' directive globally to suppress this message\n[Mon Feb 17 18:59:04.303034 2020] [mpm_event:notice] [pid 1:tid 140712288770920] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Mon Feb 17 18:59:04.303165 2020] [core:notice] [pid 1:tid 140712288770920] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1637
Feb 17 18:59:06.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete rc e2e-test-httpd-rc --namespace=kubectl-3650'
Feb 17 18:59:06.958: INFO: stderr: ""
Feb 17 18:59:06.958: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:59:06.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3650" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":278,"completed":219,"skipped":3523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:59:06.974: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:59:07.131: INFO: Creating ReplicaSet my-hostname-basic-37779e74-d849-47df-8aca-f92b07125f0e
Feb 17 18:59:07.141: INFO: Pod name my-hostname-basic-37779e74-d849-47df-8aca-f92b07125f0e: Found 0 pods out of 1
Feb 17 18:59:12.145: INFO: Pod name my-hostname-basic-37779e74-d849-47df-8aca-f92b07125f0e: Found 1 pods out of 1
Feb 17 18:59:12.145: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-37779e74-d849-47df-8aca-f92b07125f0e" is running
Feb 17 18:59:12.149: INFO: Pod "my-hostname-basic-37779e74-d849-47df-8aca-f92b07125f0e-hwck6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 18:59:07 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 18:59:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 18:59:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 18:59:07 +0000 UTC Reason: Message:}])
Feb 17 18:59:12.149: INFO: Trying to dial the pod
Feb 17 18:59:17.166: INFO: Controller my-hostname-basic-37779e74-d849-47df-8aca-f92b07125f0e: Got expected result from replica 1 [my-hostname-basic-37779e74-d849-47df-8aca-f92b07125f0e-hwck6]: "my-hostname-basic-37779e74-d849-47df-8aca-f92b07125f0e-hwck6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:59:17.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-504" for this suite.

• [SLOW TEST:10.205 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":278,"completed":220,"skipped":3550,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:59:17.180: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-9101
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 18:59:17.340: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Creating first CR 
Feb 17 18:59:22.466: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T18:59:22Z generation:1 name:name1 resourceVersion:26401 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a2d7c221-5878-4e89-8638-2dd753cf5855] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Feb 17 18:59:32.473: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T18:59:32Z generation:1 name:name2 resourceVersion:26440 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:62d0ad58-a46f-4d23-856d-1c00975b11a7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Feb 17 18:59:42.480: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T18:59:22Z generation:2 name:name1 resourceVersion:26476 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a2d7c221-5878-4e89-8638-2dd753cf5855] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Feb 17 18:59:52.487: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T18:59:32Z generation:2 name:name2 resourceVersion:26513 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:62d0ad58-a46f-4d23-856d-1c00975b11a7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Feb 17 19:00:02.499: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T18:59:22Z generation:2 name:name1 resourceVersion:26546 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a2d7c221-5878-4e89-8638-2dd753cf5855] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Feb 17 19:00:12.513: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T18:59:32Z generation:2 name:name2 resourceVersion:26580 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:62d0ad58-a46f-4d23-856d-1c00975b11a7] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:00:23.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9101" for this suite.

• [SLOW TEST:65.865 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":278,"completed":221,"skipped":3578,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:00:23.045: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9226
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:00:25.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9226" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":222,"skipped":3620,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:00:25.249: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-38
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-38
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-38
Feb 17 19:00:25.431: INFO: Found 0 stateful pods, waiting for 1
Feb 17 19:00:35.436: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 17 19:00:35.464: INFO: Deleting all statefulset in ns statefulset-38
Feb 17 19:00:35.477: INFO: Scaling statefulset ss to 0
Feb 17 19:00:55.503: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 19:00:55.508: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:00:55.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-38" for this suite.

• [SLOW TEST:30.290 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":278,"completed":223,"skipped":3636,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:00:55.541: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9117
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Feb 17 19:01:05.742: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0217 19:01:05.742243      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:01:05.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9117" for this suite.

• [SLOW TEST:10.214 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":278,"completed":224,"skipped":3691,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:01:05.755: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6712
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb 17 19:01:05.913: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 17 19:01:12.963: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:01:12.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6712" for this suite.

• [SLOW TEST:7.225 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":278,"completed":225,"skipped":3703,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:01:12.980: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5798
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Feb 17 19:01:13.143: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 19:01:21.146: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:01:38.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5798" for this suite.

• [SLOW TEST:25.132 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":278,"completed":226,"skipped":3731,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:01:38.112: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4794
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Feb 17 19:01:42.799: INFO: Successfully updated pod "adopt-release-hkvwm"
STEP: Checking that the Job readopts the Pod
Feb 17 19:01:42.799: INFO: Waiting up to 15m0s for pod "adopt-release-hkvwm" in namespace "job-4794" to be "adopted"
Feb 17 19:01:42.804: INFO: Pod "adopt-release-hkvwm": Phase="Running", Reason="", readiness=true. Elapsed: 4.905107ms
Feb 17 19:01:44.809: INFO: Pod "adopt-release-hkvwm": Phase="Running", Reason="", readiness=true. Elapsed: 2.009993687s
Feb 17 19:01:44.809: INFO: Pod "adopt-release-hkvwm" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Feb 17 19:01:45.321: INFO: Successfully updated pod "adopt-release-hkvwm"
STEP: Checking that the Job releases the Pod
Feb 17 19:01:45.321: INFO: Waiting up to 15m0s for pod "adopt-release-hkvwm" in namespace "job-4794" to be "released"
Feb 17 19:01:45.327: INFO: Pod "adopt-release-hkvwm": Phase="Running", Reason="", readiness=true. Elapsed: 5.218934ms
Feb 17 19:01:47.333: INFO: Pod "adopt-release-hkvwm": Phase="Running", Reason="", readiness=true. Elapsed: 2.011210127s
Feb 17 19:01:47.333: INFO: Pod "adopt-release-hkvwm" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:01:47.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4794" for this suite.

• [SLOW TEST:9.234 seconds]
[sig-apps] Job
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":278,"completed":227,"skipped":3735,"failed":0}
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:01:47.347: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8971
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Feb 17 19:01:47.498: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-713147157 proxy --unix-socket=/tmp/kubectl-proxy-unix621196464/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:01:47.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8971" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":278,"completed":228,"skipped":3735,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:01:47.563: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4820
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 17 19:01:47.736: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4820 /api/v1/namespaces/watch-4820/configmaps/e2e-watch-test-label-changed bdd93a5d-777b-4e2a-ae95-87d8a4f46e6f 27190 0 2020-02-17 19:01:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 17 19:01:47.736: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4820 /api/v1/namespaces/watch-4820/configmaps/e2e-watch-test-label-changed bdd93a5d-777b-4e2a-ae95-87d8a4f46e6f 27191 0 2020-02-17 19:01:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 17 19:01:47.736: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4820 /api/v1/namespaces/watch-4820/configmaps/e2e-watch-test-label-changed bdd93a5d-777b-4e2a-ae95-87d8a4f46e6f 27192 0 2020-02-17 19:01:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 17 19:01:57.775: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4820 /api/v1/namespaces/watch-4820/configmaps/e2e-watch-test-label-changed bdd93a5d-777b-4e2a-ae95-87d8a4f46e6f 27262 0 2020-02-17 19:01:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 17 19:01:57.775: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4820 /api/v1/namespaces/watch-4820/configmaps/e2e-watch-test-label-changed bdd93a5d-777b-4e2a-ae95-87d8a4f46e6f 27263 0 2020-02-17 19:01:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 17 19:01:57.775: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4820 /api/v1/namespaces/watch-4820/configmaps/e2e-watch-test-label-changed bdd93a5d-777b-4e2a-ae95-87d8a4f46e6f 27264 0 2020-02-17 19:01:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:01:57.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4820" for this suite.

• [SLOW TEST:10.230 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":278,"completed":229,"skipped":3738,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:01:57.793: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6169
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Feb 17 19:02:02.516: INFO: Successfully updated pod "annotationupdate3161424e-eab9-4fdc-8049-95d95a417302"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:02:04.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6169" for this suite.

• [SLOW TEST:6.761 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":278,"completed":230,"skipped":3745,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:02:04.554: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8493
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 19:02:05.692: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 19:02:07.707: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562925, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562925, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562925, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562925, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 19:02:10.732: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:02:25.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8493" for this suite.
STEP: Destroying namespace "webhook-8493-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:21.491 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":278,"completed":231,"skipped":3771,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:02:26.045: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8848
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Feb 17 19:02:26.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 create -f - --namespace=kubectl-8848'
Feb 17 19:02:27.228: INFO: stderr: ""
Feb 17 19:02:27.228: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Feb 17 19:02:28.235: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 19:02:28.235: INFO: Found 0 / 1
Feb 17 19:02:29.233: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 19:02:29.233: INFO: Found 1 / 1
Feb 17 19:02:29.233: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 17 19:02:29.237: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 19:02:29.237: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 17 19:02:29.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 patch pod agnhost-master-ntpkh --namespace=kubectl-8848 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 17 19:02:29.306: INFO: stderr: ""
Feb 17 19:02:29.306: INFO: stdout: "pod/agnhost-master-ntpkh patched\n"
STEP: checking annotations
Feb 17 19:02:29.312: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 19:02:29.312: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:02:29.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8848" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":278,"completed":232,"skipped":3783,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:02:29.327: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 19:02:29.485: INFO: Waiting up to 5m0s for pod "downwardapi-volume-603c3dee-d92e-4ab2-9843-f8ce776b5468" in namespace "projected-2344" to be "success or failure"
Feb 17 19:02:29.489: INFO: Pod "downwardapi-volume-603c3dee-d92e-4ab2-9843-f8ce776b5468": Phase="Pending", Reason="", readiness=false. Elapsed: 3.921609ms
Feb 17 19:02:31.496: INFO: Pod "downwardapi-volume-603c3dee-d92e-4ab2-9843-f8ce776b5468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010992691s
Feb 17 19:02:33.502: INFO: Pod "downwardapi-volume-603c3dee-d92e-4ab2-9843-f8ce776b5468": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016902108s
STEP: Saw pod success
Feb 17 19:02:33.502: INFO: Pod "downwardapi-volume-603c3dee-d92e-4ab2-9843-f8ce776b5468" satisfied condition "success or failure"
Feb 17 19:02:33.506: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod downwardapi-volume-603c3dee-d92e-4ab2-9843-f8ce776b5468 container client-container: <nil>
STEP: delete the pod
Feb 17 19:02:33.542: INFO: Waiting for pod downwardapi-volume-603c3dee-d92e-4ab2-9843-f8ce776b5468 to disappear
Feb 17 19:02:33.546: INFO: Pod downwardapi-volume-603c3dee-d92e-4ab2-9843-f8ce776b5468 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:02:33.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2344" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":233,"skipped":3837,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:02:33.562: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5979
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-1c288fc9-7d92-49f4-ab60-68d18b8351c4
STEP: Creating a pod to test consume secrets
Feb 17 19:02:33.726: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a8853eae-4f93-41b3-9ace-9b94b981e22d" in namespace "projected-5979" to be "success or failure"
Feb 17 19:02:33.732: INFO: Pod "pod-projected-secrets-a8853eae-4f93-41b3-9ace-9b94b981e22d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.261949ms
Feb 17 19:02:35.737: INFO: Pod "pod-projected-secrets-a8853eae-4f93-41b3-9ace-9b94b981e22d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010302913s
Feb 17 19:02:37.741: INFO: Pod "pod-projected-secrets-a8853eae-4f93-41b3-9ace-9b94b981e22d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014928844s
STEP: Saw pod success
Feb 17 19:02:37.741: INFO: Pod "pod-projected-secrets-a8853eae-4f93-41b3-9ace-9b94b981e22d" satisfied condition "success or failure"
Feb 17 19:02:37.746: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod pod-projected-secrets-a8853eae-4f93-41b3-9ace-9b94b981e22d container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 19:02:37.772: INFO: Waiting for pod pod-projected-secrets-a8853eae-4f93-41b3-9ace-9b94b981e22d to disappear
Feb 17 19:02:37.777: INFO: Pod pod-projected-secrets-a8853eae-4f93-41b3-9ace-9b94b981e22d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:02:37.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5979" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":278,"completed":234,"skipped":3864,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:02:37.791: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-772
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Feb 17 19:02:37.944: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 19:02:45.932: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:03:03.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-772" for this suite.

• [SLOW TEST:26.038 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":278,"completed":235,"skipped":3864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:03:03.829: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5321
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 19:03:03.989: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 17 19:03:11.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-5321 create -f -'
Feb 17 19:03:13.090: INFO: stderr: ""
Feb 17 19:03:13.090: INFO: stdout: "e2e-test-crd-publish-openapi-5513-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 17 19:03:13.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-5321 delete e2e-test-crd-publish-openapi-5513-crds test-cr'
Feb 17 19:03:13.166: INFO: stderr: ""
Feb 17 19:03:13.166: INFO: stdout: "e2e-test-crd-publish-openapi-5513-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 17 19:03:13.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-5321 apply -f -'
Feb 17 19:03:13.380: INFO: stderr: ""
Feb 17 19:03:13.381: INFO: stdout: "e2e-test-crd-publish-openapi-5513-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 17 19:03:13.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 --namespace=crd-publish-openapi-5321 delete e2e-test-crd-publish-openapi-5513-crds test-cr'
Feb 17 19:03:13.498: INFO: stderr: ""
Feb 17 19:03:13.498: INFO: stdout: "e2e-test-crd-publish-openapi-5513-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Feb 17 19:03:13.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 explain e2e-test-crd-publish-openapi-5513-crds'
Feb 17 19:03:13.691: INFO: stderr: ""
Feb 17 19:03:13.691: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5513-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:03:16.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5321" for this suite.

• [SLOW TEST:12.846 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":278,"completed":236,"skipped":3888,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:03:16.675: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-8475
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:03:18.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8475" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":278,"completed":237,"skipped":3918,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:03:18.913: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1256
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Feb 17 19:03:19.067: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:03:42.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1256" for this suite.

• [SLOW TEST:23.390 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":278,"completed":238,"skipped":3951,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:03:42.303: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-761
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:04:42.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-761" for this suite.

• [SLOW TEST:60.182 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":278,"completed":239,"skipped":3952,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:04:42.485: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3770
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1596
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 19:04:42.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3770'
Feb 17 19:04:42.718: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 17 19:04:42.718: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1602
Feb 17 19:04:44.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete deployment e2e-test-httpd-deployment --namespace=kubectl-3770'
Feb 17 19:04:44.804: INFO: stderr: ""
Feb 17 19:04:44.804: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:04:44.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3770" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":278,"completed":240,"skipped":3965,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:04:44.818: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8206
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-c02f235d-4894-4dc0-973a-73bab8db963d in namespace container-probe-8206
Feb 17 19:04:48.994: INFO: Started pod test-webserver-c02f235d-4894-4dc0-973a-73bab8db963d in namespace container-probe-8206
STEP: checking the pod's current state and verifying that restartCount is present
Feb 17 19:04:48.999: INFO: Initial restart count of pod test-webserver-c02f235d-4894-4dc0-973a-73bab8db963d is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:08:49.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8206" for this suite.

• [SLOW TEST:244.845 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":278,"completed":241,"skipped":3990,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:08:49.663: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5195
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 17 19:08:50.144: INFO: Pod name wrapped-volume-race-fd819c5d-cadd-4cb1-809c-8cf74399189c: Found 0 pods out of 5
Feb 17 19:08:55.151: INFO: Pod name wrapped-volume-race-fd819c5d-cadd-4cb1-809c-8cf74399189c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-fd819c5d-cadd-4cb1-809c-8cf74399189c in namespace emptydir-wrapper-5195, will wait for the garbage collector to delete the pods
Feb 17 19:09:05.246: INFO: Deleting ReplicationController wrapped-volume-race-fd819c5d-cadd-4cb1-809c-8cf74399189c took: 11.710317ms
Feb 17 19:09:05.746: INFO: Terminating ReplicationController wrapped-volume-race-fd819c5d-cadd-4cb1-809c-8cf74399189c pods took: 500.243755ms
STEP: Creating RC which spawns configmap-volume pods
Feb 17 19:09:20.068: INFO: Pod name wrapped-volume-race-ca8f916e-81f7-4799-95cb-4fca32bd87c1: Found 0 pods out of 5
Feb 17 19:09:25.075: INFO: Pod name wrapped-volume-race-ca8f916e-81f7-4799-95cb-4fca32bd87c1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ca8f916e-81f7-4799-95cb-4fca32bd87c1 in namespace emptydir-wrapper-5195, will wait for the garbage collector to delete the pods
Feb 17 19:09:35.169: INFO: Deleting ReplicationController wrapped-volume-race-ca8f916e-81f7-4799-95cb-4fca32bd87c1 took: 11.597301ms
Feb 17 19:09:35.669: INFO: Terminating ReplicationController wrapped-volume-race-ca8f916e-81f7-4799-95cb-4fca32bd87c1 pods took: 500.26516ms
STEP: Creating RC which spawns configmap-volume pods
Feb 17 19:09:49.588: INFO: Pod name wrapped-volume-race-06e4b168-aba0-4994-86ff-3ebd2d67c159: Found 0 pods out of 5
Feb 17 19:09:54.602: INFO: Pod name wrapped-volume-race-06e4b168-aba0-4994-86ff-3ebd2d67c159: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-06e4b168-aba0-4994-86ff-3ebd2d67c159 in namespace emptydir-wrapper-5195, will wait for the garbage collector to delete the pods
Feb 17 19:10:06.715: INFO: Deleting ReplicationController wrapped-volume-race-06e4b168-aba0-4994-86ff-3ebd2d67c159 took: 13.952805ms
Feb 17 19:10:07.215: INFO: Terminating ReplicationController wrapped-volume-race-06e4b168-aba0-4994-86ff-3ebd2d67c159 pods took: 500.240308ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:10:19.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5195" for this suite.

• [SLOW TEST:89.964 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":278,"completed":242,"skipped":3991,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:10:19.628: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2655
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 17 19:10:22.327: INFO: Successfully updated pod "pod-update-activedeadlineseconds-924dfb80-7827-4b9b-80ef-bc18cc2f4d68"
Feb 17 19:10:22.327: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-924dfb80-7827-4b9b-80ef-bc18cc2f4d68" in namespace "pods-2655" to be "terminated due to deadline exceeded"
Feb 17 19:10:22.331: INFO: Pod "pod-update-activedeadlineseconds-924dfb80-7827-4b9b-80ef-bc18cc2f4d68": Phase="Running", Reason="", readiness=true. Elapsed: 4.288083ms
Feb 17 19:10:24.336: INFO: Pod "pod-update-activedeadlineseconds-924dfb80-7827-4b9b-80ef-bc18cc2f4d68": Phase="Running", Reason="", readiness=true. Elapsed: 2.009379802s
Feb 17 19:10:26.341: INFO: Pod "pod-update-activedeadlineseconds-924dfb80-7827-4b9b-80ef-bc18cc2f4d68": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.013847745s
Feb 17 19:10:26.341: INFO: Pod "pod-update-activedeadlineseconds-924dfb80-7827-4b9b-80ef-bc18cc2f4d68" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:10:26.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2655" for this suite.

• [SLOW TEST:6.730 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":278,"completed":243,"skipped":3998,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:10:26.358: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5855
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-6f3dba58-5342-47e8-b257-228fb963713c
STEP: Creating a pod to test consume secrets
Feb 17 19:10:26.537: INFO: Waiting up to 5m0s for pod "pod-secrets-fe27e467-64b2-4449-8536-46f948a8c679" in namespace "secrets-5855" to be "success or failure"
Feb 17 19:10:26.541: INFO: Pod "pod-secrets-fe27e467-64b2-4449-8536-46f948a8c679": Phase="Pending", Reason="", readiness=false. Elapsed: 3.976411ms
Feb 17 19:10:28.546: INFO: Pod "pod-secrets-fe27e467-64b2-4449-8536-46f948a8c679": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00913206s
Feb 17 19:10:30.552: INFO: Pod "pod-secrets-fe27e467-64b2-4449-8536-46f948a8c679": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014839453s
STEP: Saw pod success
Feb 17 19:10:30.552: INFO: Pod "pod-secrets-fe27e467-64b2-4449-8536-46f948a8c679" satisfied condition "success or failure"
Feb 17 19:10:30.556: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod pod-secrets-fe27e467-64b2-4449-8536-46f948a8c679 container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 19:10:30.591: INFO: Waiting for pod pod-secrets-fe27e467-64b2-4449-8536-46f948a8c679 to disappear
Feb 17 19:10:30.595: INFO: Pod pod-secrets-fe27e467-64b2-4449-8536-46f948a8c679 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:10:30.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5855" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":278,"completed":244,"skipped":4032,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:10:30.610: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-262
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-262
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-262
STEP: Deleting pre-stop pod
Feb 17 19:10:41.825: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:10:41.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-262" for this suite.

• [SLOW TEST:11.239 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":278,"completed":245,"skipped":4054,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:10:41.849: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Feb 17 19:10:42.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 create -f - --namespace=kubectl-545'
Feb 17 19:10:42.213: INFO: stderr: ""
Feb 17 19:10:42.213: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 19:10:42.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-545'
Feb 17 19:10:42.279: INFO: stderr: ""
Feb 17 19:10:42.279: INFO: stdout: "update-demo-nautilus-nx7cz update-demo-nautilus-qxwnf "
Feb 17 19:10:42.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-nx7cz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-545'
Feb 17 19:10:42.342: INFO: stderr: ""
Feb 17 19:10:42.343: INFO: stdout: ""
Feb 17 19:10:42.343: INFO: update-demo-nautilus-nx7cz is created but not running
Feb 17 19:10:47.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-545'
Feb 17 19:10:47.410: INFO: stderr: ""
Feb 17 19:10:47.410: INFO: stdout: "update-demo-nautilus-nx7cz update-demo-nautilus-qxwnf "
Feb 17 19:10:47.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-nx7cz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-545'
Feb 17 19:10:47.474: INFO: stderr: ""
Feb 17 19:10:47.474: INFO: stdout: "true"
Feb 17 19:10:47.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-nx7cz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-545'
Feb 17 19:10:47.538: INFO: stderr: ""
Feb 17 19:10:47.538: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 19:10:47.538: INFO: validating pod update-demo-nautilus-nx7cz
Feb 17 19:10:47.544: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 19:10:47.544: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 19:10:47.544: INFO: update-demo-nautilus-nx7cz is verified up and running
Feb 17 19:10:47.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-qxwnf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-545'
Feb 17 19:10:47.605: INFO: stderr: ""
Feb 17 19:10:47.605: INFO: stdout: "true"
Feb 17 19:10:47.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-qxwnf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-545'
Feb 17 19:10:47.670: INFO: stderr: ""
Feb 17 19:10:47.670: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 19:10:47.670: INFO: validating pod update-demo-nautilus-qxwnf
Feb 17 19:10:47.677: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 19:10:47.677: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 19:10:47.677: INFO: update-demo-nautilus-qxwnf is verified up and running
STEP: rolling-update to new replication controller
Feb 17 19:10:47.679: INFO: scanned /root for discovery docs: <nil>
Feb 17 19:10:47.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-545'
Feb 17 19:11:11.139: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 17 19:11:11.139: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 19:11:11.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-545'
Feb 17 19:11:11.212: INFO: stderr: ""
Feb 17 19:11:11.212: INFO: stdout: "update-demo-kitten-h5qb6 update-demo-kitten-ksqcf "
Feb 17 19:11:11.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-kitten-h5qb6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-545'
Feb 17 19:11:11.272: INFO: stderr: ""
Feb 17 19:11:11.272: INFO: stdout: "true"
Feb 17 19:11:11.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-kitten-h5qb6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-545'
Feb 17 19:11:11.334: INFO: stderr: ""
Feb 17 19:11:11.334: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 17 19:11:11.334: INFO: validating pod update-demo-kitten-h5qb6
Feb 17 19:11:11.343: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 17 19:11:11.343: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 17 19:11:11.343: INFO: update-demo-kitten-h5qb6 is verified up and running
Feb 17 19:11:11.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-kitten-ksqcf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-545'
Feb 17 19:11:11.403: INFO: stderr: ""
Feb 17 19:11:11.403: INFO: stdout: "true"
Feb 17 19:11:11.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-kitten-ksqcf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-545'
Feb 17 19:11:11.467: INFO: stderr: ""
Feb 17 19:11:11.467: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 17 19:11:11.467: INFO: validating pod update-demo-kitten-ksqcf
Feb 17 19:11:11.474: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 17 19:11:11.474: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 17 19:11:11.474: INFO: update-demo-kitten-ksqcf is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:11:11.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-545" for this suite.

• [SLOW TEST:29.639 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":278,"completed":246,"skipped":4055,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:11:11.488: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7441
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:11:18.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7441" for this suite.

• [SLOW TEST:7.195 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":278,"completed":247,"skipped":4068,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:11:18.683: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5268
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-56e374e0-fcb1-4b9d-b71d-7375182c57f1
STEP: Creating a pod to test consume configMaps
Feb 17 19:11:18.856: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-60a822db-7c30-417e-9032-edfc9182fe00" in namespace "projected-5268" to be "success or failure"
Feb 17 19:11:18.865: INFO: Pod "pod-projected-configmaps-60a822db-7c30-417e-9032-edfc9182fe00": Phase="Pending", Reason="", readiness=false. Elapsed: 9.579484ms
Feb 17 19:11:20.870: INFO: Pod "pod-projected-configmaps-60a822db-7c30-417e-9032-edfc9182fe00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013951616s
Feb 17 19:11:22.875: INFO: Pod "pod-projected-configmaps-60a822db-7c30-417e-9032-edfc9182fe00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019180872s
STEP: Saw pod success
Feb 17 19:11:22.875: INFO: Pod "pod-projected-configmaps-60a822db-7c30-417e-9032-edfc9182fe00" satisfied condition "success or failure"
Feb 17 19:11:22.879: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod pod-projected-configmaps-60a822db-7c30-417e-9032-edfc9182fe00 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 19:11:22.916: INFO: Waiting for pod pod-projected-configmaps-60a822db-7c30-417e-9032-edfc9182fe00 to disappear
Feb 17 19:11:22.920: INFO: Pod pod-projected-configmaps-60a822db-7c30-417e-9032-edfc9182fe00 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:11:22.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5268" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":278,"completed":248,"skipped":4075,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:11:22.935: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7468
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 19:11:23.104: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e17b8125-62ca-4bfa-8ab5-3cdd2ca54226" in namespace "projected-7468" to be "success or failure"
Feb 17 19:11:23.111: INFO: Pod "downwardapi-volume-e17b8125-62ca-4bfa-8ab5-3cdd2ca54226": Phase="Pending", Reason="", readiness=false. Elapsed: 7.717919ms
Feb 17 19:11:25.117: INFO: Pod "downwardapi-volume-e17b8125-62ca-4bfa-8ab5-3cdd2ca54226": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013175349s
STEP: Saw pod success
Feb 17 19:11:25.117: INFO: Pod "downwardapi-volume-e17b8125-62ca-4bfa-8ab5-3cdd2ca54226" satisfied condition "success or failure"
Feb 17 19:11:25.122: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod downwardapi-volume-e17b8125-62ca-4bfa-8ab5-3cdd2ca54226 container client-container: <nil>
STEP: delete the pod
Feb 17 19:11:25.153: INFO: Waiting for pod downwardapi-volume-e17b8125-62ca-4bfa-8ab5-3cdd2ca54226 to disappear
Feb 17 19:11:25.161: INFO: Pod downwardapi-volume-e17b8125-62ca-4bfa-8ab5-3cdd2ca54226 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:11:25.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7468" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":278,"completed":249,"skipped":4084,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:11:25.175: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9845
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1788
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 19:11:25.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9845'
Feb 17 19:11:25.411: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 17 19:11:25.411: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1793
Feb 17 19:11:25.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete jobs e2e-test-httpd-job --namespace=kubectl-9845'
Feb 17 19:11:25.492: INFO: stderr: ""
Feb 17 19:11:25.492: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:11:25.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9845" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":278,"completed":250,"skipped":4092,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:11:25.511: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-1716cd62-574d-444e-96b5-b466f66d822e
STEP: Creating a pod to test consume configMaps
Feb 17 19:11:25.679: INFO: Waiting up to 5m0s for pod "pod-configmaps-31c4bbc4-18f4-415d-9cac-c3e7298f63f5" in namespace "configmap-3002" to be "success or failure"
Feb 17 19:11:25.683: INFO: Pod "pod-configmaps-31c4bbc4-18f4-415d-9cac-c3e7298f63f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.51934ms
Feb 17 19:11:27.689: INFO: Pod "pod-configmaps-31c4bbc4-18f4-415d-9cac-c3e7298f63f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009627024s
STEP: Saw pod success
Feb 17 19:11:27.689: INFO: Pod "pod-configmaps-31c4bbc4-18f4-415d-9cac-c3e7298f63f5" satisfied condition "success or failure"
Feb 17 19:11:27.694: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod pod-configmaps-31c4bbc4-18f4-415d-9cac-c3e7298f63f5 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 19:11:27.720: INFO: Waiting for pod pod-configmaps-31c4bbc4-18f4-415d-9cac-c3e7298f63f5 to disappear
Feb 17 19:11:27.724: INFO: Pod pod-configmaps-31c4bbc4-18f4-415d-9cac-c3e7298f63f5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:11:27.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3002" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":251,"skipped":4108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:11:27.747: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2304
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-6xs6
STEP: Creating a pod to test atomic-volume-subpath
Feb 17 19:11:27.925: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-6xs6" in namespace "subpath-2304" to be "success or failure"
Feb 17 19:11:27.929: INFO: Pod "pod-subpath-test-downwardapi-6xs6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.783916ms
Feb 17 19:11:29.935: INFO: Pod "pod-subpath-test-downwardapi-6xs6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00949429s
Feb 17 19:11:31.940: INFO: Pod "pod-subpath-test-downwardapi-6xs6": Phase="Running", Reason="", readiness=true. Elapsed: 4.014909581s
Feb 17 19:11:33.946: INFO: Pod "pod-subpath-test-downwardapi-6xs6": Phase="Running", Reason="", readiness=true. Elapsed: 6.02048772s
Feb 17 19:11:35.951: INFO: Pod "pod-subpath-test-downwardapi-6xs6": Phase="Running", Reason="", readiness=true. Elapsed: 8.026126746s
Feb 17 19:11:37.956: INFO: Pod "pod-subpath-test-downwardapi-6xs6": Phase="Running", Reason="", readiness=true. Elapsed: 10.030785732s
Feb 17 19:11:39.962: INFO: Pod "pod-subpath-test-downwardapi-6xs6": Phase="Running", Reason="", readiness=true. Elapsed: 12.037455225s
Feb 17 19:11:41.968: INFO: Pod "pod-subpath-test-downwardapi-6xs6": Phase="Running", Reason="", readiness=true. Elapsed: 14.042744704s
Feb 17 19:11:43.973: INFO: Pod "pod-subpath-test-downwardapi-6xs6": Phase="Running", Reason="", readiness=true. Elapsed: 16.047648054s
Feb 17 19:11:45.978: INFO: Pod "pod-subpath-test-downwardapi-6xs6": Phase="Running", Reason="", readiness=true. Elapsed: 18.052758634s
Feb 17 19:11:47.983: INFO: Pod "pod-subpath-test-downwardapi-6xs6": Phase="Running", Reason="", readiness=true. Elapsed: 20.058348012s
Feb 17 19:11:49.988: INFO: Pod "pod-subpath-test-downwardapi-6xs6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.063261407s
STEP: Saw pod success
Feb 17 19:11:49.988: INFO: Pod "pod-subpath-test-downwardapi-6xs6" satisfied condition "success or failure"
Feb 17 19:11:49.992: INFO: Trying to get logs from node ip-172-16-86-181.ec2.internal pod pod-subpath-test-downwardapi-6xs6 container test-container-subpath-downwardapi-6xs6: <nil>
STEP: delete the pod
Feb 17 19:11:50.020: INFO: Waiting for pod pod-subpath-test-downwardapi-6xs6 to disappear
Feb 17 19:11:50.026: INFO: Pod pod-subpath-test-downwardapi-6xs6 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-6xs6
Feb 17 19:11:50.026: INFO: Deleting pod "pod-subpath-test-downwardapi-6xs6" in namespace "subpath-2304"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:11:50.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2304" for this suite.

• [SLOW TEST:22.297 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":278,"completed":252,"skipped":4130,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:11:50.043: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6581
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:12:03.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6581" for this suite.

• [SLOW TEST:13.276 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":278,"completed":253,"skipped":4153,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:12:03.319: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Feb 17 19:12:03.476: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 17 19:12:03.496: INFO: Waiting for terminating namespaces to be deleted...
Feb 17 19:12:03.500: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-58-212.ec2.internal before test
Feb 17 19:12:03.516: INFO: canal-v8cfc from kube-system started at 2020-02-17 17:53:10 +0000 UTC (4 container statuses recorded)
Feb 17 19:12:03.516: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 19:12:03.516: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 17 19:12:03.516: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Feb 17 19:12:03.516: INFO: 	Container update-network-condition ready: true, restart count 0
Feb 17 19:12:03.516: INFO: node-local-dns-rvfkr from kube-system started at 2020-02-17 17:53:20 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.516: INFO: 	Container node-cache ready: true, restart count 0
Feb 17 19:12:03.516: INFO: sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-zlh59 from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:03.516: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 17 19:12:03.516: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 19:12:03.516: INFO: k8s-api-haproxy-b9ba254b202c3ae80f0c096eb09e3391b752ed191356ffd2fb26712eff44ce1b-ip-172-16-58-212.ec2.internal from kube-system started at 2020-02-17 17:52:08 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.516: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Feb 17 19:12:03.516: INFO: kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-58-212.ec2.internal from kube-system started at 2020-02-17 17:52:08 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.516: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 19:12:03.516: INFO: kublr-node-name-reporter-2e4a883ef03ca181a5002964babb2c122cef2ef3f405e26061a11af75a5d837f-ip-172-16-58-212.ec2.internal from kube-system started at 2020-02-17 17:52:08 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.516: INFO: 	Container main ready: true, restart count 0
Feb 17 19:12:03.516: INFO: coredns-6c578c8bf7-88w67 from kube-system started at 2020-02-17 17:53:27 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.516: INFO: 	Container coredns ready: true, restart count 0
Feb 17 19:12:03.516: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-71-125.ec2.internal before test
Feb 17 19:12:03.535: INFO: canal-9jpnm from kube-system started at 2020-02-17 17:53:09 +0000 UTC (4 container statuses recorded)
Feb 17 19:12:03.535: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 19:12:03.535: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 17 19:12:03.535: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Feb 17 19:12:03.535: INFO: 	Container update-network-condition ready: true, restart count 0
Feb 17 19:12:03.535: INFO: kublr-system-shell-5b9bd5c865-lmlsp from kube-system started at 2020-02-17 17:54:02 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.535: INFO: 	Container shell ready: true, restart count 0
Feb 17 19:12:03.535: INFO: sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-d99zx from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:03.535: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 17 19:12:03.535: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 19:12:03.535: INFO: kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-71-125.ec2.internal from kube-system started at 2020-02-17 17:51:59 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.535: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 19:12:03.535: INFO: k8s-api-haproxy-92dea82227153ce8905e2adce84945a2a46a1ff902bbfb5846f597d0315ec0e1-ip-172-16-71-125.ec2.internal from kube-system started at 2020-02-17 17:51:59 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.535: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Feb 17 19:12:03.535: INFO: sonobuoy from sonobuoy started at 2020-02-17 18:03:16 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.535: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 17 19:12:03.535: INFO: sonobuoy-e2e-job-20e37349fda64c29 from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:03.535: INFO: 	Container e2e ready: true, restart count 0
Feb 17 19:12:03.535: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 19:12:03.535: INFO: kublr-node-name-reporter-3cd39b06f3a526166a2bdbb7d0aa396f06e89576f16de214708f3a78d146fdd2-ip-172-16-71-125.ec2.internal from kube-system started at 2020-02-17 17:51:59 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.535: INFO: 	Container main ready: true, restart count 0
Feb 17 19:12:03.535: INFO: node-local-dns-b4j9f from kube-system started at 2020-02-17 17:53:19 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.535: INFO: 	Container node-cache ready: true, restart count 0
Feb 17 19:12:03.535: INFO: 
Logging pods the kubelet thinks is on node ip-172-16-86-181.ec2.internal before test
Feb 17 19:12:03.543: INFO: kube-proxy-68804c3ed9b62448ba98b520700f520f8b710bfa2c12efd791c73311feb78209-ip-172-16-86-181.ec2.internal from kube-system started at 2020-02-17 17:51:50 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.543: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 19:12:03.543: INFO: kublr-node-name-reporter-d9895b0065d8993619f85e1df198fa878363f17b2ecacbc683c208bcec5934f7-ip-172-16-86-181.ec2.internal from kube-system started at 2020-02-17 17:51:50 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.543: INFO: 	Container main ready: true, restart count 0
Feb 17 19:12:03.543: INFO: coredns-6c578c8bf7-bwb2d from kube-system started at 2020-02-17 17:53:34 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.543: INFO: 	Container coredns ready: true, restart count 0
Feb 17 19:12:03.543: INFO: metrics-server-v0.3.6-7b66f9c8dc-n74zk from kube-system started at 2020-02-17 17:53:36 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:03.543: INFO: 	Container metrics-server ready: true, restart count 0
Feb 17 19:12:03.543: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Feb 17 19:12:03.543: INFO: k8s-api-haproxy-b3a73b75b08bcd3ee17799fee24d51365c32935f7350cdcde3e2adb2935e8f65-ip-172-16-86-181.ec2.internal from kube-system started at 2020-02-17 17:51:50 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.543: INFO: 	Container k8s-api-haproxy ready: true, restart count 0
Feb 17 19:12:03.543: INFO: canal-ng4fk from kube-system started at 2020-02-17 17:53:14 +0000 UTC (4 container statuses recorded)
Feb 17 19:12:03.543: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 19:12:03.543: INFO: 	Container kube-flannel ready: true, restart count 0
Feb 17 19:12:03.543: INFO: 	Container monitor-blackhole-route ready: true, restart count 0
Feb 17 19:12:03.543: INFO: 	Container update-network-condition ready: true, restart count 0
Feb 17 19:12:03.543: INFO: node-local-dns-7prxg from kube-system started at 2020-02-17 17:53:34 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:03.543: INFO: 	Container node-cache ready: true, restart count 0
Feb 17 19:12:03.543: INFO: sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-q7qcp from sonobuoy started at 2020-02-17 18:03:21 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:03.543: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 17 19:12:03.543: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-28d120e6-2b40-4b5d-86d4-51ddc8d6a95a 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-28d120e6-2b40-4b5d-86d4-51ddc8d6a95a off the node ip-172-16-58-212.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-28d120e6-2b40-4b5d-86d4-51ddc8d6a95a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:12:17.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2690" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:14.365 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":278,"completed":254,"skipped":4172,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:12:17.684: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Feb 17 19:12:21.863: INFO: Pod pod-hostip-e3ecbf6e-0e89-4668-9c0c-2dfdff39c115 has hostIP: 172.16.86.181
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:12:21.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6186" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":278,"completed":255,"skipped":4180,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:12:21.883: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5662
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 19:12:22.954: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 19:12:24.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563542, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563542, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563543, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563542, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 19:12:27.996: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:12:29.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5662" for this suite.
STEP: Destroying namespace "webhook-5662-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.309 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":278,"completed":256,"skipped":4187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:12:29.192: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9776
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:12:45.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9776" for this suite.

• [SLOW TEST:16.248 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":278,"completed":257,"skipped":4232,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:12:45.440: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6768
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 19:12:45.593: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:12:51.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6768" for this suite.

• [SLOW TEST:5.758 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:47
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":278,"completed":258,"skipped":4246,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:12:51.198: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5296
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 19:12:51.425: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf0029b6-6dbe-4691-b639-45a6eb667e3d" in namespace "downward-api-5296" to be "success or failure"
Feb 17 19:12:51.432: INFO: Pod "downwardapi-volume-cf0029b6-6dbe-4691-b639-45a6eb667e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.662471ms
Feb 17 19:12:53.438: INFO: Pod "downwardapi-volume-cf0029b6-6dbe-4691-b639-45a6eb667e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012540674s
Feb 17 19:12:55.444: INFO: Pod "downwardapi-volume-cf0029b6-6dbe-4691-b639-45a6eb667e3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018217565s
STEP: Saw pod success
Feb 17 19:12:55.444: INFO: Pod "downwardapi-volume-cf0029b6-6dbe-4691-b639-45a6eb667e3d" satisfied condition "success or failure"
Feb 17 19:12:55.448: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downwardapi-volume-cf0029b6-6dbe-4691-b639-45a6eb667e3d container client-container: <nil>
STEP: delete the pod
Feb 17 19:12:55.476: INFO: Waiting for pod downwardapi-volume-cf0029b6-6dbe-4691-b639-45a6eb667e3d to disappear
Feb 17 19:12:55.479: INFO: Pod downwardapi-volume-cf0029b6-6dbe-4691-b639-45a6eb667e3d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:12:55.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5296" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":278,"completed":259,"skipped":4248,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:12:55.495: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 19:12:56.038: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 19:12:58.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563576, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563576, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563576, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563576, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 19:13:01.080: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Feb 17 19:13:06.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 attach --namespace=webhook-1973 to-be-attached-pod -i -c=container1'
Feb 17 19:13:06.214: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:13:06.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1973" for this suite.
STEP: Destroying namespace "webhook-1973-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.830 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":278,"completed":260,"skipped":4263,"failed":0}
S
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:13:06.325: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4773
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:13:31.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4773" for this suite.

• [SLOW TEST:25.505 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":278,"completed":261,"skipped":4264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:13:31.831: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5626
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Feb 17 19:13:31.985: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Feb 17 19:13:31.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 create -f - --namespace=kubectl-5626'
Feb 17 19:13:32.906: INFO: stderr: ""
Feb 17 19:13:32.906: INFO: stdout: "service/agnhost-slave created\n"
Feb 17 19:13:32.906: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Feb 17 19:13:32.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 create -f - --namespace=kubectl-5626'
Feb 17 19:13:33.121: INFO: stderr: ""
Feb 17 19:13:33.121: INFO: stdout: "service/agnhost-master created\n"
Feb 17 19:13:33.121: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 17 19:13:33.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 create -f - --namespace=kubectl-5626'
Feb 17 19:13:33.274: INFO: stderr: ""
Feb 17 19:13:33.274: INFO: stdout: "service/frontend created\n"
Feb 17 19:13:33.275: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Feb 17 19:13:33.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 create -f - --namespace=kubectl-5626'
Feb 17 19:13:33.471: INFO: stderr: ""
Feb 17 19:13:33.471: INFO: stdout: "deployment.apps/frontend created\n"
Feb 17 19:13:33.471: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 17 19:13:33.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 create -f - --namespace=kubectl-5626'
Feb 17 19:13:33.676: INFO: stderr: ""
Feb 17 19:13:33.676: INFO: stdout: "deployment.apps/agnhost-master created\n"
Feb 17 19:13:33.676: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 17 19:13:33.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 create -f - --namespace=kubectl-5626'
Feb 17 19:13:33.876: INFO: stderr: ""
Feb 17 19:13:33.876: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Feb 17 19:13:33.876: INFO: Waiting for all frontend pods to be Running.
Feb 17 19:13:38.927: INFO: Waiting for frontend to serve content.
Feb 17 19:13:43.955: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
Feb 17 19:13:49.993: INFO: Trying to add a new entry to the guestbook.
Feb 17 19:13:50.006: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 17 19:13:50.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete --grace-period=0 --force -f - --namespace=kubectl-5626'
Feb 17 19:13:50.115: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 19:13:50.115: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 17 19:13:50.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete --grace-period=0 --force -f - --namespace=kubectl-5626'
Feb 17 19:13:50.210: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 19:13:50.210: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 17 19:13:50.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete --grace-period=0 --force -f - --namespace=kubectl-5626'
Feb 17 19:13:50.305: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 19:13:50.305: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 17 19:13:50.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete --grace-period=0 --force -f - --namespace=kubectl-5626'
Feb 17 19:13:50.379: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 19:13:50.379: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 17 19:13:50.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete --grace-period=0 --force -f - --namespace=kubectl-5626'
Feb 17 19:13:50.455: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 19:13:50.455: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 17 19:13:50.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete --grace-period=0 --force -f - --namespace=kubectl-5626'
Feb 17 19:13:50.526: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 19:13:50.526: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:13:50.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5626" for this suite.

• [SLOW TEST:18.712 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:386
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":278,"completed":262,"skipped":4287,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:13:50.542: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-3225
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 17 19:13:52.013: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 17 19:13:54.027: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563632, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563632, loc:(*time.Location)(0x7db7bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563632, loc:(*time.Location)(0x7db7bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563632, loc:(*time.Location)(0x7db7bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 19:13:57.055: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 19:13:57.060: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 19:14:32.698: INFO: error waiting for conversion to succeed during setup: conversion webhook for stable.example.com/v2, Kind=E2e-test-crd-webhook-4744-crd failed: Post https://e2e-test-crd-conversion-webhook.crd-webhook-3225.svc:9443/crdconvert?timeout=30s: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:14:34.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3225" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

• [SLOW TEST:43.927 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":278,"completed":263,"skipped":4292,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:14:34.470: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4386
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Feb 17 19:14:34.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 create -f - --namespace=kubectl-4386'
Feb 17 19:14:34.909: INFO: stderr: ""
Feb 17 19:14:34.909: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 19:14:34.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4386'
Feb 17 19:14:34.980: INFO: stderr: ""
Feb 17 19:14:34.980: INFO: stdout: "update-demo-nautilus-9wjhm update-demo-nautilus-vds68 "
Feb 17 19:14:34.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-9wjhm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4386'
Feb 17 19:14:35.083: INFO: stderr: ""
Feb 17 19:14:35.083: INFO: stdout: ""
Feb 17 19:14:35.083: INFO: update-demo-nautilus-9wjhm is created but not running
Feb 17 19:14:40.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4386'
Feb 17 19:14:40.150: INFO: stderr: ""
Feb 17 19:14:40.150: INFO: stdout: "update-demo-nautilus-9wjhm update-demo-nautilus-vds68 "
Feb 17 19:14:40.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-9wjhm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4386'
Feb 17 19:14:40.211: INFO: stderr: ""
Feb 17 19:14:40.211: INFO: stdout: "true"
Feb 17 19:14:40.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-9wjhm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4386'
Feb 17 19:14:40.275: INFO: stderr: ""
Feb 17 19:14:40.275: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 19:14:40.276: INFO: validating pod update-demo-nautilus-9wjhm
Feb 17 19:14:40.282: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 19:14:40.282: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 19:14:40.282: INFO: update-demo-nautilus-9wjhm is verified up and running
Feb 17 19:14:40.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-vds68 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4386'
Feb 17 19:14:40.347: INFO: stderr: ""
Feb 17 19:14:40.347: INFO: stdout: "true"
Feb 17 19:14:40.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods update-demo-nautilus-vds68 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4386'
Feb 17 19:14:40.411: INFO: stderr: ""
Feb 17 19:14:40.411: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 19:14:40.411: INFO: validating pod update-demo-nautilus-vds68
Feb 17 19:14:40.418: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 19:14:40.418: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 19:14:40.418: INFO: update-demo-nautilus-vds68 is verified up and running
STEP: using delete to clean up resources
Feb 17 19:14:40.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 delete --grace-period=0 --force -f - --namespace=kubectl-4386'
Feb 17 19:14:40.501: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 19:14:40.501: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 17 19:14:40.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4386'
Feb 17 19:14:40.576: INFO: stderr: "No resources found in kubectl-4386 namespace.\n"
Feb 17 19:14:40.576: INFO: stdout: ""
Feb 17 19:14:40.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 get pods -l name=update-demo --namespace=kubectl-4386 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 17 19:14:40.649: INFO: stderr: ""
Feb 17 19:14:40.649: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:14:40.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4386" for this suite.

• [SLOW TEST:6.205 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:328
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":278,"completed":264,"skipped":4300,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:14:40.675: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3094
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-3094
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 17 19:14:40.870: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 17 19:15:03.011: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.222:8080/dial?request=hostname&protocol=http&host=100.96.1.221&port=8080&tries=1'] Namespace:pod-network-test-3094 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 19:15:03.011: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 19:15:03.141: INFO: Waiting for responses: map[]
Feb 17 19:15:03.146: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.222:8080/dial?request=hostname&protocol=http&host=100.96.0.54&port=8080&tries=1'] Namespace:pod-network-test-3094 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 19:15:03.146: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 19:15:03.269: INFO: Waiting for responses: map[]
Feb 17 19:15:03.274: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.222:8080/dial?request=hostname&protocol=http&host=100.96.5.103&port=8080&tries=1'] Namespace:pod-network-test-3094 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 19:15:03.274: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
Feb 17 19:15:03.397: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:15:03.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3094" for this suite.

• [SLOW TEST:22.738 seconds]
[sig-network] Networking
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":265,"skipped":4317,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:15:03.413: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4393
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 19:15:03.568: INFO: Creating deployment "webserver-deployment"
Feb 17 19:15:03.576: INFO: Waiting for observed generation 1
Feb 17 19:15:05.590: INFO: Waiting for all required pods to come up
Feb 17 19:15:05.596: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 17 19:15:07.611: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 17 19:15:07.618: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 17 19:15:07.628: INFO: Updating deployment webserver-deployment
Feb 17 19:15:07.628: INFO: Waiting for observed generation 2
Feb 17 19:15:09.644: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 17 19:15:09.648: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 17 19:15:09.651: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 17 19:15:09.663: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 17 19:15:09.663: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 17 19:15:09.667: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 17 19:15:09.675: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 17 19:15:09.675: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 17 19:15:09.692: INFO: Updating deployment webserver-deployment
Feb 17 19:15:09.692: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 17 19:15:09.709: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 17 19:15:09.716: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Feb 17 19:15:09.726: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4393 /apis/apps/v1/namespaces/deployment-4393/deployments/webserver-deployment e8110d54-49bc-47f1-bd23-412e0fff0f25 32739 3 2020-02-17 19:15:03 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002def258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-02-17 19:15:07 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-02-17 19:15:09 +0000 UTC,LastTransitionTime:2020-02-17 19:15:09 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 17 19:15:09.744: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-4393 /apis/apps/v1/namespaces/deployment-4393/replicasets/webserver-deployment-c7997dcc8 58373d7e-51de-4be4-a998-ef6ce65a30bd 32737 3 2020-02-17 19:15:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e8110d54-49bc-47f1-bd23-412e0fff0f25 0xc002def7a7 0xc002def7a8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002def818 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 19:15:09.744: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 17 19:15:09.744: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-4393 /apis/apps/v1/namespaces/deployment-4393/replicasets/webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 32735 3 2020-02-17 19:15:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e8110d54-49bc-47f1-bd23-412e0fff0f25 0xc002def6b7 0xc002def6b8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002def748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 17 19:15:09.787: INFO: Pod "webserver-deployment-595b5b9587-2hk88" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2hk88 webserver-deployment-595b5b9587- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-595b5b9587-2hk88 c698481e-3121-4afd-bf56-08ff1e77e8f2 32588 0 2020-02-17 19:15:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.96.5.105/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 0xc003016cd7 0xc003016cd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-86-181.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.86.181,PodIP:100.96.5.105,StartTime:2020-02-17 19:15:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 19:15:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a429ec2d25aeb21bdaf2eaf401685544398e44dfb856ce991575f67ee77fbac7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.5.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.787: INFO: Pod "webserver-deployment-595b5b9587-5pjpf" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5pjpf webserver-deployment-595b5b9587- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-595b5b9587-5pjpf ff4a2a87-b417-4370-aafa-655e52d61d51 32617 0 2020-02-17 19:15:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.96.0.55/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 0xc003016e57 0xc003016e58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-71-125.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.71.125,PodIP:100.96.0.55,StartTime:2020-02-17 19:15:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 19:15:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://7e1e0e1eb513c17d141687d2151442894d36fca4b76f72906cf86d9073f38780,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.787: INFO: Pod "webserver-deployment-595b5b9587-6lbp8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6lbp8 webserver-deployment-595b5b9587- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-595b5b9587-6lbp8 184ffe7d-d404-4edb-be70-1c7475f58179 32612 0 2020-02-17 19:15:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.96.0.57/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 0xc003016fd0 0xc003016fd1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-71-125.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.71.125,PodIP:100.96.0.57,StartTime:2020-02-17 19:15:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 19:15:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ac97f859bff9fe774bbe320723c68ae372842d81514c3a3a7b36ecf0942a29e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.787: INFO: Pod "webserver-deployment-595b5b9587-8xf7h" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8xf7h webserver-deployment-595b5b9587- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-595b5b9587-8xf7h c0aa7fbe-5231-4cce-9b12-6daef30253cc 32749 0 2020-02-17 19:15:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 0xc003017130 0xc003017131}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.788: INFO: Pod "webserver-deployment-595b5b9587-d82tb" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-d82tb webserver-deployment-595b5b9587- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-595b5b9587-d82tb 9089beb1-e815-4df6-a82c-8c333576d531 32751 0 2020-02-17 19:15:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 0xc003017217 0xc003017218}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.788: INFO: Pod "webserver-deployment-595b5b9587-gpljg" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-gpljg webserver-deployment-595b5b9587- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-595b5b9587-gpljg 319296b8-2068-4037-ad9f-b962120df0be 32747 0 2020-02-17 19:15:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 0xc003017307 0xc003017308}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-58-212.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.788: INFO: Pod "webserver-deployment-595b5b9587-jgnzt" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jgnzt webserver-deployment-595b5b9587- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-595b5b9587-jgnzt f0ad3fdf-1a9d-4488-9e09-0528707d20ae 32742 0 2020-02-17 19:15:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 0xc003017410 0xc003017411}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-58-212.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.788: INFO: Pod "webserver-deployment-595b5b9587-nps94" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-nps94 webserver-deployment-595b5b9587- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-595b5b9587-nps94 7de50030-9f05-497c-b307-1f5af360cf16 32755 0 2020-02-17 19:15:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 0xc003017510 0xc003017511}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.788: INFO: Pod "webserver-deployment-595b5b9587-pffdj" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pffdj webserver-deployment-595b5b9587- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-595b5b9587-pffdj 5834727d-d69e-4d50-8602-f1fe17b8e42d 32591 0 2020-02-17 19:15:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.96.5.104/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 0xc003017607 0xc003017608}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-86-181.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.86.181,PodIP:100.96.5.104,StartTime:2020-02-17 19:15:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 19:15:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d06c5355352c6c54753a43418d28cf36bc4d976739719d753bcf31424b490f8a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.5.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.788: INFO: Pod "webserver-deployment-595b5b9587-qdqlk" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qdqlk webserver-deployment-595b5b9587- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-595b5b9587-qdqlk 3c60c385-9cba-4671-90cc-b05f2a79475b 32614 0 2020-02-17 19:15:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.96.0.56/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 0xc003017787 0xc003017788}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-71-125.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.71.125,PodIP:100.96.0.56,StartTime:2020-02-17 19:15:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 19:15:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://952b6165dd2cdfc364bdd35bc71eac06a5fb012a653f67415e22ee2c3fde0af6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.788: INFO: Pod "webserver-deployment-595b5b9587-qk897" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qk897 webserver-deployment-595b5b9587- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-595b5b9587-qk897 d25d9c4a-800b-4238-9d7c-d16c3b7c6f40 32603 0 2020-02-17 19:15:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.96.1.223/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 0xc003017900 0xc003017901}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-58-212.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.58.212,PodIP:100.96.1.223,StartTime:2020-02-17 19:15:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 19:15:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a233435c85e2ee288e93cd005983f98485851f9b84b7aaf06bcd488ab8b90a83,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.788: INFO: Pod "webserver-deployment-595b5b9587-qq9pk" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qq9pk webserver-deployment-595b5b9587- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-595b5b9587-qq9pk a2293718-4122-4cc6-82b7-48cb9979f79f 32744 0 2020-02-17 19:15:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 0xc003017a67 0xc003017a68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.789: INFO: Pod "webserver-deployment-595b5b9587-tprsp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tprsp webserver-deployment-595b5b9587- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-595b5b9587-tprsp 7a2ac187-8a19-4637-8533-0c9f8b0bdb14 32594 0 2020-02-17 19:15:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.96.5.106/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 0xc003017b67 0xc003017b68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-86-181.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.86.181,PodIP:100.96.5.106,StartTime:2020-02-17 19:15:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 19:15:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8aa147a10f8d6f7f9b88c7d869a879e1df04d9035d59180963f9d44865463f74,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.5.106,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.789: INFO: Pod "webserver-deployment-595b5b9587-z6zp9" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-z6zp9 webserver-deployment-595b5b9587- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-595b5b9587-z6zp9 de297a4e-2f68-4d69-86dc-8bb424422188 32601 0 2020-02-17 19:15:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.96.1.225/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 0xc003017ce7 0xc003017ce8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-58-212.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.58.212,PodIP:100.96.1.225,StartTime:2020-02-17 19:15:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 19:15:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9efc3158f81758622e13139fbce05aac12a89978e69050f170467fd8ff95cf83,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.225,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.789: INFO: Pod "webserver-deployment-595b5b9587-zd8tp" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zd8tp webserver-deployment-595b5b9587- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-595b5b9587-zd8tp 810422ba-3aeb-4f45-949a-e18330bbf412 32753 0 2020-02-17 19:15:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 4ecc70eb-c717-47a0-87d9-4856409fcab1 0xc003017e57 0xc003017e58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.789: INFO: Pod "webserver-deployment-c7997dcc8-2dhnj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-2dhnj webserver-deployment-c7997dcc8- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-c7997dcc8-2dhnj 3770986a-fc0c-422a-b17e-0afa4b2d6875 32709 0 2020-02-17 19:15:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.96.5.107/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 58373d7e-51de-4be4-a998-ef6ce65a30bd 0xc003017f57 0xc003017f58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-86-181.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.86.181,PodIP:,StartTime:2020-02-17 19:15:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.789: INFO: Pod "webserver-deployment-c7997dcc8-48cxx" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-48cxx webserver-deployment-c7997dcc8- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-c7997dcc8-48cxx baaf94b5-ffb9-4501-bdd9-8ba6e20415e1 32746 0 2020-02-17 19:15:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 58373d7e-51de-4be4-a998-ef6ce65a30bd 0xc0038000c7 0xc0038000c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.789: INFO: Pod "webserver-deployment-c7997dcc8-hxrhh" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-hxrhh webserver-deployment-c7997dcc8- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-c7997dcc8-hxrhh 69e0b548-adb0-4bb0-8af2-aeabf7593042 32732 0 2020-02-17 19:15:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.96.0.58/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 58373d7e-51de-4be4-a998-ef6ce65a30bd 0xc0038001d7 0xc0038001d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-71-125.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.71.125,PodIP:100.96.0.58,StartTime:2020-02-17 19:15:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login',},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.789: INFO: Pod "webserver-deployment-c7997dcc8-nzl5l" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-nzl5l webserver-deployment-c7997dcc8- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-c7997dcc8-nzl5l ed90fb93-d020-4017-9a24-f3213b60313b 32723 0 2020-02-17 19:15:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.96.1.227/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 58373d7e-51de-4be4-a998-ef6ce65a30bd 0xc0038004f0 0xc0038004f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-58-212.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.58.212,PodIP:,StartTime:2020-02-17 19:15:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.789: INFO: Pod "webserver-deployment-c7997dcc8-sbhdt" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-sbhdt webserver-deployment-c7997dcc8- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-c7997dcc8-sbhdt 144df3cb-34f4-435c-94f7-f57889483ff9 32724 0 2020-02-17 19:15:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.96.5.108/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 58373d7e-51de-4be4-a998-ef6ce65a30bd 0xc003800667 0xc003800668}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-86-181.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.86.181,PodIP:,StartTime:2020-02-17 19:15:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.789: INFO: Pod "webserver-deployment-c7997dcc8-skzv6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-skzv6 webserver-deployment-c7997dcc8- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-c7997dcc8-skzv6 5b601ec5-accc-4f31-bbf8-362f3b916166 32752 0 2020-02-17 19:15:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 58373d7e-51de-4be4-a998-ef6ce65a30bd 0xc0038007d7 0xc0038007d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.790: INFO: Pod "webserver-deployment-c7997dcc8-w4bsv" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-w4bsv webserver-deployment-c7997dcc8- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-c7997dcc8-w4bsv c49ff1c4-4bcd-411f-a933-4996a17574fe 32748 0 2020-02-17 19:15:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 58373d7e-51de-4be4-a998-ef6ce65a30bd 0xc0038008d7 0xc0038008d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-71-125.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 19:15:09.790: INFO: Pod "webserver-deployment-c7997dcc8-zv2qd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zv2qd webserver-deployment-c7997dcc8- deployment-4393 /api/v1/namespaces/deployment-4393/pods/webserver-deployment-c7997dcc8-zv2qd 299f7c3f-7b50-4e72-aa59-753defbf8120 32729 0 2020-02-17 19:15:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.96.1.228/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 58373d7e-51de-4be4-a998-ef6ce65a30bd 0xc003800a00 0xc003800a01}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wbvgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wbvgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wbvgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-16-58-212.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:15:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.58.212,PodIP:,StartTime:2020-02-17 19:15:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:15:09.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4393" for this suite.

• [SLOW TEST:6.478 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":278,"completed":266,"skipped":4324,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:15:09.892: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4290
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:15:10.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4290" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":278,"completed":267,"skipped":4339,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:15:10.221: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6430
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6430.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6430.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6430.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6430.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6430.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6430.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 19:15:16.479: INFO: DNS probes using dns-6430/dns-test-75563985-bc33-466d-9790-db5bd75586cc succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:15:16.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6430" for this suite.

• [SLOW TEST:6.338 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":278,"completed":268,"skipped":4385,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:15:16.560: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8606
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8606 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8606;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8606 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8606;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8606.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8606.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8606.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8606.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8606.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8606.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8606.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8606.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8606.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8606.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8606.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8606.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8606.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 72.41.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.41.72_udp@PTR;check="$$(dig +tcp +noall +answer +search 72.41.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.41.72_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8606 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8606;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8606 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8606;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8606.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8606.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8606.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8606.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8606.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8606.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8606.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8606.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8606.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8606.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8606.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8606.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8606.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 72.41.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.41.72_udp@PTR;check="$$(dig +tcp +noall +answer +search 72.41.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.41.72_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 19:15:20.914: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:20.919: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:20.929: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:20.938: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:20.944: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:20.949: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:20.983: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:20.987: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:20.992: INFO: Unable to read jessie_udp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:20.996: INFO: Unable to read jessie_tcp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:21.001: INFO: Unable to read jessie_udp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:21.006: INFO: Unable to read jessie_tcp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:21.010: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:21.015: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:21.043: INFO: Lookups using dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.dns-8606 wheezy_tcp@dns-test-service.dns-8606.svc wheezy_udp@_http._tcp.dns-test-service.dns-8606.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8606.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8606 jessie_tcp@dns-test-service.dns-8606 jessie_udp@dns-test-service.dns-8606.svc jessie_tcp@dns-test-service.dns-8606.svc jessie_udp@_http._tcp.dns-test-service.dns-8606.svc jessie_tcp@_http._tcp.dns-test-service.dns-8606.svc]

Feb 17 19:15:26.050: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:26.055: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:26.065: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:26.074: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:26.083: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:26.117: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:26.122: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:26.126: INFO: Unable to read jessie_udp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:26.131: INFO: Unable to read jessie_tcp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:26.136: INFO: Unable to read jessie_udp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:26.141: INFO: Unable to read jessie_tcp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:26.145: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:26.150: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:26.177: INFO: Lookups using dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.dns-8606 wheezy_tcp@dns-test-service.dns-8606.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8606.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8606 jessie_tcp@dns-test-service.dns-8606 jessie_udp@dns-test-service.dns-8606.svc jessie_tcp@dns-test-service.dns-8606.svc jessie_udp@_http._tcp.dns-test-service.dns-8606.svc jessie_tcp@_http._tcp.dns-test-service.dns-8606.svc]

Feb 17 19:15:31.050: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:31.055: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:31.064: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:31.074: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:31.083: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:31.117: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:31.122: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:31.127: INFO: Unable to read jessie_udp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:31.132: INFO: Unable to read jessie_tcp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:31.140: INFO: Unable to read jessie_udp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:31.145: INFO: Unable to read jessie_tcp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:31.150: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:31.155: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:31.183: INFO: Lookups using dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.dns-8606 wheezy_tcp@dns-test-service.dns-8606.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8606.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8606 jessie_tcp@dns-test-service.dns-8606 jessie_udp@dns-test-service.dns-8606.svc jessie_tcp@dns-test-service.dns-8606.svc jessie_udp@_http._tcp.dns-test-service.dns-8606.svc jessie_tcp@_http._tcp.dns-test-service.dns-8606.svc]

Feb 17 19:15:36.050: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:36.056: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:36.065: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:36.075: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:36.085: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:36.120: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:36.124: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:36.130: INFO: Unable to read jessie_udp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:36.136: INFO: Unable to read jessie_tcp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:36.141: INFO: Unable to read jessie_udp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:36.147: INFO: Unable to read jessie_tcp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:36.152: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:36.156: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:36.185: INFO: Lookups using dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.dns-8606 wheezy_tcp@dns-test-service.dns-8606.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8606.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8606 jessie_tcp@dns-test-service.dns-8606 jessie_udp@dns-test-service.dns-8606.svc jessie_tcp@dns-test-service.dns-8606.svc jessie_udp@_http._tcp.dns-test-service.dns-8606.svc jessie_tcp@_http._tcp.dns-test-service.dns-8606.svc]

Feb 17 19:15:41.051: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:41.055: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:41.066: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:41.075: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:41.087: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:41.129: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:41.136: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:41.140: INFO: Unable to read jessie_udp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:41.153: INFO: Unable to read jessie_tcp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:41.158: INFO: Unable to read jessie_udp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:41.163: INFO: Unable to read jessie_tcp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:41.167: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:41.172: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:41.201: INFO: Lookups using dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.dns-8606 wheezy_tcp@dns-test-service.dns-8606.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8606.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8606 jessie_tcp@dns-test-service.dns-8606 jessie_udp@dns-test-service.dns-8606.svc jessie_tcp@dns-test-service.dns-8606.svc jessie_udp@_http._tcp.dns-test-service.dns-8606.svc jessie_tcp@_http._tcp.dns-test-service.dns-8606.svc]

Feb 17 19:15:46.050: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:46.055: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:46.065: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:46.075: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:46.085: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:46.118: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:46.124: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:46.128: INFO: Unable to read jessie_udp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:46.134: INFO: Unable to read jessie_tcp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:46.139: INFO: Unable to read jessie_udp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:46.144: INFO: Unable to read jessie_tcp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:46.150: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:46.154: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:46.183: INFO: Lookups using dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.dns-8606 wheezy_tcp@dns-test-service.dns-8606.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8606.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8606 jessie_tcp@dns-test-service.dns-8606 jessie_udp@dns-test-service.dns-8606.svc jessie_tcp@dns-test-service.dns-8606.svc jessie_udp@_http._tcp.dns-test-service.dns-8606.svc jessie_tcp@_http._tcp.dns-test-service.dns-8606.svc]

Feb 17 19:15:51.051: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:51.056: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:51.066: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:51.077: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:51.087: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:51.122: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:51.126: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:51.131: INFO: Unable to read jessie_udp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:51.136: INFO: Unable to read jessie_tcp@dns-test-service.dns-8606 from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:51.141: INFO: Unable to read jessie_udp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:51.146: INFO: Unable to read jessie_tcp@dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:51.150: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:51.154: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8606.svc from pod dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363: the server could not find the requested resource (get pods dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363)
Feb 17 19:15:51.183: INFO: Lookups using dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.dns-8606 wheezy_tcp@dns-test-service.dns-8606.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8606.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8606 jessie_tcp@dns-test-service.dns-8606 jessie_udp@dns-test-service.dns-8606.svc jessie_tcp@dns-test-service.dns-8606.svc jessie_udp@_http._tcp.dns-test-service.dns-8606.svc jessie_tcp@_http._tcp.dns-test-service.dns-8606.svc]

Feb 17 19:15:56.184: INFO: DNS probes using dns-8606/dns-test-846a7941-7ede-42d6-bf8f-2b4a3e275363 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:15:56.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8606" for this suite.

• [SLOW TEST:39.740 seconds]
[sig-network] DNS
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":278,"completed":269,"skipped":4413,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:15:56.301: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7333
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 19:15:56.479: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e8b736b1-5bf9-424a-bfba-ae1da7dc12a2" in namespace "downward-api-7333" to be "success or failure"
Feb 17 19:15:56.491: INFO: Pod "downwardapi-volume-e8b736b1-5bf9-424a-bfba-ae1da7dc12a2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.434684ms
Feb 17 19:15:58.496: INFO: Pod "downwardapi-volume-e8b736b1-5bf9-424a-bfba-ae1da7dc12a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016396665s
Feb 17 19:16:00.502: INFO: Pod "downwardapi-volume-e8b736b1-5bf9-424a-bfba-ae1da7dc12a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023035657s
STEP: Saw pod success
Feb 17 19:16:00.502: INFO: Pod "downwardapi-volume-e8b736b1-5bf9-424a-bfba-ae1da7dc12a2" satisfied condition "success or failure"
Feb 17 19:16:00.507: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downwardapi-volume-e8b736b1-5bf9-424a-bfba-ae1da7dc12a2 container client-container: <nil>
STEP: delete the pod
Feb 17 19:16:00.546: INFO: Waiting for pod downwardapi-volume-e8b736b1-5bf9-424a-bfba-ae1da7dc12a2 to disappear
Feb 17 19:16:00.550: INFO: Pod downwardapi-volume-e8b736b1-5bf9-424a-bfba-ae1da7dc12a2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:16:00.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7333" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":278,"completed":270,"skipped":4423,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:16:00.567: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-218
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 19:16:00.731: INFO: Waiting up to 5m0s for pod "downwardapi-volume-86f109d3-7c73-4a04-a202-94d096f0da1c" in namespace "projected-218" to be "success or failure"
Feb 17 19:16:00.735: INFO: Pod "downwardapi-volume-86f109d3-7c73-4a04-a202-94d096f0da1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.548747ms
Feb 17 19:16:02.741: INFO: Pod "downwardapi-volume-86f109d3-7c73-4a04-a202-94d096f0da1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010787508s
Feb 17 19:16:04.747: INFO: Pod "downwardapi-volume-86f109d3-7c73-4a04-a202-94d096f0da1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016592139s
STEP: Saw pod success
Feb 17 19:16:04.747: INFO: Pod "downwardapi-volume-86f109d3-7c73-4a04-a202-94d096f0da1c" satisfied condition "success or failure"
Feb 17 19:16:04.752: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downwardapi-volume-86f109d3-7c73-4a04-a202-94d096f0da1c container client-container: <nil>
STEP: delete the pod
Feb 17 19:16:04.782: INFO: Waiting for pod downwardapi-volume-86f109d3-7c73-4a04-a202-94d096f0da1c to disappear
Feb 17 19:16:04.788: INFO: Pod downwardapi-volume-86f109d3-7c73-4a04-a202-94d096f0da1c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:16:04.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-218" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":278,"completed":271,"skipped":4438,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:16:04.804: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2847
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 19:16:04.999: INFO: Create a RollingUpdate DaemonSet
Feb 17 19:16:05.005: INFO: Check that daemon pods launch on every node of the cluster
Feb 17 19:16:05.016: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:05.016: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:05.016: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:05.020: INFO: Number of nodes with available pods: 0
Feb 17 19:16:05.020: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 19:16:06.028: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:06.028: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:06.028: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:06.033: INFO: Number of nodes with available pods: 0
Feb 17 19:16:06.033: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 19:16:07.028: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:07.028: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:07.028: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:07.032: INFO: Number of nodes with available pods: 0
Feb 17 19:16:07.032: INFO: Node ip-172-16-58-212.ec2.internal is running more than one daemon pod
Feb 17 19:16:08.027: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:08.027: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:08.027: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:08.031: INFO: Number of nodes with available pods: 3
Feb 17 19:16:08.031: INFO: Number of running nodes: 3, number of available pods: 3
Feb 17 19:16:08.031: INFO: Update the DaemonSet to trigger a rollout
Feb 17 19:16:08.042: INFO: Updating DaemonSet daemon-set
Feb 17 19:16:18.063: INFO: Roll back the DaemonSet before rollout is complete
Feb 17 19:16:18.073: INFO: Updating DaemonSet daemon-set
Feb 17 19:16:18.073: INFO: Make sure DaemonSet rollback is complete
Feb 17 19:16:18.082: INFO: Wrong image for pod: daemon-set-b8rb8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 17 19:16:18.082: INFO: Pod daemon-set-b8rb8 is not available
Feb 17 19:16:18.088: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:18.088: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:18.088: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:19.094: INFO: Wrong image for pod: daemon-set-b8rb8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 17 19:16:19.094: INFO: Pod daemon-set-b8rb8 is not available
Feb 17 19:16:19.100: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:19.100: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:19.100: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:20.093: INFO: Wrong image for pod: daemon-set-b8rb8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 17 19:16:20.093: INFO: Pod daemon-set-b8rb8 is not available
Feb 17 19:16:20.100: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:20.100: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:20.100: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:21.098: INFO: Pod daemon-set-hgcb9 is not available
Feb 17 19:16:21.104: INFO: DaemonSet pods can't tolerate node ip-172-16-10-143.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:21.104: INFO: DaemonSet pods can't tolerate node ip-172-16-7-140.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Feb 17 19:16:21.104: INFO: DaemonSet pods can't tolerate node ip-172-16-9-66.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2847, will wait for the garbage collector to delete the pods
Feb 17 19:16:21.178: INFO: Deleting DaemonSet.extensions daemon-set took: 11.174467ms
Feb 17 19:16:21.679: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.244305ms
Feb 17 19:16:29.983: INFO: Number of nodes with available pods: 0
Feb 17 19:16:29.983: INFO: Number of running nodes: 0, number of available pods: 0
Feb 17 19:16:29.987: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2847/daemonsets","resourceVersion":"33653"},"items":null}

Feb 17 19:16:29.992: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2847/pods","resourceVersion":"33653"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:16:30.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2847" for this suite.

• [SLOW TEST:25.227 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":278,"completed":272,"skipped":4453,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:16:30.032: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7480
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-7480
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7480
STEP: Creating statefulset with conflicting port in namespace statefulset-7480
STEP: Waiting until pod test-pod will start running in namespace statefulset-7480
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7480
Feb 17 19:16:34.226: INFO: Observed stateful pod in namespace: statefulset-7480, name: ss-0, uid: 167467ad-dbec-4d07-9192-87da82965534, status phase: Pending. Waiting for statefulset controller to delete.
Feb 17 19:16:34.550: INFO: Observed stateful pod in namespace: statefulset-7480, name: ss-0, uid: 167467ad-dbec-4d07-9192-87da82965534, status phase: Failed. Waiting for statefulset controller to delete.
Feb 17 19:16:34.559: INFO: Observed stateful pod in namespace: statefulset-7480, name: ss-0, uid: 167467ad-dbec-4d07-9192-87da82965534, status phase: Failed. Waiting for statefulset controller to delete.
Feb 17 19:16:34.566: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7480
STEP: Removing pod with conflicting port in namespace statefulset-7480
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7480 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 17 19:16:38.607: INFO: Deleting all statefulset in ns statefulset-7480
Feb 17 19:16:38.611: INFO: Scaling statefulset ss to 0
Feb 17 19:16:58.630: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 19:16:58.635: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:16:58.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7480" for this suite.

• [SLOW TEST:28.636 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":278,"completed":273,"skipped":4498,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:16:58.668: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7398
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Feb 17 19:16:58.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 create -f - --namespace=kubectl-7398'
Feb 17 19:16:59.018: INFO: stderr: ""
Feb 17 19:16:59.018: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Feb 17 19:16:59.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 create -f - --namespace=kubectl-7398'
Feb 17 19:16:59.238: INFO: stderr: ""
Feb 17 19:16:59.238: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Feb 17 19:17:00.244: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 19:17:00.244: INFO: Found 0 / 1
Feb 17 19:17:01.244: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 19:17:01.244: INFO: Found 1 / 1
Feb 17 19:17:01.244: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 17 19:17:01.248: INFO: Selector matched 1 pods for map[app:agnhost]
Feb 17 19:17:01.248: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 17 19:17:01.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 describe pod agnhost-master-x76tq --namespace=kubectl-7398'
Feb 17 19:17:01.333: INFO: stderr: ""
Feb 17 19:17:01.333: INFO: stdout: "Name:         agnhost-master-x76tq\nNamespace:    kubectl-7398\nPriority:     0\nNode:         ip-172-16-58-212.ec2.internal/172.16.58.212\nStart Time:   Mon, 17 Feb 2020 19:16:59 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 100.96.1.244/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           100.96.1.244\nIPs:\n  IP:           100.96.1.244\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://7669dbdc8bff34af6088521b2f2f0433ae8080675a634eccd33c9c8920b56aaf\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 17 Feb 2020 19:17:00 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zpqwd (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-zpqwd:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-zpqwd\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                    Message\n  ----    ------     ----  ----                                    -------\n  Normal  Scheduled  2s    default-scheduler                       Successfully assigned kubectl-7398/agnhost-master-x76tq to ip-172-16-58-212.ec2.internal\n  Normal  Pulled     1s    kubelet, ip-172-16-58-212.ec2.internal  Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    1s    kubelet, ip-172-16-58-212.ec2.internal  Created container agnhost-master\n  Normal  Started    1s    kubelet, ip-172-16-58-212.ec2.internal  Started container agnhost-master\n"
Feb 17 19:17:01.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 describe rc agnhost-master --namespace=kubectl-7398'
Feb 17 19:17:01.423: INFO: stderr: ""
Feb 17 19:17:01.423: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-7398\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-master-x76tq\n"
Feb 17 19:17:01.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 describe service agnhost-master --namespace=kubectl-7398'
Feb 17 19:17:01.505: INFO: stderr: ""
Feb 17 19:17:01.505: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-7398\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                100.66.56.245\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.96.1.244:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 17 19:17:01.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 describe node ip-172-16-10-143.ec2.internal'
Feb 17 19:17:01.621: INFO: stderr: ""
Feb 17 19:17:01.621: INFO: stdout: "Name:               ip-172-16-10-143.ec2.internal\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t2.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1e\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-16-10-143\n                    kubernetes.io/os=linux\n                    kublr.io/location=aws1\n                    kublr.io/node-group=master\n                    kublr.io/node-identifier=e920dd1f-1364-479c-889f-6bb7fdcb8c89\n                    kublr.io/node-ordinal=2\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=t2.large\n                    topology.kubernetes.io/region=us-east-1\n                    topology.kubernetes.io/zone=us-east-1e\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"a6:a1:b5:71:20:a7\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.16.10.143\n                    kublr.io/agent-status:\n                      {\"clusterName\":\"dvi-conformance17-2-1581961502\",\"location\":\"aws1\",\"nodeGroup\":\"master\",\"nodeOrdinal\":2,\"nodeIdentifier\":\"e920dd1f-1364-479...\n                    kublr.io/location: aws1\n                    kublr.io/node-group: master\n                    kublr.io/node-identifier: e920dd1f-1364-479c-889f-6bb7fdcb8c89\n                    kublr.io/node-ordinal: 2\n                    kublr.io/seeder-status:\n                      {\"clusterName\":\"dvi-conformance17-2-1581961502\",\"location\":\"aws1\",\"nodeGroup\":\"master\",\"nodeOrdinal\":2,\"nodeIdentifier\":\"e920dd1f-1364-479...\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.96.3.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 17 Feb 2020 17:53:10 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-16-10-143.ec2.internal\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 17 Feb 2020 19:16:54 +0000\nConditions:\n  Type                             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                             ------  -----------------                 ------------------                ------                       -------\n  KublrSeederAgentReady            True    Mon, 17 Feb 2020 19:16:37 +0000   Mon, 17 Feb 2020 17:51:59 +0000   KublrAgentRunning            Kublr agent is running\n  KublrSeederInstanceReady         True    Mon, 17 Feb 2020 19:16:36 +0000   Mon, 17 Feb 2020 17:50:08 +0000   SeederRunning                Seeder is running\n  KublrSeederSecretStoreReady      True    Mon, 17 Feb 2020 19:16:36 +0000   Mon, 17 Feb 2020 17:50:08 +0000   SecretStoreOk                Secret store is running and accessible\n  KublrAgentContainerEngineReady   True    Mon, 17 Feb 2020 19:16:40 +0000   Mon, 17 Feb 2020 17:52:09 +0000   DockerRunning                Docker is running\n  KublrAgentInstanceReady          True    Mon, 17 Feb 2020 19:16:39 +0000   Mon, 17 Feb 2020 17:51:40 +0000   AgentRunning                 Agent is running\n  KublrAgentKubeletReady           True    Mon, 17 Feb 2020 19:16:39 +0000   Mon, 17 Feb 2020 17:52:09 +0000   KubeletRunning               Kubelet is running\n  KublrAgentReady                  True    Mon, 17 Feb 2020 19:16:40 +0000   Mon, 17 Feb 2020 17:51:41 +0000   AgentRunning                 Agent is running\n  MemoryPressure                   False   Mon, 17 Feb 2020 19:13:43 +0000   Mon, 17 Feb 2020 17:53:10 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure                     False   Mon, 17 Feb 2020 19:13:43 +0000   Mon, 17 Feb 2020 17:53:10 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure                      False   Mon, 17 Feb 2020 19:13:43 +0000   Mon, 17 Feb 2020 17:53:10 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                            True    Mon, 17 Feb 2020 19:13:43 +0000   Mon, 17 Feb 2020 17:53:20 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.16.10.143\n  ExternalIP:   54.160.70.105\n  Hostname:     ip-172-16-10-143.ec2.internal\n  InternalDNS:  ip-172-16-10-143.ec2.internal\n  ExternalDNS:  ec2-54-160-70-105.compute-1.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         2\n  ephemeral-storage:           40593612Ki\n  hugepages-2Mi:               0\n  memory:                      8166712Ki\n  pods:                        110\nAllocatable:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         1900m\n  ephemeral-storage:           35263589110\n  hugepages-2Mi:               0\n  memory:                      7015736Ki\n  pods:                        110\nSystem Info:\n  Machine ID:                 9d917b7f6c8c4cf1933ec34ee4d5b057\n  System UUID:                EC2EAA51-A02C-A449-7DAD-AD83CFCE38B6\n  Boot ID:                    4efe79d6-8890-458f-a648-5840cf406b88\n  Kernel Version:             4.15.0-1058-aws\n  OS Image:                   Ubuntu 18.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.9.7\n  Kubelet Version:            v1.17.3\n  Kube-Proxy Version:         v1.17.3\nPodCIDR:                      100.96.3.0/24\nPodCIDRs:                     100.96.3.0/24\nProviderID:                   aws:///us-east-1e/i-00d783fbeb0c58344\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                                                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                                                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-fvs4b                                                                                                                50m (2%)      0 (0%)      114Mi (1%)       825Mi (12%)    83m\n  kube-system                 cert-updater-73aca1b35c2c355a73b23ddea987e3c951929e69b390d18b08afd1387c6ef121-ip-172-16-10-143.ec2.internal                100m (5%)     100m (5%)   32Mi (0%)        128Mi (1%)     83m\n  kube-system                 k8s-api-haproxy-b9ba254b202c3ae80f0c096eb09e3391b752ed191356ffd2fb26712eff44ce1b-ip-172-16-10-143.ec2.internal             1m (0%)       0 (0%)      20Mi (0%)        20Mi (0%)      82m\n  kube-system                 k8s-etcd-71b8f72ed4d551f5d4fe4ad4727118ae725aaa2e2f08a1672b27bd1a263048b7-ip-172-16-10-143.ec2.internal                    100m (5%)     1 (52%)     200Mi (2%)       1536Mi (22%)   83m\n  kube-system                 k8s-master-467f1464162df9940cfa2cbecb45d328a5c76fac86356cb6753a931f1dd6e9ec-ip-172-16-10-143.ec2.internal                  160m (8%)     3 (157%)    380Mi (5%)       3Gi (44%)      82m\n  kube-system                 kube-addon-manager-3ede9d47526b218891e60768efcac00bdca2f6fd9e50740866a5a589312981ea-ip-172-16-10-143.ec2.internal          50m (2%)      1 (52%)     64Mi (0%)        128Mi (1%)     83m\n  kube-system                 kube-dns-autoscaler-56588bbd55-vmd4r                                                                                       1m (0%)       0 (0%)      10Mi (0%)        32Mi (0%)      83m\n  kube-system                 kube-proxy-1f95d4834de72fc7a192f21f307450e3e3a9902d1319965d85b9dbdd2c5447ee-ip-172-16-10-143.ec2.internal                  5m (0%)       250m (13%)  24Mi (0%)        512Mi (7%)     83m\n  kube-system                 kublr-label-master-node-ip-172-16-10-143.ec2.internal                                                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         83m\n  kube-system                 kublr-node-name-reporter-f07aa9dab010c4e5ea03c8ba1fc360304baa13c0cf3ac11dec07c23cfcd9fc80-ip-172-16-10-143.ec2.internal    0 (0%)        0 (0%)      32Mi (0%)        32Mi (0%)      83m\n  kube-system                 node-local-dns-8jsbp                                                                                                       25m (1%)      0 (0%)      5Mi (0%)         30Mi (0%)      83m\n  kubernetes-dashboard        dashboard-metrics-scraper-698f5bb4db-hqlfh                                                                                 5m (0%)       50m (2%)    10Mi (0%)        100Mi (1%)     83m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-d7ef5c0c6da94b4f-xq8rw                                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         73m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests     Limits\n  --------                    --------     ------\n  cpu                         497m (26%)   5400m (284%)\n  memory                      891Mi (13%)  6415Mi (93%)\n  ephemeral-storage           0 (0%)       0 (0%)\n  attachable-volumes-aws-ebs  0            0\nEvents:                       <none>\n"
Feb 17 19:17:01.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-713147157 describe namespace kubectl-7398'
Feb 17 19:17:01.701: INFO: stderr: ""
Feb 17 19:17:01.701: INFO: stdout: "Name:         kubectl-7398\nLabels:       e2e-framework=kubectl\n              e2e-run=4311e4a8-a409-4e97-b195-a6af62106842\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:17:01.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7398" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":278,"completed":274,"skipped":4499,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:17:01.716: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-1755
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Feb 17 19:17:01.893: INFO: Found 0 stateful pods, waiting for 3
Feb 17 19:17:11.899: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 19:17:11.899: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 19:17:11.899: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 17 19:17:11.932: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 17 19:17:21.973: INFO: Updating stateful set ss2
Feb 17 19:17:21.984: INFO: Waiting for Pod statefulset-1755/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Feb 17 19:17:32.053: INFO: Found 2 stateful pods, waiting for 3
Feb 17 19:17:42.059: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 19:17:42.059: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 19:17:42.059: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 17 19:17:42.089: INFO: Updating stateful set ss2
Feb 17 19:17:42.097: INFO: Waiting for Pod statefulset-1755/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 19:17:52.128: INFO: Updating stateful set ss2
Feb 17 19:17:52.140: INFO: Waiting for StatefulSet statefulset-1755/ss2 to complete update
Feb 17 19:17:52.140: INFO: Waiting for Pod statefulset-1755/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Feb 17 19:18:02.150: INFO: Deleting all statefulset in ns statefulset-1755
Feb 17 19:18:02.155: INFO: Scaling statefulset ss2 to 0
Feb 17 19:18:22.184: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 19:18:22.189: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:18:22.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1755" for this suite.

• [SLOW TEST:80.515 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":278,"completed":275,"skipped":4513,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:18:22.231: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8609
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 17 19:18:24.433: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:18:24.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8609" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":278,"completed":276,"skipped":4536,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:18:24.470: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6995
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Feb 17 19:18:24.633: INFO: Waiting up to 5m0s for pod "downwardapi-volume-40c37871-067f-4991-945a-9c92fe417d5d" in namespace "downward-api-6995" to be "success or failure"
Feb 17 19:18:24.640: INFO: Pod "downwardapi-volume-40c37871-067f-4991-945a-9c92fe417d5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.967781ms
Feb 17 19:18:26.646: INFO: Pod "downwardapi-volume-40c37871-067f-4991-945a-9c92fe417d5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013741057s
STEP: Saw pod success
Feb 17 19:18:26.646: INFO: Pod "downwardapi-volume-40c37871-067f-4991-945a-9c92fe417d5d" satisfied condition "success or failure"
Feb 17 19:18:26.650: INFO: Trying to get logs from node ip-172-16-58-212.ec2.internal pod downwardapi-volume-40c37871-067f-4991-945a-9c92fe417d5d container client-container: <nil>
STEP: delete the pod
Feb 17 19:18:26.687: INFO: Waiting for pod downwardapi-volume-40c37871-067f-4991-945a-9c92fe417d5d to disappear
Feb 17 19:18:26.690: INFO: Pod downwardapi-volume-40c37871-067f-4991-945a-9c92fe417d5d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:18:26.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6995" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":278,"completed":277,"skipped":4549,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:18:26.705: INFO: >>> kubeConfig: /tmp/kubeconfig-713147157
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7247
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:18:43.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7247" for this suite.

• [SLOW TEST:17.222 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.3-beta.0.40+c94b9acd4b784f/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":278,"completed":278,"skipped":4564,"failed":0}
SFeb 17 19:18:43.927: INFO: Running AfterSuite actions on all nodes
Feb 17 19:18:43.928: INFO: Running AfterSuite actions on node 1
Feb 17 19:18:43.928: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":278,"completed":278,"skipped":4565,"failed":0}

Ran 278 of 4843 Specs in 4504.854 seconds
SUCCESS! -- 278 Passed | 0 Failed | 0 Pending | 4565 Skipped
PASS

Ginkgo ran 1 suite in 1h15m6.230296575s
Test Suite Passed
