I0713 12:29:50.608160      20 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-680009899
I0713 12:29:50.608184      20 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0713 12:29:50.608273      20 e2e.go:109] Starting e2e run "46cd4b81-82f3-45d6-af66-8f57539d2678" on Ginkgo node 1
{"msg":"Test Suite starting","total":280,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1594643388 - Will randomize all specs
Will run 280 of 4843 specs

Jul 13 12:29:50.619: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 12:29:50.621: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 13 12:29:50.642: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 13 12:29:50.690: INFO: 15 / 15 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 13 12:29:50.690: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Jul 13 12:29:50.690: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 13 12:29:50.700: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jul 13 12:29:50.700: INFO: e2e test version: v1.17.6
Jul 13 12:29:50.702: INFO: kube-apiserver version: v1.17.6
Jul 13 12:29:50.702: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 12:29:50.712: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:29:50.729: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename taint-multiple-pods
Jul 13 12:29:50.847: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:344
Jul 13 12:29:50.851: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 13 12:30:50.891: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 12:30:50.895: INFO: Starting informer...
STEP: Starting pods...
Jul 13 12:30:51.119: INFO: Pod1 is running on ip-10-0-3-16.us-east-2.compute.internal. Tainting Node
Jul 13 12:30:53.344: INFO: Pod2 is running on ip-10-0-3-16.us-east-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jul 13 12:31:00.141: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jul 13 12:31:31.250: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:31:31.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6332" for this suite.

â€¢ [SLOW TEST:100.579 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":280,"completed":1,"skipped":21,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:31:31.311: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 12:31:31.383: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 13 12:31:36.389: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 13 12:31:36.389: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 13 12:31:38.395: INFO: Creating deployment "test-rollover-deployment"
Jul 13 12:31:38.405: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 13 12:31:40.414: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 13 12:31:40.422: INFO: Ensure that both replica sets have 1 created replica
Jul 13 12:31:40.431: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 13 12:31:40.440: INFO: Updating deployment test-rollover-deployment
Jul 13 12:31:40.440: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 13 12:31:42.453: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 13 12:31:42.461: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 13 12:31:42.469: INFO: all replica sets need to contain the pod-template-hash label
Jul 13 12:31:42.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240300, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 12:31:44.478: INFO: all replica sets need to contain the pod-template-hash label
Jul 13 12:31:44.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240302, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 12:31:46.487: INFO: all replica sets need to contain the pod-template-hash label
Jul 13 12:31:46.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240302, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 12:31:48.478: INFO: all replica sets need to contain the pod-template-hash label
Jul 13 12:31:48.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240302, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 12:31:50.486: INFO: all replica sets need to contain the pod-template-hash label
Jul 13 12:31:50.486: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240302, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 12:31:52.481: INFO: all replica sets need to contain the pod-template-hash label
Jul 13 12:31:52.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240302, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240298, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 12:31:54.480: INFO: 
Jul 13 12:31:54.480: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jul 13 12:31:54.493: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-4012 /apis/apps/v1/namespaces/deployment-4012/deployments/test-rollover-deployment 05f0954d-0461-4f67-b691-a98566e66b80 212683 2 2020-07-13 12:31:38 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002eb6ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-07-13 12:31:38 +0000 UTC,LastTransitionTime:2020-07-13 12:31:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-07-13 12:31:52 +0000 UTC,LastTransitionTime:2020-07-13 12:31:38 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 13 12:31:54.497: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-4012 /apis/apps/v1/namespaces/deployment-4012/replicasets/test-rollover-deployment-574d6dfbff fee2473a-aa32-4fd7-9e7a-1898cc2054a0 212673 2 2020-07-13 12:31:40 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 05f0954d-0461-4f67-b691-a98566e66b80 0xc002eb6f47 0xc002eb6f48}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002eb6fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 13 12:31:54.497: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 13 12:31:54.497: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-4012 /apis/apps/v1/namespaces/deployment-4012/replicasets/test-rollover-controller cfafd100-0b99-45fd-bdc0-39032a3f789a 212682 2 2020-07-13 12:31:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 05f0954d-0461-4f67-b691-a98566e66b80 0xc002eb6e77 0xc002eb6e78}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002eb6ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 13 12:31:54.497: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-4012 /apis/apps/v1/namespaces/deployment-4012/replicasets/test-rollover-deployment-f6c94f66c aeed10f0-b9db-4d0c-9bea-c36f8d22056c 212611 2 2020-07-13 12:31:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 05f0954d-0461-4f67-b691-a98566e66b80 0xc002eb7020 0xc002eb7021}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002eb7098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 13 12:31:54.502: INFO: Pod "test-rollover-deployment-574d6dfbff-rgtvf" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-rgtvf test-rollover-deployment-574d6dfbff- deployment-4012 /api/v1/namespaces/deployment-4012/pods/test-rollover-deployment-574d6dfbff-rgtvf 69116dff-d4f2-4c83-8318-f23bcbebd91c 212640 0 2020-07-13 12:31:40 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[cni.projectcalico.org/podIP:10.20.23.169/32] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff fee2473a-aa32-4fd7-9e7a-1898cc2054a0 0xc002ece9a7 0xc002ece9a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wfjz9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wfjz9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wfjz9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-156.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 12:31:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 12:31:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 12:31:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 12:31:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.156,PodIP:10.20.23.169,StartTime:2020-07-13 12:31:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-13 12:31:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://41afe043b4dfbf7e819de3cf819a2c28d01e919d08c5247574a4981fae8e0987,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.23.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:31:54.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4012" for this suite.

â€¢ [SLOW TEST:23.204 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":280,"completed":2,"skipped":74,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:31:54.515: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 13 12:31:54.567: INFO: Waiting up to 5m0s for pod "pod-7ba40a8c-7239-4c96-b88d-de2b9afd2bb2" in namespace "emptydir-402" to be "success or failure"
Jul 13 12:31:54.576: INFO: Pod "pod-7ba40a8c-7239-4c96-b88d-de2b9afd2bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.350397ms
Jul 13 12:31:56.582: INFO: Pod "pod-7ba40a8c-7239-4c96-b88d-de2b9afd2bb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014797647s
Jul 13 12:31:58.587: INFO: Pod "pod-7ba40a8c-7239-4c96-b88d-de2b9afd2bb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019710854s
STEP: Saw pod success
Jul 13 12:31:58.587: INFO: Pod "pod-7ba40a8c-7239-4c96-b88d-de2b9afd2bb2" satisfied condition "success or failure"
Jul 13 12:31:58.591: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-7ba40a8c-7239-4c96-b88d-de2b9afd2bb2 container test-container: <nil>
STEP: delete the pod
Jul 13 12:31:58.641: INFO: Waiting for pod pod-7ba40a8c-7239-4c96-b88d-de2b9afd2bb2 to disappear
Jul 13 12:31:58.647: INFO: Pod pod-7ba40a8c-7239-4c96-b88d-de2b9afd2bb2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:31:58.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-402" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":3,"skipped":87,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:31:58.662: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 12:31:58.697: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 13 12:32:00.752: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:32:00.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1017" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":280,"completed":4,"skipped":104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:32:00.795: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:32:06.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5461" for this suite.
STEP: Destroying namespace "nsdeletetest-2064" for this suite.
Jul 13 12:32:07.000: INFO: Namespace nsdeletetest-2064 was already deleted
STEP: Destroying namespace "nsdeletetest-7717" for this suite.

â€¢ [SLOW TEST:6.212 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":280,"completed":5,"skipped":168,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:32:07.009: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 12:32:07.420: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 13 12:32:09.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240327, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240327, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240327, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240327, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 12:32:12.462: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 12:32:12.467: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9807-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:32:13.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5173" for this suite.
STEP: Destroying namespace "webhook-5173-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.858 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":280,"completed":6,"skipped":174,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:32:13.868: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 12:32:13.928: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 13 12:32:17.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-1111 create -f -'
Jul 13 12:32:18.699: INFO: stderr: ""
Jul 13 12:32:18.699: INFO: stdout: "e2e-test-crd-publish-openapi-3345-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 13 12:32:18.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-1111 delete e2e-test-crd-publish-openapi-3345-crds test-cr'
Jul 13 12:32:18.816: INFO: stderr: ""
Jul 13 12:32:18.816: INFO: stdout: "e2e-test-crd-publish-openapi-3345-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jul 13 12:32:18.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-1111 apply -f -'
Jul 13 12:32:19.075: INFO: stderr: ""
Jul 13 12:32:19.075: INFO: stdout: "e2e-test-crd-publish-openapi-3345-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 13 12:32:19.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-1111 delete e2e-test-crd-publish-openapi-3345-crds test-cr'
Jul 13 12:32:19.190: INFO: stderr: ""
Jul 13 12:32:19.190: INFO: stdout: "e2e-test-crd-publish-openapi-3345-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jul 13 12:32:19.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 explain e2e-test-crd-publish-openapi-3345-crds'
Jul 13 12:32:19.417: INFO: stderr: ""
Jul 13 12:32:19.417: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3345-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:32:23.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1111" for this suite.

â€¢ [SLOW TEST:9.182 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":280,"completed":7,"skipped":209,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:32:23.051: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jul 13 12:32:23.092: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jul 13 12:32:36.979: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 12:32:41.202: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:32:55.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7601" for this suite.

â€¢ [SLOW TEST:32.919 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":280,"completed":8,"skipped":244,"failed":0}
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:32:55.970: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 12:32:56.041: INFO: Waiting up to 5m0s for pod "busybox-user-65534-6f41b270-da5a-48c2-9638-08f4a2950a68" in namespace "security-context-test-3053" to be "success or failure"
Jul 13 12:32:56.047: INFO: Pod "busybox-user-65534-6f41b270-da5a-48c2-9638-08f4a2950a68": Phase="Pending", Reason="", readiness=false. Elapsed: 6.225442ms
Jul 13 12:32:58.064: INFO: Pod "busybox-user-65534-6f41b270-da5a-48c2-9638-08f4a2950a68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022682101s
Jul 13 12:32:58.064: INFO: Pod "busybox-user-65534-6f41b270-da5a-48c2-9638-08f4a2950a68" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:32:58.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3053" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":9,"skipped":244,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:32:58.080: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jul 13 12:33:02.689: INFO: Successfully updated pod "labelsupdatec6230211-649e-4614-8529-e6443d6a7972"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:33:04.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2089" for this suite.

â€¢ [SLOW TEST:6.645 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":10,"skipped":251,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:33:04.725: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 12:33:04.801: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 13 12:33:04.822: INFO: Number of nodes with available pods: 0
Jul 13 12:33:04.822: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 12:33:05.835: INFO: Number of nodes with available pods: 0
Jul 13 12:33:05.835: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 12:33:06.835: INFO: Number of nodes with available pods: 1
Jul 13 12:33:06.835: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 12:33:07.836: INFO: Number of nodes with available pods: 5
Jul 13 12:33:07.836: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 13 12:33:07.884: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:07.884: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:07.884: INFO: Wrong image for pod: daemon-set-pn4vh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:07.884: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:07.884: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:08.900: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:08.900: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:08.900: INFO: Wrong image for pod: daemon-set-pn4vh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:08.900: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:08.900: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:09.902: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:09.902: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:09.902: INFO: Wrong image for pod: daemon-set-pn4vh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:09.902: INFO: Pod daemon-set-pn4vh is not available
Jul 13 12:33:09.902: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:09.902: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:10.906: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:10.906: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:10.906: INFO: Wrong image for pod: daemon-set-pn4vh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:10.906: INFO: Pod daemon-set-pn4vh is not available
Jul 13 12:33:10.906: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:10.906: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:11.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:11.902: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:11.902: INFO: Wrong image for pod: daemon-set-pn4vh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:11.902: INFO: Pod daemon-set-pn4vh is not available
Jul 13 12:33:11.902: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:11.902: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:12.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:12.901: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:12.901: INFO: Wrong image for pod: daemon-set-pn4vh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:12.901: INFO: Pod daemon-set-pn4vh is not available
Jul 13 12:33:12.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:12.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:13.902: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:13.902: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:13.902: INFO: Wrong image for pod: daemon-set-pn4vh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:13.902: INFO: Pod daemon-set-pn4vh is not available
Jul 13 12:33:13.902: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:13.902: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:14.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:14.901: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:14.901: INFO: Wrong image for pod: daemon-set-pn4vh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:14.901: INFO: Pod daemon-set-pn4vh is not available
Jul 13 12:33:14.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:14.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:15.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:15.901: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:15.901: INFO: Wrong image for pod: daemon-set-pn4vh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:15.901: INFO: Pod daemon-set-pn4vh is not available
Jul 13 12:33:15.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:15.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:16.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:16.901: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:16.901: INFO: Wrong image for pod: daemon-set-pn4vh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:16.901: INFO: Pod daemon-set-pn4vh is not available
Jul 13 12:33:16.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:16.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:17.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:17.901: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:17.901: INFO: Wrong image for pod: daemon-set-pn4vh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:17.901: INFO: Pod daemon-set-pn4vh is not available
Jul 13 12:33:17.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:17.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:18.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:18.901: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:18.901: INFO: Wrong image for pod: daemon-set-pn4vh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:18.901: INFO: Pod daemon-set-pn4vh is not available
Jul 13 12:33:18.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:18.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:19.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:19.901: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:19.901: INFO: Wrong image for pod: daemon-set-pn4vh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:19.901: INFO: Pod daemon-set-pn4vh is not available
Jul 13 12:33:19.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:19.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:20.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:20.901: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:20.901: INFO: Wrong image for pod: daemon-set-pn4vh. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:20.901: INFO: Pod daemon-set-pn4vh is not available
Jul 13 12:33:20.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:20.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:21.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:21.902: INFO: Pod daemon-set-6jv4f is not available
Jul 13 12:33:21.902: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:21.902: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:21.902: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:22.906: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:22.906: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:22.906: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:22.906: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:23.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:23.901: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:23.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:23.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:24.902: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:24.902: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:24.902: INFO: Pod daemon-set-gzqmb is not available
Jul 13 12:33:24.902: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:24.902: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:25.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:25.901: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:25.901: INFO: Pod daemon-set-gzqmb is not available
Jul 13 12:33:25.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:25.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:26.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:26.901: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:26.901: INFO: Pod daemon-set-gzqmb is not available
Jul 13 12:33:26.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:26.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:27.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:27.901: INFO: Wrong image for pod: daemon-set-gzqmb. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:27.901: INFO: Pod daemon-set-gzqmb is not available
Jul 13 12:33:27.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:27.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:28.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:28.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:28.901: INFO: Pod daemon-set-v6ftc is not available
Jul 13 12:33:28.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:29.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:29.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:29.901: INFO: Pod daemon-set-v6ftc is not available
Jul 13 12:33:29.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:30.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:30.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:30.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:31.901: INFO: Wrong image for pod: daemon-set-4wzfg. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:31.901: INFO: Pod daemon-set-4wzfg is not available
Jul 13 12:33:31.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:31.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:32.901: INFO: Pod daemon-set-spzqs is not available
Jul 13 12:33:32.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:32.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:33.901: INFO: Pod daemon-set-spzqs is not available
Jul 13 12:33:33.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:33.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:34.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:34.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:35.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:35.901: INFO: Pod daemon-set-ttzcm is not available
Jul 13 12:33:35.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:36.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:36.901: INFO: Pod daemon-set-ttzcm is not available
Jul 13 12:33:36.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:37.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:37.901: INFO: Pod daemon-set-ttzcm is not available
Jul 13 12:33:37.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:38.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:38.901: INFO: Pod daemon-set-ttzcm is not available
Jul 13 12:33:38.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:39.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:39.901: INFO: Pod daemon-set-ttzcm is not available
Jul 13 12:33:39.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:40.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:40.901: INFO: Pod daemon-set-ttzcm is not available
Jul 13 12:33:40.902: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:41.904: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:41.915: INFO: Pod daemon-set-ttzcm is not available
Jul 13 12:33:41.915: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:42.901: INFO: Wrong image for pod: daemon-set-ttzcm. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:42.901: INFO: Pod daemon-set-ttzcm is not available
Jul 13 12:33:42.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:43.901: INFO: Pod daemon-set-fgx85 is not available
Jul 13 12:33:43.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:44.901: INFO: Pod daemon-set-fgx85 is not available
Jul 13 12:33:44.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:45.901: INFO: Pod daemon-set-fgx85 is not available
Jul 13 12:33:45.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:46.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:47.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:47.901: INFO: Pod daemon-set-wv2wr is not available
Jul 13 12:33:48.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:48.901: INFO: Pod daemon-set-wv2wr is not available
Jul 13 12:33:49.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:49.901: INFO: Pod daemon-set-wv2wr is not available
Jul 13 12:33:50.901: INFO: Wrong image for pod: daemon-set-wv2wr. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jul 13 12:33:50.901: INFO: Pod daemon-set-wv2wr is not available
Jul 13 12:33:51.904: INFO: Pod daemon-set-jf5rk is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 13 12:33:51.958: INFO: Number of nodes with available pods: 4
Jul 13 12:33:51.959: INFO: Node ip-10-0-3-16.us-east-2.compute.internal is running more than one daemon pod
Jul 13 12:33:52.972: INFO: Number of nodes with available pods: 4
Jul 13 12:33:52.973: INFO: Node ip-10-0-3-16.us-east-2.compute.internal is running more than one daemon pod
Jul 13 12:33:53.973: INFO: Number of nodes with available pods: 5
Jul 13 12:33:53.973: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3088, will wait for the garbage collector to delete the pods
Jul 13 12:33:54.056: INFO: Deleting DaemonSet.extensions daemon-set took: 9.188958ms
Jul 13 12:33:54.856: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.259025ms
Jul 13 12:34:03.663: INFO: Number of nodes with available pods: 0
Jul 13 12:34:03.663: INFO: Number of running nodes: 0, number of available pods: 0
Jul 13 12:34:03.667: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3088/daemonsets","resourceVersion":"213790"},"items":null}

Jul 13 12:34:03.670: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3088/pods","resourceVersion":"213790"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:34:03.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3088" for this suite.

â€¢ [SLOW TEST:58.986 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":280,"completed":11,"skipped":264,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:34:03.712: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 12:34:04.198: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 13 12:34:06.211: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240444, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240444, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240444, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240444, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 12:34:09.236: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:34:19.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8646" for this suite.
STEP: Destroying namespace "webhook-8646-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:15.923 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":280,"completed":12,"skipped":274,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:34:19.634: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Jul 13 12:34:19.699: INFO: Waiting up to 5m0s for pod "client-containers-9128e21d-f894-442e-b650-1491968d4604" in namespace "containers-9270" to be "success or failure"
Jul 13 12:34:19.705: INFO: Pod "client-containers-9128e21d-f894-442e-b650-1491968d4604": Phase="Pending", Reason="", readiness=false. Elapsed: 5.743871ms
Jul 13 12:34:21.710: INFO: Pod "client-containers-9128e21d-f894-442e-b650-1491968d4604": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010825468s
STEP: Saw pod success
Jul 13 12:34:21.710: INFO: Pod "client-containers-9128e21d-f894-442e-b650-1491968d4604" satisfied condition "success or failure"
Jul 13 12:34:21.715: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod client-containers-9128e21d-f894-442e-b650-1491968d4604 container test-container: <nil>
STEP: delete the pod
Jul 13 12:34:21.743: INFO: Waiting for pod client-containers-9128e21d-f894-442e-b650-1491968d4604 to disappear
Jul 13 12:34:21.751: INFO: Pod client-containers-9128e21d-f894-442e-b650-1491968d4604 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:34:21.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9270" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":280,"completed":13,"skipped":281,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:34:21.774: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:34:44.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5386" for this suite.

â€¢ [SLOW TEST:22.354 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  blackbox test
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":280,"completed":14,"skipped":286,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:34:44.128: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:34:44.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6411" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
â€¢{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":280,"completed":15,"skipped":297,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:34:44.185: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9196.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9196.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9196.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9196.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9196.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9196.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9196.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 13 12:34:48.297: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:48.302: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:48.307: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:48.313: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:48.335: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:48.341: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:48.346: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:48.351: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:48.361: INFO: Lookups using dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9196.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9196.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local jessie_udp@dns-test-service-2.dns-9196.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9196.svc.cluster.local]

Jul 13 12:34:53.367: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:53.372: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:53.378: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:53.384: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:53.399: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:53.404: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:53.410: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:53.415: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:53.425: INFO: Lookups using dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9196.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9196.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local jessie_udp@dns-test-service-2.dns-9196.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9196.svc.cluster.local]

Jul 13 12:34:58.368: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:58.374: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:58.379: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:58.386: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:58.405: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:58.410: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:58.415: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:58.421: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:34:58.432: INFO: Lookups using dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9196.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9196.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local jessie_udp@dns-test-service-2.dns-9196.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9196.svc.cluster.local]

Jul 13 12:35:03.369: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:03.382: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:03.388: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:03.394: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:03.410: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:03.415: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:03.420: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:03.426: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:03.438: INFO: Lookups using dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9196.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9196.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local jessie_udp@dns-test-service-2.dns-9196.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9196.svc.cluster.local]

Jul 13 12:35:08.369: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:08.374: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:08.380: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:08.386: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:08.402: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:08.407: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:08.413: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:08.418: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:08.428: INFO: Lookups using dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9196.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9196.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local jessie_udp@dns-test-service-2.dns-9196.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9196.svc.cluster.local]

Jul 13 12:35:13.367: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:13.373: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:13.378: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:13.383: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:13.399: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:13.404: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:13.410: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:13.415: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9196.svc.cluster.local from pod dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a: the server could not find the requested resource (get pods dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a)
Jul 13 12:35:13.426: INFO: Lookups using dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9196.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9196.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9196.svc.cluster.local jessie_udp@dns-test-service-2.dns-9196.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9196.svc.cluster.local]

Jul 13 12:35:18.433: INFO: DNS probes using dns-9196/dns-test-c615b90d-bd7a-4c78-af6f-59e7d939aa4a succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:35:18.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9196" for this suite.

â€¢ [SLOW TEST:34.309 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":280,"completed":16,"skipped":303,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:35:18.496: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 12:35:18.548: INFO: Waiting up to 5m0s for pod "downwardapi-volume-95207341-9b29-4e39-a08a-30b5529ec65a" in namespace "downward-api-7201" to be "success or failure"
Jul 13 12:35:18.562: INFO: Pod "downwardapi-volume-95207341-9b29-4e39-a08a-30b5529ec65a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.168381ms
Jul 13 12:35:20.567: INFO: Pod "downwardapi-volume-95207341-9b29-4e39-a08a-30b5529ec65a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019365246s
STEP: Saw pod success
Jul 13 12:35:20.567: INFO: Pod "downwardapi-volume-95207341-9b29-4e39-a08a-30b5529ec65a" satisfied condition "success or failure"
Jul 13 12:35:20.571: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod downwardapi-volume-95207341-9b29-4e39-a08a-30b5529ec65a container client-container: <nil>
STEP: delete the pod
Jul 13 12:35:20.614: INFO: Waiting for pod downwardapi-volume-95207341-9b29-4e39-a08a-30b5529ec65a to disappear
Jul 13 12:35:20.618: INFO: Pod downwardapi-volume-95207341-9b29-4e39-a08a-30b5529ec65a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:35:20.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7201" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":17,"skipped":307,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:35:20.634: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:35:33.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3403" for this suite.

â€¢ [SLOW TEST:13.154 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":280,"completed":18,"skipped":313,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:35:33.789: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 13 12:35:33.847: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4552 /api/v1/namespaces/watch-4552/configmaps/e2e-watch-test-label-changed c37ba24d-12fa-41e2-aaa5-a4f5f8cf8b71 214501 0 2020-07-13 12:35:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 13 12:35:33.847: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4552 /api/v1/namespaces/watch-4552/configmaps/e2e-watch-test-label-changed c37ba24d-12fa-41e2-aaa5-a4f5f8cf8b71 214502 0 2020-07-13 12:35:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 13 12:35:33.848: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4552 /api/v1/namespaces/watch-4552/configmaps/e2e-watch-test-label-changed c37ba24d-12fa-41e2-aaa5-a4f5f8cf8b71 214503 0 2020-07-13 12:35:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 13 12:35:43.888: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4552 /api/v1/namespaces/watch-4552/configmaps/e2e-watch-test-label-changed c37ba24d-12fa-41e2-aaa5-a4f5f8cf8b71 214545 0 2020-07-13 12:35:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 13 12:35:43.888: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4552 /api/v1/namespaces/watch-4552/configmaps/e2e-watch-test-label-changed c37ba24d-12fa-41e2-aaa5-a4f5f8cf8b71 214546 0 2020-07-13 12:35:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul 13 12:35:43.888: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4552 /api/v1/namespaces/watch-4552/configmaps/e2e-watch-test-label-changed c37ba24d-12fa-41e2-aaa5-a4f5f8cf8b71 214547 0 2020-07-13 12:35:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:35:43.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4552" for this suite.

â€¢ [SLOW TEST:10.113 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":280,"completed":19,"skipped":331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:35:43.905: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-2197
STEP: creating replication controller nodeport-test in namespace services-2197
I0713 12:35:43.988091      20 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-2197, replica count: 2
I0713 12:35:47.041551      20 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 13 12:35:47.041: INFO: Creating new exec pod
Jul 13 12:35:52.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-2197 execpodg9wm4 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jul 13 12:35:52.262: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul 13 12:35:52.262: INFO: stdout: ""
Jul 13 12:35:52.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-2197 execpodg9wm4 -- /bin/sh -x -c nc -zv -t -w 2 10.21.221.129 80'
Jul 13 12:35:52.498: INFO: stderr: "+ nc -zv -t -w 2 10.21.221.129 80\nConnection to 10.21.221.129 80 port [tcp/http] succeeded!\n"
Jul 13 12:35:52.498: INFO: stdout: ""
Jul 13 12:35:52.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-2197 execpodg9wm4 -- /bin/sh -x -c nc -zv -t -w 2 10.0.1.82 31087'
Jul 13 12:35:52.697: INFO: stderr: "+ nc -zv -t -w 2 10.0.1.82 31087\nConnection to 10.0.1.82 31087 port [tcp/31087] succeeded!\n"
Jul 13 12:35:52.697: INFO: stdout: ""
Jul 13 12:35:52.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-2197 execpodg9wm4 -- /bin/sh -x -c nc -zv -t -w 2 10.0.2.22 31087'
Jul 13 12:35:52.927: INFO: stderr: "+ nc -zv -t -w 2 10.0.2.22 31087\nConnection to 10.0.2.22 31087 port [tcp/31087] succeeded!\n"
Jul 13 12:35:52.927: INFO: stdout: ""
Jul 13 12:35:52.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-2197 execpodg9wm4 -- /bin/sh -x -c nc -zv -t -w 2 18.191.135.33 31087'
Jul 13 12:35:53.187: INFO: stderr: "+ nc -zv -t -w 2 18.191.135.33 31087\nConnection to 18.191.135.33 31087 port [tcp/31087] succeeded!\n"
Jul 13 12:35:53.187: INFO: stdout: ""
Jul 13 12:35:53.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-2197 execpodg9wm4 -- /bin/sh -x -c nc -zv -t -w 2 18.191.151.100 31087'
Jul 13 12:35:53.432: INFO: stderr: "+ nc -zv -t -w 2 18.191.151.100 31087\nConnection to 18.191.151.100 31087 port [tcp/31087] succeeded!\n"
Jul 13 12:35:53.432: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:35:53.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2197" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:9.542 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":280,"completed":20,"skipped":425,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:35:53.447: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 12:35:53.507: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a719471-cfce-4cf5-ad65-9b6d592ffb6f" in namespace "downward-api-379" to be "success or failure"
Jul 13 12:35:53.515: INFO: Pod "downwardapi-volume-1a719471-cfce-4cf5-ad65-9b6d592ffb6f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.372169ms
Jul 13 12:35:55.520: INFO: Pod "downwardapi-volume-1a719471-cfce-4cf5-ad65-9b6d592ffb6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013236223s
STEP: Saw pod success
Jul 13 12:35:55.520: INFO: Pod "downwardapi-volume-1a719471-cfce-4cf5-ad65-9b6d592ffb6f" satisfied condition "success or failure"
Jul 13 12:35:55.524: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod downwardapi-volume-1a719471-cfce-4cf5-ad65-9b6d592ffb6f container client-container: <nil>
STEP: delete the pod
Jul 13 12:35:55.568: INFO: Waiting for pod downwardapi-volume-1a719471-cfce-4cf5-ad65-9b6d592ffb6f to disappear
Jul 13 12:35:55.572: INFO: Pod downwardapi-volume-1a719471-cfce-4cf5-ad65-9b6d592ffb6f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:35:55.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-379" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":21,"skipped":429,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:35:55.590: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-3c23ba9e-7d76-4ab2-a580-692453371439
STEP: Creating a pod to test consume secrets
Jul 13 12:35:55.649: INFO: Waiting up to 5m0s for pod "pod-secrets-4394d696-8672-4a5c-b4e6-e6563b37d6a6" in namespace "secrets-2538" to be "success or failure"
Jul 13 12:35:55.656: INFO: Pod "pod-secrets-4394d696-8672-4a5c-b4e6-e6563b37d6a6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.617777ms
Jul 13 12:35:57.660: INFO: Pod "pod-secrets-4394d696-8672-4a5c-b4e6-e6563b37d6a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01130781s
STEP: Saw pod success
Jul 13 12:35:57.660: INFO: Pod "pod-secrets-4394d696-8672-4a5c-b4e6-e6563b37d6a6" satisfied condition "success or failure"
Jul 13 12:35:57.664: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-secrets-4394d696-8672-4a5c-b4e6-e6563b37d6a6 container secret-env-test: <nil>
STEP: delete the pod
Jul 13 12:35:57.687: INFO: Waiting for pod pod-secrets-4394d696-8672-4a5c-b4e6-e6563b37d6a6 to disappear
Jul 13 12:35:57.690: INFO: Pod pod-secrets-4394d696-8672-4a5c-b4e6-e6563b37d6a6 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:35:57.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2538" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":280,"completed":22,"skipped":463,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:35:57.707: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:36:01.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4183" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":280,"completed":23,"skipped":489,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:36:01.798: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:36:05.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7706" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":280,"completed":24,"skipped":537,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:36:05.882: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 13 12:36:05.939: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7352 /api/v1/namespaces/watch-7352/configmaps/e2e-watch-test-watch-closed b4e582b1-a54b-4482-a48b-8416113c1a73 214829 0 2020-07-13 12:36:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 13 12:36:05.939: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7352 /api/v1/namespaces/watch-7352/configmaps/e2e-watch-test-watch-closed b4e582b1-a54b-4482-a48b-8416113c1a73 214830 0 2020-07-13 12:36:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 13 12:36:05.957: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7352 /api/v1/namespaces/watch-7352/configmaps/e2e-watch-test-watch-closed b4e582b1-a54b-4482-a48b-8416113c1a73 214831 0 2020-07-13 12:36:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 13 12:36:05.958: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7352 /api/v1/namespaces/watch-7352/configmaps/e2e-watch-test-watch-closed b4e582b1-a54b-4482-a48b-8416113c1a73 214832 0 2020-07-13 12:36:05 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:36:05.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7352" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":280,"completed":25,"skipped":547,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:36:05.973: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Jul 13 12:36:06.548: INFO: created pod pod-service-account-defaultsa
Jul 13 12:36:06.548: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 13 12:36:06.563: INFO: created pod pod-service-account-mountsa
Jul 13 12:36:06.563: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 13 12:36:06.573: INFO: created pod pod-service-account-nomountsa
Jul 13 12:36:06.573: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 13 12:36:06.589: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 13 12:36:06.589: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 13 12:36:06.616: INFO: created pod pod-service-account-mountsa-mountspec
Jul 13 12:36:06.616: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 13 12:36:06.631: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 13 12:36:06.631: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 13 12:36:06.654: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 13 12:36:06.654: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 13 12:36:06.676: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 13 12:36:06.676: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 13 12:36:06.706: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 13 12:36:06.706: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:36:06.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2174" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":280,"completed":26,"skipped":554,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:36:06.767: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 13 12:36:06.882: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9200 /api/v1/namespaces/watch-9200/configmaps/e2e-watch-test-configmap-a 41421fd6-4ae1-436a-bd70-dd3df53b5c09 214888 0 2020-07-13 12:36:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 13 12:36:06.883: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9200 /api/v1/namespaces/watch-9200/configmaps/e2e-watch-test-configmap-a 41421fd6-4ae1-436a-bd70-dd3df53b5c09 214888 0 2020-07-13 12:36:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 13 12:36:16.893: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9200 /api/v1/namespaces/watch-9200/configmaps/e2e-watch-test-configmap-a 41421fd6-4ae1-436a-bd70-dd3df53b5c09 215054 0 2020-07-13 12:36:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 13 12:36:16.893: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9200 /api/v1/namespaces/watch-9200/configmaps/e2e-watch-test-configmap-a 41421fd6-4ae1-436a-bd70-dd3df53b5c09 215054 0 2020-07-13 12:36:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 13 12:36:26.904: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9200 /api/v1/namespaces/watch-9200/configmaps/e2e-watch-test-configmap-a 41421fd6-4ae1-436a-bd70-dd3df53b5c09 215082 0 2020-07-13 12:36:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 13 12:36:26.904: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9200 /api/v1/namespaces/watch-9200/configmaps/e2e-watch-test-configmap-a 41421fd6-4ae1-436a-bd70-dd3df53b5c09 215082 0 2020-07-13 12:36:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 13 12:36:36.914: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9200 /api/v1/namespaces/watch-9200/configmaps/e2e-watch-test-configmap-a 41421fd6-4ae1-436a-bd70-dd3df53b5c09 215121 0 2020-07-13 12:36:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 13 12:36:36.914: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9200 /api/v1/namespaces/watch-9200/configmaps/e2e-watch-test-configmap-a 41421fd6-4ae1-436a-bd70-dd3df53b5c09 215121 0 2020-07-13 12:36:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 13 12:36:46.923: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9200 /api/v1/namespaces/watch-9200/configmaps/e2e-watch-test-configmap-b 39700889-3552-4f43-98eb-acce26c44d35 215159 0 2020-07-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 13 12:36:46.923: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9200 /api/v1/namespaces/watch-9200/configmaps/e2e-watch-test-configmap-b 39700889-3552-4f43-98eb-acce26c44d35 215159 0 2020-07-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 13 12:36:56.933: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9200 /api/v1/namespaces/watch-9200/configmaps/e2e-watch-test-configmap-b 39700889-3552-4f43-98eb-acce26c44d35 215198 0 2020-07-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 13 12:36:56.933: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9200 /api/v1/namespaces/watch-9200/configmaps/e2e-watch-test-configmap-b 39700889-3552-4f43-98eb-acce26c44d35 215198 0 2020-07-13 12:36:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:37:06.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9200" for this suite.

â€¢ [SLOW TEST:60.181 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":280,"completed":27,"skipped":584,"failed":0}
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:37:06.949: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Jul 13 12:37:07.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 create -f - --namespace=kubectl-7979'
Jul 13 12:37:07.325: INFO: stderr: ""
Jul 13 12:37:07.325: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 13 12:37:07.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7979'
Jul 13 12:37:07.424: INFO: stderr: ""
Jul 13 12:37:07.424: INFO: stdout: "update-demo-nautilus-ht4tw update-demo-nautilus-zps69 "
Jul 13 12:37:07.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-ht4tw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7979'
Jul 13 12:37:07.500: INFO: stderr: ""
Jul 13 12:37:07.500: INFO: stdout: ""
Jul 13 12:37:07.501: INFO: update-demo-nautilus-ht4tw is created but not running
Jul 13 12:37:12.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7979'
Jul 13 12:37:12.581: INFO: stderr: ""
Jul 13 12:37:12.581: INFO: stdout: "update-demo-nautilus-ht4tw update-demo-nautilus-zps69 "
Jul 13 12:37:12.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-ht4tw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7979'
Jul 13 12:37:12.661: INFO: stderr: ""
Jul 13 12:37:12.661: INFO: stdout: "true"
Jul 13 12:37:12.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-ht4tw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7979'
Jul 13 12:37:12.759: INFO: stderr: ""
Jul 13 12:37:12.759: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 12:37:12.759: INFO: validating pod update-demo-nautilus-ht4tw
Jul 13 12:37:12.770: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 12:37:12.770: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 12:37:12.770: INFO: update-demo-nautilus-ht4tw is verified up and running
Jul 13 12:37:12.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-zps69 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7979'
Jul 13 12:37:12.889: INFO: stderr: ""
Jul 13 12:37:12.889: INFO: stdout: "true"
Jul 13 12:37:12.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-zps69 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7979'
Jul 13 12:37:13.002: INFO: stderr: ""
Jul 13 12:37:13.002: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 12:37:13.002: INFO: validating pod update-demo-nautilus-zps69
Jul 13 12:37:13.018: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 12:37:13.018: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 12:37:13.018: INFO: update-demo-nautilus-zps69 is verified up and running
STEP: rolling-update to new replication controller
Jul 13 12:37:13.020: INFO: scanned /root for discovery docs: <nil>
Jul 13 12:37:13.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7979'
Jul 13 12:37:35.536: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 13 12:37:35.536: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 13 12:37:35.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7979'
Jul 13 12:37:35.623: INFO: stderr: ""
Jul 13 12:37:35.623: INFO: stdout: "update-demo-kitten-6fdtt update-demo-kitten-cx849 "
Jul 13 12:37:35.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-kitten-6fdtt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7979'
Jul 13 12:37:35.698: INFO: stderr: ""
Jul 13 12:37:35.698: INFO: stdout: "true"
Jul 13 12:37:35.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-kitten-6fdtt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7979'
Jul 13 12:37:35.778: INFO: stderr: ""
Jul 13 12:37:35.778: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 13 12:37:35.778: INFO: validating pod update-demo-kitten-6fdtt
Jul 13 12:37:35.786: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 13 12:37:35.786: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 13 12:37:35.786: INFO: update-demo-kitten-6fdtt is verified up and running
Jul 13 12:37:35.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-kitten-cx849 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7979'
Jul 13 12:37:35.858: INFO: stderr: ""
Jul 13 12:37:35.858: INFO: stdout: "true"
Jul 13 12:37:35.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-kitten-cx849 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7979'
Jul 13 12:37:35.937: INFO: stderr: ""
Jul 13 12:37:35.937: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 13 12:37:35.937: INFO: validating pod update-demo-kitten-cx849
Jul 13 12:37:35.944: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 13 12:37:35.944: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 13 12:37:35.944: INFO: update-demo-kitten-cx849 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:37:35.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7979" for this suite.

â€¢ [SLOW TEST:29.012 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":280,"completed":28,"skipped":584,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:37:35.961: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Jul 13 12:37:36.008: INFO: Waiting up to 5m0s for pod "var-expansion-c2ffa2e1-6568-437b-85c4-efeb29e1abe2" in namespace "var-expansion-7642" to be "success or failure"
Jul 13 12:37:36.015: INFO: Pod "var-expansion-c2ffa2e1-6568-437b-85c4-efeb29e1abe2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.481001ms
Jul 13 12:37:38.021: INFO: Pod "var-expansion-c2ffa2e1-6568-437b-85c4-efeb29e1abe2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012962658s
STEP: Saw pod success
Jul 13 12:37:38.021: INFO: Pod "var-expansion-c2ffa2e1-6568-437b-85c4-efeb29e1abe2" satisfied condition "success or failure"
Jul 13 12:37:38.025: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod var-expansion-c2ffa2e1-6568-437b-85c4-efeb29e1abe2 container dapi-container: <nil>
STEP: delete the pod
Jul 13 12:37:38.069: INFO: Waiting for pod var-expansion-c2ffa2e1-6568-437b-85c4-efeb29e1abe2 to disappear
Jul 13 12:37:38.073: INFO: Pod var-expansion-c2ffa2e1-6568-437b-85c4-efeb29e1abe2 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:37:38.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7642" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":280,"completed":29,"skipped":587,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:37:38.094: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 12:37:38.656: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 13 12:37:40.669: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240658, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240658, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240658, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240658, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 12:37:43.704: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:37:43.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9116" for this suite.
STEP: Destroying namespace "webhook-9116-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.773 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":280,"completed":30,"skipped":591,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:37:43.868: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8295
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8295
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8295
Jul 13 12:37:43.947: INFO: Found 0 stateful pods, waiting for 1
Jul 13 12:37:53.952: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 13 12:37:53.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-8295 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 13 12:37:54.232: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 13 12:37:54.232: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 13 12:37:54.232: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 13 12:37:54.239: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 13 12:38:04.245: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 12:38:04.245: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 12:38:04.262: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998465s
Jul 13 12:38:05.267: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995853043s
Jul 13 12:38:06.273: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990324754s
Jul 13 12:38:07.278: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.98504751s
Jul 13 12:38:08.283: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979971016s
Jul 13 12:38:09.289: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974978995s
Jul 13 12:38:10.294: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.969156429s
Jul 13 12:38:11.299: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.96400021s
Jul 13 12:38:12.306: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.958824359s
Jul 13 12:38:13.315: INFO: Verifying statefulset ss doesn't scale past 1 for another 951.69576ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8295
Jul 13 12:38:14.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-8295 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:38:14.509: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 13 12:38:14.509: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 13 12:38:14.509: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 13 12:38:14.514: INFO: Found 1 stateful pods, waiting for 3
Jul 13 12:38:24.520: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 12:38:24.520: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 12:38:24.520: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 13 12:38:24.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-8295 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 13 12:38:24.719: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 13 12:38:24.719: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 13 12:38:24.719: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 13 12:38:24.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-8295 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 13 12:38:25.014: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 13 12:38:25.014: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 13 12:38:25.014: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 13 12:38:25.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-8295 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 13 12:38:25.216: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 13 12:38:25.217: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 13 12:38:25.217: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 13 12:38:25.217: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 12:38:25.221: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 13 12:38:35.230: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 12:38:35.230: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 12:38:35.230: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 12:38:35.250: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998495s
Jul 13 12:38:36.255: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991167756s
Jul 13 12:38:37.260: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985845427s
Jul 13 12:38:38.266: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980407659s
Jul 13 12:38:39.272: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974410144s
Jul 13 12:38:40.278: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968814946s
Jul 13 12:38:41.283: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963226165s
Jul 13 12:38:42.289: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957204702s
Jul 13 12:38:43.295: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.951579532s
Jul 13 12:38:44.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 945.85098ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8295
Jul 13 12:38:45.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-8295 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:38:45.506: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 13 12:38:45.506: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 13 12:38:45.506: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 13 12:38:45.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-8295 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:38:45.723: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 13 12:38:45.723: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 13 12:38:45.723: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 13 12:38:45.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-8295 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:38:45.927: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 13 12:38:45.927: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 13 12:38:45.927: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 13 12:38:45.927: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jul 13 12:39:05.947: INFO: Deleting all statefulset in ns statefulset-8295
Jul 13 12:39:05.953: INFO: Scaling statefulset ss to 0
Jul 13 12:39:05.967: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 12:39:05.971: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:39:05.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8295" for this suite.

â€¢ [SLOW TEST:82.139 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":280,"completed":31,"skipped":606,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:39:06.008: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 12:39:06.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 create -f - --namespace=kubectl-3100'
Jul 13 12:39:06.341: INFO: stderr: ""
Jul 13 12:39:06.341: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Jul 13 12:39:06.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 create -f - --namespace=kubectl-3100'
Jul 13 12:39:06.581: INFO: stderr: ""
Jul 13 12:39:06.582: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jul 13 12:39:07.587: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 13 12:39:07.587: INFO: Found 0 / 1
Jul 13 12:39:08.587: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 13 12:39:08.587: INFO: Found 0 / 1
Jul 13 12:39:09.588: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 13 12:39:09.588: INFO: Found 1 / 1
Jul 13 12:39:09.588: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 13 12:39:09.591: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 13 12:39:09.592: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 13 12:39:09.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 describe pod agnhost-master-rslfh --namespace=kubectl-3100'
Jul 13 12:39:09.707: INFO: stderr: ""
Jul 13 12:39:09.707: INFO: stdout: "Name:         agnhost-master-rslfh\nNamespace:    kubectl-3100\nPriority:     0\nNode:         ip-10-0-2-22.us-east-2.compute.internal/10.0.2.22\nStart Time:   Mon, 13 Jul 2020 12:39:06 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 10.20.168.135/32\nStatus:       Running\nIP:           10.20.168.135\nIPs:\n  IP:           10.20.168.135\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://7d8c35c7ee6974725808f72c3acea89ac0d571a0d1f60dc1ce7033e4802e7660\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 13 Jul 2020 12:39:07 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-wtkqm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-wtkqm:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-wtkqm\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                              Message\n  ----    ------     ----       ----                                              -------\n  Normal  Scheduled  <unknown>  default-scheduler                                 Successfully assigned kubectl-3100/agnhost-master-rslfh to ip-10-0-2-22.us-east-2.compute.internal\n  Normal  Pulled     2s         kubelet, ip-10-0-2-22.us-east-2.compute.internal  Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    2s         kubelet, ip-10-0-2-22.us-east-2.compute.internal  Created container agnhost-master\n  Normal  Started    2s         kubelet, ip-10-0-2-22.us-east-2.compute.internal  Started container agnhost-master\n"
Jul 13 12:39:09.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 describe rc agnhost-master --namespace=kubectl-3100'
Jul 13 12:39:09.807: INFO: stderr: ""
Jul 13 12:39:09.807: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-3100\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-master-rslfh\n"
Jul 13 12:39:09.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 describe service agnhost-master --namespace=kubectl-3100'
Jul 13 12:39:09.905: INFO: stderr: ""
Jul 13 12:39:09.905: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-3100\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.21.81.65\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.20.168.135:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 13 12:39:09.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 describe node ip-10-0-1-82.us-east-2.compute.internal'
Jul 13 12:39:10.033: INFO: stderr: ""
Jul 13 12:39:10.033: INFO: stdout: "Name:               ip-10-0-1-82.us-east-2.compute.internal\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t2.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-2\n                    failure-domain.beta.kubernetes.io/zone=us-east-2a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-1-82.us-east-2.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=t2.medium\n                    topology.kubernetes.io/region=us-east-2\n                    topology.kubernetes.io/zone=us-east-2a\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.1.82/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.20.171.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 12 Jul 2020 18:20:45 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-1-82.us-east-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 13 Jul 2020 12:39:00 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sun, 12 Jul 2020 18:21:35 +0000   Sun, 12 Jul 2020 18:21:35 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 13 Jul 2020 12:35:32 +0000   Sun, 12 Jul 2020 18:20:45 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 13 Jul 2020 12:35:32 +0000   Sun, 12 Jul 2020 18:20:45 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 13 Jul 2020 12:35:32 +0000   Sun, 12 Jul 2020 18:20:45 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 13 Jul 2020 12:35:32 +0000   Sun, 12 Jul 2020 18:21:28 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.0.1.82\n  ExternalIP:   18.191.135.33\n  Hostname:     ip-10-0-1-82.us-east-2.compute.internal\n  InternalDNS:  ip-10-0-1-82.us-east-2.compute.internal\n  ExternalDNS:  ec2-18-191-135-33.us-east-2.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         2\n  ephemeral-storage:           50758604Ki\n  hugepages-2Mi:               0\n  memory:                      4038260Ki\n  pods:                        200\nAllocatable:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         2\n  ephemeral-storage:           46779129369\n  hugepages-2Mi:               0\n  memory:                      3935860Ki\n  pods:                        200\nSystem Info:\n  Machine ID:                 c0aa75d8f21b4364b70bc9549122b6b6\n  System UUID:                EC282F3A-61AC-C559-B649-CE558D80852F\n  Boot ID:                    6eebf3db-52e9-494f-b7de-e8e44e84f54e\n  Kernel Version:             4.15.0-1054-aws\n  OS Image:                   Ubuntu 18.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.11\n  Kubelet Version:            v1.17.6\n  Kube-Proxy Version:         v1.17.6\nPodCIDR:                      10.20.0.0/24\nPodCIDRs:                     10.20.0.0/24\nProviderID:                   aws:///us-east-2a/i-0a742de6cb8c1c9fa\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-6d84f9d87d-zqq65                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         18h\n  kube-system                 calico-node-xqfbv                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         18h\n  kube-system                 calico-typha-569c98c8c5-q4jds                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         18h\n  kube-system                 calicoctl                                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         18h\n  kube-system                 k8s-master-ip-10-0-1-82.us-east-2.compute.internal         0 (0%)        0 (0%)      0 (0%)           0 (0%)         18h\n  kubernetes-dashboard        dashboard-metrics-scraper-c79c65bb7-zg4zf                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         18h\n  kubernetes-dashboard        kubernetes-dashboard-6b7d67fff4-s2c44                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         18h\n  pf9-monitoring              node-exporter-gqvhs                                        102m (5%)     250m (12%)  180Mi (4%)       180Mi (4%)     18h\n  platform9-system            pf9-sentry-865fcbbb89-jhgf5                                500m (25%)    500m (25%)  128Mi (3%)       128Mi (3%)     18h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-46t76    0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m24s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         852m (42%)  750m (37%)\n  memory                      308Mi (8%)  308Mi (8%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:                       <none>\n"
Jul 13 12:39:10.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 describe namespace kubectl-3100'
Jul 13 12:39:10.119: INFO: stderr: ""
Jul 13 12:39:10.119: INFO: stdout: "Name:         kubectl-3100\nLabels:       e2e-framework=kubectl\n              e2e-run=46cd4b81-82f3-45d6-af66-8f57539d2678\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:39:10.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3100" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":280,"completed":32,"skipped":607,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:39:10.135: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 12:39:10.184: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7d6f4d5-f63f-4733-bd3c-06cf93b10ca7" in namespace "projected-1974" to be "success or failure"
Jul 13 12:39:10.188: INFO: Pod "downwardapi-volume-a7d6f4d5-f63f-4733-bd3c-06cf93b10ca7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.36626ms
Jul 13 12:39:12.201: INFO: Pod "downwardapi-volume-a7d6f4d5-f63f-4733-bd3c-06cf93b10ca7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016983463s
STEP: Saw pod success
Jul 13 12:39:12.201: INFO: Pod "downwardapi-volume-a7d6f4d5-f63f-4733-bd3c-06cf93b10ca7" satisfied condition "success or failure"
Jul 13 12:39:12.209: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod downwardapi-volume-a7d6f4d5-f63f-4733-bd3c-06cf93b10ca7 container client-container: <nil>
STEP: delete the pod
Jul 13 12:39:12.257: INFO: Waiting for pod downwardapi-volume-a7d6f4d5-f63f-4733-bd3c-06cf93b10ca7 to disappear
Jul 13 12:39:12.260: INFO: Pod downwardapi-volume-a7d6f4d5-f63f-4733-bd3c-06cf93b10ca7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:39:12.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1974" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":33,"skipped":615,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:39:12.276: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jul 13 12:39:12.318: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:39:15.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1889" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":280,"completed":34,"skipped":616,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:39:15.287: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3701
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3701
STEP: creating replication controller externalsvc in namespace services-3701
I0713 12:39:15.412857      20 runners.go:189] Created replication controller with name: externalsvc, namespace: services-3701, replica count: 2
I0713 12:39:18.464923      20 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jul 13 12:39:18.505: INFO: Creating new exec pod
Jul 13 12:39:20.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-3701 execpodch2gx -- /bin/sh -x -c nslookup nodeport-service'
Jul 13 12:39:20.782: INFO: stderr: "+ nslookup nodeport-service\n"
Jul 13 12:39:20.782: INFO: stdout: "Server:\t\t10.21.0.10\nAddress:\t10.21.0.10#53\n\nnodeport-service.services-3701.svc.cluster.local\tcanonical name = externalsvc.services-3701.svc.cluster.local.\nName:\texternalsvc.services-3701.svc.cluster.local\nAddress: 10.21.146.41\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3701, will wait for the garbage collector to delete the pods
Jul 13 12:39:20.848: INFO: Deleting ReplicationController externalsvc took: 10.479013ms
Jul 13 12:39:21.548: INFO: Terminating ReplicationController externalsvc pods took: 700.249313ms
Jul 13 12:39:33.686: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:39:33.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3701" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:18.441 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":280,"completed":35,"skipped":629,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:39:33.728: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jul 13 12:39:33.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 create -f - --namespace=kubectl-8623'
Jul 13 12:39:34.000: INFO: stderr: ""
Jul 13 12:39:34.000: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 13 12:39:34.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8623'
Jul 13 12:39:34.098: INFO: stderr: ""
Jul 13 12:39:34.098: INFO: stdout: "update-demo-nautilus-dgf9d update-demo-nautilus-gww77 "
Jul 13 12:39:34.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-dgf9d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8623'
Jul 13 12:39:34.170: INFO: stderr: ""
Jul 13 12:39:34.170: INFO: stdout: ""
Jul 13 12:39:34.170: INFO: update-demo-nautilus-dgf9d is created but not running
Jul 13 12:39:39.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8623'
Jul 13 12:39:39.247: INFO: stderr: ""
Jul 13 12:39:39.247: INFO: stdout: "update-demo-nautilus-dgf9d update-demo-nautilus-gww77 "
Jul 13 12:39:39.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-dgf9d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8623'
Jul 13 12:39:39.322: INFO: stderr: ""
Jul 13 12:39:39.322: INFO: stdout: "true"
Jul 13 12:39:39.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-dgf9d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8623'
Jul 13 12:39:39.402: INFO: stderr: ""
Jul 13 12:39:39.402: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 12:39:39.402: INFO: validating pod update-demo-nautilus-dgf9d
Jul 13 12:39:39.411: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 12:39:39.411: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 12:39:39.411: INFO: update-demo-nautilus-dgf9d is verified up and running
Jul 13 12:39:39.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-gww77 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8623'
Jul 13 12:39:39.500: INFO: stderr: ""
Jul 13 12:39:39.500: INFO: stdout: "true"
Jul 13 12:39:39.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-gww77 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8623'
Jul 13 12:39:39.573: INFO: stderr: ""
Jul 13 12:39:39.574: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 12:39:39.574: INFO: validating pod update-demo-nautilus-gww77
Jul 13 12:39:39.583: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 12:39:39.583: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 12:39:39.583: INFO: update-demo-nautilus-gww77 is verified up and running
STEP: scaling down the replication controller
Jul 13 12:39:39.585: INFO: scanned /root for discovery docs: <nil>
Jul 13 12:39:39.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-8623'
Jul 13 12:39:40.685: INFO: stderr: ""
Jul 13 12:39:40.685: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 13 12:39:40.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8623'
Jul 13 12:39:40.759: INFO: stderr: ""
Jul 13 12:39:40.759: INFO: stdout: "update-demo-nautilus-dgf9d update-demo-nautilus-gww77 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 13 12:39:45.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8623'
Jul 13 12:39:45.866: INFO: stderr: ""
Jul 13 12:39:45.866: INFO: stdout: "update-demo-nautilus-dgf9d "
Jul 13 12:39:45.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-dgf9d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8623'
Jul 13 12:39:45.967: INFO: stderr: ""
Jul 13 12:39:45.967: INFO: stdout: "true"
Jul 13 12:39:45.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-dgf9d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8623'
Jul 13 12:39:46.074: INFO: stderr: ""
Jul 13 12:39:46.074: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 12:39:46.074: INFO: validating pod update-demo-nautilus-dgf9d
Jul 13 12:39:46.080: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 12:39:46.080: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 12:39:46.080: INFO: update-demo-nautilus-dgf9d is verified up and running
STEP: scaling up the replication controller
Jul 13 12:39:46.083: INFO: scanned /root for discovery docs: <nil>
Jul 13 12:39:46.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-8623'
Jul 13 12:39:47.249: INFO: stderr: ""
Jul 13 12:39:47.249: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 13 12:39:47.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8623'
Jul 13 12:39:47.332: INFO: stderr: ""
Jul 13 12:39:47.332: INFO: stdout: "update-demo-nautilus-dgf9d update-demo-nautilus-rq697 "
Jul 13 12:39:47.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-dgf9d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8623'
Jul 13 12:39:47.447: INFO: stderr: ""
Jul 13 12:39:47.447: INFO: stdout: "true"
Jul 13 12:39:47.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-dgf9d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8623'
Jul 13 12:39:47.527: INFO: stderr: ""
Jul 13 12:39:47.527: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 12:39:47.527: INFO: validating pod update-demo-nautilus-dgf9d
Jul 13 12:39:47.534: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 12:39:47.534: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 12:39:47.534: INFO: update-demo-nautilus-dgf9d is verified up and running
Jul 13 12:39:47.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-rq697 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8623'
Jul 13 12:39:47.619: INFO: stderr: ""
Jul 13 12:39:47.619: INFO: stdout: ""
Jul 13 12:39:47.619: INFO: update-demo-nautilus-rq697 is created but not running
Jul 13 12:39:52.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8623'
Jul 13 12:39:52.711: INFO: stderr: ""
Jul 13 12:39:52.711: INFO: stdout: "update-demo-nautilus-dgf9d update-demo-nautilus-rq697 "
Jul 13 12:39:52.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-dgf9d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8623'
Jul 13 12:39:52.808: INFO: stderr: ""
Jul 13 12:39:52.808: INFO: stdout: "true"
Jul 13 12:39:52.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-dgf9d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8623'
Jul 13 12:39:52.916: INFO: stderr: ""
Jul 13 12:39:52.916: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 12:39:52.916: INFO: validating pod update-demo-nautilus-dgf9d
Jul 13 12:39:52.922: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 12:39:52.922: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 12:39:52.922: INFO: update-demo-nautilus-dgf9d is verified up and running
Jul 13 12:39:52.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-rq697 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8623'
Jul 13 12:39:53.034: INFO: stderr: ""
Jul 13 12:39:53.034: INFO: stdout: "true"
Jul 13 12:39:53.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-rq697 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8623'
Jul 13 12:39:53.148: INFO: stderr: ""
Jul 13 12:39:53.148: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 12:39:53.148: INFO: validating pod update-demo-nautilus-rq697
Jul 13 12:39:53.156: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 12:39:53.156: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 12:39:53.156: INFO: update-demo-nautilus-rq697 is verified up and running
STEP: using delete to clean up resources
Jul 13 12:39:53.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete --grace-period=0 --force -f - --namespace=kubectl-8623'
Jul 13 12:39:53.255: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 12:39:53.255: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 13 12:39:53.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8623'
Jul 13 12:39:53.336: INFO: stderr: "No resources found in kubectl-8623 namespace.\n"
Jul 13 12:39:53.336: INFO: stdout: ""
Jul 13 12:39:53.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -l name=update-demo --namespace=kubectl-8623 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 13 12:39:53.421: INFO: stderr: ""
Jul 13 12:39:53.421: INFO: stdout: "update-demo-nautilus-dgf9d\nupdate-demo-nautilus-rq697\n"
Jul 13 12:39:53.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8623'
Jul 13 12:39:54.058: INFO: stderr: "No resources found in kubectl-8623 namespace.\n"
Jul 13 12:39:54.058: INFO: stdout: ""
Jul 13 12:39:54.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -l name=update-demo --namespace=kubectl-8623 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 13 12:39:54.157: INFO: stderr: ""
Jul 13 12:39:54.157: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:39:54.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8623" for this suite.

â€¢ [SLOW TEST:20.443 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":280,"completed":36,"skipped":643,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:39:54.172: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 12:39:54.206: INFO: Creating deployment "test-recreate-deployment"
Jul 13 12:39:54.213: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 13 12:39:54.234: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 13 12:39:56.242: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 13 12:39:56.246: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240794, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240794, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240794, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730240794, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 12:39:58.251: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 13 12:39:58.261: INFO: Updating deployment test-recreate-deployment
Jul 13 12:39:58.261: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jul 13 12:39:58.386: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-7519 /apis/apps/v1/namespaces/deployment-7519/deployments/test-recreate-deployment 8dc221fe-0828-4166-bd51-f9ba9e08b4cf 216655 2 2020-07-13 12:39:54 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0020652f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-07-13 12:39:58 +0000 UTC,LastTransitionTime:2020-07-13 12:39:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-07-13 12:39:58 +0000 UTC,LastTransitionTime:2020-07-13 12:39:54 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jul 13 12:39:58.390: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-7519 /apis/apps/v1/namespaces/deployment-7519/replicasets/test-recreate-deployment-5f94c574ff 2d89906f-b8b6-4804-a2b4-f5034d6a2d3e 216654 1 2020-07-13 12:39:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 8dc221fe-0828-4166-bd51-f9ba9e08b4cf 0xc00248d227 0xc00248d228}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00248d288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 13 12:39:58.390: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 13 12:39:58.390: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-7519 /apis/apps/v1/namespaces/deployment-7519/replicasets/test-recreate-deployment-799c574856 2c1bb2fa-25d3-4df8-b350-1c316187f28f 216643 2 2020-07-13 12:39:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 8dc221fe-0828-4166-bd51-f9ba9e08b4cf 0xc00248d2f7 0xc00248d2f8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00248d368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 13 12:39:58.395: INFO: Pod "test-recreate-deployment-5f94c574ff-rzs5w" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-rzs5w test-recreate-deployment-5f94c574ff- deployment-7519 /api/v1/namespaces/deployment-7519/pods/test-recreate-deployment-5f94c574ff-rzs5w afb8abe8-8915-4ba8-a9a1-339e9a85ebc4 216656 0 2020-07-13 12:39:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 2d89906f-b8b6-4804-a2b4-f5034d6a2d3e 0xc0020656b7 0xc0020656b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zkxhr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zkxhr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zkxhr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-22.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 12:39:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 12:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 12:39:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 12:39:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.22,PodIP:,StartTime:2020-07-13 12:39:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:39:58.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7519" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":37,"skipped":658,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:39:58.409: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 12:39:58.448: INFO: Creating ReplicaSet my-hostname-basic-7ed0d7c1-6c41-495b-a42f-f431e10d1683
Jul 13 12:39:58.459: INFO: Pod name my-hostname-basic-7ed0d7c1-6c41-495b-a42f-f431e10d1683: Found 0 pods out of 1
Jul 13 12:40:03.470: INFO: Pod name my-hostname-basic-7ed0d7c1-6c41-495b-a42f-f431e10d1683: Found 1 pods out of 1
Jul 13 12:40:03.470: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-7ed0d7c1-6c41-495b-a42f-f431e10d1683" is running
Jul 13 12:40:03.474: INFO: Pod "my-hostname-basic-7ed0d7c1-6c41-495b-a42f-f431e10d1683-vhgk6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-13 12:39:58 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-13 12:40:00 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-13 12:40:00 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-13 12:39:58 +0000 UTC Reason: Message:}])
Jul 13 12:40:03.474: INFO: Trying to dial the pod
Jul 13 12:40:08.491: INFO: Controller my-hostname-basic-7ed0d7c1-6c41-495b-a42f-f431e10d1683: Got expected result from replica 1 [my-hostname-basic-7ed0d7c1-6c41-495b-a42f-f431e10d1683-vhgk6]: "my-hostname-basic-7ed0d7c1-6c41-495b-a42f-f431e10d1683-vhgk6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:40:08.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6566" for this suite.

â€¢ [SLOW TEST:10.096 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":38,"skipped":663,"failed":0}
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:40:08.505: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 13 12:40:12.591: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 12:40:12.597: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 13 12:40:14.598: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 12:40:14.603: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 13 12:40:16.598: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 12:40:16.603: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 13 12:40:18.598: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 12:40:18.603: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 13 12:40:20.598: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 12:40:20.603: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 13 12:40:22.598: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 13 12:40:22.603: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:40:22.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8318" for this suite.

â€¢ [SLOW TEST:14.134 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":280,"completed":39,"skipped":663,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:40:22.640: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-2470a3d6-79ca-4f3d-ae48-3f341173fe55 in namespace container-probe-9835
Jul 13 12:40:24.733: INFO: Started pod busybox-2470a3d6-79ca-4f3d-ae48-3f341173fe55 in namespace container-probe-9835
STEP: checking the pod's current state and verifying that restartCount is present
Jul 13 12:40:24.738: INFO: Initial restart count of pod busybox-2470a3d6-79ca-4f3d-ae48-3f341173fe55 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:44:25.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9835" for this suite.

â€¢ [SLOW TEST:242.761 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":40,"skipped":674,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:44:25.403: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 12:44:25.460: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 13 12:44:30.466: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 13 12:44:30.466: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jul 13 12:44:34.504: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9305 /apis/apps/v1/namespaces/deployment-9305/deployments/test-cleanup-deployment af3c0dee-b479-4fc7-8b30-c965af50360f 217746 1 2020-07-13 12:44:30 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002f827b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-07-13 12:44:30 +0000 UTC,LastTransitionTime:2020-07-13 12:44:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-55ffc6b7b6" has successfully progressed.,LastUpdateTime:2020-07-13 12:44:32 +0000 UTC,LastTransitionTime:2020-07-13 12:44:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 13 12:44:34.513: INFO: New ReplicaSet "test-cleanup-deployment-55ffc6b7b6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6  deployment-9305 /apis/apps/v1/namespaces/deployment-9305/replicasets/test-cleanup-deployment-55ffc6b7b6 7bef698d-7bd5-4e64-885b-48cfcee393f6 217736 1 2020-07-13 12:44:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment af3c0dee-b479-4fc7-8b30-c965af50360f 0xc002f82bf7 0xc002f82bf8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55ffc6b7b6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002f82c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 13 12:44:34.521: INFO: Pod "test-cleanup-deployment-55ffc6b7b6-lz6zn" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6-lz6zn test-cleanup-deployment-55ffc6b7b6- deployment-9305 /api/v1/namespaces/deployment-9305/pods/test-cleanup-deployment-55ffc6b7b6-lz6zn a584f4fe-2359-43a3-8232-e895def96c9a 217735 0 2020-07-13 12:44:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[cni.projectcalico.org/podIP:10.20.168.139/32] [{apps/v1 ReplicaSet test-cleanup-deployment-55ffc6b7b6 7bef698d-7bd5-4e64-885b-48cfcee393f6 0xc002f83047 0xc002f83048}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-b9bxd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-b9bxd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-b9bxd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-22.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 12:44:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 12:44:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 12:44:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 12:44:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.22,PodIP:10.20.168.139,StartTime:2020-07-13 12:44:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-13 12:44:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://29652929f2e4849cc52616ff1c8f84959e7912fbba12c747f74eba3d9f4b241a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.168.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:44:34.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9305" for this suite.

â€¢ [SLOW TEST:9.132 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":280,"completed":41,"skipped":676,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:44:34.536: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0713 12:45:14.618411      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 13 12:45:14.618: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:45:14.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2637" for this suite.

â€¢ [SLOW TEST:40.096 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":280,"completed":42,"skipped":715,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:45:14.633: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-a4e93e74-128d-4ee4-8d9a-f9db22a6df4b in namespace container-probe-9888
Jul 13 12:45:16.704: INFO: Started pod liveness-a4e93e74-128d-4ee4-8d9a-f9db22a6df4b in namespace container-probe-9888
STEP: checking the pod's current state and verifying that restartCount is present
Jul 13 12:45:16.708: INFO: Initial restart count of pod liveness-a4e93e74-128d-4ee4-8d9a-f9db22a6df4b is 0
Jul 13 12:45:32.759: INFO: Restart count of pod container-probe-9888/liveness-a4e93e74-128d-4ee4-8d9a-f9db22a6df4b is now 1 (16.050951918s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:45:32.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9888" for this suite.

â€¢ [SLOW TEST:18.162 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":43,"skipped":752,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:45:32.796: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 12:45:32.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 version'
Jul 13 12:45:32.962: INFO: stderr: ""
Jul 13 12:45:32.962: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.6\", GitCommit:\"d32e40e20d167e103faf894261614c5b45c44198\", GitTreeState:\"clean\", BuildDate:\"2020-05-20T13:16:24Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.6\", GitCommit:\"d32e40e20d167e103faf894261614c5b45c44198\", GitTreeState:\"clean\", BuildDate:\"2020-05-20T13:08:34Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:45:32.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7791" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":280,"completed":44,"skipped":767,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:45:32.976: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-55bb2184-664b-48fa-a875-25a7f9dea79c in namespace container-probe-7671
Jul 13 12:45:35.044: INFO: Started pod liveness-55bb2184-664b-48fa-a875-25a7f9dea79c in namespace container-probe-7671
STEP: checking the pod's current state and verifying that restartCount is present
Jul 13 12:45:35.048: INFO: Initial restart count of pod liveness-55bb2184-664b-48fa-a875-25a7f9dea79c is 0
Jul 13 12:45:55.101: INFO: Restart count of pod container-probe-7671/liveness-55bb2184-664b-48fa-a875-25a7f9dea79c is now 1 (20.053175888s elapsed)
Jul 13 12:46:15.158: INFO: Restart count of pod container-probe-7671/liveness-55bb2184-664b-48fa-a875-25a7f9dea79c is now 2 (40.109488413s elapsed)
Jul 13 12:46:35.211: INFO: Restart count of pod container-probe-7671/liveness-55bb2184-664b-48fa-a875-25a7f9dea79c is now 3 (1m0.162864003s elapsed)
Jul 13 12:46:55.262: INFO: Restart count of pod container-probe-7671/liveness-55bb2184-664b-48fa-a875-25a7f9dea79c is now 4 (1m20.2136498s elapsed)
Jul 13 12:47:55.425: INFO: Restart count of pod container-probe-7671/liveness-55bb2184-664b-48fa-a875-25a7f9dea79c is now 5 (2m20.377338833s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:47:55.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7671" for this suite.

â€¢ [SLOW TEST:142.486 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":280,"completed":45,"skipped":790,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:47:55.464: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 12:47:55.528: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f7fbe6f-3fd7-42d3-b15f-2aaa7a80e1c5" in namespace "projected-3094" to be "success or failure"
Jul 13 12:47:55.533: INFO: Pod "downwardapi-volume-5f7fbe6f-3fd7-42d3-b15f-2aaa7a80e1c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.858804ms
Jul 13 12:47:57.538: INFO: Pod "downwardapi-volume-5f7fbe6f-3fd7-42d3-b15f-2aaa7a80e1c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009530504s
STEP: Saw pod success
Jul 13 12:47:57.538: INFO: Pod "downwardapi-volume-5f7fbe6f-3fd7-42d3-b15f-2aaa7a80e1c5" satisfied condition "success or failure"
Jul 13 12:47:57.542: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod downwardapi-volume-5f7fbe6f-3fd7-42d3-b15f-2aaa7a80e1c5 container client-container: <nil>
STEP: delete the pod
Jul 13 12:47:57.584: INFO: Waiting for pod downwardapi-volume-5f7fbe6f-3fd7-42d3-b15f-2aaa7a80e1c5 to disappear
Jul 13 12:47:57.590: INFO: Pod downwardapi-volume-5f7fbe6f-3fd7-42d3-b15f-2aaa7a80e1c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:47:57.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3094" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":46,"skipped":816,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:47:57.604: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 12:47:57.652: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38edd806-4f22-4cfe-93de-1b180e9871e9" in namespace "downward-api-8035" to be "success or failure"
Jul 13 12:47:57.660: INFO: Pod "downwardapi-volume-38edd806-4f22-4cfe-93de-1b180e9871e9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.206374ms
Jul 13 12:47:59.666: INFO: Pod "downwardapi-volume-38edd806-4f22-4cfe-93de-1b180e9871e9": Phase="Running", Reason="", readiness=true. Elapsed: 2.013634345s
Jul 13 12:48:01.671: INFO: Pod "downwardapi-volume-38edd806-4f22-4cfe-93de-1b180e9871e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019184043s
STEP: Saw pod success
Jul 13 12:48:01.671: INFO: Pod "downwardapi-volume-38edd806-4f22-4cfe-93de-1b180e9871e9" satisfied condition "success or failure"
Jul 13 12:48:01.675: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod downwardapi-volume-38edd806-4f22-4cfe-93de-1b180e9871e9 container client-container: <nil>
STEP: delete the pod
Jul 13 12:48:01.703: INFO: Waiting for pod downwardapi-volume-38edd806-4f22-4cfe-93de-1b180e9871e9 to disappear
Jul 13 12:48:01.707: INFO: Pod downwardapi-volume-38edd806-4f22-4cfe-93de-1b180e9871e9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:48:01.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8035" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":47,"skipped":850,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:48:01.722: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-7484
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 13 12:48:01.762: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 13 12:48:25.958: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.171.230 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7484 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 12:48:25.959: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 12:48:27.111: INFO: Found all expected endpoints: [netserver-0]
Jul 13 12:48:27.116: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.92.101 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7484 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 12:48:27.116: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 12:48:28.298: INFO: Found all expected endpoints: [netserver-1]
Jul 13 12:48:28.302: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.168.147 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7484 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 12:48:28.302: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 12:48:29.419: INFO: Found all expected endpoints: [netserver-2]
Jul 13 12:48:29.425: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.23.182 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7484 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 12:48:29.425: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 12:48:30.544: INFO: Found all expected endpoints: [netserver-3]
Jul 13 12:48:30.549: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.20.234.32 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7484 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 12:48:30.549: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 12:48:31.665: INFO: Found all expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:48:31.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7484" for this suite.

â€¢ [SLOW TEST:29.957 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":48,"skipped":870,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:48:31.681: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 12:48:33.780: INFO: Waiting up to 5m0s for pod "client-envvars-58caf6ef-82ba-4efd-a88e-1f8986fc7941" in namespace "pods-8857" to be "success or failure"
Jul 13 12:48:33.787: INFO: Pod "client-envvars-58caf6ef-82ba-4efd-a88e-1f8986fc7941": Phase="Pending", Reason="", readiness=false. Elapsed: 7.337021ms
Jul 13 12:48:35.792: INFO: Pod "client-envvars-58caf6ef-82ba-4efd-a88e-1f8986fc7941": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011987432s
STEP: Saw pod success
Jul 13 12:48:35.792: INFO: Pod "client-envvars-58caf6ef-82ba-4efd-a88e-1f8986fc7941" satisfied condition "success or failure"
Jul 13 12:48:35.796: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod client-envvars-58caf6ef-82ba-4efd-a88e-1f8986fc7941 container env3cont: <nil>
STEP: delete the pod
Jul 13 12:48:35.818: INFO: Waiting for pod client-envvars-58caf6ef-82ba-4efd-a88e-1f8986fc7941 to disappear
Jul 13 12:48:35.822: INFO: Pod client-envvars-58caf6ef-82ba-4efd-a88e-1f8986fc7941 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:48:35.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8857" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":280,"completed":49,"skipped":886,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:48:35.837: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-2821
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-2821
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2821
Jul 13 12:48:35.891: INFO: Found 0 stateful pods, waiting for 1
Jul 13 12:48:45.896: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 13 12:48:45.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 13 12:48:46.983: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 13 12:48:46.983: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 13 12:48:46.983: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 13 12:48:46.988: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 13 12:48:56.994: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 12:48:56.994: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 12:48:57.013: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 13 12:48:57.013: INFO: ss-0  ip-10-0-2-22.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:35 +0000 UTC  }]
Jul 13 12:48:57.013: INFO: 
Jul 13 12:48:57.013: INFO: StatefulSet ss has not reached scale 3, at 1
Jul 13 12:48:58.019: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993955993s
Jul 13 12:48:59.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988630257s
Jul 13 12:49:00.032: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982404403s
Jul 13 12:49:01.037: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.975084958s
Jul 13 12:49:02.048: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969933262s
Jul 13 12:49:03.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.959442919s
Jul 13 12:49:04.062: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.951178223s
Jul 13 12:49:05.067: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.945353593s
Jul 13 12:49:06.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 939.726846ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2821
Jul 13 12:49:07.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:49:07.276: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 13 12:49:07.276: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 13 12:49:07.276: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 13 12:49:07.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:49:07.477: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 13 12:49:07.477: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 13 12:49:07.477: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 13 12:49:07.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:49:07.695: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 13 12:49:07.695: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 13 12:49:07.695: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 13 12:49:07.701: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 12:49:07.701: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 12:49:07.701: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 13 12:49:07.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 13 12:49:07.916: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 13 12:49:07.916: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 13 12:49:07.916: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 13 12:49:07.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 13 12:49:08.114: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 13 12:49:08.114: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 13 12:49:08.114: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 13 12:49:08.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 13 12:49:08.327: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 13 12:49:08.327: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 13 12:49:08.327: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 13 12:49:08.327: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 12:49:08.339: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jul 13 12:49:18.348: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 12:49:18.348: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 12:49:18.348: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 13 12:49:18.365: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 13 12:49:18.366: INFO: ss-0  ip-10-0-2-22.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:35 +0000 UTC  }]
Jul 13 12:49:18.366: INFO: ss-1  ip-10-0-3-16.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  }]
Jul 13 12:49:18.366: INFO: ss-2  ip-10-0-1-82.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  }]
Jul 13 12:49:18.366: INFO: 
Jul 13 12:49:18.366: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 13 12:49:19.372: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 13 12:49:19.372: INFO: ss-0  ip-10-0-2-22.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:35 +0000 UTC  }]
Jul 13 12:49:19.372: INFO: ss-1  ip-10-0-3-16.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  }]
Jul 13 12:49:19.372: INFO: ss-2  ip-10-0-1-82.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  }]
Jul 13 12:49:19.372: INFO: 
Jul 13 12:49:19.372: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 13 12:49:20.377: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 13 12:49:20.377: INFO: ss-0  ip-10-0-2-22.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:35 +0000 UTC  }]
Jul 13 12:49:20.377: INFO: ss-1  ip-10-0-3-16.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  }]
Jul 13 12:49:20.377: INFO: ss-2  ip-10-0-1-82.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  }]
Jul 13 12:49:20.377: INFO: 
Jul 13 12:49:20.377: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 13 12:49:21.383: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 13 12:49:21.383: INFO: ss-0  ip-10-0-2-22.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:35 +0000 UTC  }]
Jul 13 12:49:21.383: INFO: ss-1  ip-10-0-3-16.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  }]
Jul 13 12:49:21.383: INFO: 
Jul 13 12:49:21.383: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 13 12:49:22.389: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 13 12:49:22.389: INFO: ss-0  ip-10-0-2-22.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:35 +0000 UTC  }]
Jul 13 12:49:22.389: INFO: ss-1  ip-10-0-3-16.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  }]
Jul 13 12:49:22.389: INFO: 
Jul 13 12:49:22.389: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 13 12:49:23.394: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 13 12:49:23.394: INFO: ss-0  ip-10-0-2-22.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:35 +0000 UTC  }]
Jul 13 12:49:23.394: INFO: ss-1  ip-10-0-3-16.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  }]
Jul 13 12:49:23.394: INFO: 
Jul 13 12:49:23.394: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 13 12:49:24.399: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 13 12:49:24.399: INFO: ss-1  ip-10-0-3-16.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  }]
Jul 13 12:49:24.399: INFO: 
Jul 13 12:49:24.399: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 13 12:49:25.405: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 13 12:49:25.405: INFO: ss-1  ip-10-0-3-16.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  }]
Jul 13 12:49:25.405: INFO: 
Jul 13 12:49:25.405: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 13 12:49:26.414: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 13 12:49:26.414: INFO: ss-1  ip-10-0-3-16.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  }]
Jul 13 12:49:26.414: INFO: 
Jul 13 12:49:26.414: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 13 12:49:27.419: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 13 12:49:27.419: INFO: ss-1  ip-10-0-3-16.us-east-2.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:49:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-13 12:48:57 +0000 UTC  }]
Jul 13 12:49:27.419: INFO: 
Jul 13 12:49:27.419: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2821
Jul 13 12:49:28.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:49:28.524: INFO: rc: 1
Jul 13 12:49:28.524: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jul 13 12:49:38.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:49:38.601: INFO: rc: 1
Jul 13 12:49:38.601: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:49:48.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:49:48.675: INFO: rc: 1
Jul 13 12:49:48.675: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:49:58.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:49:58.752: INFO: rc: 1
Jul 13 12:49:58.752: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:50:08.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:50:08.901: INFO: rc: 1
Jul 13 12:50:08.901: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:50:18.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:50:18.984: INFO: rc: 1
Jul 13 12:50:18.984: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:50:28.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:50:29.103: INFO: rc: 1
Jul 13 12:50:29.103: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:50:39.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:50:39.185: INFO: rc: 1
Jul 13 12:50:39.185: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:50:49.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:50:49.269: INFO: rc: 1
Jul 13 12:50:49.269: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:50:59.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:50:59.349: INFO: rc: 1
Jul 13 12:50:59.349: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:51:09.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:51:09.427: INFO: rc: 1
Jul 13 12:51:09.427: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:51:19.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:51:19.512: INFO: rc: 1
Jul 13 12:51:19.512: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:51:29.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:51:29.592: INFO: rc: 1
Jul 13 12:51:29.592: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:51:39.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:51:39.667: INFO: rc: 1
Jul 13 12:51:39.667: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:51:49.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:51:49.742: INFO: rc: 1
Jul 13 12:51:49.742: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:51:59.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:51:59.822: INFO: rc: 1
Jul 13 12:51:59.822: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:52:09.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:52:09.903: INFO: rc: 1
Jul 13 12:52:09.903: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:52:19.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:52:19.986: INFO: rc: 1
Jul 13 12:52:19.986: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:52:29.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:52:30.095: INFO: rc: 1
Jul 13 12:52:30.095: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:52:40.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:52:40.173: INFO: rc: 1
Jul 13 12:52:40.173: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:52:50.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:52:50.251: INFO: rc: 1
Jul 13 12:52:50.251: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:53:00.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:53:00.327: INFO: rc: 1
Jul 13 12:53:00.327: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:53:10.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:53:10.410: INFO: rc: 1
Jul 13 12:53:10.410: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:53:20.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:53:20.485: INFO: rc: 1
Jul 13 12:53:20.485: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:53:30.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:53:30.559: INFO: rc: 1
Jul 13 12:53:30.559: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:53:40.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:53:40.637: INFO: rc: 1
Jul 13 12:53:40.637: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:53:50.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:53:50.737: INFO: rc: 1
Jul 13 12:53:50.737: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:54:00.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:54:00.842: INFO: rc: 1
Jul 13 12:54:00.842: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:54:10.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:54:10.916: INFO: rc: 1
Jul 13 12:54:10.916: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:54:20.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:54:20.991: INFO: rc: 1
Jul 13 12:54:20.991: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jul 13 12:54:30.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-2821 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 12:54:31.072: INFO: rc: 1
Jul 13 12:54:31.072: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Jul 13 12:54:31.072: INFO: Scaling statefulset ss to 0
Jul 13 12:54:31.084: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jul 13 12:54:31.088: INFO: Deleting all statefulset in ns statefulset-2821
Jul 13 12:54:31.091: INFO: Scaling statefulset ss to 0
Jul 13 12:54:31.103: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 12:54:31.106: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:54:31.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2821" for this suite.

â€¢ [SLOW TEST:355.300 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":280,"completed":50,"skipped":934,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:54:31.138: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 12:54:31.761: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 13 12:54:33.773: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730241671, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730241671, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730241671, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730241671, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 12:54:36.795: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 12:54:36.800: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1764-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:54:37.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8586" for this suite.
STEP: Destroying namespace "webhook-8586-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.884 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":280,"completed":51,"skipped":947,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:54:38.025: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 12:54:38.764: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 13 12:54:40.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730241678, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730241678, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730241678, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730241678, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 12:54:43.800: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:54:43.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4873" for this suite.
STEP: Destroying namespace "webhook-4873-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.965 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":280,"completed":52,"skipped":952,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:54:43.991: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Jul 13 12:54:44.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 api-versions'
Jul 13 12:54:44.226: INFO: stderr: ""
Jul 13 12:54:44.226: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:54:44.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-523" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":280,"completed":53,"skipped":973,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:54:44.240: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-e6f846eb-b37e-4259-b554-bb70f949c754
STEP: Creating a pod to test consume configMaps
Jul 13 12:54:44.292: INFO: Waiting up to 5m0s for pod "pod-configmaps-4a31811c-2a59-4796-bc89-f71a910099cd" in namespace "configmap-3889" to be "success or failure"
Jul 13 12:54:44.298: INFO: Pod "pod-configmaps-4a31811c-2a59-4796-bc89-f71a910099cd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.424885ms
Jul 13 12:54:46.303: INFO: Pod "pod-configmaps-4a31811c-2a59-4796-bc89-f71a910099cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010537864s
STEP: Saw pod success
Jul 13 12:54:46.303: INFO: Pod "pod-configmaps-4a31811c-2a59-4796-bc89-f71a910099cd" satisfied condition "success or failure"
Jul 13 12:54:46.306: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-configmaps-4a31811c-2a59-4796-bc89-f71a910099cd container configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 12:54:46.349: INFO: Waiting for pod pod-configmaps-4a31811c-2a59-4796-bc89-f71a910099cd to disappear
Jul 13 12:54:46.353: INFO: Pod pod-configmaps-4a31811c-2a59-4796-bc89-f71a910099cd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:54:46.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3889" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":54,"skipped":999,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:54:46.368: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-29fb6cba-a1f5-483a-bb63-ef82e9b02fce
STEP: Creating secret with name s-test-opt-upd-858a2668-a725-420f-8196-1973f9923335
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-29fb6cba-a1f5-483a-bb63-ef82e9b02fce
STEP: Updating secret s-test-opt-upd-858a2668-a725-420f-8196-1973f9923335
STEP: Creating secret with name s-test-opt-create-51259f46-f721-4b90-88b5-67c06cc2dd10
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:54:52.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6812" for this suite.

â€¢ [SLOW TEST:6.200 seconds]
[sig-storage] Secrets
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":55,"skipped":1000,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:54:52.569: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1357
STEP: creating an pod
Jul 13 12:54:52.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-9719 -- logs-generator --log-lines-total 100 --run-duration 20s'
Jul 13 12:54:52.696: INFO: stderr: ""
Jul 13 12:54:52.696: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Jul 13 12:54:52.696: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jul 13 12:54:52.697: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9719" to be "running and ready, or succeeded"
Jul 13 12:54:52.705: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.42692ms
Jul 13 12:54:54.710: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01346974s
Jul 13 12:54:56.715: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.018469201s
Jul 13 12:54:56.715: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jul 13 12:54:56.715: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jul 13 12:54:56.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 logs logs-generator logs-generator --namespace=kubectl-9719'
Jul 13 12:54:56.818: INFO: stderr: ""
Jul 13 12:54:56.818: INFO: stdout: "I0713 12:54:53.921088       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/2f7b 395\nI0713 12:54:54.121323       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/t9d 562\nI0713 12:54:54.321339       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/l9w 587\nI0713 12:54:54.521274       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/p9pn 402\nI0713 12:54:54.721262       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/twc 443\nI0713 12:54:54.921358       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/mjl 417\nI0713 12:54:55.121269       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/98xl 250\nI0713 12:54:55.321290       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/dwl 313\nI0713 12:54:55.521354       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/p2cx 308\nI0713 12:54:55.721347       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/dpfv 486\nI0713 12:54:55.921372       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/rbvr 326\nI0713 12:54:56.121279       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/69v 517\nI0713 12:54:56.321338       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/j7kv 365\nI0713 12:54:56.521287       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/8kgl 490\nI0713 12:54:56.721175       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/7fxd 322\n"
STEP: limiting log lines
Jul 13 12:54:56.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 logs logs-generator logs-generator --namespace=kubectl-9719 --tail=1'
Jul 13 12:54:56.920: INFO: stderr: ""
Jul 13 12:54:56.920: INFO: stdout: "I0713 12:54:56.721175       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/7fxd 322\n"
Jul 13 12:54:56.920: INFO: got output "I0713 12:54:56.721175       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/7fxd 322\n"
STEP: limiting log bytes
Jul 13 12:54:56.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 logs logs-generator logs-generator --namespace=kubectl-9719 --limit-bytes=1'
Jul 13 12:54:57.006: INFO: stderr: ""
Jul 13 12:54:57.007: INFO: stdout: "I"
Jul 13 12:54:57.007: INFO: got output "I"
STEP: exposing timestamps
Jul 13 12:54:57.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 logs logs-generator logs-generator --namespace=kubectl-9719 --tail=1 --timestamps'
Jul 13 12:54:57.096: INFO: stderr: ""
Jul 13 12:54:57.096: INFO: stdout: "2020-07-13T12:54:56.921306952Z I0713 12:54:56.921206       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/4rj 470\n"
Jul 13 12:54:57.096: INFO: got output "2020-07-13T12:54:56.921306952Z I0713 12:54:56.921206       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/4rj 470\n"
STEP: restricting to a time range
Jul 13 12:54:59.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 logs logs-generator logs-generator --namespace=kubectl-9719 --since=1s'
Jul 13 12:54:59.693: INFO: stderr: ""
Jul 13 12:54:59.693: INFO: stdout: "I0713 12:54:58.721274       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/v5rm 288\nI0713 12:54:58.921362       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/sp5 387\nI0713 12:54:59.121317       1 logs_generator.go:76] 26 POST /api/v1/namespaces/default/pods/s5w 301\nI0713 12:54:59.321327       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/t2mr 266\nI0713 12:54:59.521312       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/m29t 581\n"
Jul 13 12:54:59.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 logs logs-generator logs-generator --namespace=kubectl-9719 --since=24h'
Jul 13 12:54:59.781: INFO: stderr: ""
Jul 13 12:54:59.781: INFO: stdout: "I0713 12:54:53.921088       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/2f7b 395\nI0713 12:54:54.121323       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/t9d 562\nI0713 12:54:54.321339       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/l9w 587\nI0713 12:54:54.521274       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/p9pn 402\nI0713 12:54:54.721262       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/twc 443\nI0713 12:54:54.921358       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/mjl 417\nI0713 12:54:55.121269       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/98xl 250\nI0713 12:54:55.321290       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/dwl 313\nI0713 12:54:55.521354       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/p2cx 308\nI0713 12:54:55.721347       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/dpfv 486\nI0713 12:54:55.921372       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/rbvr 326\nI0713 12:54:56.121279       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/69v 517\nI0713 12:54:56.321338       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/j7kv 365\nI0713 12:54:56.521287       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/8kgl 490\nI0713 12:54:56.721175       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/7fxd 322\nI0713 12:54:56.921206       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/4rj 470\nI0713 12:54:57.121462       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/2775 532\nI0713 12:54:57.321332       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/gjv 598\nI0713 12:54:57.521337       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/4vl 268\nI0713 12:54:57.721289       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/vntv 395\nI0713 12:54:57.921298       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/tl5 215\nI0713 12:54:58.121275       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/hlc 536\nI0713 12:54:58.321317       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/ppw 227\nI0713 12:54:58.521334       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/zzx 579\nI0713 12:54:58.721274       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/v5rm 288\nI0713 12:54:58.921362       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/sp5 387\nI0713 12:54:59.121317       1 logs_generator.go:76] 26 POST /api/v1/namespaces/default/pods/s5w 301\nI0713 12:54:59.321327       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/t2mr 266\nI0713 12:54:59.521312       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/m29t 581\nI0713 12:54:59.721358       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/gwqj 590\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1363
Jul 13 12:54:59.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete pod logs-generator --namespace=kubectl-9719'
Jul 13 12:55:01.855: INFO: stderr: ""
Jul 13 12:55:01.855: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 12:55:01.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9719" for this suite.

â€¢ [SLOW TEST:9.312 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1353
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":280,"completed":56,"skipped":1003,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 12:55:01.881: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jul 13 12:55:01.925: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 13 12:55:01.942: INFO: Waiting for terminating namespaces to be deleted...
Jul 13 12:55:01.947: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-1-82.us-east-2.compute.internal before test
Jul 13 12:55:01.971: INFO: k8s-master-ip-10-0-1-82.us-east-2.compute.internal from kube-system started at 2020-07-12 18:21:28 +0000 UTC (3 container statuses recorded)
Jul 13 12:55:01.971: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 13 12:55:01.971: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jul 13 12:55:01.971: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 13 12:55:01.971: INFO: kubernetes-dashboard-6b7d67fff4-s2c44 from kubernetes-dashboard started at 2020-07-12 18:21:32 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:01.971: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 13 12:55:01.971: INFO: node-exporter-gqvhs from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:01.971: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 12:55:01.971: INFO: calico-kube-controllers-6d84f9d87d-zqq65 from kube-system started at 2020-07-12 18:21:34 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:01.971: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 13 12:55:01.971: INFO: calico-node-xqfbv from kube-system started at 2020-07-12 18:21:00 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:01.971: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 12:55:01.971: INFO: pf9-sentry-865fcbbb89-jhgf5 from platform9-system started at 2020-07-12 18:21:32 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:01.971: INFO: 	Container pf9-sentry ready: true, restart count 0
Jul 13 12:55:01.971: INFO: calico-typha-569c98c8c5-q4jds from kube-system started at 2020-07-12 18:21:34 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:01.971: INFO: 	Container calico-typha ready: true, restart count 0
Jul 13 12:55:01.971: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-46t76 from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 12:55:01.971: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 12:55:01.971: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 12:55:01.971: INFO: dashboard-metrics-scraper-c79c65bb7-zg4zf from kubernetes-dashboard started at 2020-07-12 18:21:32 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:01.971: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 13 12:55:01.971: INFO: calicoctl from kube-system started at 2020-07-12 18:21:42 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:01.971: INFO: 	Container calicoctl ready: true, restart count 0
Jul 13 12:55:01.971: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-2-177.us-east-2.compute.internal before test
Jul 13 12:55:01.996: INFO: prometheus-system-0 from pf9-monitoring started at 2020-07-12 18:29:23 +0000 UTC (3 container statuses recorded)
Jul 13 12:55:01.996: INFO: 	Container prometheus ready: true, restart count 1
Jul 13 12:55:01.996: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 13 12:55:01.996: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 13 12:55:01.996: INFO: prometheus-operator-546b677c-z58nz from pf9-operators started at 2020-07-12 18:28:38 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:01.996: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 13 12:55:01.996: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-5rzwn from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 12:55:01.996: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 12:55:01.996: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 12:55:01.996: INFO: calico-node-2cwb2 from kube-system started at 2020-07-12 18:23:38 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:01.996: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 12:55:01.996: INFO: node-exporter-zcpws from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:01.996: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 12:55:01.996: INFO: platform9-operators-pw6hx from pf9-olm started at 2020-07-12 18:27:46 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:01.996: INFO: 	Container registry-server ready: true, restart count 0
Jul 13 12:55:01.996: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-2-22.us-east-2.compute.internal before test
Jul 13 12:55:02.006: INFO: calico-node-hvjtr from kube-system started at 2020-07-12 18:22:47 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.006: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 12:55:02.006: INFO: coredns-5b985c544f-78zhd from kube-system started at 2020-07-12 18:23:41 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.006: INFO: 	Container coredns ready: true, restart count 0
Jul 13 12:55:02.006: INFO: metrics-server-v0.3.6-75cdf48d5b-fwjq5 from kube-system started at 2020-07-13 10:20:26 +0000 UTC (2 container statuses recorded)
Jul 13 12:55:02.006: INFO: 	Container metrics-server ready: true, restart count 0
Jul 13 12:55:02.006: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jul 13 12:55:02.006: INFO: pod-secrets-6c7af700-c069-496b-ba25-45e7f8bcd126 from secrets-6812 started at 2020-07-13 12:54:46 +0000 UTC (3 container statuses recorded)
Jul 13 12:55:02.006: INFO: 	Container creates-volume-test ready: false, restart count 0
Jul 13 12:55:02.006: INFO: 	Container dels-volume-test ready: false, restart count 0
Jul 13 12:55:02.006: INFO: 	Container upds-volume-test ready: false, restart count 0
Jul 13 12:55:02.006: INFO: calico-typha-569c98c8c5-7skpm from kube-system started at 2020-07-12 18:23:42 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.006: INFO: 	Container calico-typha ready: true, restart count 0
Jul 13 12:55:02.006: INFO: node-exporter-dqvmz from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.006: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 12:55:02.006: INFO: packageserver-6cb6cd5d8b-9qxzn from pf9-olm started at 2020-07-13 11:17:40 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.006: INFO: 	Container packageserver ready: true, restart count 0
Jul 13 12:55:02.006: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-x2g8b from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 12:55:02.006: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 12:55:02.006: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 12:55:02.006: INFO: k8s-master-ip-10-0-2-22.us-east-2.compute.internal from kube-system started at 2020-07-12 18:24:03 +0000 UTC (3 container statuses recorded)
Jul 13 12:55:02.007: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 13 12:55:02.007: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 13 12:55:02.007: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 13 12:55:02.007: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-3-156.us-east-2.compute.internal before test
Jul 13 12:55:02.070: INFO: catalog-operator-5898bbb7d6-hrfcd from pf9-olm started at 2020-07-12 18:27:19 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.070: INFO: 	Container catalog-operator ready: true, restart count 0
Jul 13 12:55:02.070: INFO: calico-typha-569c98c8c5-6ppkg from kube-system started at 2020-07-13 10:20:26 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.070: INFO: 	Container calico-typha ready: true, restart count 0
Jul 13 12:55:02.070: INFO: kube-state-metrics-595cb5cc-x66jk from pf9-monitoring started at 2020-07-12 18:27:36 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.070: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 13 12:55:02.070: INFO: olm-operator-76d446f94c-6bntr from pf9-olm started at 2020-07-12 18:27:21 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.070: INFO: 	Container olm-operator ready: true, restart count 0
Jul 13 12:55:02.070: INFO: node-exporter-9tw9c from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.070: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 12:55:02.070: INFO: monhelper-776667cb66-tsccr from pf9-operators started at 2020-07-12 18:28:38 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.070: INFO: 	Container monhelper ready: true, restart count 1
Jul 13 12:55:02.070: INFO: packageserver-6cb6cd5d8b-59d8z from pf9-olm started at 2020-07-13 11:17:44 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.070: INFO: 	Container packageserver ready: true, restart count 0
Jul 13 12:55:02.070: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-p99pp from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 12:55:02.070: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 12:55:02.070: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 12:55:02.070: INFO: calico-node-x6qds from kube-system started at 2020-07-12 18:23:38 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.070: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 12:55:02.070: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2020-07-12 18:29:23 +0000 UTC (2 container statuses recorded)
Jul 13 12:55:02.070: INFO: 	Container alertmanager ready: true, restart count 0
Jul 13 12:55:02.070: INFO: 	Container config-reloader ready: true, restart count 0
Jul 13 12:55:02.070: INFO: grafana-7c58bb84d4-pkfb5 from pf9-monitoring started at 2020-07-12 18:29:23 +0000 UTC (2 container statuses recorded)
Jul 13 12:55:02.070: INFO: 	Container grafana ready: true, restart count 0
Jul 13 12:55:02.070: INFO: 	Container proxy ready: true, restart count 0
Jul 13 12:55:02.070: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-3-16.us-east-2.compute.internal before test
Jul 13 12:55:02.080: INFO: calico-node-rgskg from kube-system started at 2020-07-12 18:21:41 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.080: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 12:55:02.080: INFO: k8s-master-ip-10-0-3-16.us-east-2.compute.internal from kube-system started at 2020-07-12 18:21:41 +0000 UTC (3 container statuses recorded)
Jul 13 12:55:02.080: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 13 12:55:02.080: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 13 12:55:02.080: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 13 12:55:02.080: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-5tb2r from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 12:55:02.080: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 12:55:02.080: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 12:55:02.080: INFO: node-exporter-tzmdk from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.080: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 12:55:02.080: INFO: sonobuoy from sonobuoy started at 2020-07-13 12:29:45 +0000 UTC (1 container statuses recorded)
Jul 13 12:55:02.080: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 13 12:55:02.080: INFO: sonobuoy-e2e-job-03adefc2964045c0 from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 12:55:02.080: INFO: 	Container e2e ready: true, restart count 0
Jul 13 12:55:02.080: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d125aaad-6a50-435a-a5db-0d857de7ac8b 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-d125aaad-6a50-435a-a5db-0d857de7ac8b off the node ip-10-0-3-16.us-east-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d125aaad-6a50-435a-a5db-0d857de7ac8b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:00:06.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8831" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

â€¢ [SLOW TEST:304.373 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":280,"completed":57,"skipped":1006,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:00:06.257: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 13 13:00:06.314: INFO: Waiting up to 5m0s for pod "pod-cae35987-59aa-4496-a7c1-d8c190ab6221" in namespace "emptydir-630" to be "success or failure"
Jul 13 13:00:06.330: INFO: Pod "pod-cae35987-59aa-4496-a7c1-d8c190ab6221": Phase="Pending", Reason="", readiness=false. Elapsed: 16.611285ms
Jul 13 13:00:08.335: INFO: Pod "pod-cae35987-59aa-4496-a7c1-d8c190ab6221": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021546195s
STEP: Saw pod success
Jul 13 13:00:08.335: INFO: Pod "pod-cae35987-59aa-4496-a7c1-d8c190ab6221" satisfied condition "success or failure"
Jul 13 13:00:08.339: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-cae35987-59aa-4496-a7c1-d8c190ab6221 container test-container: <nil>
STEP: delete the pod
Jul 13 13:00:08.386: INFO: Waiting for pod pod-cae35987-59aa-4496-a7c1-d8c190ab6221 to disappear
Jul 13 13:00:08.390: INFO: Pod pod-cae35987-59aa-4496-a7c1-d8c190ab6221 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:00:08.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-630" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":58,"skipped":1017,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:00:08.405: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1626
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 13 13:00:08.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-980'
Jul 13 13:00:09.370: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 13 13:00:09.370: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1631
Jul 13 13:00:13.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete deployment e2e-test-httpd-deployment --namespace=kubectl-980'
Jul 13 13:00:13.496: INFO: stderr: ""
Jul 13 13:00:13.496: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:00:13.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-980" for this suite.

â€¢ [SLOW TEST:5.110 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1622
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":280,"completed":59,"skipped":1070,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:00:13.516: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9811
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-9811
I0713 13:00:13.656723      20 runners.go:189] Created replication controller with name: externalname-service, namespace: services-9811, replica count: 2
Jul 13 13:00:16.708: INFO: Creating new exec pod
I0713 13:00:16.708068      20 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 13 13:00:21.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-9811 execpodnbxkx -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jul 13 13:00:21.945: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 13 13:00:21.945: INFO: stdout: ""
Jul 13 13:00:21.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-9811 execpodnbxkx -- /bin/sh -x -c nc -zv -t -w 2 10.21.69.202 80'
Jul 13 13:00:22.225: INFO: stderr: "+ nc -zv -t -w 2 10.21.69.202 80\nConnection to 10.21.69.202 80 port [tcp/http] succeeded!\n"
Jul 13 13:00:22.225: INFO: stdout: ""
Jul 13 13:00:22.225: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:00:22.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9811" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:8.759 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":280,"completed":60,"skipped":1095,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:00:22.275: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:00:22.375: INFO: (0) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 44.930192ms)
Jul 13 13:00:22.382: INFO: (1) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 6.670947ms)
Jul 13 13:00:22.388: INFO: (2) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.962019ms)
Jul 13 13:00:22.395: INFO: (3) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 7.359725ms)
Jul 13 13:00:22.401: INFO: (4) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 6.427996ms)
Jul 13 13:00:22.407: INFO: (5) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.768904ms)
Jul 13 13:00:22.415: INFO: (6) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 7.561012ms)
Jul 13 13:00:22.421: INFO: (7) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.776943ms)
Jul 13 13:00:22.426: INFO: (8) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.77718ms)
Jul 13 13:00:22.432: INFO: (9) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.732834ms)
Jul 13 13:00:22.438: INFO: (10) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.359952ms)
Jul 13 13:00:22.445: INFO: (11) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 7.537896ms)
Jul 13 13:00:22.454: INFO: (12) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 9.321931ms)
Jul 13 13:00:22.460: INFO: (13) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.39059ms)
Jul 13 13:00:22.466: INFO: (14) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.992143ms)
Jul 13 13:00:22.471: INFO: (15) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.540808ms)
Jul 13 13:00:22.479: INFO: (16) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 7.629949ms)
Jul 13 13:00:22.486: INFO: (17) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 6.640569ms)
Jul 13 13:00:22.492: INFO: (18) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 6.153654ms)
Jul 13 13:00:22.497: INFO: (19) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.33792ms)
[AfterEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:00:22.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4271" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":280,"completed":61,"skipped":1113,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:00:22.513: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jul 13 13:00:25.134: INFO: Successfully updated pod "annotationupdate90de9b9d-2f5f-47b8-9a17-581947cef9e4"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:00:27.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2702" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":62,"skipped":1115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:00:27.172: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jul 13 13:00:27.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 create -f - --namespace=kubectl-6014'
Jul 13 13:00:27.460: INFO: stderr: ""
Jul 13 13:00:27.460: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 13 13:00:27.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6014'
Jul 13 13:00:27.547: INFO: stderr: ""
Jul 13 13:00:27.548: INFO: stdout: "update-demo-nautilus-nn5n7 update-demo-nautilus-plg77 "
Jul 13 13:00:27.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-nn5n7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6014'
Jul 13 13:00:27.634: INFO: stderr: ""
Jul 13 13:00:27.635: INFO: stdout: ""
Jul 13 13:00:27.635: INFO: update-demo-nautilus-nn5n7 is created but not running
Jul 13 13:00:32.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6014'
Jul 13 13:00:32.730: INFO: stderr: ""
Jul 13 13:00:32.730: INFO: stdout: "update-demo-nautilus-nn5n7 update-demo-nautilus-plg77 "
Jul 13 13:00:32.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-nn5n7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6014'
Jul 13 13:00:32.829: INFO: stderr: ""
Jul 13 13:00:32.829: INFO: stdout: "true"
Jul 13 13:00:32.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-nn5n7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6014'
Jul 13 13:00:32.912: INFO: stderr: ""
Jul 13 13:00:32.913: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 13:00:32.913: INFO: validating pod update-demo-nautilus-nn5n7
Jul 13 13:00:32.921: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 13:00:32.921: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 13:00:32.921: INFO: update-demo-nautilus-nn5n7 is verified up and running
Jul 13 13:00:32.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-plg77 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6014'
Jul 13 13:00:33.000: INFO: stderr: ""
Jul 13 13:00:33.000: INFO: stdout: "true"
Jul 13 13:00:33.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods update-demo-nautilus-plg77 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6014'
Jul 13 13:00:33.075: INFO: stderr: ""
Jul 13 13:00:33.075: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 13 13:00:33.075: INFO: validating pod update-demo-nautilus-plg77
Jul 13 13:00:33.089: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 13 13:00:33.089: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 13 13:00:33.089: INFO: update-demo-nautilus-plg77 is verified up and running
STEP: using delete to clean up resources
Jul 13 13:00:33.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete --grace-period=0 --force -f - --namespace=kubectl-6014'
Jul 13 13:00:33.177: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 13:00:33.177: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 13 13:00:33.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6014'
Jul 13 13:00:33.286: INFO: stderr: "No resources found in kubectl-6014 namespace.\n"
Jul 13 13:00:33.286: INFO: stdout: ""
Jul 13 13:00:33.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -l name=update-demo --namespace=kubectl-6014 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 13 13:00:33.367: INFO: stderr: ""
Jul 13 13:00:33.367: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:00:33.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6014" for this suite.

â€¢ [SLOW TEST:6.210 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":280,"completed":63,"skipped":1137,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:00:33.382: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 13 13:00:41.494: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3838 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:00:41.494: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:00:41.628: INFO: Exec stderr: ""
Jul 13 13:00:41.628: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3838 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:00:41.628: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:00:41.746: INFO: Exec stderr: ""
Jul 13 13:00:41.746: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3838 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:00:41.746: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:00:41.875: INFO: Exec stderr: ""
Jul 13 13:00:41.875: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3838 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:00:41.875: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:00:42.004: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 13 13:00:42.004: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3838 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:00:42.005: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:00:42.130: INFO: Exec stderr: ""
Jul 13 13:00:42.130: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3838 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:00:42.130: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:00:42.261: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 13 13:00:42.261: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3838 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:00:42.261: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:00:42.389: INFO: Exec stderr: ""
Jul 13 13:00:42.390: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3838 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:00:42.390: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:00:42.520: INFO: Exec stderr: ""
Jul 13 13:00:42.520: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3838 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:00:42.520: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:00:42.644: INFO: Exec stderr: ""
Jul 13 13:00:42.644: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3838 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:00:42.644: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:00:42.767: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:00:42.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3838" for this suite.

â€¢ [SLOW TEST:9.400 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":64,"skipped":1145,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:00:42.782: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jul 13 13:00:42.823: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 13 13:00:42.839: INFO: Waiting for terminating namespaces to be deleted...
Jul 13 13:00:42.843: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-1-82.us-east-2.compute.internal before test
Jul 13 13:00:42.859: INFO: k8s-master-ip-10-0-1-82.us-east-2.compute.internal from kube-system started at 2020-07-12 18:21:28 +0000 UTC (3 container statuses recorded)
Jul 13 13:00:42.859: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 13 13:00:42.859: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jul 13 13:00:42.859: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 13 13:00:42.859: INFO: kubernetes-dashboard-6b7d67fff4-s2c44 from kubernetes-dashboard started at 2020-07-12 18:21:32 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.859: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 13 13:00:42.859: INFO: node-exporter-gqvhs from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.859: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:00:42.859: INFO: calico-kube-controllers-6d84f9d87d-zqq65 from kube-system started at 2020-07-12 18:21:34 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.859: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 13 13:00:42.859: INFO: calico-node-xqfbv from kube-system started at 2020-07-12 18:21:00 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.859: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:00:42.859: INFO: pf9-sentry-865fcbbb89-jhgf5 from platform9-system started at 2020-07-12 18:21:32 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.859: INFO: 	Container pf9-sentry ready: true, restart count 0
Jul 13 13:00:42.859: INFO: calico-typha-569c98c8c5-q4jds from kube-system started at 2020-07-12 18:21:34 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.859: INFO: 	Container calico-typha ready: true, restart count 0
Jul 13 13:00:42.859: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-46t76 from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:00:42.859: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:00:42.859: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:00:42.859: INFO: dashboard-metrics-scraper-c79c65bb7-zg4zf from kubernetes-dashboard started at 2020-07-12 18:21:32 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.859: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 13 13:00:42.859: INFO: calicoctl from kube-system started at 2020-07-12 18:21:42 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.859: INFO: 	Container calicoctl ready: true, restart count 0
Jul 13 13:00:42.859: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-2-177.us-east-2.compute.internal before test
Jul 13 13:00:42.867: INFO: prometheus-system-0 from pf9-monitoring started at 2020-07-12 18:29:23 +0000 UTC (3 container statuses recorded)
Jul 13 13:00:42.867: INFO: 	Container prometheus ready: true, restart count 1
Jul 13 13:00:42.867: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 13 13:00:42.867: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 13 13:00:42.867: INFO: calico-node-2cwb2 from kube-system started at 2020-07-12 18:23:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.867: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:00:42.867: INFO: node-exporter-zcpws from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.867: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:00:42.867: INFO: platform9-operators-pw6hx from pf9-olm started at 2020-07-12 18:27:46 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.867: INFO: 	Container registry-server ready: true, restart count 0
Jul 13 13:00:42.867: INFO: prometheus-operator-546b677c-z58nz from pf9-operators started at 2020-07-12 18:28:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.867: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 13 13:00:42.867: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-5rzwn from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:00:42.867: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:00:42.867: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:00:42.867: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-2-22.us-east-2.compute.internal before test
Jul 13 13:00:42.877: INFO: calico-typha-569c98c8c5-7skpm from kube-system started at 2020-07-12 18:23:42 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.877: INFO: 	Container calico-typha ready: true, restart count 0
Jul 13 13:00:42.877: INFO: node-exporter-dqvmz from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.877: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:00:42.877: INFO: packageserver-6cb6cd5d8b-9qxzn from pf9-olm started at 2020-07-13 11:17:40 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.877: INFO: 	Container packageserver ready: true, restart count 0
Jul 13 13:00:42.877: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-x2g8b from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:00:42.877: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:00:42.877: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:00:42.877: INFO: update-demo-nautilus-plg77 from kubectl-6014 started at 2020-07-13 13:00:27 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.877: INFO: 	Container update-demo ready: false, restart count 0
Jul 13 13:00:42.877: INFO: test-pod from e2e-kubelet-etc-hosts-3838 started at 2020-07-13 13:00:33 +0000 UTC (3 container statuses recorded)
Jul 13 13:00:42.877: INFO: 	Container busybox-1 ready: true, restart count 0
Jul 13 13:00:42.877: INFO: 	Container busybox-2 ready: true, restart count 0
Jul 13 13:00:42.877: INFO: 	Container busybox-3 ready: true, restart count 0
Jul 13 13:00:42.877: INFO: k8s-master-ip-10-0-2-22.us-east-2.compute.internal from kube-system started at 2020-07-12 18:24:03 +0000 UTC (3 container statuses recorded)
Jul 13 13:00:42.877: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 13 13:00:42.877: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 13 13:00:42.877: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 13 13:00:42.877: INFO: calico-node-hvjtr from kube-system started at 2020-07-12 18:22:47 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.877: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:00:42.877: INFO: coredns-5b985c544f-78zhd from kube-system started at 2020-07-12 18:23:41 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.877: INFO: 	Container coredns ready: true, restart count 0
Jul 13 13:00:42.877: INFO: metrics-server-v0.3.6-75cdf48d5b-fwjq5 from kube-system started at 2020-07-13 10:20:26 +0000 UTC (2 container statuses recorded)
Jul 13 13:00:42.877: INFO: 	Container metrics-server ready: true, restart count 0
Jul 13 13:00:42.877: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jul 13 13:00:42.877: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-3-156.us-east-2.compute.internal before test
Jul 13 13:00:42.897: INFO: calico-node-x6qds from kube-system started at 2020-07-12 18:23:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.897: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:00:42.897: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2020-07-12 18:29:23 +0000 UTC (2 container statuses recorded)
Jul 13 13:00:42.897: INFO: 	Container alertmanager ready: true, restart count 0
Jul 13 13:00:42.897: INFO: 	Container config-reloader ready: true, restart count 0
Jul 13 13:00:42.897: INFO: grafana-7c58bb84d4-pkfb5 from pf9-monitoring started at 2020-07-12 18:29:23 +0000 UTC (2 container statuses recorded)
Jul 13 13:00:42.897: INFO: 	Container grafana ready: true, restart count 0
Jul 13 13:00:42.897: INFO: 	Container proxy ready: true, restart count 0
Jul 13 13:00:42.897: INFO: catalog-operator-5898bbb7d6-hrfcd from pf9-olm started at 2020-07-12 18:27:19 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.897: INFO: 	Container catalog-operator ready: true, restart count 0
Jul 13 13:00:42.897: INFO: calico-typha-569c98c8c5-6ppkg from kube-system started at 2020-07-13 10:20:26 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.897: INFO: 	Container calico-typha ready: true, restart count 0
Jul 13 13:00:42.897: INFO: kube-state-metrics-595cb5cc-x66jk from pf9-monitoring started at 2020-07-12 18:27:36 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.897: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 13 13:00:42.897: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-p99pp from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:00:42.897: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:00:42.897: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:00:42.897: INFO: olm-operator-76d446f94c-6bntr from pf9-olm started at 2020-07-12 18:27:21 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.897: INFO: 	Container olm-operator ready: true, restart count 0
Jul 13 13:00:42.897: INFO: node-exporter-9tw9c from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.897: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:00:42.897: INFO: monhelper-776667cb66-tsccr from pf9-operators started at 2020-07-12 18:28:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.897: INFO: 	Container monhelper ready: true, restart count 1
Jul 13 13:00:42.897: INFO: packageserver-6cb6cd5d8b-59d8z from pf9-olm started at 2020-07-13 11:17:44 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.897: INFO: 	Container packageserver ready: true, restart count 0
Jul 13 13:00:42.897: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-3-16.us-east-2.compute.internal before test
Jul 13 13:00:42.908: INFO: calico-node-rgskg from kube-system started at 2020-07-12 18:21:41 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.908: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:00:42.908: INFO: k8s-master-ip-10-0-3-16.us-east-2.compute.internal from kube-system started at 2020-07-12 18:21:41 +0000 UTC (3 container statuses recorded)
Jul 13 13:00:42.908: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 13 13:00:42.908: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 13 13:00:42.908: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 13 13:00:42.908: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-5tb2r from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:00:42.908: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:00:42.908: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:00:42.908: INFO: test-host-network-pod from e2e-kubelet-etc-hosts-3838 started at 2020-07-13 13:00:37 +0000 UTC (2 container statuses recorded)
Jul 13 13:00:42.908: INFO: 	Container busybox-1 ready: true, restart count 0
Jul 13 13:00:42.908: INFO: 	Container busybox-2 ready: true, restart count 0
Jul 13 13:00:42.908: INFO: node-exporter-tzmdk from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.908: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:00:42.908: INFO: sonobuoy from sonobuoy started at 2020-07-13 12:29:45 +0000 UTC (1 container statuses recorded)
Jul 13 13:00:42.908: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 13 13:00:42.908: INFO: sonobuoy-e2e-job-03adefc2964045c0 from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:00:42.908: INFO: 	Container e2e ready: true, restart count 0
Jul 13 13:00:42.908: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-64cdeebf-4369-4f36-9a26-c947762a296f 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-64cdeebf-4369-4f36-9a26-c947762a296f off the node ip-10-0-3-156.us-east-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-64cdeebf-4369-4f36-9a26-c947762a296f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:00:53.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3647" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

â€¢ [SLOW TEST:10.266 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":280,"completed":65,"skipped":1218,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:00:53.049: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8758
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jul 13 13:00:53.112: INFO: Found 0 stateful pods, waiting for 3
Jul 13 13:01:03.118: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 13:01:03.118: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 13:01:03.118: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 13:01:03.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-8758 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 13 13:01:03.363: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 13 13:01:03.363: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 13 13:01:03.363: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jul 13 13:01:13.405: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 13 13:01:23.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-8758 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 13:01:23.682: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 13 13:01:23.682: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 13 13:01:23.682: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Jul 13 13:01:43.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-8758 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 13 13:01:43.920: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 13 13:01:43.920: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 13 13:01:43.920: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 13 13:01:53.958: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 13 13:02:03.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=statefulset-8758 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 13 13:02:04.229: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 13 13:02:04.229: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 13 13:02:04.229: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jul 13 13:02:24.256: INFO: Deleting all statefulset in ns statefulset-8758
Jul 13 13:02:24.259: INFO: Scaling statefulset ss2 to 0
Jul 13 13:02:44.281: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 13:02:44.285: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:02:44.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8758" for this suite.

â€¢ [SLOW TEST:111.274 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":280,"completed":66,"skipped":1247,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:02:44.324: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-d77ced85-b5a6-4e59-b1c4-1f342a4b3193
STEP: Creating a pod to test consume configMaps
Jul 13 13:02:44.379: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d3c7b302-c7fa-4b14-9599-55ec96414a65" in namespace "projected-6875" to be "success or failure"
Jul 13 13:02:44.384: INFO: Pod "pod-projected-configmaps-d3c7b302-c7fa-4b14-9599-55ec96414a65": Phase="Pending", Reason="", readiness=false. Elapsed: 5.640984ms
Jul 13 13:02:46.390: INFO: Pod "pod-projected-configmaps-d3c7b302-c7fa-4b14-9599-55ec96414a65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01074307s
STEP: Saw pod success
Jul 13 13:02:46.390: INFO: Pod "pod-projected-configmaps-d3c7b302-c7fa-4b14-9599-55ec96414a65" satisfied condition "success or failure"
Jul 13 13:02:46.393: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-projected-configmaps-d3c7b302-c7fa-4b14-9599-55ec96414a65 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 13:02:46.540: INFO: Waiting for pod pod-projected-configmaps-d3c7b302-c7fa-4b14-9599-55ec96414a65 to disappear
Jul 13 13:02:46.544: INFO: Pod pod-projected-configmaps-d3c7b302-c7fa-4b14-9599-55ec96414a65 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:02:46.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6875" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":67,"skipped":1251,"failed":0}
SSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:02:46.562: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:02:46.607: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-23cedad0-5a8e-4589-85e6-89a31e4f0f47" in namespace "security-context-test-6416" to be "success or failure"
Jul 13 13:02:46.619: INFO: Pod "busybox-readonly-false-23cedad0-5a8e-4589-85e6-89a31e4f0f47": Phase="Pending", Reason="", readiness=false. Elapsed: 11.48836ms
Jul 13 13:02:48.624: INFO: Pod "busybox-readonly-false-23cedad0-5a8e-4589-85e6-89a31e4f0f47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016600687s
Jul 13 13:02:48.624: INFO: Pod "busybox-readonly-false-23cedad0-5a8e-4589-85e6-89a31e4f0f47" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:02:48.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6416" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":280,"completed":68,"skipped":1256,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:02:48.639: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 13 13:02:48.686: INFO: Waiting up to 5m0s for pod "pod-74d1b929-e91d-4c17-95e7-0551ebd2e4f5" in namespace "emptydir-3018" to be "success or failure"
Jul 13 13:02:48.698: INFO: Pod "pod-74d1b929-e91d-4c17-95e7-0551ebd2e4f5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.511398ms
Jul 13 13:02:50.703: INFO: Pod "pod-74d1b929-e91d-4c17-95e7-0551ebd2e4f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016661543s
Jul 13 13:02:52.708: INFO: Pod "pod-74d1b929-e91d-4c17-95e7-0551ebd2e4f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021427612s
STEP: Saw pod success
Jul 13 13:02:52.708: INFO: Pod "pod-74d1b929-e91d-4c17-95e7-0551ebd2e4f5" satisfied condition "success or failure"
Jul 13 13:02:52.712: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-74d1b929-e91d-4c17-95e7-0551ebd2e4f5 container test-container: <nil>
STEP: delete the pod
Jul 13 13:02:52.741: INFO: Waiting for pod pod-74d1b929-e91d-4c17-95e7-0551ebd2e4f5 to disappear
Jul 13 13:02:52.744: INFO: Pod pod-74d1b929-e91d-4c17-95e7-0551ebd2e4f5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:02:52.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3018" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":69,"skipped":1263,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:02:52.766: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jul 13 13:02:52.806: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 13 13:02:52.823: INFO: Waiting for terminating namespaces to be deleted...
Jul 13 13:02:52.827: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-1-82.us-east-2.compute.internal before test
Jul 13 13:02:52.843: INFO: dashboard-metrics-scraper-c79c65bb7-zg4zf from kubernetes-dashboard started at 2020-07-12 18:21:32 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.843: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 13 13:02:52.843: INFO: calicoctl from kube-system started at 2020-07-12 18:21:42 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.843: INFO: 	Container calicoctl ready: true, restart count 0
Jul 13 13:02:52.843: INFO: k8s-master-ip-10-0-1-82.us-east-2.compute.internal from kube-system started at 2020-07-12 18:21:28 +0000 UTC (3 container statuses recorded)
Jul 13 13:02:52.843: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 13 13:02:52.843: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jul 13 13:02:52.843: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 13 13:02:52.843: INFO: kubernetes-dashboard-6b7d67fff4-s2c44 from kubernetes-dashboard started at 2020-07-12 18:21:32 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.843: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 13 13:02:52.843: INFO: node-exporter-gqvhs from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.843: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:02:52.843: INFO: calico-kube-controllers-6d84f9d87d-zqq65 from kube-system started at 2020-07-12 18:21:34 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.843: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 13 13:02:52.843: INFO: calico-node-xqfbv from kube-system started at 2020-07-12 18:21:00 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.843: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:02:52.843: INFO: pf9-sentry-865fcbbb89-jhgf5 from platform9-system started at 2020-07-12 18:21:32 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.843: INFO: 	Container pf9-sentry ready: true, restart count 0
Jul 13 13:02:52.843: INFO: calico-typha-569c98c8c5-q4jds from kube-system started at 2020-07-12 18:21:34 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.843: INFO: 	Container calico-typha ready: true, restart count 0
Jul 13 13:02:52.843: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-46t76 from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:02:52.843: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:02:52.843: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:02:52.843: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-2-177.us-east-2.compute.internal before test
Jul 13 13:02:52.871: INFO: prometheus-system-0 from pf9-monitoring started at 2020-07-12 18:29:23 +0000 UTC (3 container statuses recorded)
Jul 13 13:02:52.871: INFO: 	Container prometheus ready: true, restart count 1
Jul 13 13:02:52.871: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 13 13:02:52.871: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 13 13:02:52.871: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-5rzwn from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:02:52.871: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:02:52.871: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:02:52.871: INFO: calico-node-2cwb2 from kube-system started at 2020-07-12 18:23:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.871: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:02:52.871: INFO: node-exporter-zcpws from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.871: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:02:52.871: INFO: platform9-operators-pw6hx from pf9-olm started at 2020-07-12 18:27:46 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.871: INFO: 	Container registry-server ready: true, restart count 0
Jul 13 13:02:52.871: INFO: prometheus-operator-546b677c-z58nz from pf9-operators started at 2020-07-12 18:28:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.871: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 13 13:02:52.871: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-2-22.us-east-2.compute.internal before test
Jul 13 13:02:52.881: INFO: k8s-master-ip-10-0-2-22.us-east-2.compute.internal from kube-system started at 2020-07-12 18:24:03 +0000 UTC (3 container statuses recorded)
Jul 13 13:02:52.881: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 13 13:02:52.881: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 13 13:02:52.881: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 13 13:02:52.881: INFO: calico-node-hvjtr from kube-system started at 2020-07-12 18:22:47 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.881: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:02:52.881: INFO: coredns-5b985c544f-78zhd from kube-system started at 2020-07-12 18:23:41 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.881: INFO: 	Container coredns ready: true, restart count 0
Jul 13 13:02:52.881: INFO: metrics-server-v0.3.6-75cdf48d5b-fwjq5 from kube-system started at 2020-07-13 10:20:26 +0000 UTC (2 container statuses recorded)
Jul 13 13:02:52.881: INFO: 	Container metrics-server ready: true, restart count 0
Jul 13 13:02:52.881: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jul 13 13:02:52.881: INFO: calico-typha-569c98c8c5-7skpm from kube-system started at 2020-07-12 18:23:42 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.881: INFO: 	Container calico-typha ready: true, restart count 0
Jul 13 13:02:52.881: INFO: node-exporter-dqvmz from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.881: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:02:52.881: INFO: packageserver-6cb6cd5d8b-9qxzn from pf9-olm started at 2020-07-13 11:17:40 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.881: INFO: 	Container packageserver ready: true, restart count 0
Jul 13 13:02:52.881: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-x2g8b from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:02:52.881: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:02:52.881: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:02:52.881: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-3-156.us-east-2.compute.internal before test
Jul 13 13:02:52.905: INFO: olm-operator-76d446f94c-6bntr from pf9-olm started at 2020-07-12 18:27:21 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.905: INFO: 	Container olm-operator ready: true, restart count 0
Jul 13 13:02:52.905: INFO: node-exporter-9tw9c from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.905: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:02:52.905: INFO: monhelper-776667cb66-tsccr from pf9-operators started at 2020-07-12 18:28:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.905: INFO: 	Container monhelper ready: true, restart count 1
Jul 13 13:02:52.905: INFO: packageserver-6cb6cd5d8b-59d8z from pf9-olm started at 2020-07-13 11:17:44 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.905: INFO: 	Container packageserver ready: true, restart count 0
Jul 13 13:02:52.905: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-p99pp from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:02:52.905: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:02:52.905: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:02:52.905: INFO: calico-node-x6qds from kube-system started at 2020-07-12 18:23:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.905: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:02:52.905: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2020-07-12 18:29:23 +0000 UTC (2 container statuses recorded)
Jul 13 13:02:52.905: INFO: 	Container alertmanager ready: true, restart count 0
Jul 13 13:02:52.905: INFO: 	Container config-reloader ready: true, restart count 0
Jul 13 13:02:52.905: INFO: grafana-7c58bb84d4-pkfb5 from pf9-monitoring started at 2020-07-12 18:29:23 +0000 UTC (2 container statuses recorded)
Jul 13 13:02:52.905: INFO: 	Container grafana ready: true, restart count 0
Jul 13 13:02:52.905: INFO: 	Container proxy ready: true, restart count 0
Jul 13 13:02:52.905: INFO: catalog-operator-5898bbb7d6-hrfcd from pf9-olm started at 2020-07-12 18:27:19 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.905: INFO: 	Container catalog-operator ready: true, restart count 0
Jul 13 13:02:52.905: INFO: calico-typha-569c98c8c5-6ppkg from kube-system started at 2020-07-13 10:20:26 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.905: INFO: 	Container calico-typha ready: true, restart count 0
Jul 13 13:02:52.905: INFO: kube-state-metrics-595cb5cc-x66jk from pf9-monitoring started at 2020-07-12 18:27:36 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.906: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 13 13:02:52.906: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-3-16.us-east-2.compute.internal before test
Jul 13 13:02:52.927: INFO: busybox-readonly-false-23cedad0-5a8e-4589-85e6-89a31e4f0f47 from security-context-test-6416 started at 2020-07-13 13:02:46 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.927: INFO: 	Container busybox-readonly-false-23cedad0-5a8e-4589-85e6-89a31e4f0f47 ready: false, restart count 0
Jul 13 13:02:52.927: INFO: calico-node-rgskg from kube-system started at 2020-07-12 18:21:41 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.927: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:02:52.927: INFO: k8s-master-ip-10-0-3-16.us-east-2.compute.internal from kube-system started at 2020-07-12 18:21:41 +0000 UTC (3 container statuses recorded)
Jul 13 13:02:52.927: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 13 13:02:52.927: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 13 13:02:52.927: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 13 13:02:52.927: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-5tb2r from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:02:52.927: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:02:52.927: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:02:52.927: INFO: node-exporter-tzmdk from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.927: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:02:52.927: INFO: sonobuoy from sonobuoy started at 2020-07-13 12:29:45 +0000 UTC (1 container statuses recorded)
Jul 13 13:02:52.927: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 13 13:02:52.927: INFO: sonobuoy-e2e-job-03adefc2964045c0 from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:02:52.927: INFO: 	Container e2e ready: true, restart count 0
Jul 13 13:02:52.927: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node ip-10-0-1-82.us-east-2.compute.internal
STEP: verifying the node has the label node ip-10-0-2-177.us-east-2.compute.internal
STEP: verifying the node has the label node ip-10-0-2-22.us-east-2.compute.internal
STEP: verifying the node has the label node ip-10-0-3-156.us-east-2.compute.internal
STEP: verifying the node has the label node ip-10-0-3-16.us-east-2.compute.internal
Jul 13 13:02:53.028: INFO: Pod calico-kube-controllers-6d84f9d87d-zqq65 requesting resource cpu=0m on Node ip-10-0-1-82.us-east-2.compute.internal
Jul 13 13:02:53.028: INFO: Pod calico-node-2cwb2 requesting resource cpu=250m on Node ip-10-0-2-177.us-east-2.compute.internal
Jul 13 13:02:53.028: INFO: Pod calico-node-hvjtr requesting resource cpu=250m on Node ip-10-0-2-22.us-east-2.compute.internal
Jul 13 13:02:53.028: INFO: Pod calico-node-rgskg requesting resource cpu=250m on Node ip-10-0-3-16.us-east-2.compute.internal
Jul 13 13:02:53.028: INFO: Pod calico-node-x6qds requesting resource cpu=250m on Node ip-10-0-3-156.us-east-2.compute.internal
Jul 13 13:02:53.028: INFO: Pod calico-node-xqfbv requesting resource cpu=250m on Node ip-10-0-1-82.us-east-2.compute.internal
Jul 13 13:02:53.028: INFO: Pod calico-typha-569c98c8c5-6ppkg requesting resource cpu=0m on Node ip-10-0-3-156.us-east-2.compute.internal
Jul 13 13:02:53.028: INFO: Pod calico-typha-569c98c8c5-7skpm requesting resource cpu=0m on Node ip-10-0-2-22.us-east-2.compute.internal
Jul 13 13:02:53.028: INFO: Pod calico-typha-569c98c8c5-q4jds requesting resource cpu=0m on Node ip-10-0-1-82.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod calicoctl requesting resource cpu=0m on Node ip-10-0-1-82.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod coredns-5b985c544f-78zhd requesting resource cpu=100m on Node ip-10-0-2-22.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod k8s-master-ip-10-0-1-82.us-east-2.compute.internal requesting resource cpu=0m on Node ip-10-0-1-82.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod k8s-master-ip-10-0-2-22.us-east-2.compute.internal requesting resource cpu=0m on Node ip-10-0-2-22.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod k8s-master-ip-10-0-3-16.us-east-2.compute.internal requesting resource cpu=0m on Node ip-10-0-3-16.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod metrics-server-v0.3.6-75cdf48d5b-fwjq5 requesting resource cpu=53m on Node ip-10-0-2-22.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod dashboard-metrics-scraper-c79c65bb7-zg4zf requesting resource cpu=0m on Node ip-10-0-1-82.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod kubernetes-dashboard-6b7d67fff4-s2c44 requesting resource cpu=0m on Node ip-10-0-1-82.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod alertmanager-sysalert-0 requesting resource cpu=200m on Node ip-10-0-3-156.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod grafana-7c58bb84d4-pkfb5 requesting resource cpu=100m on Node ip-10-0-3-156.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod kube-state-metrics-595cb5cc-x66jk requesting resource cpu=100m on Node ip-10-0-3-156.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod node-exporter-9tw9c requesting resource cpu=102m on Node ip-10-0-3-156.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod node-exporter-dqvmz requesting resource cpu=102m on Node ip-10-0-2-22.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod node-exporter-gqvhs requesting resource cpu=102m on Node ip-10-0-1-82.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod node-exporter-tzmdk requesting resource cpu=102m on Node ip-10-0-3-16.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod node-exporter-zcpws requesting resource cpu=102m on Node ip-10-0-2-177.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod prometheus-system-0 requesting resource cpu=700m on Node ip-10-0-2-177.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod catalog-operator-5898bbb7d6-hrfcd requesting resource cpu=10m on Node ip-10-0-3-156.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod olm-operator-76d446f94c-6bntr requesting resource cpu=10m on Node ip-10-0-3-156.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod packageserver-6cb6cd5d8b-59d8z requesting resource cpu=10m on Node ip-10-0-3-156.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod packageserver-6cb6cd5d8b-9qxzn requesting resource cpu=10m on Node ip-10-0-2-22.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod platform9-operators-pw6hx requesting resource cpu=10m on Node ip-10-0-2-177.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod monhelper-776667cb66-tsccr requesting resource cpu=50m on Node ip-10-0-3-156.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod prometheus-operator-546b677c-z58nz requesting resource cpu=100m on Node ip-10-0-2-177.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod pf9-sentry-865fcbbb89-jhgf5 requesting resource cpu=500m on Node ip-10-0-1-82.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-3-16.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod sonobuoy-e2e-job-03adefc2964045c0 requesting resource cpu=0m on Node ip-10-0-3-16.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-46t76 requesting resource cpu=0m on Node ip-10-0-1-82.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-5rzwn requesting resource cpu=0m on Node ip-10-0-2-177.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-5tb2r requesting resource cpu=0m on Node ip-10-0-3-16.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-p99pp requesting resource cpu=0m on Node ip-10-0-3-156.us-east-2.compute.internal
Jul 13 13:02:53.029: INFO: Pod sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-x2g8b requesting resource cpu=0m on Node ip-10-0-2-22.us-east-2.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
Jul 13 13:02:53.029: INFO: Creating a pod which consumes cpu=817m on Node ip-10-0-3-156.us-east-2.compute.internal
Jul 13 13:02:53.037: INFO: Creating a pod which consumes cpu=1153m on Node ip-10-0-3-16.us-east-2.compute.internal
Jul 13 13:02:53.056: INFO: Creating a pod which consumes cpu=803m on Node ip-10-0-1-82.us-east-2.compute.internal
Jul 13 13:02:53.074: INFO: Creating a pod which consumes cpu=586m on Node ip-10-0-2-177.us-east-2.compute.internal
Jul 13 13:02:53.093: INFO: Creating a pod which consumes cpu=1039m on Node ip-10-0-2-22.us-east-2.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0d61a164-9406-45f2-b049-25a22aac5f37.162151849b1fc9bc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-595/filler-pod-0d61a164-9406-45f2-b049-25a22aac5f37 to ip-10-0-3-156.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0d61a164-9406-45f2-b049-25a22aac5f37.16215184da8d63fb], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0d61a164-9406-45f2-b049-25a22aac5f37.16215184e1b6e61e], Reason = [Created], Message = [Created container filler-pod-0d61a164-9406-45f2-b049-25a22aac5f37]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0d61a164-9406-45f2-b049-25a22aac5f37.16215184ec467602], Reason = [Started], Message = [Started container filler-pod-0d61a164-9406-45f2-b049-25a22aac5f37]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57e8e5cd-ed82-4556-b537-0f447a9c3346.162151849f7c9f0c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-595/filler-pod-57e8e5cd-ed82-4556-b537-0f447a9c3346 to ip-10-0-2-22.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57e8e5cd-ed82-4556-b537-0f447a9c3346.16215184dc6df093], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57e8e5cd-ed82-4556-b537-0f447a9c3346.16215184e5308463], Reason = [Created], Message = [Created container filler-pod-57e8e5cd-ed82-4556-b537-0f447a9c3346]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-57e8e5cd-ed82-4556-b537-0f447a9c3346.16215184f1c46fd5], Reason = [Started], Message = [Started container filler-pod-57e8e5cd-ed82-4556-b537-0f447a9c3346]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d0f8419e-f789-4eae-a919-964743d97ec9.162151849e823c5b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-595/filler-pod-d0f8419e-f789-4eae-a919-964743d97ec9 to ip-10-0-2-177.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d0f8419e-f789-4eae-a919-964743d97ec9.16215184d71bef1c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d0f8419e-f789-4eae-a919-964743d97ec9.16215184e0069ed1], Reason = [Created], Message = [Created container filler-pod-d0f8419e-f789-4eae-a919-964743d97ec9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d0f8419e-f789-4eae-a919-964743d97ec9.16215184ea682c33], Reason = [Started], Message = [Started container filler-pod-d0f8419e-f789-4eae-a919-964743d97ec9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-da9db07f-dffb-4c4f-9872-761f5a5dbe2f.162151849c15ff30], Reason = [Scheduled], Message = [Successfully assigned sched-pred-595/filler-pod-da9db07f-dffb-4c4f-9872-761f5a5dbe2f to ip-10-0-3-16.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-da9db07f-dffb-4c4f-9872-761f5a5dbe2f.16215184d9d8fee2], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-da9db07f-dffb-4c4f-9872-761f5a5dbe2f.16215184e2c826b0], Reason = [Created], Message = [Created container filler-pod-da9db07f-dffb-4c4f-9872-761f5a5dbe2f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-da9db07f-dffb-4c4f-9872-761f5a5dbe2f.16215184ee275a00], Reason = [Started], Message = [Started container filler-pod-da9db07f-dffb-4c4f-9872-761f5a5dbe2f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e5352e34-eb2e-43bc-810b-372363262eff.162151849d5a1837], Reason = [Scheduled], Message = [Successfully assigned sched-pred-595/filler-pod-e5352e34-eb2e-43bc-810b-372363262eff to ip-10-0-1-82.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e5352e34-eb2e-43bc-810b-372363262eff.16215184e1bc2e4e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e5352e34-eb2e-43bc-810b-372363262eff.16215184ea50cfdb], Reason = [Created], Message = [Created container filler-pod-e5352e34-eb2e-43bc-810b-372363262eff]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e5352e34-eb2e-43bc-810b-372363262eff.16215184f615ce9d], Reason = [Started], Message = [Started container filler-pod-e5352e34-eb2e-43bc-810b-372363262eff]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16215185914dc700], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: removing the label node off the node ip-10-0-1-82.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-2-177.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-2-22.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-3-156.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-3-16.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:02:58.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-595" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

â€¢ [SLOW TEST:5.536 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":280,"completed":70,"skipped":1284,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:02:58.303: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-zpz7
STEP: Creating a pod to test atomic-volume-subpath
Jul 13 13:02:58.366: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zpz7" in namespace "subpath-5685" to be "success or failure"
Jul 13 13:02:58.371: INFO: Pod "pod-subpath-test-configmap-zpz7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.855031ms
Jul 13 13:03:00.376: INFO: Pod "pod-subpath-test-configmap-zpz7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009873933s
Jul 13 13:03:02.383: INFO: Pod "pod-subpath-test-configmap-zpz7": Phase="Running", Reason="", readiness=true. Elapsed: 4.017190872s
Jul 13 13:03:04.392: INFO: Pod "pod-subpath-test-configmap-zpz7": Phase="Running", Reason="", readiness=true. Elapsed: 6.026260539s
Jul 13 13:03:06.398: INFO: Pod "pod-subpath-test-configmap-zpz7": Phase="Running", Reason="", readiness=true. Elapsed: 8.031686095s
Jul 13 13:03:08.403: INFO: Pod "pod-subpath-test-configmap-zpz7": Phase="Running", Reason="", readiness=true. Elapsed: 10.036560788s
Jul 13 13:03:10.407: INFO: Pod "pod-subpath-test-configmap-zpz7": Phase="Running", Reason="", readiness=true. Elapsed: 12.04135467s
Jul 13 13:03:12.412: INFO: Pod "pod-subpath-test-configmap-zpz7": Phase="Running", Reason="", readiness=true. Elapsed: 14.046409678s
Jul 13 13:03:14.418: INFO: Pod "pod-subpath-test-configmap-zpz7": Phase="Running", Reason="", readiness=true. Elapsed: 16.051888599s
Jul 13 13:03:16.423: INFO: Pod "pod-subpath-test-configmap-zpz7": Phase="Running", Reason="", readiness=true. Elapsed: 18.05702068s
Jul 13 13:03:18.428: INFO: Pod "pod-subpath-test-configmap-zpz7": Phase="Running", Reason="", readiness=true. Elapsed: 20.062471932s
Jul 13 13:03:20.433: INFO: Pod "pod-subpath-test-configmap-zpz7": Phase="Running", Reason="", readiness=true. Elapsed: 22.067079266s
Jul 13 13:03:22.438: INFO: Pod "pod-subpath-test-configmap-zpz7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.07169195s
STEP: Saw pod success
Jul 13 13:03:22.438: INFO: Pod "pod-subpath-test-configmap-zpz7" satisfied condition "success or failure"
Jul 13 13:03:22.442: INFO: Trying to get logs from node ip-10-0-2-177.us-east-2.compute.internal pod pod-subpath-test-configmap-zpz7 container test-container-subpath-configmap-zpz7: <nil>
STEP: delete the pod
Jul 13 13:03:22.471: INFO: Waiting for pod pod-subpath-test-configmap-zpz7 to disappear
Jul 13 13:03:22.478: INFO: Pod pod-subpath-test-configmap-zpz7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zpz7
Jul 13 13:03:22.478: INFO: Deleting pod "pod-subpath-test-configmap-zpz7" in namespace "subpath-5685"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:03:22.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5685" for this suite.

â€¢ [SLOW TEST:24.196 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":280,"completed":71,"skipped":1292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:03:22.500: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:03:22.558: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:03:26.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5550" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":280,"completed":72,"skipped":1320,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:03:26.629: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 13 13:03:26.680: INFO: Waiting up to 5m0s for pod "pod-35f38936-4b58-4158-8ad2-250f04de7156" in namespace "emptydir-3948" to be "success or failure"
Jul 13 13:03:26.692: INFO: Pod "pod-35f38936-4b58-4158-8ad2-250f04de7156": Phase="Pending", Reason="", readiness=false. Elapsed: 12.639895ms
Jul 13 13:03:28.697: INFO: Pod "pod-35f38936-4b58-4158-8ad2-250f04de7156": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017492933s
Jul 13 13:03:30.702: INFO: Pod "pod-35f38936-4b58-4158-8ad2-250f04de7156": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022618714s
STEP: Saw pod success
Jul 13 13:03:30.702: INFO: Pod "pod-35f38936-4b58-4158-8ad2-250f04de7156" satisfied condition "success or failure"
Jul 13 13:03:30.706: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-35f38936-4b58-4158-8ad2-250f04de7156 container test-container: <nil>
STEP: delete the pod
Jul 13 13:03:30.734: INFO: Waiting for pod pod-35f38936-4b58-4158-8ad2-250f04de7156 to disappear
Jul 13 13:03:30.739: INFO: Pod pod-35f38936-4b58-4158-8ad2-250f04de7156 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:03:30.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3948" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":73,"skipped":1322,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:03:30.753: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:03:30.843: INFO: Create a RollingUpdate DaemonSet
Jul 13 13:03:30.849: INFO: Check that daemon pods launch on every node of the cluster
Jul 13 13:03:30.868: INFO: Number of nodes with available pods: 0
Jul 13 13:03:30.868: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:03:31.904: INFO: Number of nodes with available pods: 0
Jul 13 13:03:31.907: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:03:32.880: INFO: Number of nodes with available pods: 4
Jul 13 13:03:32.880: INFO: Node ip-10-0-3-16.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:03:33.880: INFO: Number of nodes with available pods: 5
Jul 13 13:03:33.880: INFO: Number of running nodes: 5, number of available pods: 5
Jul 13 13:03:33.880: INFO: Update the DaemonSet to trigger a rollout
Jul 13 13:03:33.891: INFO: Updating DaemonSet daemon-set
Jul 13 13:03:36.927: INFO: Roll back the DaemonSet before rollout is complete
Jul 13 13:03:36.937: INFO: Updating DaemonSet daemon-set
Jul 13 13:03:36.938: INFO: Make sure DaemonSet rollback is complete
Jul 13 13:03:36.947: INFO: Wrong image for pod: daemon-set-6fhp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 13 13:03:36.947: INFO: Pod daemon-set-6fhp5 is not available
Jul 13 13:03:37.959: INFO: Wrong image for pod: daemon-set-6fhp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jul 13 13:03:37.959: INFO: Pod daemon-set-6fhp5 is not available
Jul 13 13:03:38.962: INFO: Pod daemon-set-vnwt4 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2452, will wait for the garbage collector to delete the pods
Jul 13 13:03:39.044: INFO: Deleting DaemonSet.extensions daemon-set took: 9.638452ms
Jul 13 13:03:39.744: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.230535ms
Jul 13 13:03:51.355: INFO: Number of nodes with available pods: 0
Jul 13 13:03:51.355: INFO: Number of running nodes: 0, number of available pods: 0
Jul 13 13:03:51.359: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2452/daemonsets","resourceVersion":"223974"},"items":null}

Jul 13 13:03:51.362: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2452/pods","resourceVersion":"223974"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:03:51.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2452" for this suite.

â€¢ [SLOW TEST:20.647 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":280,"completed":74,"skipped":1330,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:03:51.401: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 13:03:51.459: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d194f338-6ca7-49e2-9893-f4698d400648" in namespace "projected-3704" to be "success or failure"
Jul 13 13:03:51.464: INFO: Pod "downwardapi-volume-d194f338-6ca7-49e2-9893-f4698d400648": Phase="Pending", Reason="", readiness=false. Elapsed: 4.710195ms
Jul 13 13:03:53.468: INFO: Pod "downwardapi-volume-d194f338-6ca7-49e2-9893-f4698d400648": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009262422s
Jul 13 13:03:55.473: INFO: Pod "downwardapi-volume-d194f338-6ca7-49e2-9893-f4698d400648": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014215482s
STEP: Saw pod success
Jul 13 13:03:55.473: INFO: Pod "downwardapi-volume-d194f338-6ca7-49e2-9893-f4698d400648" satisfied condition "success or failure"
Jul 13 13:03:55.477: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod downwardapi-volume-d194f338-6ca7-49e2-9893-f4698d400648 container client-container: <nil>
STEP: delete the pod
Jul 13 13:03:55.505: INFO: Waiting for pod downwardapi-volume-d194f338-6ca7-49e2-9893-f4698d400648 to disappear
Jul 13 13:03:55.510: INFO: Pod downwardapi-volume-d194f338-6ca7-49e2-9893-f4698d400648 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:03:55.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3704" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":75,"skipped":1351,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:03:55.524: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 13 13:04:05.683: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0713 13:04:05.683060      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:04:05.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7389" for this suite.

â€¢ [SLOW TEST:10.171 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":280,"completed":76,"skipped":1365,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:04:05.697: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-7efcb9fa-7ce8-4be1-81cb-08f625ccf9de
STEP: Creating a pod to test consume configMaps
Jul 13 13:04:05.753: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4e230dff-5d0f-4572-a6ce-d85191d1a94a" in namespace "projected-7875" to be "success or failure"
Jul 13 13:04:05.762: INFO: Pod "pod-projected-configmaps-4e230dff-5d0f-4572-a6ce-d85191d1a94a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.24854ms
Jul 13 13:04:07.767: INFO: Pod "pod-projected-configmaps-4e230dff-5d0f-4572-a6ce-d85191d1a94a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013915179s
STEP: Saw pod success
Jul 13 13:04:07.767: INFO: Pod "pod-projected-configmaps-4e230dff-5d0f-4572-a6ce-d85191d1a94a" satisfied condition "success or failure"
Jul 13 13:04:07.770: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-projected-configmaps-4e230dff-5d0f-4572-a6ce-d85191d1a94a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 13:04:07.804: INFO: Waiting for pod pod-projected-configmaps-4e230dff-5d0f-4572-a6ce-d85191d1a94a to disappear
Jul 13 13:04:07.808: INFO: Pod pod-projected-configmaps-4e230dff-5d0f-4572-a6ce-d85191d1a94a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:04:07.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7875" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":77,"skipped":1373,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:04:07.822: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 13:04:07.872: INFO: Waiting up to 5m0s for pod "downwardapi-volume-69cc08e0-0444-4a9e-b877-563e46068eea" in namespace "downward-api-4695" to be "success or failure"
Jul 13 13:04:07.878: INFO: Pod "downwardapi-volume-69cc08e0-0444-4a9e-b877-563e46068eea": Phase="Pending", Reason="", readiness=false. Elapsed: 5.239699ms
Jul 13 13:04:09.884: INFO: Pod "downwardapi-volume-69cc08e0-0444-4a9e-b877-563e46068eea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010953045s
Jul 13 13:04:11.890: INFO: Pod "downwardapi-volume-69cc08e0-0444-4a9e-b877-563e46068eea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017824573s
STEP: Saw pod success
Jul 13 13:04:11.890: INFO: Pod "downwardapi-volume-69cc08e0-0444-4a9e-b877-563e46068eea" satisfied condition "success or failure"
Jul 13 13:04:11.895: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod downwardapi-volume-69cc08e0-0444-4a9e-b877-563e46068eea container client-container: <nil>
STEP: delete the pod
Jul 13 13:04:11.929: INFO: Waiting for pod downwardapi-volume-69cc08e0-0444-4a9e-b877-563e46068eea to disappear
Jul 13 13:04:11.933: INFO: Pod downwardapi-volume-69cc08e0-0444-4a9e-b877-563e46068eea no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:04:11.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4695" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":78,"skipped":1374,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:04:11.948: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1681
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 13 13:04:11.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6715'
Jul 13 13:04:12.071: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 13 13:04:12.071: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
Jul 13 13:04:12.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete jobs e2e-test-httpd-job --namespace=kubectl-6715'
Jul 13 13:04:12.181: INFO: stderr: ""
Jul 13 13:04:12.181: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:04:12.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6715" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":280,"completed":79,"skipped":1399,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:04:12.195: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:04:23.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2214" for this suite.

â€¢ [SLOW TEST:11.115 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":280,"completed":80,"skipped":1425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:04:23.311: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 13 13:04:29.403: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:04:29.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0713 13:04:29.403775      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-8123" for this suite.

â€¢ [SLOW TEST:6.108 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":280,"completed":81,"skipped":1457,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:04:29.419: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:163
Jul 13 13:04:29.457: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 13 13:05:29.491: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:05:29.495: INFO: Starting informer...
STEP: Starting pod...
Jul 13 13:05:29.713: INFO: Pod is running on ip-10-0-3-16.us-east-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jul 13 13:05:29.732: INFO: Pod wasn't evicted. Proceeding
Jul 13 13:05:29.733: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jul 13 13:06:44.763: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:06:44.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4442" for this suite.

â€¢ [SLOW TEST:135.360 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":280,"completed":82,"skipped":1476,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:06:44.779: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 13:06:44.835: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c44fc82-5a28-4894-9c67-fc41d6560277" in namespace "projected-3063" to be "success or failure"
Jul 13 13:06:44.844: INFO: Pod "downwardapi-volume-3c44fc82-5a28-4894-9c67-fc41d6560277": Phase="Pending", Reason="", readiness=false. Elapsed: 8.751719ms
Jul 13 13:06:46.849: INFO: Pod "downwardapi-volume-3c44fc82-5a28-4894-9c67-fc41d6560277": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013928853s
STEP: Saw pod success
Jul 13 13:06:46.849: INFO: Pod "downwardapi-volume-3c44fc82-5a28-4894-9c67-fc41d6560277" satisfied condition "success or failure"
Jul 13 13:06:46.853: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod downwardapi-volume-3c44fc82-5a28-4894-9c67-fc41d6560277 container client-container: <nil>
STEP: delete the pod
Jul 13 13:06:46.901: INFO: Waiting for pod downwardapi-volume-3c44fc82-5a28-4894-9c67-fc41d6560277 to disappear
Jul 13 13:06:46.904: INFO: Pod downwardapi-volume-3c44fc82-5a28-4894-9c67-fc41d6560277 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:06:46.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3063" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":280,"completed":83,"skipped":1491,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:06:46.920: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 13:06:47.601: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 13 13:06:49.614: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242407, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242407, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242407, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242407, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 13:06:52.641: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:06:52.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8746" for this suite.
STEP: Destroying namespace "webhook-8746-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.811 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":280,"completed":84,"skipped":1499,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:06:52.732: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 13 13:06:52.781: INFO: Waiting up to 5m0s for pod "pod-42302437-8d1d-474c-93c5-3dbc4a60cf02" in namespace "emptydir-5561" to be "success or failure"
Jul 13 13:06:52.791: INFO: Pod "pod-42302437-8d1d-474c-93c5-3dbc4a60cf02": Phase="Pending", Reason="", readiness=false. Elapsed: 10.28599ms
Jul 13 13:06:54.797: INFO: Pod "pod-42302437-8d1d-474c-93c5-3dbc4a60cf02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015488827s
Jul 13 13:06:56.802: INFO: Pod "pod-42302437-8d1d-474c-93c5-3dbc4a60cf02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02062889s
STEP: Saw pod success
Jul 13 13:06:56.802: INFO: Pod "pod-42302437-8d1d-474c-93c5-3dbc4a60cf02" satisfied condition "success or failure"
Jul 13 13:06:56.805: INFO: Trying to get logs from node ip-10-0-3-156.us-east-2.compute.internal pod pod-42302437-8d1d-474c-93c5-3dbc4a60cf02 container test-container: <nil>
STEP: delete the pod
Jul 13 13:06:56.860: INFO: Waiting for pod pod-42302437-8d1d-474c-93c5-3dbc4a60cf02 to disappear
Jul 13 13:06:56.864: INFO: Pod pod-42302437-8d1d-474c-93c5-3dbc4a60cf02 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:06:56.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5561" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":85,"skipped":1514,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:06:56.878: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0713 13:06:57.535800      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 13 13:06:57.535: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:06:57.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5976" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":280,"completed":86,"skipped":1519,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:06:57.548: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:06:57.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3550" for this suite.
â€¢{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":280,"completed":87,"skipped":1564,"failed":0}

------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:06:57.673: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:06:57.736: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6782
I0713 13:06:57.752029      20 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6782, replica count: 1
I0713 13:06:58.802529      20 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0713 13:06:59.802769      20 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0713 13:07:00.803003      20 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 13 13:07:00.921: INFO: Created: latency-svc-lxxrs
Jul 13 13:07:00.932: INFO: Got endpoints: latency-svc-lxxrs [28.88929ms]
Jul 13 13:07:00.952: INFO: Created: latency-svc-nxc2z
Jul 13 13:07:00.968: INFO: Got endpoints: latency-svc-nxc2z [36.132406ms]
Jul 13 13:07:00.971: INFO: Created: latency-svc-g4t8k
Jul 13 13:07:00.983: INFO: Got endpoints: latency-svc-g4t8k [50.858593ms]
Jul 13 13:07:00.987: INFO: Created: latency-svc-w4r5h
Jul 13 13:07:00.997: INFO: Got endpoints: latency-svc-w4r5h [64.646343ms]
Jul 13 13:07:01.000: INFO: Created: latency-svc-dsw96
Jul 13 13:07:01.014: INFO: Got endpoints: latency-svc-dsw96 [82.111666ms]
Jul 13 13:07:01.015: INFO: Created: latency-svc-s6mdq
Jul 13 13:07:01.030: INFO: Got endpoints: latency-svc-s6mdq [97.135652ms]
Jul 13 13:07:01.044: INFO: Created: latency-svc-m67q8
Jul 13 13:07:01.051: INFO: Created: latency-svc-2d4n9
Jul 13 13:07:01.066: INFO: Got endpoints: latency-svc-m67q8 [133.49496ms]
Jul 13 13:07:01.070: INFO: Got endpoints: latency-svc-2d4n9 [136.957795ms]
Jul 13 13:07:01.083: INFO: Created: latency-svc-mlqwx
Jul 13 13:07:01.102: INFO: Got endpoints: latency-svc-mlqwx [169.758447ms]
Jul 13 13:07:01.108: INFO: Created: latency-svc-x6nct
Jul 13 13:07:01.116: INFO: Got endpoints: latency-svc-x6nct [182.986944ms]
Jul 13 13:07:01.125: INFO: Created: latency-svc-k22vm
Jul 13 13:07:01.141: INFO: Got endpoints: latency-svc-k22vm [208.571721ms]
Jul 13 13:07:01.145: INFO: Created: latency-svc-dprkw
Jul 13 13:07:01.155: INFO: Got endpoints: latency-svc-dprkw [222.575706ms]
Jul 13 13:07:01.159: INFO: Created: latency-svc-8ft26
Jul 13 13:07:01.172: INFO: Got endpoints: latency-svc-8ft26 [238.973106ms]
Jul 13 13:07:01.176: INFO: Created: latency-svc-5m95z
Jul 13 13:07:01.193: INFO: Got endpoints: latency-svc-5m95z [259.896182ms]
Jul 13 13:07:01.195: INFO: Created: latency-svc-mmtk4
Jul 13 13:07:01.210: INFO: Got endpoints: latency-svc-mmtk4 [276.809284ms]
Jul 13 13:07:01.212: INFO: Created: latency-svc-qg9zx
Jul 13 13:07:01.229: INFO: Got endpoints: latency-svc-qg9zx [295.914113ms]
Jul 13 13:07:01.246: INFO: Created: latency-svc-hmw67
Jul 13 13:07:01.249: INFO: Created: latency-svc-znrtk
Jul 13 13:07:01.267: INFO: Got endpoints: latency-svc-znrtk [284.237911ms]
Jul 13 13:07:01.267: INFO: Got endpoints: latency-svc-hmw67 [299.128621ms]
Jul 13 13:07:01.274: INFO: Created: latency-svc-7b5mf
Jul 13 13:07:01.290: INFO: Created: latency-svc-z4wts
Jul 13 13:07:01.290: INFO: Got endpoints: latency-svc-7b5mf [292.049369ms]
Jul 13 13:07:01.308: INFO: Got endpoints: latency-svc-z4wts [293.278789ms]
Jul 13 13:07:01.312: INFO: Created: latency-svc-bhgnn
Jul 13 13:07:01.321: INFO: Got endpoints: latency-svc-bhgnn [290.985431ms]
Jul 13 13:07:01.352: INFO: Created: latency-svc-8skp5
Jul 13 13:07:01.356: INFO: Got endpoints: latency-svc-8skp5 [290.361033ms]
Jul 13 13:07:01.358: INFO: Created: latency-svc-h4969
Jul 13 13:07:01.385: INFO: Created: latency-svc-tj9l8
Jul 13 13:07:01.390: INFO: Got endpoints: latency-svc-tj9l8 [287.335465ms]
Jul 13 13:07:01.405: INFO: Got endpoints: latency-svc-h4969 [335.33611ms]
Jul 13 13:07:01.417: INFO: Created: latency-svc-2lpcx
Jul 13 13:07:01.446: INFO: Created: latency-svc-gpvbh
Jul 13 13:07:01.447: INFO: Created: latency-svc-rw29t
Jul 13 13:07:01.447: INFO: Got endpoints: latency-svc-2lpcx [331.04372ms]
Jul 13 13:07:01.447: INFO: Got endpoints: latency-svc-rw29t [305.409411ms]
Jul 13 13:07:01.458: INFO: Got endpoints: latency-svc-gpvbh [302.337354ms]
Jul 13 13:07:01.463: INFO: Created: latency-svc-sf7hv
Jul 13 13:07:01.477: INFO: Got endpoints: latency-svc-sf7hv [305.575297ms]
Jul 13 13:07:01.482: INFO: Created: latency-svc-njxd4
Jul 13 13:07:01.494: INFO: Got endpoints: latency-svc-njxd4 [300.770044ms]
Jul 13 13:07:01.498: INFO: Created: latency-svc-pgfv9
Jul 13 13:07:01.512: INFO: Got endpoints: latency-svc-pgfv9 [302.604852ms]
Jul 13 13:07:01.513: INFO: Created: latency-svc-65vd8
Jul 13 13:07:01.529: INFO: Got endpoints: latency-svc-65vd8 [299.785308ms]
Jul 13 13:07:01.530: INFO: Created: latency-svc-vcwfc
Jul 13 13:07:01.549: INFO: Created: latency-svc-qk6lc
Jul 13 13:07:01.550: INFO: Got endpoints: latency-svc-vcwfc [282.282123ms]
Jul 13 13:07:01.562: INFO: Got endpoints: latency-svc-qk6lc [294.784316ms]
Jul 13 13:07:01.564: INFO: Created: latency-svc-c6tcn
Jul 13 13:07:01.584: INFO: Got endpoints: latency-svc-c6tcn [293.776305ms]
Jul 13 13:07:01.591: INFO: Created: latency-svc-95469
Jul 13 13:07:01.614: INFO: Got endpoints: latency-svc-95469 [305.573933ms]
Jul 13 13:07:01.617: INFO: Created: latency-svc-z2l58
Jul 13 13:07:01.625: INFO: Got endpoints: latency-svc-z2l58 [304.583871ms]
Jul 13 13:07:01.640: INFO: Created: latency-svc-4fc6q
Jul 13 13:07:01.653: INFO: Got endpoints: latency-svc-4fc6q [295.835343ms]
Jul 13 13:07:01.654: INFO: Created: latency-svc-cjdvp
Jul 13 13:07:01.673: INFO: Got endpoints: latency-svc-cjdvp [282.815221ms]
Jul 13 13:07:01.679: INFO: Created: latency-svc-l6gcz
Jul 13 13:07:01.681: INFO: Got endpoints: latency-svc-l6gcz [276.49871ms]
Jul 13 13:07:01.696: INFO: Created: latency-svc-wkpx2
Jul 13 13:07:01.709: INFO: Created: latency-svc-22hln
Jul 13 13:07:01.720: INFO: Got endpoints: latency-svc-wkpx2 [273.150517ms]
Jul 13 13:07:01.721: INFO: Got endpoints: latency-svc-22hln [274.717964ms]
Jul 13 13:07:01.734: INFO: Created: latency-svc-758kw
Jul 13 13:07:01.742: INFO: Created: latency-svc-dzrrc
Jul 13 13:07:01.746: INFO: Got endpoints: latency-svc-758kw [288.303542ms]
Jul 13 13:07:01.760: INFO: Got endpoints: latency-svc-dzrrc [282.870952ms]
Jul 13 13:07:01.768: INFO: Created: latency-svc-dq986
Jul 13 13:07:01.786: INFO: Got endpoints: latency-svc-dq986 [292.427432ms]
Jul 13 13:07:01.799: INFO: Created: latency-svc-fzprp
Jul 13 13:07:01.808: INFO: Got endpoints: latency-svc-fzprp [295.400081ms]
Jul 13 13:07:01.831: INFO: Created: latency-svc-jmxx4
Jul 13 13:07:01.847: INFO: Created: latency-svc-hwtv9
Jul 13 13:07:01.847: INFO: Got endpoints: latency-svc-jmxx4 [318.810174ms]
Jul 13 13:07:01.852: INFO: Got endpoints: latency-svc-hwtv9 [301.946939ms]
Jul 13 13:07:01.865: INFO: Created: latency-svc-9hvh9
Jul 13 13:07:01.872: INFO: Got endpoints: latency-svc-9hvh9 [309.930802ms]
Jul 13 13:07:01.879: INFO: Created: latency-svc-lkcjq
Jul 13 13:07:01.891: INFO: Got endpoints: latency-svc-lkcjq [307.573285ms]
Jul 13 13:07:01.902: INFO: Created: latency-svc-rxsk9
Jul 13 13:07:01.921: INFO: Created: latency-svc-nj6d8
Jul 13 13:07:01.951: INFO: Created: latency-svc-kn4qz
Jul 13 13:07:01.951: INFO: Got endpoints: latency-svc-rxsk9 [337.209983ms]
Jul 13 13:07:01.965: INFO: Created: latency-svc-nx8dm
Jul 13 13:07:01.975: INFO: Created: latency-svc-7h29t
Jul 13 13:07:01.985: INFO: Got endpoints: latency-svc-nj6d8 [359.882977ms]
Jul 13 13:07:01.995: INFO: Created: latency-svc-b96jt
Jul 13 13:07:02.011: INFO: Created: latency-svc-75vhl
Jul 13 13:07:02.025: INFO: Created: latency-svc-sw8fp
Jul 13 13:07:02.031: INFO: Got endpoints: latency-svc-kn4qz [378.04829ms]
Jul 13 13:07:02.043: INFO: Created: latency-svc-5t44d
Jul 13 13:07:02.062: INFO: Created: latency-svc-9fvgc
Jul 13 13:07:02.080: INFO: Created: latency-svc-rrss4
Jul 13 13:07:02.090: INFO: Got endpoints: latency-svc-nx8dm [417.321938ms]
Jul 13 13:07:02.114: INFO: Created: latency-svc-46dfh
Jul 13 13:07:02.145: INFO: Created: latency-svc-ftxj2
Jul 13 13:07:02.145: INFO: Got endpoints: latency-svc-7h29t [463.278258ms]
Jul 13 13:07:02.162: INFO: Created: latency-svc-rfvrd
Jul 13 13:07:02.185: INFO: Created: latency-svc-hj4pr
Jul 13 13:07:02.194: INFO: Got endpoints: latency-svc-b96jt [473.550468ms]
Jul 13 13:07:02.217: INFO: Created: latency-svc-msgpx
Jul 13 13:07:02.232: INFO: Created: latency-svc-kk24f
Jul 13 13:07:02.238: INFO: Got endpoints: latency-svc-75vhl [516.058675ms]
Jul 13 13:07:02.253: INFO: Created: latency-svc-t5jd6
Jul 13 13:07:02.272: INFO: Created: latency-svc-f2k9t
Jul 13 13:07:02.287: INFO: Got endpoints: latency-svc-sw8fp [540.673752ms]
Jul 13 13:07:02.290: INFO: Created: latency-svc-wdrjm
Jul 13 13:07:02.308: INFO: Created: latency-svc-ggz9t
Jul 13 13:07:02.317: INFO: Created: latency-svc-ml9xv
Jul 13 13:07:02.330: INFO: Created: latency-svc-jxbwh
Jul 13 13:07:02.332: INFO: Got endpoints: latency-svc-5t44d [571.808983ms]
Jul 13 13:07:02.363: INFO: Created: latency-svc-lj2sc
Jul 13 13:07:02.383: INFO: Got endpoints: latency-svc-9fvgc [597.270086ms]
Jul 13 13:07:02.402: INFO: Created: latency-svc-z6gql
Jul 13 13:07:02.436: INFO: Got endpoints: latency-svc-rrss4 [628.560573ms]
Jul 13 13:07:02.458: INFO: Created: latency-svc-94fzn
Jul 13 13:07:02.488: INFO: Got endpoints: latency-svc-46dfh [640.079755ms]
Jul 13 13:07:02.515: INFO: Created: latency-svc-8xlsn
Jul 13 13:07:02.530: INFO: Got endpoints: latency-svc-ftxj2 [678.31769ms]
Jul 13 13:07:02.550: INFO: Created: latency-svc-grjxc
Jul 13 13:07:02.590: INFO: Got endpoints: latency-svc-rfvrd [717.72537ms]
Jul 13 13:07:02.646: INFO: Created: latency-svc-9ck6r
Jul 13 13:07:02.651: INFO: Got endpoints: latency-svc-hj4pr [759.393639ms]
Jul 13 13:07:02.694: INFO: Got endpoints: latency-svc-msgpx [743.306078ms]
Jul 13 13:07:02.720: INFO: Created: latency-svc-jb4jc
Jul 13 13:07:02.749: INFO: Got endpoints: latency-svc-kk24f [763.759232ms]
Jul 13 13:07:02.758: INFO: Created: latency-svc-g525w
Jul 13 13:07:02.781: INFO: Created: latency-svc-clzq2
Jul 13 13:07:02.795: INFO: Got endpoints: latency-svc-t5jd6 [763.59947ms]
Jul 13 13:07:02.830: INFO: Created: latency-svc-6wk8x
Jul 13 13:07:02.834: INFO: Got endpoints: latency-svc-f2k9t [743.452099ms]
Jul 13 13:07:02.869: INFO: Created: latency-svc-z9b8j
Jul 13 13:07:02.882: INFO: Got endpoints: latency-svc-wdrjm [736.740773ms]
Jul 13 13:07:02.910: INFO: Created: latency-svc-bm424
Jul 13 13:07:02.934: INFO: Got endpoints: latency-svc-ggz9t [740.054509ms]
Jul 13 13:07:02.957: INFO: Created: latency-svc-wjtb4
Jul 13 13:07:02.980: INFO: Got endpoints: latency-svc-ml9xv [742.741948ms]
Jul 13 13:07:03.005: INFO: Created: latency-svc-x6jdf
Jul 13 13:07:03.030: INFO: Got endpoints: latency-svc-jxbwh [743.185372ms]
Jul 13 13:07:03.058: INFO: Created: latency-svc-g2bxc
Jul 13 13:07:03.080: INFO: Got endpoints: latency-svc-lj2sc [748.216346ms]
Jul 13 13:07:03.103: INFO: Created: latency-svc-jrjx7
Jul 13 13:07:03.134: INFO: Got endpoints: latency-svc-z6gql [750.44908ms]
Jul 13 13:07:03.159: INFO: Created: latency-svc-x8n29
Jul 13 13:07:03.183: INFO: Got endpoints: latency-svc-94fzn [746.673883ms]
Jul 13 13:07:03.204: INFO: Created: latency-svc-qdhlg
Jul 13 13:07:03.232: INFO: Got endpoints: latency-svc-8xlsn [744.414808ms]
Jul 13 13:07:03.254: INFO: Created: latency-svc-52j5f
Jul 13 13:07:03.281: INFO: Got endpoints: latency-svc-grjxc [750.802092ms]
Jul 13 13:07:03.303: INFO: Created: latency-svc-8v7mr
Jul 13 13:07:03.329: INFO: Got endpoints: latency-svc-9ck6r [738.646825ms]
Jul 13 13:07:03.354: INFO: Created: latency-svc-svj7j
Jul 13 13:07:03.382: INFO: Got endpoints: latency-svc-jb4jc [730.93028ms]
Jul 13 13:07:03.401: INFO: Created: latency-svc-75qd2
Jul 13 13:07:03.436: INFO: Got endpoints: latency-svc-g525w [741.689402ms]
Jul 13 13:07:03.457: INFO: Created: latency-svc-p6cqw
Jul 13 13:07:03.479: INFO: Got endpoints: latency-svc-clzq2 [729.924847ms]
Jul 13 13:07:03.499: INFO: Created: latency-svc-bzrkt
Jul 13 13:07:03.531: INFO: Got endpoints: latency-svc-6wk8x [735.445755ms]
Jul 13 13:07:03.550: INFO: Created: latency-svc-tlrn8
Jul 13 13:07:03.580: INFO: Got endpoints: latency-svc-z9b8j [746.631819ms]
Jul 13 13:07:03.606: INFO: Created: latency-svc-wvn4t
Jul 13 13:07:03.630: INFO: Got endpoints: latency-svc-bm424 [748.377467ms]
Jul 13 13:07:03.649: INFO: Created: latency-svc-g4cws
Jul 13 13:07:03.679: INFO: Got endpoints: latency-svc-wjtb4 [745.36689ms]
Jul 13 13:07:03.698: INFO: Created: latency-svc-2dmf7
Jul 13 13:07:03.730: INFO: Got endpoints: latency-svc-x6jdf [749.055565ms]
Jul 13 13:07:03.749: INFO: Created: latency-svc-n7z9f
Jul 13 13:07:03.780: INFO: Got endpoints: latency-svc-g2bxc [749.484902ms]
Jul 13 13:07:03.802: INFO: Created: latency-svc-28tqt
Jul 13 13:07:03.829: INFO: Got endpoints: latency-svc-jrjx7 [748.892609ms]
Jul 13 13:07:03.847: INFO: Created: latency-svc-l59tf
Jul 13 13:07:03.880: INFO: Got endpoints: latency-svc-x8n29 [745.444707ms]
Jul 13 13:07:03.899: INFO: Created: latency-svc-qhhz5
Jul 13 13:07:03.930: INFO: Got endpoints: latency-svc-qdhlg [746.293469ms]
Jul 13 13:07:03.958: INFO: Created: latency-svc-87jb6
Jul 13 13:07:03.980: INFO: Got endpoints: latency-svc-52j5f [747.589316ms]
Jul 13 13:07:04.001: INFO: Created: latency-svc-wkfzk
Jul 13 13:07:04.031: INFO: Got endpoints: latency-svc-8v7mr [749.655868ms]
Jul 13 13:07:04.049: INFO: Created: latency-svc-jf2p6
Jul 13 13:07:04.080: INFO: Got endpoints: latency-svc-svj7j [751.260735ms]
Jul 13 13:07:04.099: INFO: Created: latency-svc-gxx6w
Jul 13 13:07:04.130: INFO: Got endpoints: latency-svc-75qd2 [748.201876ms]
Jul 13 13:07:04.149: INFO: Created: latency-svc-5x87v
Jul 13 13:07:04.180: INFO: Got endpoints: latency-svc-p6cqw [743.793321ms]
Jul 13 13:07:04.197: INFO: Created: latency-svc-pfn8g
Jul 13 13:07:04.238: INFO: Got endpoints: latency-svc-bzrkt [758.92019ms]
Jul 13 13:07:04.259: INFO: Created: latency-svc-b7fwz
Jul 13 13:07:04.279: INFO: Got endpoints: latency-svc-tlrn8 [748.24561ms]
Jul 13 13:07:04.299: INFO: Created: latency-svc-mrkxs
Jul 13 13:07:04.330: INFO: Got endpoints: latency-svc-wvn4t [750.073218ms]
Jul 13 13:07:04.351: INFO: Created: latency-svc-zgr7j
Jul 13 13:07:04.381: INFO: Got endpoints: latency-svc-g4cws [750.577848ms]
Jul 13 13:07:04.400: INFO: Created: latency-svc-65tw7
Jul 13 13:07:04.430: INFO: Got endpoints: latency-svc-2dmf7 [750.318787ms]
Jul 13 13:07:04.450: INFO: Created: latency-svc-lq8kl
Jul 13 13:07:04.482: INFO: Got endpoints: latency-svc-n7z9f [751.856119ms]
Jul 13 13:07:04.503: INFO: Created: latency-svc-85t9n
Jul 13 13:07:04.530: INFO: Got endpoints: latency-svc-28tqt [750.842656ms]
Jul 13 13:07:04.548: INFO: Created: latency-svc-zsl4x
Jul 13 13:07:04.580: INFO: Got endpoints: latency-svc-l59tf [750.269401ms]
Jul 13 13:07:04.599: INFO: Created: latency-svc-dkv9x
Jul 13 13:07:04.630: INFO: Got endpoints: latency-svc-qhhz5 [750.032395ms]
Jul 13 13:07:04.648: INFO: Created: latency-svc-gxd66
Jul 13 13:07:04.679: INFO: Got endpoints: latency-svc-87jb6 [749.232592ms]
Jul 13 13:07:04.699: INFO: Created: latency-svc-hw2rk
Jul 13 13:07:04.729: INFO: Got endpoints: latency-svc-wkfzk [749.178969ms]
Jul 13 13:07:04.750: INFO: Created: latency-svc-lk9s8
Jul 13 13:07:04.780: INFO: Got endpoints: latency-svc-jf2p6 [749.118001ms]
Jul 13 13:07:04.801: INFO: Created: latency-svc-qf8ts
Jul 13 13:07:04.830: INFO: Got endpoints: latency-svc-gxx6w [749.629948ms]
Jul 13 13:07:04.850: INFO: Created: latency-svc-wpzsd
Jul 13 13:07:04.880: INFO: Got endpoints: latency-svc-5x87v [749.461194ms]
Jul 13 13:07:04.901: INFO: Created: latency-svc-2c7px
Jul 13 13:07:04.929: INFO: Got endpoints: latency-svc-pfn8g [749.023132ms]
Jul 13 13:07:04.948: INFO: Created: latency-svc-kj8bq
Jul 13 13:07:04.980: INFO: Got endpoints: latency-svc-b7fwz [740.996741ms]
Jul 13 13:07:04.998: INFO: Created: latency-svc-lqjxt
Jul 13 13:07:05.030: INFO: Got endpoints: latency-svc-mrkxs [750.65782ms]
Jul 13 13:07:05.049: INFO: Created: latency-svc-dzxzb
Jul 13 13:07:05.080: INFO: Got endpoints: latency-svc-zgr7j [749.239033ms]
Jul 13 13:07:05.101: INFO: Created: latency-svc-5vwnv
Jul 13 13:07:05.129: INFO: Got endpoints: latency-svc-65tw7 [748.259917ms]
Jul 13 13:07:05.147: INFO: Created: latency-svc-fbkrj
Jul 13 13:07:05.180: INFO: Got endpoints: latency-svc-lq8kl [749.630778ms]
Jul 13 13:07:05.200: INFO: Created: latency-svc-l2jvc
Jul 13 13:07:05.230: INFO: Got endpoints: latency-svc-85t9n [748.47884ms]
Jul 13 13:07:05.250: INFO: Created: latency-svc-dnp9w
Jul 13 13:07:05.279: INFO: Got endpoints: latency-svc-zsl4x [748.697241ms]
Jul 13 13:07:05.299: INFO: Created: latency-svc-hvkbz
Jul 13 13:07:05.331: INFO: Got endpoints: latency-svc-dkv9x [750.741455ms]
Jul 13 13:07:05.349: INFO: Created: latency-svc-2dvph
Jul 13 13:07:05.380: INFO: Got endpoints: latency-svc-gxd66 [750.471874ms]
Jul 13 13:07:05.399: INFO: Created: latency-svc-4s7sn
Jul 13 13:07:05.429: INFO: Got endpoints: latency-svc-hw2rk [750.113694ms]
Jul 13 13:07:05.450: INFO: Created: latency-svc-s7gxn
Jul 13 13:07:05.480: INFO: Got endpoints: latency-svc-lk9s8 [751.141018ms]
Jul 13 13:07:05.500: INFO: Created: latency-svc-jlhc9
Jul 13 13:07:05.529: INFO: Got endpoints: latency-svc-qf8ts [748.920998ms]
Jul 13 13:07:05.548: INFO: Created: latency-svc-665m7
Jul 13 13:07:05.580: INFO: Got endpoints: latency-svc-wpzsd [749.951215ms]
Jul 13 13:07:05.599: INFO: Created: latency-svc-dvl9p
Jul 13 13:07:05.631: INFO: Got endpoints: latency-svc-2c7px [751.033413ms]
Jul 13 13:07:05.649: INFO: Created: latency-svc-sb7vz
Jul 13 13:07:05.680: INFO: Got endpoints: latency-svc-kj8bq [750.867325ms]
Jul 13 13:07:05.700: INFO: Created: latency-svc-kvzbw
Jul 13 13:07:05.730: INFO: Got endpoints: latency-svc-lqjxt [750.171808ms]
Jul 13 13:07:05.749: INFO: Created: latency-svc-jq8jq
Jul 13 13:07:05.781: INFO: Got endpoints: latency-svc-dzxzb [750.778404ms]
Jul 13 13:07:05.804: INFO: Created: latency-svc-tvmzh
Jul 13 13:07:05.830: INFO: Got endpoints: latency-svc-5vwnv [749.782877ms]
Jul 13 13:07:05.849: INFO: Created: latency-svc-btnlj
Jul 13 13:07:05.880: INFO: Got endpoints: latency-svc-fbkrj [750.290335ms]
Jul 13 13:07:05.902: INFO: Created: latency-svc-qd8rg
Jul 13 13:07:05.930: INFO: Got endpoints: latency-svc-l2jvc [750.131716ms]
Jul 13 13:07:05.950: INFO: Created: latency-svc-fmfdf
Jul 13 13:07:05.981: INFO: Got endpoints: latency-svc-dnp9w [750.135056ms]
Jul 13 13:07:06.000: INFO: Created: latency-svc-79htj
Jul 13 13:07:06.029: INFO: Got endpoints: latency-svc-hvkbz [749.877885ms]
Jul 13 13:07:06.047: INFO: Created: latency-svc-v86gd
Jul 13 13:07:06.080: INFO: Got endpoints: latency-svc-2dvph [748.849907ms]
Jul 13 13:07:06.103: INFO: Created: latency-svc-96rvl
Jul 13 13:07:06.131: INFO: Got endpoints: latency-svc-4s7sn [750.340443ms]
Jul 13 13:07:06.149: INFO: Created: latency-svc-x7pl7
Jul 13 13:07:06.180: INFO: Got endpoints: latency-svc-s7gxn [750.745548ms]
Jul 13 13:07:06.199: INFO: Created: latency-svc-4xlnk
Jul 13 13:07:06.229: INFO: Got endpoints: latency-svc-jlhc9 [749.007912ms]
Jul 13 13:07:06.250: INFO: Created: latency-svc-nzwpv
Jul 13 13:07:06.279: INFO: Got endpoints: latency-svc-665m7 [749.961236ms]
Jul 13 13:07:06.299: INFO: Created: latency-svc-48zdc
Jul 13 13:07:06.329: INFO: Got endpoints: latency-svc-dvl9p [749.133648ms]
Jul 13 13:07:06.348: INFO: Created: latency-svc-dwp52
Jul 13 13:07:06.380: INFO: Got endpoints: latency-svc-sb7vz [748.891005ms]
Jul 13 13:07:06.400: INFO: Created: latency-svc-k7tmx
Jul 13 13:07:06.431: INFO: Got endpoints: latency-svc-kvzbw [750.738156ms]
Jul 13 13:07:06.449: INFO: Created: latency-svc-z7bgc
Jul 13 13:07:06.481: INFO: Got endpoints: latency-svc-jq8jq [751.39555ms]
Jul 13 13:07:06.500: INFO: Created: latency-svc-946dj
Jul 13 13:07:06.530: INFO: Got endpoints: latency-svc-tvmzh [748.951586ms]
Jul 13 13:07:06.550: INFO: Created: latency-svc-j4qjr
Jul 13 13:07:06.586: INFO: Got endpoints: latency-svc-btnlj [756.502576ms]
Jul 13 13:07:06.609: INFO: Created: latency-svc-q5dsn
Jul 13 13:07:06.629: INFO: Got endpoints: latency-svc-qd8rg [749.757168ms]
Jul 13 13:07:06.650: INFO: Created: latency-svc-lcprs
Jul 13 13:07:06.680: INFO: Got endpoints: latency-svc-fmfdf [749.591916ms]
Jul 13 13:07:06.699: INFO: Created: latency-svc-2t785
Jul 13 13:07:06.730: INFO: Got endpoints: latency-svc-79htj [749.314122ms]
Jul 13 13:07:06.751: INFO: Created: latency-svc-67kr9
Jul 13 13:07:06.779: INFO: Got endpoints: latency-svc-v86gd [749.930432ms]
Jul 13 13:07:06.799: INFO: Created: latency-svc-6kztn
Jul 13 13:07:06.831: INFO: Got endpoints: latency-svc-96rvl [751.859417ms]
Jul 13 13:07:06.856: INFO: Created: latency-svc-sqfx4
Jul 13 13:07:06.880: INFO: Got endpoints: latency-svc-x7pl7 [749.181893ms]
Jul 13 13:07:06.903: INFO: Created: latency-svc-pxcqf
Jul 13 13:07:06.932: INFO: Got endpoints: latency-svc-4xlnk [751.671393ms]
Jul 13 13:07:06.959: INFO: Created: latency-svc-nkdhw
Jul 13 13:07:06.979: INFO: Got endpoints: latency-svc-nzwpv [749.854971ms]
Jul 13 13:07:07.002: INFO: Created: latency-svc-xcch6
Jul 13 13:07:07.029: INFO: Got endpoints: latency-svc-48zdc [749.796251ms]
Jul 13 13:07:07.053: INFO: Created: latency-svc-jgbhn
Jul 13 13:07:07.080: INFO: Got endpoints: latency-svc-dwp52 [750.15521ms]
Jul 13 13:07:07.100: INFO: Created: latency-svc-jpsv8
Jul 13 13:07:07.130: INFO: Got endpoints: latency-svc-k7tmx [749.806133ms]
Jul 13 13:07:07.150: INFO: Created: latency-svc-222j2
Jul 13 13:07:07.180: INFO: Got endpoints: latency-svc-z7bgc [749.395352ms]
Jul 13 13:07:07.204: INFO: Created: latency-svc-b8g27
Jul 13 13:07:07.230: INFO: Got endpoints: latency-svc-946dj [748.101092ms]
Jul 13 13:07:07.250: INFO: Created: latency-svc-dn42p
Jul 13 13:07:07.284: INFO: Got endpoints: latency-svc-j4qjr [754.401465ms]
Jul 13 13:07:07.307: INFO: Created: latency-svc-stnmp
Jul 13 13:07:07.330: INFO: Got endpoints: latency-svc-q5dsn [743.880215ms]
Jul 13 13:07:07.350: INFO: Created: latency-svc-lskth
Jul 13 13:07:07.381: INFO: Got endpoints: latency-svc-lcprs [751.356125ms]
Jul 13 13:07:07.401: INFO: Created: latency-svc-np5g2
Jul 13 13:07:07.440: INFO: Got endpoints: latency-svc-2t785 [759.884275ms]
Jul 13 13:07:07.459: INFO: Created: latency-svc-lt88j
Jul 13 13:07:07.480: INFO: Got endpoints: latency-svc-67kr9 [750.25903ms]
Jul 13 13:07:07.504: INFO: Created: latency-svc-2f86n
Jul 13 13:07:07.530: INFO: Got endpoints: latency-svc-6kztn [750.183484ms]
Jul 13 13:07:07.554: INFO: Created: latency-svc-75pqd
Jul 13 13:07:07.580: INFO: Got endpoints: latency-svc-sqfx4 [748.735193ms]
Jul 13 13:07:07.601: INFO: Created: latency-svc-vt767
Jul 13 13:07:07.629: INFO: Got endpoints: latency-svc-pxcqf [749.33023ms]
Jul 13 13:07:07.650: INFO: Created: latency-svc-nsmtr
Jul 13 13:07:07.681: INFO: Got endpoints: latency-svc-nkdhw [748.770897ms]
Jul 13 13:07:07.700: INFO: Created: latency-svc-cf8kt
Jul 13 13:07:07.730: INFO: Got endpoints: latency-svc-xcch6 [750.501494ms]
Jul 13 13:07:07.750: INFO: Created: latency-svc-mlrhr
Jul 13 13:07:07.780: INFO: Got endpoints: latency-svc-jgbhn [750.462171ms]
Jul 13 13:07:07.800: INFO: Created: latency-svc-m6tp2
Jul 13 13:07:07.830: INFO: Got endpoints: latency-svc-jpsv8 [750.091874ms]
Jul 13 13:07:07.849: INFO: Created: latency-svc-q7hkx
Jul 13 13:07:07.883: INFO: Got endpoints: latency-svc-222j2 [753.572445ms]
Jul 13 13:07:07.902: INFO: Created: latency-svc-wtfsq
Jul 13 13:07:07.931: INFO: Got endpoints: latency-svc-b8g27 [750.574094ms]
Jul 13 13:07:07.949: INFO: Created: latency-svc-nlcdr
Jul 13 13:07:07.979: INFO: Got endpoints: latency-svc-dn42p [749.644864ms]
Jul 13 13:07:08.001: INFO: Created: latency-svc-z4mjl
Jul 13 13:07:08.030: INFO: Got endpoints: latency-svc-stnmp [745.708411ms]
Jul 13 13:07:08.050: INFO: Created: latency-svc-hmnlx
Jul 13 13:07:08.081: INFO: Got endpoints: latency-svc-lskth [750.446995ms]
Jul 13 13:07:08.109: INFO: Created: latency-svc-59m6s
Jul 13 13:07:08.130: INFO: Got endpoints: latency-svc-np5g2 [749.363416ms]
Jul 13 13:07:08.149: INFO: Created: latency-svc-lbjmt
Jul 13 13:07:08.180: INFO: Got endpoints: latency-svc-lt88j [740.677783ms]
Jul 13 13:07:08.200: INFO: Created: latency-svc-jg6nt
Jul 13 13:07:08.230: INFO: Got endpoints: latency-svc-2f86n [748.987942ms]
Jul 13 13:07:08.250: INFO: Created: latency-svc-kncrm
Jul 13 13:07:08.280: INFO: Got endpoints: latency-svc-75pqd [749.965375ms]
Jul 13 13:07:08.300: INFO: Created: latency-svc-74dnp
Jul 13 13:07:08.335: INFO: Got endpoints: latency-svc-vt767 [754.341244ms]
Jul 13 13:07:08.354: INFO: Created: latency-svc-d4qvr
Jul 13 13:07:08.379: INFO: Got endpoints: latency-svc-nsmtr [749.881799ms]
Jul 13 13:07:08.398: INFO: Created: latency-svc-pmcnq
Jul 13 13:07:08.432: INFO: Got endpoints: latency-svc-cf8kt [751.586915ms]
Jul 13 13:07:08.451: INFO: Created: latency-svc-v8l2m
Jul 13 13:07:08.480: INFO: Got endpoints: latency-svc-mlrhr [750.518105ms]
Jul 13 13:07:08.501: INFO: Created: latency-svc-7qfzl
Jul 13 13:07:08.530: INFO: Got endpoints: latency-svc-m6tp2 [749.943827ms]
Jul 13 13:07:08.551: INFO: Created: latency-svc-b8srg
Jul 13 13:07:08.580: INFO: Got endpoints: latency-svc-q7hkx [750.188559ms]
Jul 13 13:07:08.598: INFO: Created: latency-svc-rpfcr
Jul 13 13:07:08.630: INFO: Got endpoints: latency-svc-wtfsq [746.72238ms]
Jul 13 13:07:08.648: INFO: Created: latency-svc-lsb84
Jul 13 13:07:08.679: INFO: Got endpoints: latency-svc-nlcdr [748.228886ms]
Jul 13 13:07:08.698: INFO: Created: latency-svc-k2hnn
Jul 13 13:07:08.735: INFO: Got endpoints: latency-svc-z4mjl [755.861709ms]
Jul 13 13:07:08.756: INFO: Created: latency-svc-zslg8
Jul 13 13:07:08.780: INFO: Got endpoints: latency-svc-hmnlx [749.736274ms]
Jul 13 13:07:08.830: INFO: Got endpoints: latency-svc-59m6s [748.8624ms]
Jul 13 13:07:08.886: INFO: Got endpoints: latency-svc-lbjmt [756.199294ms]
Jul 13 13:07:08.930: INFO: Got endpoints: latency-svc-jg6nt [749.754081ms]
Jul 13 13:07:08.979: INFO: Got endpoints: latency-svc-kncrm [749.578797ms]
Jul 13 13:07:09.030: INFO: Got endpoints: latency-svc-74dnp [749.900324ms]
Jul 13 13:07:09.079: INFO: Got endpoints: latency-svc-d4qvr [744.751257ms]
Jul 13 13:07:09.133: INFO: Got endpoints: latency-svc-pmcnq [754.004471ms]
Jul 13 13:07:09.180: INFO: Got endpoints: latency-svc-v8l2m [747.237493ms]
Jul 13 13:07:09.229: INFO: Got endpoints: latency-svc-7qfzl [748.708699ms]
Jul 13 13:07:09.280: INFO: Got endpoints: latency-svc-b8srg [749.90898ms]
Jul 13 13:07:09.330: INFO: Got endpoints: latency-svc-rpfcr [750.265913ms]
Jul 13 13:07:09.381: INFO: Got endpoints: latency-svc-lsb84 [750.548473ms]
Jul 13 13:07:09.429: INFO: Got endpoints: latency-svc-k2hnn [749.761534ms]
Jul 13 13:07:09.479: INFO: Got endpoints: latency-svc-zslg8 [743.927245ms]
Jul 13 13:07:09.479: INFO: Latencies: [36.132406ms 50.858593ms 64.646343ms 82.111666ms 97.135652ms 133.49496ms 136.957795ms 169.758447ms 182.986944ms 208.571721ms 222.575706ms 238.973106ms 259.896182ms 273.150517ms 274.717964ms 276.49871ms 276.809284ms 282.282123ms 282.815221ms 282.870952ms 284.237911ms 287.335465ms 288.303542ms 290.361033ms 290.985431ms 292.049369ms 292.427432ms 293.278789ms 293.776305ms 294.784316ms 295.400081ms 295.835343ms 295.914113ms 299.128621ms 299.785308ms 300.770044ms 301.946939ms 302.337354ms 302.604852ms 304.583871ms 305.409411ms 305.573933ms 305.575297ms 307.573285ms 309.930802ms 318.810174ms 331.04372ms 335.33611ms 337.209983ms 359.882977ms 378.04829ms 417.321938ms 463.278258ms 473.550468ms 516.058675ms 540.673752ms 571.808983ms 597.270086ms 628.560573ms 640.079755ms 678.31769ms 717.72537ms 729.924847ms 730.93028ms 735.445755ms 736.740773ms 738.646825ms 740.054509ms 740.677783ms 740.996741ms 741.689402ms 742.741948ms 743.185372ms 743.306078ms 743.452099ms 743.793321ms 743.880215ms 743.927245ms 744.414808ms 744.751257ms 745.36689ms 745.444707ms 745.708411ms 746.293469ms 746.631819ms 746.673883ms 746.72238ms 747.237493ms 747.589316ms 748.101092ms 748.201876ms 748.216346ms 748.228886ms 748.24561ms 748.259917ms 748.377467ms 748.47884ms 748.697241ms 748.708699ms 748.735193ms 748.770897ms 748.849907ms 748.8624ms 748.891005ms 748.892609ms 748.920998ms 748.951586ms 748.987942ms 749.007912ms 749.023132ms 749.055565ms 749.118001ms 749.133648ms 749.178969ms 749.181893ms 749.232592ms 749.239033ms 749.314122ms 749.33023ms 749.363416ms 749.395352ms 749.461194ms 749.484902ms 749.578797ms 749.591916ms 749.629948ms 749.630778ms 749.644864ms 749.655868ms 749.736274ms 749.754081ms 749.757168ms 749.761534ms 749.782877ms 749.796251ms 749.806133ms 749.854971ms 749.877885ms 749.881799ms 749.900324ms 749.90898ms 749.930432ms 749.943827ms 749.951215ms 749.961236ms 749.965375ms 750.032395ms 750.073218ms 750.091874ms 750.113694ms 750.131716ms 750.135056ms 750.15521ms 750.171808ms 750.183484ms 750.188559ms 750.25903ms 750.265913ms 750.269401ms 750.290335ms 750.318787ms 750.340443ms 750.446995ms 750.44908ms 750.462171ms 750.471874ms 750.501494ms 750.518105ms 750.548473ms 750.574094ms 750.577848ms 750.65782ms 750.738156ms 750.741455ms 750.745548ms 750.778404ms 750.802092ms 750.842656ms 750.867325ms 751.033413ms 751.141018ms 751.260735ms 751.356125ms 751.39555ms 751.586915ms 751.671393ms 751.856119ms 751.859417ms 753.572445ms 754.004471ms 754.341244ms 754.401465ms 755.861709ms 756.199294ms 756.502576ms 758.92019ms 759.393639ms 759.884275ms 763.59947ms 763.759232ms]
Jul 13 13:07:09.479: INFO: 50 %ile: 748.770897ms
Jul 13 13:07:09.479: INFO: 90 %ile: 751.141018ms
Jul 13 13:07:09.479: INFO: 99 %ile: 763.59947ms
Jul 13 13:07:09.479: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:07:09.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6782" for this suite.

â€¢ [SLOW TEST:11.832 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":280,"completed":88,"skipped":1564,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:07:09.505: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 13:07:09.559: INFO: Waiting up to 5m0s for pod "downwardapi-volume-92af69f9-a7cb-4a1f-a54b-cb4ba505c22e" in namespace "projected-6435" to be "success or failure"
Jul 13 13:07:09.568: INFO: Pod "downwardapi-volume-92af69f9-a7cb-4a1f-a54b-cb4ba505c22e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.746716ms
Jul 13 13:07:11.572: INFO: Pod "downwardapi-volume-92af69f9-a7cb-4a1f-a54b-cb4ba505c22e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013560786s
Jul 13 13:07:13.578: INFO: Pod "downwardapi-volume-92af69f9-a7cb-4a1f-a54b-cb4ba505c22e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019434377s
STEP: Saw pod success
Jul 13 13:07:13.578: INFO: Pod "downwardapi-volume-92af69f9-a7cb-4a1f-a54b-cb4ba505c22e" satisfied condition "success or failure"
Jul 13 13:07:13.583: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod downwardapi-volume-92af69f9-a7cb-4a1f-a54b-cb4ba505c22e container client-container: <nil>
STEP: delete the pod
Jul 13 13:07:13.625: INFO: Waiting for pod downwardapi-volume-92af69f9-a7cb-4a1f-a54b-cb4ba505c22e to disappear
Jul 13 13:07:13.629: INFO: Pod downwardapi-volume-92af69f9-a7cb-4a1f-a54b-cb4ba505c22e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:07:13.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6435" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":89,"skipped":1568,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:07:13.644: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Jul 13 13:07:13.700: INFO: Waiting up to 5m0s for pod "client-containers-52e1b5c7-891a-48b4-980a-2c7fe8389396" in namespace "containers-7252" to be "success or failure"
Jul 13 13:07:13.710: INFO: Pod "client-containers-52e1b5c7-891a-48b4-980a-2c7fe8389396": Phase="Pending", Reason="", readiness=false. Elapsed: 9.211687ms
Jul 13 13:07:15.715: INFO: Pod "client-containers-52e1b5c7-891a-48b4-980a-2c7fe8389396": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014185102s
Jul 13 13:07:17.722: INFO: Pod "client-containers-52e1b5c7-891a-48b4-980a-2c7fe8389396": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021398705s
STEP: Saw pod success
Jul 13 13:07:17.722: INFO: Pod "client-containers-52e1b5c7-891a-48b4-980a-2c7fe8389396" satisfied condition "success or failure"
Jul 13 13:07:17.727: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod client-containers-52e1b5c7-891a-48b4-980a-2c7fe8389396 container test-container: <nil>
STEP: delete the pod
Jul 13 13:07:17.765: INFO: Waiting for pod client-containers-52e1b5c7-891a-48b4-980a-2c7fe8389396 to disappear
Jul 13 13:07:17.789: INFO: Pod client-containers-52e1b5c7-891a-48b4-980a-2c7fe8389396 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:07:17.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7252" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":280,"completed":90,"skipped":1578,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:07:17.808: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:07:33.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7062" for this suite.

â€¢ [SLOW TEST:16.191 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":280,"completed":91,"skipped":1615,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:07:34.001: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-2869
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2869 to expose endpoints map[]
Jul 13 13:07:34.065: INFO: Get endpoints failed (6.916384ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jul 13 13:07:35.070: INFO: successfully validated that service multi-endpoint-test in namespace services-2869 exposes endpoints map[] (1.01161771s elapsed)
STEP: Creating pod pod1 in namespace services-2869
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2869 to expose endpoints map[pod1:[100]]
Jul 13 13:07:37.126: INFO: successfully validated that service multi-endpoint-test in namespace services-2869 exposes endpoints map[pod1:[100]] (2.043297591s elapsed)
STEP: Creating pod pod2 in namespace services-2869
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2869 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 13 13:07:39.170: INFO: successfully validated that service multi-endpoint-test in namespace services-2869 exposes endpoints map[pod1:[100] pod2:[101]] (2.038383732s elapsed)
STEP: Deleting pod pod1 in namespace services-2869
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2869 to expose endpoints map[pod2:[101]]
Jul 13 13:07:40.196: INFO: successfully validated that service multi-endpoint-test in namespace services-2869 exposes endpoints map[pod2:[101]] (1.018656478s elapsed)
STEP: Deleting pod pod2 in namespace services-2869
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2869 to expose endpoints map[]
Jul 13 13:07:40.221: INFO: successfully validated that service multi-endpoint-test in namespace services-2869 exposes endpoints map[] (17.176363ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:07:40.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2869" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:6.265 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":280,"completed":92,"skipped":1625,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:07:40.266: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 13 13:07:40.312: INFO: Waiting up to 5m0s for pod "pod-b369264b-9e29-444d-ad2c-5af1cfedba68" in namespace "emptydir-4394" to be "success or failure"
Jul 13 13:07:40.319: INFO: Pod "pod-b369264b-9e29-444d-ad2c-5af1cfedba68": Phase="Pending", Reason="", readiness=false. Elapsed: 6.862733ms
Jul 13 13:07:42.324: INFO: Pod "pod-b369264b-9e29-444d-ad2c-5af1cfedba68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011855463s
Jul 13 13:07:44.329: INFO: Pod "pod-b369264b-9e29-444d-ad2c-5af1cfedba68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017001786s
STEP: Saw pod success
Jul 13 13:07:44.330: INFO: Pod "pod-b369264b-9e29-444d-ad2c-5af1cfedba68" satisfied condition "success or failure"
Jul 13 13:07:44.333: INFO: Trying to get logs from node ip-10-0-3-156.us-east-2.compute.internal pod pod-b369264b-9e29-444d-ad2c-5af1cfedba68 container test-container: <nil>
STEP: delete the pod
Jul 13 13:07:44.365: INFO: Waiting for pod pod-b369264b-9e29-444d-ad2c-5af1cfedba68 to disappear
Jul 13 13:07:44.369: INFO: Pod pod-b369264b-9e29-444d-ad2c-5af1cfedba68 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:07:44.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4394" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":93,"skipped":1645,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:07:44.385: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:07:44.425: INFO: Creating deployment "webserver-deployment"
Jul 13 13:07:44.431: INFO: Waiting for observed generation 1
Jul 13 13:07:46.442: INFO: Waiting for all required pods to come up
Jul 13 13:07:46.450: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 13 13:07:48.466: INFO: Waiting for deployment "webserver-deployment" to complete
Jul 13 13:07:48.474: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jul 13 13:07:48.486: INFO: Updating deployment webserver-deployment
Jul 13 13:07:48.486: INFO: Waiting for observed generation 2
Jul 13 13:07:50.499: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 13 13:07:50.503: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 13 13:07:50.507: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 13 13:07:50.520: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 13 13:07:50.520: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 13 13:07:50.524: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 13 13:07:50.531: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jul 13 13:07:50.531: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jul 13 13:07:50.542: INFO: Updating deployment webserver-deployment
Jul 13 13:07:50.542: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jul 13 13:07:50.560: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 13 13:07:50.569: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jul 13 13:07:50.626: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-869 /apis/apps/v1/namespaces/deployment-869/deployments/webserver-deployment 4e2c4ba4-1ece-4355-bb1f-ba57f3ba9fd0 227487 3 2020-07-13 13:07:44 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003148728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-07-13 13:07:48 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-07-13 13:07:50 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jul 13 13:07:50.683: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-869 /apis/apps/v1/namespaces/deployment-869/replicasets/webserver-deployment-c7997dcc8 c303413f-eb87-4d9c-b565-d931b4beb2a2 227473 3 2020-07-13 13:07:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 4e2c4ba4-1ece-4355-bb1f-ba57f3ba9fd0 0xc0061a1a97 0xc0061a1a98}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0061a1b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 13 13:07:50.683: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jul 13 13:07:50.683: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-869 /apis/apps/v1/namespaces/deployment-869/replicasets/webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 227471 3 2020-07-13 13:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 4e2c4ba4-1ece-4355-bb1f-ba57f3ba9fd0 0xc0061a19d7 0xc0061a19d8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0061a1a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jul 13 13:07:50.807: INFO: Pod "webserver-deployment-595b5b9587-2b2ft" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2b2ft webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-2b2ft b890ca4d-737d-4772-ac76-959663cfe25d 227513 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc003148b77 0xc003148b78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-156.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.808: INFO: Pod "webserver-deployment-595b5b9587-4t29n" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4t29n webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-4t29n 825cc936-5b76-492a-a04d-50b549db32fb 227522 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc003148c80 0xc003148c81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-177.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.809: INFO: Pod "webserver-deployment-595b5b9587-52rm8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-52rm8 webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-52rm8 c6a31a57-257e-444f-ae7a-a5c82ce865af 227300 0 2020-07-13 13:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.20.92.108/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc003148d90 0xc003148d91}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-177.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.177,PodIP:10.20.92.108,StartTime:2020-07-13 13:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-13 13:07:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2392f8e80df6058b35975d04c33ac43be14149a36465597671a024192953be0c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.92.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.814: INFO: Pod "webserver-deployment-595b5b9587-52tzf" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-52tzf webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-52tzf 112f3035-cf12-47d3-aaeb-945d20e7b387 227321 0 2020-07-13 13:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.20.168.178/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc003148f00 0xc003148f01}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-22.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.22,PodIP:10.20.168.178,StartTime:2020-07-13 13:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-13 13:07:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://79d4d6febf76d07321ef35071e8844d4b9da544e716acd36796b6a40fd22f3fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.168.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.814: INFO: Pod "webserver-deployment-595b5b9587-5d8ql" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5d8ql webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-5d8ql 7910dd1d-5765-4ec5-be62-0270913a7ffd 227328 0 2020-07-13 13:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.20.234.62/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc003149070 0xc003149071}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-16.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.16,PodIP:10.20.234.62,StartTime:2020-07-13 13:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-13 13:07:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b4f53ea876998a4515c277382a6d36d133fb4f2000732e11b5eddb12f52e256d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.234.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.816: INFO: Pod "webserver-deployment-595b5b9587-67dgb" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-67dgb webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-67dgb 79319f69-3bed-402a-8e75-06a34f8e5128 227334 0 2020-07-13 13:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.20.23.146/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc003149fd0 0xc003149fd1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-156.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.156,PodIP:10.20.23.146,StartTime:2020-07-13 13:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-13 13:07:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://21104bea7f42e4b85dc2855ae5d84d609f3bc5006b28aee5ec2b2323fcb58757,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.23.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.817: INFO: Pod "webserver-deployment-595b5b9587-8ktck" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8ktck webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-8ktck c2fd27f7-1581-4610-bdfe-0e681c9fd6f5 227529 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc0039bc160 0xc0039bc161}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-82.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.818: INFO: Pod "webserver-deployment-595b5b9587-9gf8x" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9gf8x webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-9gf8x 1f43816b-1b7e-4c6b-be56-ed949b15a2bd 227479 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc0039bc270 0xc0039bc271}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-156.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.820: INFO: Pod "webserver-deployment-595b5b9587-9qndz" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9qndz webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-9qndz 7e048f3b-bd88-4d97-8f9d-8ba13cf8781b 227530 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc0039bc380 0xc0039bc381}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-82.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.821: INFO: Pod "webserver-deployment-595b5b9587-bdjfx" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bdjfx webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-bdjfx 1da13cbc-b382-4a9b-9f24-523804e0218c 227502 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc0039bc490 0xc0039bc491}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-156.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.822: INFO: Pod "webserver-deployment-595b5b9587-crshs" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-crshs webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-crshs 25efe85c-26fa-46d5-bf02-039a9408b26a 227325 0 2020-07-13 13:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.20.234.58/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc0039bc5a0 0xc0039bc5a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-16.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.16,PodIP:10.20.234.58,StartTime:2020-07-13 13:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-13 13:07:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://5a5667acfab762ce5d844c6cb893035c64a661cfa7c7f268cac9faf439e8e6e2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.234.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.823: INFO: Pod "webserver-deployment-595b5b9587-gt4tf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-gt4tf webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-gt4tf 9a7a5039-ef95-46ee-a39c-d6b0503cd216 227509 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc0039bc700 0xc0039bc701}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-22.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.823: INFO: Pod "webserver-deployment-595b5b9587-j7pb9" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-j7pb9 webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-j7pb9 91b39eaf-cbc3-45e3-8149-66fa22f202b7 227318 0 2020-07-13 13:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.20.168.177/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc0039bc840 0xc0039bc841}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-22.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.22,PodIP:10.20.168.177,StartTime:2020-07-13 13:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-13 13:07:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ee73b6bed129fd527df90891b0b2b16a54e5dd710f478d5e255eb79a8031ac68,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.168.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.826: INFO: Pod "webserver-deployment-595b5b9587-mg5nd" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mg5nd webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-mg5nd 7717b7fa-9fd2-4757-861a-55f2df48ee94 227313 0 2020-07-13 13:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.20.171.241/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc0039bc9b0 0xc0039bc9b1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-82.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.1.82,PodIP:10.20.171.241,StartTime:2020-07-13 13:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-13 13:07:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8daf71ad43833af0cc97cc6f05ffb4e8ac92a98aa41389dbded4161c318232da,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.171.241,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.826: INFO: Pod "webserver-deployment-595b5b9587-mvj4v" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mvj4v webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-mvj4v 40fc6202-782b-4d64-85a9-a0b786973689 227343 0 2020-07-13 13:07:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:10.20.92.109/32] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc0039bcb30 0xc0039bcb31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-177.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.177,PodIP:10.20.92.109,StartTime:2020-07-13 13:07:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-13 13:07:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://3e47c85b241841d5d5237a1ecedde09a84fa7a37a90d8498c121a472dbbc10d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.92.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.826: INFO: Pod "webserver-deployment-595b5b9587-ndd22" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ndd22 webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-ndd22 b7705e5c-e781-481a-9819-55545897219e 227533 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc0039bcc90 0xc0039bcc91}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-82.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.827: INFO: Pod "webserver-deployment-595b5b9587-p9qf9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p9qf9 webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-p9qf9 a61309b1-c667-42ef-aaa5-1ac78228b80b 227493 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc0039bcd90 0xc0039bcd91}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-16.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.829: INFO: Pod "webserver-deployment-595b5b9587-s4jwx" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-s4jwx webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-s4jwx c6fd115a-0828-4310-b170-26c41f135099 227527 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc0039bcea0 0xc0039bcea1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-16.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.829: INFO: Pod "webserver-deployment-595b5b9587-vn9n2" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vn9n2 webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-vn9n2 df88f4f6-3b97-446f-aa26-daebd2b10ebc 227492 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc0039bcfa0 0xc0039bcfa1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-22.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.830: INFO: Pod "webserver-deployment-595b5b9587-xhfmg" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xhfmg webserver-deployment-595b5b9587- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-595b5b9587-xhfmg 72c3e39f-5d66-4ecf-8124-eabd7399c94b 227512 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 cb765136-8497-4de4-855d-865d9b4e4293 0xc0039bd0a0 0xc0039bd0a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-177.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.830: INFO: Pod "webserver-deployment-c7997dcc8-2kgbm" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-2kgbm webserver-deployment-c7997dcc8- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-c7997dcc8-2kgbm 71d8b31b-6a0c-461e-bfe4-a43b9de219ea 227532 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c303413f-eb87-4d9c-b565-d931b4beb2a2 0xc0039bd1a0 0xc0039bd1a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-82.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.831: INFO: Pod "webserver-deployment-c7997dcc8-55fjh" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-55fjh webserver-deployment-c7997dcc8- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-c7997dcc8-55fjh 8c1be1ce-5886-47bc-9863-fd2363224edd 227462 0 2020-07-13 13:07:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.20.92.110/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c303413f-eb87-4d9c-b565-d931b4beb2a2 0xc0039bd2c0 0xc0039bd2c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-177.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.177,PodIP:10.20.92.110,StartTime:2020-07-13 13:07:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.92.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.832: INFO: Pod "webserver-deployment-c7997dcc8-7vqbg" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-7vqbg webserver-deployment-c7997dcc8- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-c7997dcc8-7vqbg b5171292-92f9-41c2-ae18-fbee10d3eb31 227501 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c303413f-eb87-4d9c-b565-d931b4beb2a2 0xc0039bd450 0xc0039bd451}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-177.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.832: INFO: Pod "webserver-deployment-c7997dcc8-drpmj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-drpmj webserver-deployment-c7997dcc8- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-c7997dcc8-drpmj d5ffaa04-2231-4f1f-a06f-24109f42e709 227517 0 2020-07-13 13:07:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.20.234.60/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c303413f-eb87-4d9c-b565-d931b4beb2a2 0xc0039bd570 0xc0039bd571}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-16.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.16,PodIP:,StartTime:2020-07-13 13:07:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.832: INFO: Pod "webserver-deployment-c7997dcc8-f4kbh" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-f4kbh webserver-deployment-c7997dcc8- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-c7997dcc8-f4kbh 74ed3789-61b5-44b5-8610-3393ff6b606a 227496 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c303413f-eb87-4d9c-b565-d931b4beb2a2 0xc0039bd6f0 0xc0039bd6f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-16.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.832: INFO: Pod "webserver-deployment-c7997dcc8-lrb5q" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lrb5q webserver-deployment-c7997dcc8- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-c7997dcc8-lrb5q 3d77837a-c9a5-4e00-af3f-109c974c0832 227465 0 2020-07-13 13:07:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.20.23.145/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c303413f-eb87-4d9c-b565-d931b4beb2a2 0xc0039bd820 0xc0039bd821}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-156.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.156,PodIP:10.20.23.145,StartTime:2020-07-13 13:07:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.23.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.832: INFO: Pod "webserver-deployment-c7997dcc8-lxc29" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lxc29 webserver-deployment-c7997dcc8- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-c7997dcc8-lxc29 1a771a53-1d9e-42ee-9750-6c1ecc9fc563 227526 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c303413f-eb87-4d9c-b565-d931b4beb2a2 0xc0039bd9c0 0xc0039bd9c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-156.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.832: INFO: Pod "webserver-deployment-c7997dcc8-ntzdk" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ntzdk webserver-deployment-c7997dcc8- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-c7997dcc8-ntzdk d73d0495-1f76-421a-992e-5b152f07c29a 227519 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c303413f-eb87-4d9c-b565-d931b4beb2a2 0xc0039bdad0 0xc0039bdad1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-1-82.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.832: INFO: Pod "webserver-deployment-c7997dcc8-p2gxz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-p2gxz webserver-deployment-c7997dcc8- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-c7997dcc8-p2gxz 6004dff3-c773-401b-aa56-ccb2a4d7cca2 227525 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c303413f-eb87-4d9c-b565-d931b4beb2a2 0xc0039bdc10 0xc0039bdc11}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-22.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.833: INFO: Pod "webserver-deployment-c7997dcc8-q2tt9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-q2tt9 webserver-deployment-c7997dcc8- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-c7997dcc8-q2tt9 c967a0b1-1008-4f6c-abac-5da8ee75a4ef 227457 0 2020-07-13 13:07:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.20.23.147/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c303413f-eb87-4d9c-b565-d931b4beb2a2 0xc0039bdd30 0xc0039bdd31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-156.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.156,PodIP:,StartTime:2020-07-13 13:07:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.833: INFO: Pod "webserver-deployment-c7997dcc8-qltqp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qltqp webserver-deployment-c7997dcc8- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-c7997dcc8-qltqp 0dd1d79a-86d5-4068-8746-f9f52764fdd2 227534 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c303413f-eb87-4d9c-b565-d931b4beb2a2 0xc0039bde90 0xc0039bde91}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-22.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.22,PodIP:,StartTime:2020-07-13 13:07:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.833: INFO: Pod "webserver-deployment-c7997dcc8-z9r9v" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-z9r9v webserver-deployment-c7997dcc8- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-c7997dcc8-z9r9v e42e4adb-0ed9-4b5d-9b0b-0881954294c5 227472 0 2020-07-13 13:07:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:10.20.168.179/32] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c303413f-eb87-4d9c-b565-d931b4beb2a2 0xc0009f2000 0xc0009f2001}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-22.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.22,PodIP:,StartTime:2020-07-13 13:07:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 13 13:07:50.833: INFO: Pod "webserver-deployment-c7997dcc8-zdf9w" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zdf9w webserver-deployment-c7997dcc8- deployment-869 /api/v1/namespaces/deployment-869/pods/webserver-deployment-c7997dcc8-zdf9w 33606970-c0e1-4210-a73b-d21f7200779d 227520 0 2020-07-13 13:07:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c303413f-eb87-4d9c-b565-d931b4beb2a2 0xc0009f2180 0xc0009f2181}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-brq8b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-brq8b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-brq8b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-16.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:07:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:07:50.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-869" for this suite.

â€¢ [SLOW TEST:6.535 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":280,"completed":94,"skipped":1675,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:07:50.921: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 13:07:52.776: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 13 13:07:54.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242472, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242472, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242472, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242472, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 13:07:57.878: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jul 13 13:07:57.995: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:07:58.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7332" for this suite.
STEP: Destroying namespace "webhook-7332-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:7.494 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":280,"completed":95,"skipped":1692,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:07:58.415: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jul 13 13:07:58.626: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:08:03.492: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:08:17.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7352" for this suite.

â€¢ [SLOW TEST:19.560 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":280,"completed":96,"skipped":1695,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:08:17.974: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:08:22.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4091" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":97,"skipped":1712,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:08:22.092: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 13:08:23.062: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 13:08:26.099: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:08:26.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7624" for this suite.
STEP: Destroying namespace "webhook-7624-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":280,"completed":98,"skipped":1718,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:08:26.259: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-0c4f70e8-dadb-49dc-8860-06a32b91da70
STEP: Creating a pod to test consume secrets
Jul 13 13:08:26.318: INFO: Waiting up to 5m0s for pod "pod-secrets-fb7b029f-094d-4097-b187-c0c2217b1bea" in namespace "secrets-1684" to be "success or failure"
Jul 13 13:08:26.328: INFO: Pod "pod-secrets-fb7b029f-094d-4097-b187-c0c2217b1bea": Phase="Pending", Reason="", readiness=false. Elapsed: 10.839705ms
Jul 13 13:08:28.333: INFO: Pod "pod-secrets-fb7b029f-094d-4097-b187-c0c2217b1bea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015560093s
STEP: Saw pod success
Jul 13 13:08:28.333: INFO: Pod "pod-secrets-fb7b029f-094d-4097-b187-c0c2217b1bea" satisfied condition "success or failure"
Jul 13 13:08:28.337: INFO: Trying to get logs from node ip-10-0-3-156.us-east-2.compute.internal pod pod-secrets-fb7b029f-094d-4097-b187-c0c2217b1bea container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 13:08:28.363: INFO: Waiting for pod pod-secrets-fb7b029f-094d-4097-b187-c0c2217b1bea to disappear
Jul 13 13:08:28.367: INFO: Pod pod-secrets-fb7b029f-094d-4097-b187-c0c2217b1bea no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:08:28.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1684" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":99,"skipped":1721,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:08:28.382: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jul 13 13:08:32.990: INFO: Successfully updated pod "annotationupdatedeb74be5-357b-4de7-9d78-71cd57931e14"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:08:35.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5958" for this suite.

â€¢ [SLOW TEST:6.662 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":100,"skipped":1793,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:08:35.044: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-1461c44a-6341-4811-91cb-e5982626635c
STEP: Creating a pod to test consume secrets
Jul 13 13:08:35.091: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-95ff9724-ab7e-41c1-b940-4ab9b9353188" in namespace "projected-4565" to be "success or failure"
Jul 13 13:08:35.097: INFO: Pod "pod-projected-secrets-95ff9724-ab7e-41c1-b940-4ab9b9353188": Phase="Pending", Reason="", readiness=false. Elapsed: 6.264493ms
Jul 13 13:08:37.102: INFO: Pod "pod-projected-secrets-95ff9724-ab7e-41c1-b940-4ab9b9353188": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011380684s
Jul 13 13:08:39.108: INFO: Pod "pod-projected-secrets-95ff9724-ab7e-41c1-b940-4ab9b9353188": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016470919s
STEP: Saw pod success
Jul 13 13:08:39.108: INFO: Pod "pod-projected-secrets-95ff9724-ab7e-41c1-b940-4ab9b9353188" satisfied condition "success or failure"
Jul 13 13:08:39.111: INFO: Trying to get logs from node ip-10-0-3-156.us-east-2.compute.internal pod pod-projected-secrets-95ff9724-ab7e-41c1-b940-4ab9b9353188 container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 13:08:39.139: INFO: Waiting for pod pod-projected-secrets-95ff9724-ab7e-41c1-b940-4ab9b9353188 to disappear
Jul 13 13:08:39.143: INFO: Pod pod-projected-secrets-95ff9724-ab7e-41c1-b940-4ab9b9353188 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:08:39.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4565" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":101,"skipped":1800,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:08:39.157: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:08:39.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9805" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":280,"completed":102,"skipped":1801,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:08:39.264: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Jul 13 13:08:43.345: INFO: Pod pod-hostip-df0a2dfc-e7bc-4501-9d8f-8e631f91d400 has hostIP: 10.0.3.156
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:08:43.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3579" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":280,"completed":103,"skipped":1802,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:08:43.359: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jul 13 13:08:43.397: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 13 13:08:43.414: INFO: Waiting for terminating namespaces to be deleted...
Jul 13 13:08:43.422: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-1-82.us-east-2.compute.internal before test
Jul 13 13:08:43.438: INFO: calico-node-xqfbv from kube-system started at 2020-07-12 18:21:00 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.438: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:08:43.438: INFO: pf9-sentry-865fcbbb89-jhgf5 from platform9-system started at 2020-07-12 18:21:32 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.438: INFO: 	Container pf9-sentry ready: true, restart count 0
Jul 13 13:08:43.438: INFO: calico-typha-569c98c8c5-q4jds from kube-system started at 2020-07-12 18:21:34 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.438: INFO: 	Container calico-typha ready: true, restart count 0
Jul 13 13:08:43.438: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-46t76 from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:08:43.438: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:08:43.438: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:08:43.438: INFO: dashboard-metrics-scraper-c79c65bb7-zg4zf from kubernetes-dashboard started at 2020-07-12 18:21:32 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.438: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 13 13:08:43.438: INFO: calicoctl from kube-system started at 2020-07-12 18:21:42 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.438: INFO: 	Container calicoctl ready: true, restart count 0
Jul 13 13:08:43.438: INFO: k8s-master-ip-10-0-1-82.us-east-2.compute.internal from kube-system started at 2020-07-12 18:21:28 +0000 UTC (3 container statuses recorded)
Jul 13 13:08:43.438: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 13 13:08:43.438: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jul 13 13:08:43.438: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 13 13:08:43.438: INFO: kubernetes-dashboard-6b7d67fff4-s2c44 from kubernetes-dashboard started at 2020-07-12 18:21:32 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.438: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 13 13:08:43.438: INFO: node-exporter-gqvhs from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.438: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:08:43.438: INFO: calico-kube-controllers-6d84f9d87d-zqq65 from kube-system started at 2020-07-12 18:21:34 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.438: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 13 13:08:43.438: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-2-177.us-east-2.compute.internal before test
Jul 13 13:08:43.457: INFO: calico-node-2cwb2 from kube-system started at 2020-07-12 18:23:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.457: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:08:43.457: INFO: node-exporter-zcpws from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.457: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:08:43.457: INFO: platform9-operators-pw6hx from pf9-olm started at 2020-07-12 18:27:46 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.457: INFO: 	Container registry-server ready: true, restart count 0
Jul 13 13:08:43.457: INFO: prometheus-operator-546b677c-z58nz from pf9-operators started at 2020-07-12 18:28:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.457: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 13 13:08:43.457: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-5rzwn from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:08:43.457: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:08:43.457: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:08:43.457: INFO: prometheus-system-0 from pf9-monitoring started at 2020-07-12 18:29:23 +0000 UTC (3 container statuses recorded)
Jul 13 13:08:43.457: INFO: 	Container prometheus ready: true, restart count 1
Jul 13 13:08:43.457: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 13 13:08:43.457: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 13 13:08:43.457: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-2-22.us-east-2.compute.internal before test
Jul 13 13:08:43.467: INFO: calico-typha-569c98c8c5-7skpm from kube-system started at 2020-07-12 18:23:42 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.467: INFO: 	Container calico-typha ready: true, restart count 0
Jul 13 13:08:43.467: INFO: busybox-host-aliasesd38ab030-16ed-40e5-9bfd-342034fecc3b from kubelet-test-4091 started at 2020-07-13 13:08:18 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.467: INFO: 	Container busybox-host-aliasesd38ab030-16ed-40e5-9bfd-342034fecc3b ready: true, restart count 0
Jul 13 13:08:43.467: INFO: node-exporter-dqvmz from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.467: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:08:43.467: INFO: packageserver-6cb6cd5d8b-9qxzn from pf9-olm started at 2020-07-13 11:17:40 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.467: INFO: 	Container packageserver ready: true, restart count 0
Jul 13 13:08:43.467: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-x2g8b from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:08:43.467: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:08:43.467: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:08:43.467: INFO: k8s-master-ip-10-0-2-22.us-east-2.compute.internal from kube-system started at 2020-07-12 18:24:03 +0000 UTC (3 container statuses recorded)
Jul 13 13:08:43.467: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 13 13:08:43.467: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 13 13:08:43.467: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 13 13:08:43.467: INFO: calico-node-hvjtr from kube-system started at 2020-07-12 18:22:47 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.467: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:08:43.467: INFO: coredns-5b985c544f-78zhd from kube-system started at 2020-07-12 18:23:41 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.467: INFO: 	Container coredns ready: true, restart count 0
Jul 13 13:08:43.467: INFO: metrics-server-v0.3.6-75cdf48d5b-fwjq5 from kube-system started at 2020-07-13 10:20:26 +0000 UTC (2 container statuses recorded)
Jul 13 13:08:43.467: INFO: 	Container metrics-server ready: true, restart count 0
Jul 13 13:08:43.467: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jul 13 13:08:43.467: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-3-156.us-east-2.compute.internal before test
Jul 13 13:08:43.477: INFO: kube-state-metrics-595cb5cc-x66jk from pf9-monitoring started at 2020-07-12 18:27:36 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.477: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 13 13:08:43.477: INFO: olm-operator-76d446f94c-6bntr from pf9-olm started at 2020-07-12 18:27:21 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.477: INFO: 	Container olm-operator ready: true, restart count 0
Jul 13 13:08:43.477: INFO: node-exporter-9tw9c from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.477: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:08:43.477: INFO: monhelper-776667cb66-tsccr from pf9-operators started at 2020-07-12 18:28:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.477: INFO: 	Container monhelper ready: true, restart count 1
Jul 13 13:08:43.477: INFO: packageserver-6cb6cd5d8b-59d8z from pf9-olm started at 2020-07-13 11:17:44 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.477: INFO: 	Container packageserver ready: true, restart count 0
Jul 13 13:08:43.477: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-p99pp from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:08:43.477: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:08:43.477: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:08:43.477: INFO: calico-node-x6qds from kube-system started at 2020-07-12 18:23:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.477: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:08:43.477: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2020-07-12 18:29:23 +0000 UTC (2 container statuses recorded)
Jul 13 13:08:43.477: INFO: 	Container alertmanager ready: true, restart count 0
Jul 13 13:08:43.477: INFO: 	Container config-reloader ready: true, restart count 0
Jul 13 13:08:43.477: INFO: grafana-7c58bb84d4-pkfb5 from pf9-monitoring started at 2020-07-12 18:29:23 +0000 UTC (2 container statuses recorded)
Jul 13 13:08:43.477: INFO: 	Container grafana ready: true, restart count 0
Jul 13 13:08:43.477: INFO: 	Container proxy ready: true, restart count 0
Jul 13 13:08:43.477: INFO: pod-hostip-df0a2dfc-e7bc-4501-9d8f-8e631f91d400 from pods-3579 started at 2020-07-13 13:08:39 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.477: INFO: 	Container test ready: true, restart count 0
Jul 13 13:08:43.477: INFO: catalog-operator-5898bbb7d6-hrfcd from pf9-olm started at 2020-07-12 18:27:19 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.477: INFO: 	Container catalog-operator ready: true, restart count 0
Jul 13 13:08:43.477: INFO: calico-typha-569c98c8c5-6ppkg from kube-system started at 2020-07-13 10:20:26 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.477: INFO: 	Container calico-typha ready: true, restart count 0
Jul 13 13:08:43.477: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-3-16.us-east-2.compute.internal before test
Jul 13 13:08:43.491: INFO: calico-node-rgskg from kube-system started at 2020-07-12 18:21:41 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.491: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:08:43.491: INFO: annotationupdatedeb74be5-357b-4de7-9d78-71cd57931e14 from downward-api-5958 started at 2020-07-13 13:08:28 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.491: INFO: 	Container client-container ready: false, restart count 0
Jul 13 13:08:43.491: INFO: k8s-master-ip-10-0-3-16.us-east-2.compute.internal from kube-system started at 2020-07-12 18:21:41 +0000 UTC (3 container statuses recorded)
Jul 13 13:08:43.491: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 13 13:08:43.491: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 13 13:08:43.491: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 13 13:08:43.491: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-5tb2r from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:08:43.491: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:08:43.491: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:08:43.491: INFO: node-exporter-tzmdk from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.491: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:08:43.491: INFO: sonobuoy from sonobuoy started at 2020-07-13 12:29:45 +0000 UTC (1 container statuses recorded)
Jul 13 13:08:43.491: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 13 13:08:43.491: INFO: sonobuoy-e2e-job-03adefc2964045c0 from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:08:43.491: INFO: 	Container e2e ready: true, restart count 0
Jul 13 13:08:43.491: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5fde8b79-671d-4559-988f-5d7a6f6fc195 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-5fde8b79-671d-4559-988f-5d7a6f6fc195 off the node ip-10-0-2-22.us-east-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5fde8b79-671d-4559-988f-5d7a6f6fc195
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:08:47.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8064" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":280,"completed":104,"skipped":1818,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:08:47.642: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1212, will wait for the garbage collector to delete the pods
Jul 13 13:08:51.777: INFO: Deleting Job.batch foo took: 15.930603ms
Jul 13 13:08:52.477: INFO: Terminating Job.batch foo pods took: 700.239168ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:09:31.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1212" for this suite.

â€¢ [SLOW TEST:43.654 seconds]
[sig-apps] Job
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":280,"completed":105,"skipped":1845,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:09:31.296: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 13:09:31.977: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 13 13:09:33.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242571, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242571, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242572, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242571, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 13:09:37.015: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:09:37.020: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:09:38.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4932" for this suite.
STEP: Destroying namespace "webhook-4932-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:7.099 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":280,"completed":106,"skipped":1845,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:09:38.396: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 13:09:39.428: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 13 13:09:41.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242579, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242579, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242579, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242579, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 13:09:44.486: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:09:44.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7005" for this suite.
STEP: Destroying namespace "webhook-7005-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.217 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":280,"completed":107,"skipped":1857,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:09:44.614: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jul 13 13:09:44.677: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 13 13:09:44.695: INFO: Waiting for terminating namespaces to be deleted...
Jul 13 13:09:44.699: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-1-82.us-east-2.compute.internal before test
Jul 13 13:09:44.707: INFO: calicoctl from kube-system started at 2020-07-12 18:21:42 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.707: INFO: 	Container calicoctl ready: true, restart count 0
Jul 13 13:09:44.707: INFO: dashboard-metrics-scraper-c79c65bb7-zg4zf from kubernetes-dashboard started at 2020-07-12 18:21:32 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.707: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 13 13:09:44.707: INFO: kubernetes-dashboard-6b7d67fff4-s2c44 from kubernetes-dashboard started at 2020-07-12 18:21:32 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.707: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul 13 13:09:44.707: INFO: node-exporter-gqvhs from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.707: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:09:44.707: INFO: calico-kube-controllers-6d84f9d87d-zqq65 from kube-system started at 2020-07-12 18:21:34 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.707: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 13 13:09:44.707: INFO: k8s-master-ip-10-0-1-82.us-east-2.compute.internal from kube-system started at 2020-07-12 18:21:28 +0000 UTC (3 container statuses recorded)
Jul 13 13:09:44.707: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 13 13:09:44.707: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jul 13 13:09:44.707: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 13 13:09:44.707: INFO: pf9-sentry-865fcbbb89-jhgf5 from platform9-system started at 2020-07-12 18:21:32 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.707: INFO: 	Container pf9-sentry ready: true, restart count 0
Jul 13 13:09:44.707: INFO: calico-node-xqfbv from kube-system started at 2020-07-12 18:21:00 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.707: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:09:44.707: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-46t76 from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:09:44.707: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:09:44.707: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:09:44.707: INFO: calico-typha-569c98c8c5-q4jds from kube-system started at 2020-07-12 18:21:34 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.707: INFO: 	Container calico-typha ready: true, restart count 0
Jul 13 13:09:44.707: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-2-177.us-east-2.compute.internal before test
Jul 13 13:09:44.714: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-5rzwn from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:09:44.714: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:09:44.714: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:09:44.714: INFO: calico-node-2cwb2 from kube-system started at 2020-07-12 18:23:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.714: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:09:44.714: INFO: node-exporter-zcpws from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.714: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:09:44.714: INFO: platform9-operators-pw6hx from pf9-olm started at 2020-07-12 18:27:46 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.714: INFO: 	Container registry-server ready: true, restart count 0
Jul 13 13:09:44.714: INFO: prometheus-operator-546b677c-z58nz from pf9-operators started at 2020-07-12 18:28:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.714: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 13 13:09:44.714: INFO: prometheus-system-0 from pf9-monitoring started at 2020-07-12 18:29:23 +0000 UTC (3 container statuses recorded)
Jul 13 13:09:44.714: INFO: 	Container prometheus ready: true, restart count 1
Jul 13 13:09:44.714: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 13 13:09:44.714: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 13 13:09:44.714: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-2-22.us-east-2.compute.internal before test
Jul 13 13:09:44.722: INFO: calico-node-hvjtr from kube-system started at 2020-07-12 18:22:47 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.722: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:09:44.722: INFO: coredns-5b985c544f-78zhd from kube-system started at 2020-07-12 18:23:41 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.722: INFO: 	Container coredns ready: true, restart count 0
Jul 13 13:09:44.723: INFO: metrics-server-v0.3.6-75cdf48d5b-fwjq5 from kube-system started at 2020-07-13 10:20:26 +0000 UTC (2 container statuses recorded)
Jul 13 13:09:44.723: INFO: 	Container metrics-server ready: true, restart count 0
Jul 13 13:09:44.723: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jul 13 13:09:44.723: INFO: calico-typha-569c98c8c5-7skpm from kube-system started at 2020-07-12 18:23:42 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.723: INFO: 	Container calico-typha ready: true, restart count 0
Jul 13 13:09:44.723: INFO: node-exporter-dqvmz from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.723: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:09:44.723: INFO: packageserver-6cb6cd5d8b-9qxzn from pf9-olm started at 2020-07-13 11:17:40 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.723: INFO: 	Container packageserver ready: true, restart count 0
Jul 13 13:09:44.723: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-x2g8b from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:09:44.723: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:09:44.723: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:09:44.723: INFO: k8s-master-ip-10-0-2-22.us-east-2.compute.internal from kube-system started at 2020-07-12 18:24:03 +0000 UTC (3 container statuses recorded)
Jul 13 13:09:44.723: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 13 13:09:44.723: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 13 13:09:44.723: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 13 13:09:44.723: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-3-156.us-east-2.compute.internal before test
Jul 13 13:09:44.742: INFO: kube-state-metrics-595cb5cc-x66jk from pf9-monitoring started at 2020-07-12 18:27:36 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.742: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 13 13:09:44.742: INFO: olm-operator-76d446f94c-6bntr from pf9-olm started at 2020-07-12 18:27:21 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.742: INFO: 	Container olm-operator ready: true, restart count 0
Jul 13 13:09:44.742: INFO: node-exporter-9tw9c from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.742: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:09:44.742: INFO: monhelper-776667cb66-tsccr from pf9-operators started at 2020-07-12 18:28:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.742: INFO: 	Container monhelper ready: true, restart count 1
Jul 13 13:09:44.742: INFO: packageserver-6cb6cd5d8b-59d8z from pf9-olm started at 2020-07-13 11:17:44 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.742: INFO: 	Container packageserver ready: true, restart count 0
Jul 13 13:09:44.742: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-p99pp from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:09:44.742: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:09:44.742: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 13 13:09:44.743: INFO: calico-node-x6qds from kube-system started at 2020-07-12 18:23:38 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.743: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:09:44.743: INFO: alertmanager-sysalert-0 from pf9-monitoring started at 2020-07-12 18:29:23 +0000 UTC (2 container statuses recorded)
Jul 13 13:09:44.743: INFO: 	Container alertmanager ready: true, restart count 0
Jul 13 13:09:44.743: INFO: 	Container config-reloader ready: true, restart count 0
Jul 13 13:09:44.743: INFO: grafana-7c58bb84d4-pkfb5 from pf9-monitoring started at 2020-07-12 18:29:23 +0000 UTC (2 container statuses recorded)
Jul 13 13:09:44.743: INFO: 	Container grafana ready: true, restart count 0
Jul 13 13:09:44.743: INFO: 	Container proxy ready: true, restart count 0
Jul 13 13:09:44.743: INFO: catalog-operator-5898bbb7d6-hrfcd from pf9-olm started at 2020-07-12 18:27:19 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.743: INFO: 	Container catalog-operator ready: true, restart count 0
Jul 13 13:09:44.743: INFO: calico-typha-569c98c8c5-6ppkg from kube-system started at 2020-07-13 10:20:26 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.743: INFO: 	Container calico-typha ready: true, restart count 0
Jul 13 13:09:44.743: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-3-16.us-east-2.compute.internal before test
Jul 13 13:09:44.751: INFO: node-exporter-tzmdk from pf9-monitoring started at 2020-07-12 18:27:43 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.751: INFO: 	Container node-exporter ready: true, restart count 0
Jul 13 13:09:44.751: INFO: sonobuoy from sonobuoy started at 2020-07-13 12:29:45 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.751: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 13 13:09:44.751: INFO: sonobuoy-e2e-job-03adefc2964045c0 from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:09:44.751: INFO: 	Container e2e ready: true, restart count 0
Jul 13 13:09:44.751: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:09:44.751: INFO: calico-node-rgskg from kube-system started at 2020-07-12 18:21:41 +0000 UTC (1 container statuses recorded)
Jul 13 13:09:44.751: INFO: 	Container calico-node ready: true, restart count 0
Jul 13 13:09:44.751: INFO: k8s-master-ip-10-0-3-16.us-east-2.compute.internal from kube-system started at 2020-07-12 18:21:41 +0000 UTC (3 container statuses recorded)
Jul 13 13:09:44.751: INFO: 	Container kube-apiserver ready: true, restart count 0
Jul 13 13:09:44.751: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jul 13 13:09:44.751: INFO: 	Container kube-scheduler ready: true, restart count 0
Jul 13 13:09:44.751: INFO: sonobuoy-systemd-logs-daemon-set-cbb774d2622d4ce1-5tb2r from sonobuoy started at 2020-07-13 12:29:46 +0000 UTC (2 container statuses recorded)
Jul 13 13:09:44.751: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 13 13:09:44.751: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.162151e478af8b52], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:09:45.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7318" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":280,"completed":108,"skipped":1875,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:09:45.809: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3291.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3291.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3291.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3291.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3291.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3291.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 13 13:09:49.943: INFO: DNS probes using dns-3291/dns-test-79f5883d-1c77-44f9-ae83-e9a74c4b203c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:09:49.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3291" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":280,"completed":109,"skipped":1892,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:09:49.986: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:09:50.036: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-ee779edd-6b9d-437a-8b2e-2aa2154b6783" in namespace "security-context-test-9657" to be "success or failure"
Jul 13 13:09:50.043: INFO: Pod "busybox-privileged-false-ee779edd-6b9d-437a-8b2e-2aa2154b6783": Phase="Pending", Reason="", readiness=false. Elapsed: 6.87318ms
Jul 13 13:09:52.048: INFO: Pod "busybox-privileged-false-ee779edd-6b9d-437a-8b2e-2aa2154b6783": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011929594s
Jul 13 13:09:52.048: INFO: Pod "busybox-privileged-false-ee779edd-6b9d-437a-8b2e-2aa2154b6783" satisfied condition "success or failure"
Jul 13 13:09:52.064: INFO: Got logs for pod "busybox-privileged-false-ee779edd-6b9d-437a-8b2e-2aa2154b6783": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:09:52.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9657" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":110,"skipped":1923,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:09:52.081: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1754
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 13 13:09:52.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9915'
Jul 13 13:09:52.246: INFO: stderr: ""
Jul 13 13:09:52.246: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1759
Jul 13 13:09:52.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete pods e2e-test-httpd-pod --namespace=kubectl-9915'
Jul 13 13:10:01.252: INFO: stderr: ""
Jul 13 13:10:01.252: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:10:01.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9915" for this suite.

â€¢ [SLOW TEST:9.202 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1750
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":280,"completed":111,"skipped":1945,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:10:01.283: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 13 13:10:01.376: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5459 /api/v1/namespaces/watch-5459/configmaps/e2e-watch-test-resource-version a80a7398-baa9-420a-b226-aa27bfc03577 229238 0 2020-07-13 13:10:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 13 13:10:01.376: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5459 /api/v1/namespaces/watch-5459/configmaps/e2e-watch-test-resource-version a80a7398-baa9-420a-b226-aa27bfc03577 229239 0 2020-07-13 13:10:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:10:01.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5459" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":280,"completed":112,"skipped":1951,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:10:01.390: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 13:10:02.444: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 13 13:10:04.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242602, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242602, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242602, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242602, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 13:10:07.491: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jul 13 13:10:11.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 attach --namespace=webhook-6978 to-be-attached-pod -i -c=container1'
Jul 13 13:10:12.476: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:10:12.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6978" for this suite.
STEP: Destroying namespace "webhook-6978-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:11.244 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":280,"completed":113,"skipped":1955,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:10:12.635: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-5568
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5568
STEP: Creating statefulset with conflicting port in namespace statefulset-5568
STEP: Waiting until pod test-pod will start running in namespace statefulset-5568
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5568
Jul 13 13:10:16.763: INFO: Observed stateful pod in namespace: statefulset-5568, name: ss-0, uid: 92be6a34-1971-4944-b251-cccc89a9cd30, status phase: Pending. Waiting for statefulset controller to delete.
Jul 13 13:10:16.948: INFO: Observed stateful pod in namespace: statefulset-5568, name: ss-0, uid: 92be6a34-1971-4944-b251-cccc89a9cd30, status phase: Failed. Waiting for statefulset controller to delete.
Jul 13 13:10:16.958: INFO: Observed stateful pod in namespace: statefulset-5568, name: ss-0, uid: 92be6a34-1971-4944-b251-cccc89a9cd30, status phase: Failed. Waiting for statefulset controller to delete.
Jul 13 13:10:16.965: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5568
STEP: Removing pod with conflicting port in namespace statefulset-5568
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5568 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jul 13 13:10:20.998: INFO: Deleting all statefulset in ns statefulset-5568
Jul 13 13:10:21.002: INFO: Scaling statefulset ss to 0
Jul 13 13:10:31.022: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 13:10:31.026: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:10:31.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5568" for this suite.

â€¢ [SLOW TEST:18.427 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":280,"completed":114,"skipped":1976,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:10:31.065: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:10:31.115: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 13 13:10:34.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-4941 create -f -'
Jul 13 13:10:35.941: INFO: stderr: ""
Jul 13 13:10:35.941: INFO: stdout: "e2e-test-crd-publish-openapi-1200-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 13 13:10:35.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-4941 delete e2e-test-crd-publish-openapi-1200-crds test-cr'
Jul 13 13:10:36.065: INFO: stderr: ""
Jul 13 13:10:36.065: INFO: stdout: "e2e-test-crd-publish-openapi-1200-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jul 13 13:10:36.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-4941 apply -f -'
Jul 13 13:10:36.538: INFO: stderr: ""
Jul 13 13:10:36.538: INFO: stdout: "e2e-test-crd-publish-openapi-1200-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 13 13:10:36.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-4941 delete e2e-test-crd-publish-openapi-1200-crds test-cr'
Jul 13 13:10:36.623: INFO: stderr: ""
Jul 13 13:10:36.623: INFO: stdout: "e2e-test-crd-publish-openapi-1200-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 13 13:10:36.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 explain e2e-test-crd-publish-openapi-1200-crds'
Jul 13 13:10:36.930: INFO: stderr: ""
Jul 13 13:10:36.930: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1200-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:10:41.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4941" for this suite.

â€¢ [SLOW TEST:10.080 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":280,"completed":115,"skipped":1995,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:10:41.144: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-d4e0417a-caa6-4af0-a235-7483f69e16be
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-d4e0417a-caa6-4af0-a235-7483f69e16be
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:10:45.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4760" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":116,"skipped":1999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:10:45.275: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-5307
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-5307
Jul 13 13:10:45.333: INFO: Found 0 stateful pods, waiting for 1
Jul 13 13:10:55.339: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jul 13 13:10:55.371: INFO: Deleting all statefulset in ns statefulset-5307
Jul 13 13:10:55.381: INFO: Scaling statefulset ss to 0
Jul 13 13:11:15.421: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 13:11:15.425: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:11:15.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5307" for this suite.

â€¢ [SLOW TEST:30.187 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":280,"completed":117,"skipped":2058,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:11:15.462: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Jul 13 13:11:15.519: INFO: Waiting up to 5m0s for pod "var-expansion-e2d97482-6e98-4cc6-9e55-5159f4f93f3c" in namespace "var-expansion-9696" to be "success or failure"
Jul 13 13:11:15.530: INFO: Pod "var-expansion-e2d97482-6e98-4cc6-9e55-5159f4f93f3c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.915501ms
Jul 13 13:11:17.535: INFO: Pod "var-expansion-e2d97482-6e98-4cc6-9e55-5159f4f93f3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015702166s
STEP: Saw pod success
Jul 13 13:11:17.535: INFO: Pod "var-expansion-e2d97482-6e98-4cc6-9e55-5159f4f93f3c" satisfied condition "success or failure"
Jul 13 13:11:17.539: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod var-expansion-e2d97482-6e98-4cc6-9e55-5159f4f93f3c container dapi-container: <nil>
STEP: delete the pod
Jul 13 13:11:17.574: INFO: Waiting for pod var-expansion-e2d97482-6e98-4cc6-9e55-5159f4f93f3c to disappear
Jul 13 13:11:17.578: INFO: Pod var-expansion-e2d97482-6e98-4cc6-9e55-5159f4f93f3c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:11:17.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9696" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":280,"completed":118,"skipped":2061,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:11:17.593: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 13:11:18.078: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 13:11:21.115: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:11:21.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7665" for this suite.
STEP: Destroying namespace "webhook-7665-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":280,"completed":119,"skipped":2079,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:11:21.495: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jul 13 13:11:21.552: INFO: Waiting up to 5m0s for pod "downward-api-81cfedbc-a352-400b-929f-fc9067f6d4cc" in namespace "downward-api-7181" to be "success or failure"
Jul 13 13:11:21.558: INFO: Pod "downward-api-81cfedbc-a352-400b-929f-fc9067f6d4cc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.397056ms
Jul 13 13:11:23.564: INFO: Pod "downward-api-81cfedbc-a352-400b-929f-fc9067f6d4cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011927858s
STEP: Saw pod success
Jul 13 13:11:23.564: INFO: Pod "downward-api-81cfedbc-a352-400b-929f-fc9067f6d4cc" satisfied condition "success or failure"
Jul 13 13:11:23.569: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod downward-api-81cfedbc-a352-400b-929f-fc9067f6d4cc container dapi-container: <nil>
STEP: delete the pod
Jul 13 13:11:23.598: INFO: Waiting for pod downward-api-81cfedbc-a352-400b-929f-fc9067f6d4cc to disappear
Jul 13 13:11:23.602: INFO: Pod downward-api-81cfedbc-a352-400b-929f-fc9067f6d4cc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:11:23.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7181" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":280,"completed":120,"skipped":2094,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:11:23.616: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-48fc7af9-242a-416a-9326-0f9745f0ab10
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:11:25.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-637" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":121,"skipped":2107,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:11:25.743: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:11:25.781: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:11:26.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4808" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":280,"completed":122,"skipped":2120,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:11:26.351: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jul 13 13:11:26.396: INFO: Waiting up to 5m0s for pod "downward-api-c13772f4-9c93-47ee-91e2-62f358e3e29b" in namespace "downward-api-8248" to be "success or failure"
Jul 13 13:11:26.410: INFO: Pod "downward-api-c13772f4-9c93-47ee-91e2-62f358e3e29b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.607728ms
Jul 13 13:11:28.415: INFO: Pod "downward-api-c13772f4-9c93-47ee-91e2-62f358e3e29b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018806594s
STEP: Saw pod success
Jul 13 13:11:28.415: INFO: Pod "downward-api-c13772f4-9c93-47ee-91e2-62f358e3e29b" satisfied condition "success or failure"
Jul 13 13:11:28.419: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod downward-api-c13772f4-9c93-47ee-91e2-62f358e3e29b container dapi-container: <nil>
STEP: delete the pod
Jul 13 13:11:28.469: INFO: Waiting for pod downward-api-c13772f4-9c93-47ee-91e2-62f358e3e29b to disappear
Jul 13 13:11:28.474: INFO: Pod downward-api-c13772f4-9c93-47ee-91e2-62f358e3e29b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:11:28.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8248" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":280,"completed":123,"skipped":2130,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:11:28.487: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 13 13:11:28.534: INFO: Waiting up to 5m0s for pod "pod-29de0026-1181-49f8-9c03-8c130ac05f49" in namespace "emptydir-3351" to be "success or failure"
Jul 13 13:11:28.543: INFO: Pod "pod-29de0026-1181-49f8-9c03-8c130ac05f49": Phase="Pending", Reason="", readiness=false. Elapsed: 9.379502ms
Jul 13 13:11:30.548: INFO: Pod "pod-29de0026-1181-49f8-9c03-8c130ac05f49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014316852s
Jul 13 13:11:32.556: INFO: Pod "pod-29de0026-1181-49f8-9c03-8c130ac05f49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022316207s
STEP: Saw pod success
Jul 13 13:11:32.556: INFO: Pod "pod-29de0026-1181-49f8-9c03-8c130ac05f49" satisfied condition "success or failure"
Jul 13 13:11:32.561: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-29de0026-1181-49f8-9c03-8c130ac05f49 container test-container: <nil>
STEP: delete the pod
Jul 13 13:11:32.594: INFO: Waiting for pod pod-29de0026-1181-49f8-9c03-8c130ac05f49 to disappear
Jul 13 13:11:32.598: INFO: Pod pod-29de0026-1181-49f8-9c03-8c130ac05f49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:11:32.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3351" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":124,"skipped":2130,"failed":0}
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:11:32.614: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:12:32.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6076" for this suite.

â€¢ [SLOW TEST:60.083 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":280,"completed":125,"skipped":2133,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:12:32.699: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-08576cf8-df25-4a98-825f-30d9f488946b
STEP: Creating a pod to test consume configMaps
Jul 13 13:12:32.803: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d97bc286-a8c3-4d68-be64-cbd5f4ca99d4" in namespace "projected-2762" to be "success or failure"
Jul 13 13:12:32.809: INFO: Pod "pod-projected-configmaps-d97bc286-a8c3-4d68-be64-cbd5f4ca99d4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.8364ms
Jul 13 13:12:34.814: INFO: Pod "pod-projected-configmaps-d97bc286-a8c3-4d68-be64-cbd5f4ca99d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010955318s
STEP: Saw pod success
Jul 13 13:12:34.814: INFO: Pod "pod-projected-configmaps-d97bc286-a8c3-4d68-be64-cbd5f4ca99d4" satisfied condition "success or failure"
Jul 13 13:12:34.818: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-projected-configmaps-d97bc286-a8c3-4d68-be64-cbd5f4ca99d4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 13:12:34.846: INFO: Waiting for pod pod-projected-configmaps-d97bc286-a8c3-4d68-be64-cbd5f4ca99d4 to disappear
Jul 13 13:12:34.850: INFO: Pod pod-projected-configmaps-d97bc286-a8c3-4d68-be64-cbd5f4ca99d4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:12:34.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2762" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":126,"skipped":2161,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:12:34.868: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-4386/configmap-test-eb3e3dea-8e62-42da-8cb6-ceb690231855
STEP: Creating a pod to test consume configMaps
Jul 13 13:12:34.920: INFO: Waiting up to 5m0s for pod "pod-configmaps-d271f75d-89f1-4632-b9e4-dda871bf287b" in namespace "configmap-4386" to be "success or failure"
Jul 13 13:12:34.926: INFO: Pod "pod-configmaps-d271f75d-89f1-4632-b9e4-dda871bf287b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.404242ms
Jul 13 13:12:36.934: INFO: Pod "pod-configmaps-d271f75d-89f1-4632-b9e4-dda871bf287b": Phase="Running", Reason="", readiness=true. Elapsed: 2.013889334s
Jul 13 13:12:38.939: INFO: Pod "pod-configmaps-d271f75d-89f1-4632-b9e4-dda871bf287b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018785474s
STEP: Saw pod success
Jul 13 13:12:38.939: INFO: Pod "pod-configmaps-d271f75d-89f1-4632-b9e4-dda871bf287b" satisfied condition "success or failure"
Jul 13 13:12:38.943: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-configmaps-d271f75d-89f1-4632-b9e4-dda871bf287b container env-test: <nil>
STEP: delete the pod
Jul 13 13:12:38.975: INFO: Waiting for pod pod-configmaps-d271f75d-89f1-4632-b9e4-dda871bf287b to disappear
Jul 13 13:12:38.979: INFO: Pod pod-configmaps-d271f75d-89f1-4632-b9e4-dda871bf287b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:12:38.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4386" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":280,"completed":127,"skipped":2164,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:12:38.994: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 13 13:12:39.401: INFO: Pod name wrapped-volume-race-c567ab3f-facc-42fe-9b38-9fa03b7eed61: Found 0 pods out of 5
Jul 13 13:12:44.411: INFO: Pod name wrapped-volume-race-c567ab3f-facc-42fe-9b38-9fa03b7eed61: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c567ab3f-facc-42fe-9b38-9fa03b7eed61 in namespace emptydir-wrapper-6834, will wait for the garbage collector to delete the pods
Jul 13 13:12:54.510: INFO: Deleting ReplicationController wrapped-volume-race-c567ab3f-facc-42fe-9b38-9fa03b7eed61 took: 17.616161ms
Jul 13 13:12:55.310: INFO: Terminating ReplicationController wrapped-volume-race-c567ab3f-facc-42fe-9b38-9fa03b7eed61 pods took: 800.141065ms
STEP: Creating RC which spawns configmap-volume pods
Jul 13 13:13:01.738: INFO: Pod name wrapped-volume-race-48e7ad77-ab14-4489-9ffd-dc83df39ad04: Found 0 pods out of 5
Jul 13 13:13:06.746: INFO: Pod name wrapped-volume-race-48e7ad77-ab14-4489-9ffd-dc83df39ad04: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-48e7ad77-ab14-4489-9ffd-dc83df39ad04 in namespace emptydir-wrapper-6834, will wait for the garbage collector to delete the pods
Jul 13 13:13:16.838: INFO: Deleting ReplicationController wrapped-volume-race-48e7ad77-ab14-4489-9ffd-dc83df39ad04 took: 10.490677ms
Jul 13 13:13:16.938: INFO: Terminating ReplicationController wrapped-volume-race-48e7ad77-ab14-4489-9ffd-dc83df39ad04 pods took: 100.265529ms
STEP: Creating RC which spawns configmap-volume pods
Jul 13 13:13:23.561: INFO: Pod name wrapped-volume-race-95c7ff2c-44bc-48c4-989f-927b583c2f29: Found 0 pods out of 5
Jul 13 13:13:28.570: INFO: Pod name wrapped-volume-race-95c7ff2c-44bc-48c4-989f-927b583c2f29: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-95c7ff2c-44bc-48c4-989f-927b583c2f29 in namespace emptydir-wrapper-6834, will wait for the garbage collector to delete the pods
Jul 13 13:13:38.663: INFO: Deleting ReplicationController wrapped-volume-race-95c7ff2c-44bc-48c4-989f-927b583c2f29 took: 12.689854ms
Jul 13 13:13:39.463: INFO: Terminating ReplicationController wrapped-volume-race-95c7ff2c-44bc-48c4-989f-927b583c2f29 pods took: 800.26935ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:13:50.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6834" for this suite.

â€¢ [SLOW TEST:71.540 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":280,"completed":128,"skipped":2179,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:13:50.534: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 13 13:13:53.117: INFO: Successfully updated pod "pod-update-activedeadlineseconds-10cbc2e1-e472-4a22-8f15-afb3664752b3"
Jul 13 13:13:53.117: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-10cbc2e1-e472-4a22-8f15-afb3664752b3" in namespace "pods-4940" to be "terminated due to deadline exceeded"
Jul 13 13:13:53.121: INFO: Pod "pod-update-activedeadlineseconds-10cbc2e1-e472-4a22-8f15-afb3664752b3": Phase="Running", Reason="", readiness=true. Elapsed: 3.874947ms
Jul 13 13:13:55.126: INFO: Pod "pod-update-activedeadlineseconds-10cbc2e1-e472-4a22-8f15-afb3664752b3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008656991s
Jul 13 13:13:57.131: INFO: Pod "pod-update-activedeadlineseconds-10cbc2e1-e472-4a22-8f15-afb3664752b3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.014351402s
Jul 13 13:13:57.132: INFO: Pod "pod-update-activedeadlineseconds-10cbc2e1-e472-4a22-8f15-afb3664752b3" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:13:57.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4940" for this suite.

â€¢ [SLOW TEST:6.612 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":280,"completed":129,"skipped":2182,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:13:57.147: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 13:13:57.212: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3721fd09-549c-4741-a90b-650a3a1ea595" in namespace "downward-api-9843" to be "success or failure"
Jul 13 13:13:57.218: INFO: Pod "downwardapi-volume-3721fd09-549c-4741-a90b-650a3a1ea595": Phase="Pending", Reason="", readiness=false. Elapsed: 5.736706ms
Jul 13 13:13:59.224: INFO: Pod "downwardapi-volume-3721fd09-549c-4741-a90b-650a3a1ea595": Phase="Running", Reason="", readiness=true. Elapsed: 2.011472499s
Jul 13 13:14:01.229: INFO: Pod "downwardapi-volume-3721fd09-549c-4741-a90b-650a3a1ea595": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016562652s
STEP: Saw pod success
Jul 13 13:14:01.229: INFO: Pod "downwardapi-volume-3721fd09-549c-4741-a90b-650a3a1ea595" satisfied condition "success or failure"
Jul 13 13:14:01.233: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod downwardapi-volume-3721fd09-549c-4741-a90b-650a3a1ea595 container client-container: <nil>
STEP: delete the pod
Jul 13 13:14:01.290: INFO: Waiting for pod downwardapi-volume-3721fd09-549c-4741-a90b-650a3a1ea595 to disappear
Jul 13 13:14:01.295: INFO: Pod downwardapi-volume-3721fd09-549c-4741-a90b-650a3a1ea595 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:14:01.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9843" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":130,"skipped":2224,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:14:01.309: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-s46d
STEP: Creating a pod to test atomic-volume-subpath
Jul 13 13:14:01.373: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-s46d" in namespace "subpath-8264" to be "success or failure"
Jul 13 13:14:01.387: INFO: Pod "pod-subpath-test-secret-s46d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.036819ms
Jul 13 13:14:03.394: INFO: Pod "pod-subpath-test-secret-s46d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021327568s
Jul 13 13:14:05.399: INFO: Pod "pod-subpath-test-secret-s46d": Phase="Running", Reason="", readiness=true. Elapsed: 4.026509034s
Jul 13 13:14:07.405: INFO: Pod "pod-subpath-test-secret-s46d": Phase="Running", Reason="", readiness=true. Elapsed: 6.031550952s
Jul 13 13:14:09.410: INFO: Pod "pod-subpath-test-secret-s46d": Phase="Running", Reason="", readiness=true. Elapsed: 8.036797269s
Jul 13 13:14:11.415: INFO: Pod "pod-subpath-test-secret-s46d": Phase="Running", Reason="", readiness=true. Elapsed: 10.042083584s
Jul 13 13:14:13.420: INFO: Pod "pod-subpath-test-secret-s46d": Phase="Running", Reason="", readiness=true. Elapsed: 12.046635004s
Jul 13 13:14:15.425: INFO: Pod "pod-subpath-test-secret-s46d": Phase="Running", Reason="", readiness=true. Elapsed: 14.051627951s
Jul 13 13:14:17.430: INFO: Pod "pod-subpath-test-secret-s46d": Phase="Running", Reason="", readiness=true. Elapsed: 16.056649573s
Jul 13 13:14:19.434: INFO: Pod "pod-subpath-test-secret-s46d": Phase="Running", Reason="", readiness=true. Elapsed: 18.061425585s
Jul 13 13:14:21.439: INFO: Pod "pod-subpath-test-secret-s46d": Phase="Running", Reason="", readiness=true. Elapsed: 20.066110888s
Jul 13 13:14:23.444: INFO: Pod "pod-subpath-test-secret-s46d": Phase="Running", Reason="", readiness=true. Elapsed: 22.070862708s
Jul 13 13:14:25.450: INFO: Pod "pod-subpath-test-secret-s46d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.077368881s
STEP: Saw pod success
Jul 13 13:14:25.450: INFO: Pod "pod-subpath-test-secret-s46d" satisfied condition "success or failure"
Jul 13 13:14:25.458: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-subpath-test-secret-s46d container test-container-subpath-secret-s46d: <nil>
STEP: delete the pod
Jul 13 13:14:25.486: INFO: Waiting for pod pod-subpath-test-secret-s46d to disappear
Jul 13 13:14:25.491: INFO: Pod pod-subpath-test-secret-s46d no longer exists
STEP: Deleting pod pod-subpath-test-secret-s46d
Jul 13 13:14:25.491: INFO: Deleting pod "pod-subpath-test-secret-s46d" in namespace "subpath-8264"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:14:25.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8264" for this suite.

â€¢ [SLOW TEST:24.199 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":280,"completed":131,"skipped":2240,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:14:25.509: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-7a0a3755-a969-4002-8185-533b4bdaeebf
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:14:25.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9515" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":280,"completed":132,"skipped":2255,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:14:25.570: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 13 13:14:25.671: INFO: Number of nodes with available pods: 0
Jul 13 13:14:25.671: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:14:26.693: INFO: Number of nodes with available pods: 0
Jul 13 13:14:26.693: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:14:27.687: INFO: Number of nodes with available pods: 4
Jul 13 13:14:27.687: INFO: Node ip-10-0-3-16.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:14:28.684: INFO: Number of nodes with available pods: 5
Jul 13 13:14:28.684: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 13 13:14:28.711: INFO: Number of nodes with available pods: 4
Jul 13 13:14:28.711: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:14:29.724: INFO: Number of nodes with available pods: 4
Jul 13 13:14:29.725: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:14:30.729: INFO: Number of nodes with available pods: 4
Jul 13 13:14:30.729: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:14:31.725: INFO: Number of nodes with available pods: 4
Jul 13 13:14:31.725: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:14:32.723: INFO: Number of nodes with available pods: 4
Jul 13 13:14:32.723: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:14:33.728: INFO: Number of nodes with available pods: 4
Jul 13 13:14:33.728: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:14:34.724: INFO: Number of nodes with available pods: 4
Jul 13 13:14:34.724: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:14:35.728: INFO: Number of nodes with available pods: 4
Jul 13 13:14:35.728: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:14:36.725: INFO: Number of nodes with available pods: 4
Jul 13 13:14:36.725: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:14:37.724: INFO: Number of nodes with available pods: 4
Jul 13 13:14:37.724: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:14:38.723: INFO: Number of nodes with available pods: 4
Jul 13 13:14:38.723: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:14:39.724: INFO: Number of nodes with available pods: 4
Jul 13 13:14:39.724: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:14:40.723: INFO: Number of nodes with available pods: 5
Jul 13 13:14:40.724: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7099, will wait for the garbage collector to delete the pods
Jul 13 13:14:40.794: INFO: Deleting DaemonSet.extensions daemon-set took: 10.968121ms
Jul 13 13:14:41.594: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.300554ms
Jul 13 13:14:49.999: INFO: Number of nodes with available pods: 0
Jul 13 13:14:49.999: INFO: Number of running nodes: 0, number of available pods: 0
Jul 13 13:14:50.003: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7099/daemonsets","resourceVersion":"232198"},"items":null}

Jul 13 13:14:50.007: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7099/pods","resourceVersion":"232198"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:14:50.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7099" for this suite.

â€¢ [SLOW TEST:24.494 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":280,"completed":133,"skipped":2276,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:14:50.065: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 13 13:14:52.161: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:14:52.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5900" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":134,"skipped":2280,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:14:52.197: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-8444
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8444
STEP: Deleting pre-stop pod
Jul 13 13:15:01.303: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:15:01.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8444" for this suite.

â€¢ [SLOW TEST:9.134 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":280,"completed":135,"skipped":2289,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:15:01.332: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jul 13 13:15:01.376: INFO: namespace kubectl-2463
Jul 13 13:15:01.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 create -f - --namespace=kubectl-2463'
Jul 13 13:15:01.703: INFO: stderr: ""
Jul 13 13:15:01.703: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jul 13 13:15:02.711: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 13 13:15:02.711: INFO: Found 0 / 1
Jul 13 13:15:03.713: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 13 13:15:03.713: INFO: Found 1 / 1
Jul 13 13:15:03.713: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 13 13:15:03.719: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 13 13:15:03.719: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 13 13:15:03.719: INFO: wait on agnhost-master startup in kubectl-2463 
Jul 13 13:15:03.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 logs agnhost-master-dzxnz agnhost-master --namespace=kubectl-2463'
Jul 13 13:15:03.872: INFO: stderr: ""
Jul 13 13:15:03.872: INFO: stdout: "Paused\n"
STEP: exposing RC
Jul 13 13:15:03.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-2463'
Jul 13 13:15:04.034: INFO: stderr: ""
Jul 13 13:15:04.034: INFO: stdout: "service/rm2 exposed\n"
Jul 13 13:15:04.039: INFO: Service rm2 in namespace kubectl-2463 found.
STEP: exposing service
Jul 13 13:15:06.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-2463'
Jul 13 13:15:06.154: INFO: stderr: ""
Jul 13 13:15:06.155: INFO: stdout: "service/rm3 exposed\n"
Jul 13 13:15:06.162: INFO: Service rm3 in namespace kubectl-2463 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:15:08.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2463" for this suite.

â€¢ [SLOW TEST:6.854 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1188
    should create services for rc  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":280,"completed":136,"skipped":2292,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:15:08.186: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1525
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 13 13:15:08.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-8920'
Jul 13 13:15:08.370: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 13 13:15:08.370: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Jul 13 13:15:08.405: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-g48bd]
Jul 13 13:15:08.405: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-g48bd" in namespace "kubectl-8920" to be "running and ready"
Jul 13 13:15:08.409: INFO: Pod "e2e-test-httpd-rc-g48bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.126326ms
Jul 13 13:15:10.415: INFO: Pod "e2e-test-httpd-rc-g48bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.009574135s
Jul 13 13:15:10.415: INFO: Pod "e2e-test-httpd-rc-g48bd" satisfied condition "running and ready"
Jul 13 13:15:10.415: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-g48bd]
Jul 13 13:15:10.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 logs rc/e2e-test-httpd-rc --namespace=kubectl-8920'
Jul 13 13:15:10.524: INFO: stderr: ""
Jul 13 13:15:10.524: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.20.234.24. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.20.234.24. Set the 'ServerName' directive globally to suppress this message\n[Mon Jul 13 13:15:09.872372 2020] [mpm_event:notice] [pid 1:tid 140032373676904] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Mon Jul 13 13:15:09.872421 2020] [core:notice] [pid 1:tid 140032373676904] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1530
Jul 13 13:15:10.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete rc e2e-test-httpd-rc --namespace=kubectl-8920'
Jul 13 13:15:10.611: INFO: stderr: ""
Jul 13 13:15:10.611: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:15:10.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8920" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":280,"completed":137,"skipped":2297,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:15:10.625: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2763
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2763
I0713 13:15:10.718321      20 runners.go:189] Created replication controller with name: externalname-service, namespace: services-2763, replica count: 2
Jul 13 13:15:13.768: INFO: Creating new exec pod
I0713 13:15:13.768818      20 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 13 13:15:16.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-2763 execpodsffjn -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jul 13 13:15:17.035: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 13 13:15:17.035: INFO: stdout: ""
Jul 13 13:15:17.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-2763 execpodsffjn -- /bin/sh -x -c nc -zv -t -w 2 10.21.106.11 80'
Jul 13 13:15:17.242: INFO: stderr: "+ nc -zv -t -w 2 10.21.106.11 80\nConnection to 10.21.106.11 80 port [tcp/http] succeeded!\n"
Jul 13 13:15:17.242: INFO: stdout: ""
Jul 13 13:15:17.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-2763 execpodsffjn -- /bin/sh -x -c nc -zv -t -w 2 10.0.2.22 31139'
Jul 13 13:15:17.451: INFO: stderr: "+ nc -zv -t -w 2 10.0.2.22 31139\nConnection to 10.0.2.22 31139 port [tcp/31139] succeeded!\n"
Jul 13 13:15:17.451: INFO: stdout: ""
Jul 13 13:15:17.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-2763 execpodsffjn -- /bin/sh -x -c nc -zv -t -w 2 10.0.3.16 31139'
Jul 13 13:15:17.679: INFO: stderr: "+ nc -zv -t -w 2 10.0.3.16 31139\nConnection to 10.0.3.16 31139 port [tcp/31139] succeeded!\n"
Jul 13 13:15:17.679: INFO: stdout: ""
Jul 13 13:15:17.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-2763 execpodsffjn -- /bin/sh -x -c nc -zv -t -w 2 18.191.151.100 31139'
Jul 13 13:15:17.882: INFO: stderr: "+ nc -zv -t -w 2 18.191.151.100 31139\nConnection to 18.191.151.100 31139 port [tcp/31139] succeeded!\n"
Jul 13 13:15:17.882: INFO: stdout: ""
Jul 13 13:15:17.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-2763 execpodsffjn -- /bin/sh -x -c nc -zv -t -w 2 18.219.59.245 31139'
Jul 13 13:15:18.099: INFO: stderr: "+ nc -zv -t -w 2 18.219.59.245 31139\nConnection to 18.219.59.245 31139 port [tcp/31139] succeeded!\n"
Jul 13 13:15:18.099: INFO: stdout: ""
Jul 13 13:15:18.099: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:15:18.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2763" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:7.544 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":280,"completed":138,"skipped":2310,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:15:18.169: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jul 13 13:15:18.219: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Jul 13 13:15:18.569: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 13 13:15:20.658: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242918, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242918, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242918, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730242918, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 13 13:15:26.129: INFO: Waited 3.448902334s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:15:26.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2833" for this suite.

â€¢ [SLOW TEST:8.882 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":280,"completed":139,"skipped":2318,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:15:27.052: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 13 13:15:29.195: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:15:29.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6111" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":140,"skipped":2328,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:15:29.238: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-09f2215a-923b-407d-927c-096baa8f867a
STEP: Creating a pod to test consume configMaps
Jul 13 13:15:29.290: INFO: Waiting up to 5m0s for pod "pod-configmaps-6c275f96-809b-4dcd-8c07-c514a9eeaee4" in namespace "configmap-4995" to be "success or failure"
Jul 13 13:15:29.308: INFO: Pod "pod-configmaps-6c275f96-809b-4dcd-8c07-c514a9eeaee4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.673111ms
Jul 13 13:15:31.313: INFO: Pod "pod-configmaps-6c275f96-809b-4dcd-8c07-c514a9eeaee4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022731332s
STEP: Saw pod success
Jul 13 13:15:31.313: INFO: Pod "pod-configmaps-6c275f96-809b-4dcd-8c07-c514a9eeaee4" satisfied condition "success or failure"
Jul 13 13:15:31.317: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-configmaps-6c275f96-809b-4dcd-8c07-c514a9eeaee4 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 13:15:31.362: INFO: Waiting for pod pod-configmaps-6c275f96-809b-4dcd-8c07-c514a9eeaee4 to disappear
Jul 13 13:15:31.366: INFO: Pod pod-configmaps-6c275f96-809b-4dcd-8c07-c514a9eeaee4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:15:31.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4995" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":141,"skipped":2336,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:15:31.384: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-bb9b4793-e813-4407-8019-f20540378613
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:15:31.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2248" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":280,"completed":142,"skipped":2362,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:15:31.446: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 13:15:31.508: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1e1e77d-5235-450a-8318-b0c36c08109d" in namespace "downward-api-7813" to be "success or failure"
Jul 13 13:15:31.513: INFO: Pod "downwardapi-volume-f1e1e77d-5235-450a-8318-b0c36c08109d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.893923ms
Jul 13 13:15:33.521: INFO: Pod "downwardapi-volume-f1e1e77d-5235-450a-8318-b0c36c08109d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012388876s
STEP: Saw pod success
Jul 13 13:15:33.521: INFO: Pod "downwardapi-volume-f1e1e77d-5235-450a-8318-b0c36c08109d" satisfied condition "success or failure"
Jul 13 13:15:33.528: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod downwardapi-volume-f1e1e77d-5235-450a-8318-b0c36c08109d container client-container: <nil>
STEP: delete the pod
Jul 13 13:15:33.573: INFO: Waiting for pod downwardapi-volume-f1e1e77d-5235-450a-8318-b0c36c08109d to disappear
Jul 13 13:15:33.577: INFO: Pod downwardapi-volume-f1e1e77d-5235-450a-8318-b0c36c08109d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:15:33.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7813" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":143,"skipped":2388,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:15:33.596: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-403
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-403
STEP: creating replication controller externalsvc in namespace services-403
I0713 13:15:33.723501      20 runners.go:189] Created replication controller with name: externalsvc, namespace: services-403, replica count: 2
I0713 13:15:36.773922      20 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jul 13 13:15:36.815: INFO: Creating new exec pod
Jul 13 13:15:38.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 exec --namespace=services-403 execpodpf8l2 -- /bin/sh -x -c nslookup clusterip-service'
Jul 13 13:15:39.109: INFO: stderr: "+ nslookup clusterip-service\n"
Jul 13 13:15:39.109: INFO: stdout: "Server:\t\t10.21.0.10\nAddress:\t10.21.0.10#53\n\nclusterip-service.services-403.svc.cluster.local\tcanonical name = externalsvc.services-403.svc.cluster.local.\nName:\texternalsvc.services-403.svc.cluster.local\nAddress: 10.21.38.68\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-403, will wait for the garbage collector to delete the pods
Jul 13 13:15:39.173: INFO: Deleting ReplicationController externalsvc took: 9.192414ms
Jul 13 13:15:39.273: INFO: Terminating ReplicationController externalsvc pods took: 100.228353ms
Jul 13 13:15:43.800: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:15:43.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-403" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:10.238 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":280,"completed":144,"skipped":2388,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:15:43.834: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-eb094371-1961-40d1-ba39-b2e8c408f774
STEP: Creating a pod to test consume secrets
Jul 13 13:15:43.917: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1a8fa88e-78d7-48c1-83d8-51164c6b2657" in namespace "projected-846" to be "success or failure"
Jul 13 13:15:43.926: INFO: Pod "pod-projected-secrets-1a8fa88e-78d7-48c1-83d8-51164c6b2657": Phase="Pending", Reason="", readiness=false. Elapsed: 8.708597ms
Jul 13 13:15:45.934: INFO: Pod "pod-projected-secrets-1a8fa88e-78d7-48c1-83d8-51164c6b2657": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017273948s
STEP: Saw pod success
Jul 13 13:15:45.935: INFO: Pod "pod-projected-secrets-1a8fa88e-78d7-48c1-83d8-51164c6b2657" satisfied condition "success or failure"
Jul 13 13:15:45.943: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-projected-secrets-1a8fa88e-78d7-48c1-83d8-51164c6b2657 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 13 13:15:45.970: INFO: Waiting for pod pod-projected-secrets-1a8fa88e-78d7-48c1-83d8-51164c6b2657 to disappear
Jul 13 13:15:45.974: INFO: Pod pod-projected-secrets-1a8fa88e-78d7-48c1-83d8-51164c6b2657 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:15:45.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-846" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":145,"skipped":2430,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:15:45.989: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-afe52a94-ef62-41a9-a58a-6cea4db6195f
Jul 13 13:15:46.043: INFO: Pod name my-hostname-basic-afe52a94-ef62-41a9-a58a-6cea4db6195f: Found 0 pods out of 1
Jul 13 13:15:51.050: INFO: Pod name my-hostname-basic-afe52a94-ef62-41a9-a58a-6cea4db6195f: Found 1 pods out of 1
Jul 13 13:15:51.050: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-afe52a94-ef62-41a9-a58a-6cea4db6195f" are running
Jul 13 13:15:51.055: INFO: Pod "my-hostname-basic-afe52a94-ef62-41a9-a58a-6cea4db6195f-dqv9q" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-13 13:15:46 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-13 13:15:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-13 13:15:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-13 13:15:46 +0000 UTC Reason: Message:}])
Jul 13 13:15:51.055: INFO: Trying to dial the pod
Jul 13 13:15:56.077: INFO: Controller my-hostname-basic-afe52a94-ef62-41a9-a58a-6cea4db6195f: Got expected result from replica 1 [my-hostname-basic-afe52a94-ef62-41a9-a58a-6cea4db6195f-dqv9q]: "my-hostname-basic-afe52a94-ef62-41a9-a58a-6cea4db6195f-dqv9q", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:15:56.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-65" for this suite.

â€¢ [SLOW TEST:10.102 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":146,"skipped":2451,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:15:56.094: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-j4h4
STEP: Creating a pod to test atomic-volume-subpath
Jul 13 13:15:56.195: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-j4h4" in namespace "subpath-2674" to be "success or failure"
Jul 13 13:15:56.200: INFO: Pod "pod-subpath-test-configmap-j4h4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.255078ms
Jul 13 13:15:58.206: INFO: Pod "pod-subpath-test-configmap-j4h4": Phase="Running", Reason="", readiness=true. Elapsed: 2.01126755s
Jul 13 13:16:00.213: INFO: Pod "pod-subpath-test-configmap-j4h4": Phase="Running", Reason="", readiness=true. Elapsed: 4.018030435s
Jul 13 13:16:02.222: INFO: Pod "pod-subpath-test-configmap-j4h4": Phase="Running", Reason="", readiness=true. Elapsed: 6.026518443s
Jul 13 13:16:04.227: INFO: Pod "pod-subpath-test-configmap-j4h4": Phase="Running", Reason="", readiness=true. Elapsed: 8.031984077s
Jul 13 13:16:06.232: INFO: Pod "pod-subpath-test-configmap-j4h4": Phase="Running", Reason="", readiness=true. Elapsed: 10.036911835s
Jul 13 13:16:08.237: INFO: Pod "pod-subpath-test-configmap-j4h4": Phase="Running", Reason="", readiness=true. Elapsed: 12.042120097s
Jul 13 13:16:10.242: INFO: Pod "pod-subpath-test-configmap-j4h4": Phase="Running", Reason="", readiness=true. Elapsed: 14.047038858s
Jul 13 13:16:12.248: INFO: Pod "pod-subpath-test-configmap-j4h4": Phase="Running", Reason="", readiness=true. Elapsed: 16.052557797s
Jul 13 13:16:14.253: INFO: Pod "pod-subpath-test-configmap-j4h4": Phase="Running", Reason="", readiness=true. Elapsed: 18.057771476s
Jul 13 13:16:16.259: INFO: Pod "pod-subpath-test-configmap-j4h4": Phase="Running", Reason="", readiness=true. Elapsed: 20.063326703s
Jul 13 13:16:18.264: INFO: Pod "pod-subpath-test-configmap-j4h4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.068735374s
STEP: Saw pod success
Jul 13 13:16:18.264: INFO: Pod "pod-subpath-test-configmap-j4h4" satisfied condition "success or failure"
Jul 13 13:16:18.268: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-subpath-test-configmap-j4h4 container test-container-subpath-configmap-j4h4: <nil>
STEP: delete the pod
Jul 13 13:16:18.307: INFO: Waiting for pod pod-subpath-test-configmap-j4h4 to disappear
Jul 13 13:16:18.316: INFO: Pod pod-subpath-test-configmap-j4h4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-j4h4
Jul 13 13:16:18.316: INFO: Deleting pod "pod-subpath-test-configmap-j4h4" in namespace "subpath-2674"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:16:18.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2674" for this suite.

â€¢ [SLOW TEST:22.240 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":280,"completed":147,"skipped":2519,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:16:18.335: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:16:20.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5399" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":280,"completed":148,"skipped":2573,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:16:20.428: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3844.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3844.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 13 13:16:24.548: INFO: DNS probes using dns-3844/dns-test-651243e9-6489-4abd-b20b-a73ec69588fb succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:16:24.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3844" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":280,"completed":149,"skipped":2576,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:16:24.585: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-930/configmap-test-b82333c3-7307-4325-ace5-5c0bb546a500
STEP: Creating a pod to test consume configMaps
Jul 13 13:16:24.640: INFO: Waiting up to 5m0s for pod "pod-configmaps-a5d2d045-53ce-4a2b-86e9-3897e478d3e6" in namespace "configmap-930" to be "success or failure"
Jul 13 13:16:24.645: INFO: Pod "pod-configmaps-a5d2d045-53ce-4a2b-86e9-3897e478d3e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.826148ms
Jul 13 13:16:26.650: INFO: Pod "pod-configmaps-a5d2d045-53ce-4a2b-86e9-3897e478d3e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009798284s
STEP: Saw pod success
Jul 13 13:16:26.650: INFO: Pod "pod-configmaps-a5d2d045-53ce-4a2b-86e9-3897e478d3e6" satisfied condition "success or failure"
Jul 13 13:16:26.654: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-configmaps-a5d2d045-53ce-4a2b-86e9-3897e478d3e6 container env-test: <nil>
STEP: delete the pod
Jul 13 13:16:26.680: INFO: Waiting for pod pod-configmaps-a5d2d045-53ce-4a2b-86e9-3897e478d3e6 to disappear
Jul 13 13:16:26.684: INFO: Pod pod-configmaps-a5d2d045-53ce-4a2b-86e9-3897e478d3e6 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:16:26.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-930" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":150,"skipped":2577,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:16:26.703: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Jul 13 13:16:26.742: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Jul 13 13:16:26.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 create -f - --namespace=kubectl-2426'
Jul 13 13:16:27.236: INFO: stderr: ""
Jul 13 13:16:27.236: INFO: stdout: "service/agnhost-slave created\n"
Jul 13 13:16:27.236: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Jul 13 13:16:27.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 create -f - --namespace=kubectl-2426'
Jul 13 13:16:27.539: INFO: stderr: ""
Jul 13 13:16:27.539: INFO: stdout: "service/agnhost-master created\n"
Jul 13 13:16:27.539: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 13 13:16:27.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 create -f - --namespace=kubectl-2426'
Jul 13 13:16:28.027: INFO: stderr: ""
Jul 13 13:16:28.027: INFO: stdout: "service/frontend created\n"
Jul 13 13:16:28.028: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jul 13 13:16:28.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 create -f - --namespace=kubectl-2426'
Jul 13 13:16:28.368: INFO: stderr: ""
Jul 13 13:16:28.368: INFO: stdout: "deployment.apps/frontend created\n"
Jul 13 13:16:28.368: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 13 13:16:28.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 create -f - --namespace=kubectl-2426'
Jul 13 13:16:28.636: INFO: stderr: ""
Jul 13 13:16:28.636: INFO: stdout: "deployment.apps/agnhost-master created\n"
Jul 13 13:16:28.636: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 13 13:16:28.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 create -f - --namespace=kubectl-2426'
Jul 13 13:16:29.092: INFO: stderr: ""
Jul 13 13:16:29.094: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Jul 13 13:16:29.094: INFO: Waiting for all frontend pods to be Running.
Jul 13 13:16:34.144: INFO: Waiting for frontend to serve content.
Jul 13 13:16:34.156: INFO: Trying to add a new entry to the guestbook.
Jul 13 13:16:34.169: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul 13 13:16:39.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete --grace-period=0 --force -f - --namespace=kubectl-2426'
Jul 13 13:16:39.330: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 13:16:39.330: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul 13 13:16:39.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete --grace-period=0 --force -f - --namespace=kubectl-2426'
Jul 13 13:16:39.451: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 13:16:39.451: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 13 13:16:39.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete --grace-period=0 --force -f - --namespace=kubectl-2426'
Jul 13 13:16:39.574: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 13:16:39.574: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 13 13:16:39.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete --grace-period=0 --force -f - --namespace=kubectl-2426'
Jul 13 13:16:39.675: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 13:16:39.675: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 13 13:16:39.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete --grace-period=0 --force -f - --namespace=kubectl-2426'
Jul 13 13:16:39.794: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 13:16:39.794: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 13 13:16:39.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete --grace-period=0 --force -f - --namespace=kubectl-2426'
Jul 13 13:16:39.883: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 13:16:39.883: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:16:39.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2426" for this suite.

â€¢ [SLOW TEST:13.195 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:380
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":280,"completed":151,"skipped":2581,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:16:39.898: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1790
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 13 13:16:39.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6494'
Jul 13 13:16:40.023: INFO: stderr: ""
Jul 13 13:16:40.023: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jul 13 13:16:45.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pod e2e-test-httpd-pod --namespace=kubectl-6494 -o json'
Jul 13 13:16:45.216: INFO: stderr: ""
Jul 13 13:16:45.216: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.20.23.164/32\"\n        },\n        \"creationTimestamp\": \"2020-07-13T13:16:40Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6494\",\n        \"resourceVersion\": \"233621\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6494/pods/e2e-test-httpd-pod\",\n        \"uid\": \"4e27a9f2-f9e9-42d3-a9f8-f1012b007361\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-8zghr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-3-156.us-east-2.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-8zghr\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-8zghr\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-13T13:16:40Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-13T13:16:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-13T13:16:41Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-13T13:16:40Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://9ca1d920289dd9fdbd609b5b30b731c92a3d7f1aa105710d2a29f1b1923c10c7\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-07-13T13:16:41Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.3.156\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.20.23.164\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.20.23.164\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-07-13T13:16:40Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 13 13:16:45.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 replace -f - --namespace=kubectl-6494'
Jul 13 13:16:45.537: INFO: stderr: ""
Jul 13 13:16:45.537: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1795
Jul 13 13:16:45.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete pods e2e-test-httpd-pod --namespace=kubectl-6494'
Jul 13 13:16:51.010: INFO: stderr: ""
Jul 13 13:16:51.010: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:16:51.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6494" for this suite.

â€¢ [SLOW TEST:11.129 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1786
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":280,"completed":152,"skipped":2584,"failed":0}
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:16:51.027: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Jul 13 13:16:51.161: INFO: Waiting up to 5m0s for pod "client-containers-4b0f5ff2-a8b6-4fd4-ae41-da3a400691db" in namespace "containers-6582" to be "success or failure"
Jul 13 13:16:51.168: INFO: Pod "client-containers-4b0f5ff2-a8b6-4fd4-ae41-da3a400691db": Phase="Pending", Reason="", readiness=false. Elapsed: 7.165679ms
Jul 13 13:16:53.173: INFO: Pod "client-containers-4b0f5ff2-a8b6-4fd4-ae41-da3a400691db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012183887s
Jul 13 13:16:55.178: INFO: Pod "client-containers-4b0f5ff2-a8b6-4fd4-ae41-da3a400691db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017127008s
STEP: Saw pod success
Jul 13 13:16:55.178: INFO: Pod "client-containers-4b0f5ff2-a8b6-4fd4-ae41-da3a400691db" satisfied condition "success or failure"
Jul 13 13:16:55.182: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod client-containers-4b0f5ff2-a8b6-4fd4-ae41-da3a400691db container test-container: <nil>
STEP: delete the pod
Jul 13 13:16:55.209: INFO: Waiting for pod client-containers-4b0f5ff2-a8b6-4fd4-ae41-da3a400691db to disappear
Jul 13 13:16:55.212: INFO: Pod client-containers-4b0f5ff2-a8b6-4fd4-ae41-da3a400691db no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:16:55.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6582" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":280,"completed":153,"skipped":2584,"failed":0}

------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:16:55.228: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:16:55.297: INFO: (0) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 17.902381ms)
Jul 13 13:16:55.303: INFO: (1) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.744316ms)
Jul 13 13:16:55.308: INFO: (2) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.248602ms)
Jul 13 13:16:55.314: INFO: (3) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.53798ms)
Jul 13 13:16:55.319: INFO: (4) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.62998ms)
Jul 13 13:16:55.326: INFO: (5) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 6.657623ms)
Jul 13 13:16:55.332: INFO: (6) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.451097ms)
Jul 13 13:16:55.337: INFO: (7) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.376408ms)
Jul 13 13:16:55.346: INFO: (8) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 8.413336ms)
Jul 13 13:16:55.352: INFO: (9) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 6.407533ms)
Jul 13 13:16:55.358: INFO: (10) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.854155ms)
Jul 13 13:16:55.366: INFO: (11) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 7.350612ms)
Jul 13 13:16:55.375: INFO: (12) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 9.660676ms)
Jul 13 13:16:55.381: INFO: (13) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.344531ms)
Jul 13 13:16:55.386: INFO: (14) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.182591ms)
Jul 13 13:16:55.391: INFO: (15) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.345871ms)
Jul 13 13:16:55.397: INFO: (16) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.334159ms)
Jul 13 13:16:55.402: INFO: (17) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.430172ms)
Jul 13 13:16:55.410: INFO: (18) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 7.458521ms)
Jul 13 13:16:55.415: INFO: (19) /api/v1/nodes/ip-10-0-2-177.us-east-2.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.279169ms)
[AfterEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:16:55.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-875" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":280,"completed":154,"skipped":2584,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:16:55.429: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:17:03.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-844" for this suite.

â€¢ [SLOW TEST:8.065 seconds]
[sig-apps] Job
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":280,"completed":155,"skipped":2603,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:17:03.496: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-4653dd1b-6f53-448f-9f29-43ac13d95fc0
STEP: Creating a pod to test consume secrets
Jul 13 13:17:03.609: INFO: Waiting up to 5m0s for pod "pod-secrets-feddf0f4-5a9c-45cc-9cb8-2fd77cc87886" in namespace "secrets-2267" to be "success or failure"
Jul 13 13:17:03.618: INFO: Pod "pod-secrets-feddf0f4-5a9c-45cc-9cb8-2fd77cc87886": Phase="Pending", Reason="", readiness=false. Elapsed: 8.788294ms
Jul 13 13:17:05.625: INFO: Pod "pod-secrets-feddf0f4-5a9c-45cc-9cb8-2fd77cc87886": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015743537s
Jul 13 13:17:07.630: INFO: Pod "pod-secrets-feddf0f4-5a9c-45cc-9cb8-2fd77cc87886": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021355875s
STEP: Saw pod success
Jul 13 13:17:07.630: INFO: Pod "pod-secrets-feddf0f4-5a9c-45cc-9cb8-2fd77cc87886" satisfied condition "success or failure"
Jul 13 13:17:07.634: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-secrets-feddf0f4-5a9c-45cc-9cb8-2fd77cc87886 container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 13:17:07.671: INFO: Waiting for pod pod-secrets-feddf0f4-5a9c-45cc-9cb8-2fd77cc87886 to disappear
Jul 13 13:17:07.676: INFO: Pod pod-secrets-feddf0f4-5a9c-45cc-9cb8-2fd77cc87886 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:17:07.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2267" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":156,"skipped":2640,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:17:07.702: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-d4da631e-d57d-4de5-aa83-0dfe13facc4d
STEP: Creating a pod to test consume configMaps
Jul 13 13:17:07.781: INFO: Waiting up to 5m0s for pod "pod-configmaps-88cb3ffb-bda9-49ef-81af-7a8244849942" in namespace "configmap-5675" to be "success or failure"
Jul 13 13:17:07.788: INFO: Pod "pod-configmaps-88cb3ffb-bda9-49ef-81af-7a8244849942": Phase="Pending", Reason="", readiness=false. Elapsed: 7.355604ms
Jul 13 13:17:09.793: INFO: Pod "pod-configmaps-88cb3ffb-bda9-49ef-81af-7a8244849942": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012456857s
Jul 13 13:17:11.800: INFO: Pod "pod-configmaps-88cb3ffb-bda9-49ef-81af-7a8244849942": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018981854s
STEP: Saw pod success
Jul 13 13:17:11.800: INFO: Pod "pod-configmaps-88cb3ffb-bda9-49ef-81af-7a8244849942" satisfied condition "success or failure"
Jul 13 13:17:11.804: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-configmaps-88cb3ffb-bda9-49ef-81af-7a8244849942 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 13:17:11.837: INFO: Waiting for pod pod-configmaps-88cb3ffb-bda9-49ef-81af-7a8244849942 to disappear
Jul 13 13:17:11.841: INFO: Pod pod-configmaps-88cb3ffb-bda9-49ef-81af-7a8244849942 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:17:11.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5675" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":157,"skipped":2680,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:17:11.856: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-c7ee0000-69fa-4cc1-a9ca-18129d920f93
STEP: Creating configMap with name cm-test-opt-upd-ba251ec7-8b6a-4224-a962-817c24f0a93a
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c7ee0000-69fa-4cc1-a9ca-18129d920f93
STEP: Updating configmap cm-test-opt-upd-ba251ec7-8b6a-4224-a962-817c24f0a93a
STEP: Creating configMap with name cm-test-opt-create-22cf3fad-78ca-4e36-9ec5-b7cbb57bd5d1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:18:44.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8221" for this suite.

â€¢ [SLOW TEST:92.775 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":158,"skipped":2696,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:18:44.633: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-vgbq
STEP: Creating a pod to test atomic-volume-subpath
Jul 13 13:18:44.757: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-vgbq" in namespace "subpath-9728" to be "success or failure"
Jul 13 13:18:44.772: INFO: Pod "pod-subpath-test-projected-vgbq": Phase="Pending", Reason="", readiness=false. Elapsed: 15.436341ms
Jul 13 13:18:46.778: INFO: Pod "pod-subpath-test-projected-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 2.020537322s
Jul 13 13:18:48.783: INFO: Pod "pod-subpath-test-projected-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 4.025846518s
Jul 13 13:18:50.788: INFO: Pod "pod-subpath-test-projected-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 6.031272611s
Jul 13 13:18:52.794: INFO: Pod "pod-subpath-test-projected-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 8.036579336s
Jul 13 13:18:54.799: INFO: Pod "pod-subpath-test-projected-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 10.042364137s
Jul 13 13:18:56.805: INFO: Pod "pod-subpath-test-projected-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 12.047679689s
Jul 13 13:18:58.810: INFO: Pod "pod-subpath-test-projected-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 14.053004699s
Jul 13 13:19:00.815: INFO: Pod "pod-subpath-test-projected-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 16.058134215s
Jul 13 13:19:02.821: INFO: Pod "pod-subpath-test-projected-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 18.063660861s
Jul 13 13:19:04.829: INFO: Pod "pod-subpath-test-projected-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 20.071488739s
Jul 13 13:19:06.834: INFO: Pod "pod-subpath-test-projected-vgbq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.077396173s
STEP: Saw pod success
Jul 13 13:19:06.834: INFO: Pod "pod-subpath-test-projected-vgbq" satisfied condition "success or failure"
Jul 13 13:19:06.840: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-subpath-test-projected-vgbq container test-container-subpath-projected-vgbq: <nil>
STEP: delete the pod
Jul 13 13:19:06.884: INFO: Waiting for pod pod-subpath-test-projected-vgbq to disappear
Jul 13 13:19:06.889: INFO: Pod pod-subpath-test-projected-vgbq no longer exists
STEP: Deleting pod pod-subpath-test-projected-vgbq
Jul 13 13:19:06.889: INFO: Deleting pod "pod-subpath-test-projected-vgbq" in namespace "subpath-9728"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:19:06.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9728" for this suite.

â€¢ [SLOW TEST:22.273 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":280,"completed":159,"skipped":2771,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:19:06.906: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 13 13:19:06.962: INFO: Waiting up to 5m0s for pod "pod-f71090bc-bbe7-49eb-9f51-ca0dead5f563" in namespace "emptydir-6844" to be "success or failure"
Jul 13 13:19:06.972: INFO: Pod "pod-f71090bc-bbe7-49eb-9f51-ca0dead5f563": Phase="Pending", Reason="", readiness=false. Elapsed: 9.421708ms
Jul 13 13:19:08.977: INFO: Pod "pod-f71090bc-bbe7-49eb-9f51-ca0dead5f563": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014575732s
STEP: Saw pod success
Jul 13 13:19:08.977: INFO: Pod "pod-f71090bc-bbe7-49eb-9f51-ca0dead5f563" satisfied condition "success or failure"
Jul 13 13:19:08.981: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-f71090bc-bbe7-49eb-9f51-ca0dead5f563 container test-container: <nil>
STEP: delete the pod
Jul 13 13:19:09.015: INFO: Waiting for pod pod-f71090bc-bbe7-49eb-9f51-ca0dead5f563 to disappear
Jul 13 13:19:09.019: INFO: Pod pod-f71090bc-bbe7-49eb-9f51-ca0dead5f563 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:19:09.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6844" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":160,"skipped":2772,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:19:09.042: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 13 13:19:09.160: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 13 13:19:14.165: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:19:15.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9340" for this suite.

â€¢ [SLOW TEST:6.172 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":280,"completed":161,"skipped":2783,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:19:15.216: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 13 13:19:15.278: INFO: Waiting up to 5m0s for pod "pod-36076538-180e-426e-8568-04eb28d1ad0d" in namespace "emptydir-2396" to be "success or failure"
Jul 13 13:19:15.284: INFO: Pod "pod-36076538-180e-426e-8568-04eb28d1ad0d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.80847ms
Jul 13 13:19:17.289: INFO: Pod "pod-36076538-180e-426e-8568-04eb28d1ad0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011372288s
Jul 13 13:19:19.296: INFO: Pod "pod-36076538-180e-426e-8568-04eb28d1ad0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018328697s
STEP: Saw pod success
Jul 13 13:19:19.296: INFO: Pod "pod-36076538-180e-426e-8568-04eb28d1ad0d" satisfied condition "success or failure"
Jul 13 13:19:19.300: INFO: Trying to get logs from node ip-10-0-3-156.us-east-2.compute.internal pod pod-36076538-180e-426e-8568-04eb28d1ad0d container test-container: <nil>
STEP: delete the pod
Jul 13 13:19:19.346: INFO: Waiting for pod pod-36076538-180e-426e-8568-04eb28d1ad0d to disappear
Jul 13 13:19:19.350: INFO: Pod pod-36076538-180e-426e-8568-04eb28d1ad0d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:19:19.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2396" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":162,"skipped":2814,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:19:19.365: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-18ab2af6-3ecf-4e58-a471-7d5d5c43cd83
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-18ab2af6-3ecf-4e58-a471-7d5d5c43cd83
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:20:39.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5821" for this suite.

â€¢ [SLOW TEST:80.639 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":163,"skipped":2832,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:20:40.005: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-a2390c8e-129c-4ea8-a178-cb18dc107d51
STEP: Creating a pod to test consume secrets
Jul 13 13:20:40.068: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1a313ee9-9eba-448e-b518-e0c9b7565505" in namespace "projected-9631" to be "success or failure"
Jul 13 13:20:40.078: INFO: Pod "pod-projected-secrets-1a313ee9-9eba-448e-b518-e0c9b7565505": Phase="Pending", Reason="", readiness=false. Elapsed: 10.226264ms
Jul 13 13:20:42.084: INFO: Pod "pod-projected-secrets-1a313ee9-9eba-448e-b518-e0c9b7565505": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015857057s
Jul 13 13:20:44.089: INFO: Pod "pod-projected-secrets-1a313ee9-9eba-448e-b518-e0c9b7565505": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02135067s
STEP: Saw pod success
Jul 13 13:20:44.089: INFO: Pod "pod-projected-secrets-1a313ee9-9eba-448e-b518-e0c9b7565505" satisfied condition "success or failure"
Jul 13 13:20:44.093: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-projected-secrets-1a313ee9-9eba-448e-b518-e0c9b7565505 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 13 13:20:44.142: INFO: Waiting for pod pod-projected-secrets-1a313ee9-9eba-448e-b518-e0c9b7565505 to disappear
Jul 13 13:20:44.146: INFO: Pod pod-projected-secrets-1a313ee9-9eba-448e-b518-e0c9b7565505 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:20:44.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9631" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":164,"skipped":2860,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:20:44.159: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:20:55.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6583" for this suite.

â€¢ [SLOW TEST:11.152 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":280,"completed":165,"skipped":2860,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:20:55.312: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-45984fae-3932-4a64-a186-cd03257c5e00
STEP: Creating a pod to test consume configMaps
Jul 13 13:20:55.364: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8660f0b0-77f6-412b-9515-0080c6ad5c6b" in namespace "projected-7721" to be "success or failure"
Jul 13 13:20:55.370: INFO: Pod "pod-projected-configmaps-8660f0b0-77f6-412b-9515-0080c6ad5c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.902763ms
Jul 13 13:20:57.375: INFO: Pod "pod-projected-configmaps-8660f0b0-77f6-412b-9515-0080c6ad5c6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011851367s
Jul 13 13:20:59.384: INFO: Pod "pod-projected-configmaps-8660f0b0-77f6-412b-9515-0080c6ad5c6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020692639s
STEP: Saw pod success
Jul 13 13:20:59.384: INFO: Pod "pod-projected-configmaps-8660f0b0-77f6-412b-9515-0080c6ad5c6b" satisfied condition "success or failure"
Jul 13 13:20:59.388: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-projected-configmaps-8660f0b0-77f6-412b-9515-0080c6ad5c6b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 13:20:59.417: INFO: Waiting for pod pod-projected-configmaps-8660f0b0-77f6-412b-9515-0080c6ad5c6b to disappear
Jul 13 13:20:59.421: INFO: Pod pod-projected-configmaps-8660f0b0-77f6-412b-9515-0080c6ad5c6b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:20:59.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7721" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":280,"completed":166,"skipped":2883,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:20:59.444: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Jul 13 13:20:59.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=kubectl-9444 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul 13 13:21:02.460: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul 13 13:21:02.460: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:21:04.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9444" for this suite.

â€¢ [SLOW TEST:5.042 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1837
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":280,"completed":167,"skipped":2885,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:21:04.487: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:21:04.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9448" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":280,"completed":168,"skipped":2923,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:21:04.572: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jul 13 13:21:04.626: INFO: Waiting up to 5m0s for pod "downward-api-dbdbfbef-8f36-4aa1-b164-f342cfd2eded" in namespace "downward-api-9279" to be "success or failure"
Jul 13 13:21:04.632: INFO: Pod "downward-api-dbdbfbef-8f36-4aa1-b164-f342cfd2eded": Phase="Pending", Reason="", readiness=false. Elapsed: 5.74086ms
Jul 13 13:21:06.637: INFO: Pod "downward-api-dbdbfbef-8f36-4aa1-b164-f342cfd2eded": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011028012s
Jul 13 13:21:08.643: INFO: Pod "downward-api-dbdbfbef-8f36-4aa1-b164-f342cfd2eded": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016463617s
STEP: Saw pod success
Jul 13 13:21:08.643: INFO: Pod "downward-api-dbdbfbef-8f36-4aa1-b164-f342cfd2eded" satisfied condition "success or failure"
Jul 13 13:21:08.647: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod downward-api-dbdbfbef-8f36-4aa1-b164-f342cfd2eded container dapi-container: <nil>
STEP: delete the pod
Jul 13 13:21:08.676: INFO: Waiting for pod downward-api-dbdbfbef-8f36-4aa1-b164-f342cfd2eded to disappear
Jul 13 13:21:08.680: INFO: Pod downward-api-dbdbfbef-8f36-4aa1-b164-f342cfd2eded no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:21:08.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9279" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":280,"completed":169,"skipped":2956,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:21:08.696: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 13:21:09.341: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 13 13:21:11.375: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243269, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243269, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243269, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243269, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 13:21:14.398: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:21:14.403: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4413-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:21:15.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-562" for this suite.
STEP: Destroying namespace "webhook-562-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:7.037 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":280,"completed":170,"skipped":3009,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:21:15.734: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:21:31.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8042" for this suite.

â€¢ [SLOW TEST:16.222 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":280,"completed":171,"skipped":3027,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:21:31.956: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 13:21:32.009: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eed68712-fd15-45fe-a857-c4339d176f5f" in namespace "downward-api-730" to be "success or failure"
Jul 13 13:21:32.017: INFO: Pod "downwardapi-volume-eed68712-fd15-45fe-a857-c4339d176f5f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.904157ms
Jul 13 13:21:34.024: INFO: Pod "downwardapi-volume-eed68712-fd15-45fe-a857-c4339d176f5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014145027s
Jul 13 13:21:36.028: INFO: Pod "downwardapi-volume-eed68712-fd15-45fe-a857-c4339d176f5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018948425s
STEP: Saw pod success
Jul 13 13:21:36.028: INFO: Pod "downwardapi-volume-eed68712-fd15-45fe-a857-c4339d176f5f" satisfied condition "success or failure"
Jul 13 13:21:36.032: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod downwardapi-volume-eed68712-fd15-45fe-a857-c4339d176f5f container client-container: <nil>
STEP: delete the pod
Jul 13 13:21:36.060: INFO: Waiting for pod downwardapi-volume-eed68712-fd15-45fe-a857-c4339d176f5f to disappear
Jul 13 13:21:36.064: INFO: Pod downwardapi-volume-eed68712-fd15-45fe-a857-c4339d176f5f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:21:36.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-730" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":172,"skipped":3069,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:21:36.080: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:21:36.172: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"f2f65680-cb60-4e3f-b80e-07a0f10ccfe2", Controller:(*bool)(0xc002fde902), BlockOwnerDeletion:(*bool)(0xc002fde903)}}
Jul 13 13:21:36.187: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"edf3c55b-788a-4ba8-9d8d-39dbd1b04179", Controller:(*bool)(0xc003aa336a), BlockOwnerDeletion:(*bool)(0xc003aa336b)}}
Jul 13 13:21:36.202: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"363418fa-72cf-4547-b171-cc3a0c736a0f", Controller:(*bool)(0xc003aa350a), BlockOwnerDeletion:(*bool)(0xc003aa350b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:21:41.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1274" for this suite.

â€¢ [SLOW TEST:5.166 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":280,"completed":173,"skipped":3079,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:21:41.247: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jul 13 13:21:43.333: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-680009899 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul 13 13:21:53.425: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:21:53.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5295" for this suite.

â€¢ [SLOW TEST:12.196 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should be submitted and removed [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":280,"completed":174,"skipped":3097,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:21:53.443: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-5052
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 13 13:21:53.482: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 13 13:22:15.715: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.168.173:8080/dial?request=hostname&protocol=udp&host=10.20.171.250&port=8081&tries=1'] Namespace:pod-network-test-5052 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:22:15.715: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:22:15.930: INFO: Waiting for responses: map[]
Jul 13 13:22:15.952: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.168.173:8080/dial?request=hostname&protocol=udp&host=10.20.92.125&port=8081&tries=1'] Namespace:pod-network-test-5052 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:22:15.952: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:22:16.101: INFO: Waiting for responses: map[]
Jul 13 13:22:16.106: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.168.173:8080/dial?request=hostname&protocol=udp&host=10.20.168.171&port=8081&tries=1'] Namespace:pod-network-test-5052 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:22:16.106: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:22:16.239: INFO: Waiting for responses: map[]
Jul 13 13:22:16.244: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.168.173:8080/dial?request=hostname&protocol=udp&host=10.20.23.166&port=8081&tries=1'] Namespace:pod-network-test-5052 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:22:16.244: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:22:16.364: INFO: Waiting for responses: map[]
Jul 13 13:22:16.369: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.168.173:8080/dial?request=hostname&protocol=udp&host=10.20.234.38&port=8081&tries=1'] Namespace:pod-network-test-5052 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:22:16.369: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:22:16.495: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:22:16.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5052" for this suite.

â€¢ [SLOW TEST:23.067 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":175,"skipped":3119,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:22:16.511: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jul 13 13:22:16.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 create -f - --namespace=kubectl-258'
Jul 13 13:22:16.813: INFO: stderr: ""
Jul 13 13:22:16.813: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jul 13 13:22:17.817: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 13 13:22:17.817: INFO: Found 0 / 1
Jul 13 13:22:18.818: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 13 13:22:18.818: INFO: Found 1 / 1
Jul 13 13:22:18.818: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 13 13:22:18.822: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 13 13:22:18.822: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 13 13:22:18.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 patch pod agnhost-master-lcz7n --namespace=kubectl-258 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 13 13:22:18.908: INFO: stderr: ""
Jul 13 13:22:18.908: INFO: stdout: "pod/agnhost-master-lcz7n patched\n"
STEP: checking annotations
Jul 13 13:22:18.913: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 13 13:22:18.913: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:22:18.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-258" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":280,"completed":176,"skipped":3144,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:22:18.929: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:22:18.972: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:22:19.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1689" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":280,"completed":177,"skipped":3175,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:22:19.585: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jul 13 13:22:24.378: INFO: Successfully updated pod "labelsupdatee6209493-dd7b-4f54-9461-1440a4a53ea0"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:22:26.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7000" for this suite.

â€¢ [SLOW TEST:6.827 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":178,"skipped":3177,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:22:26.413: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 13:22:26.859: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 13 13:22:28.872: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243346, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243346, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243346, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243346, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 13:22:31.895: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:22:32.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2475" for this suite.
STEP: Destroying namespace "webhook-2475-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.704 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":280,"completed":179,"skipped":3182,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:22:32.118: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-0ab185d4-9974-42ff-8cab-914105943be0
STEP: Creating a pod to test consume secrets
Jul 13 13:22:32.175: INFO: Waiting up to 5m0s for pod "pod-secrets-b17f06ea-659a-443d-8abe-5b2cf1292ab3" in namespace "secrets-9835" to be "success or failure"
Jul 13 13:22:32.181: INFO: Pod "pod-secrets-b17f06ea-659a-443d-8abe-5b2cf1292ab3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086449ms
Jul 13 13:22:34.186: INFO: Pod "pod-secrets-b17f06ea-659a-443d-8abe-5b2cf1292ab3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011083299s
STEP: Saw pod success
Jul 13 13:22:34.186: INFO: Pod "pod-secrets-b17f06ea-659a-443d-8abe-5b2cf1292ab3" satisfied condition "success or failure"
Jul 13 13:22:34.191: INFO: Trying to get logs from node ip-10-0-3-156.us-east-2.compute.internal pod pod-secrets-b17f06ea-659a-443d-8abe-5b2cf1292ab3 container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 13:22:34.234: INFO: Waiting for pod pod-secrets-b17f06ea-659a-443d-8abe-5b2cf1292ab3 to disappear
Jul 13 13:22:34.239: INFO: Pod pod-secrets-b17f06ea-659a-443d-8abe-5b2cf1292ab3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:22:34.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9835" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":180,"skipped":3197,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:22:34.253: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 13:22:34.299: INFO: Waiting up to 5m0s for pod "downwardapi-volume-074e9c24-2a24-4d22-9eba-a14ec757918b" in namespace "downward-api-5671" to be "success or failure"
Jul 13 13:22:34.315: INFO: Pod "downwardapi-volume-074e9c24-2a24-4d22-9eba-a14ec757918b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.691551ms
Jul 13 13:22:36.320: INFO: Pod "downwardapi-volume-074e9c24-2a24-4d22-9eba-a14ec757918b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020874376s
STEP: Saw pod success
Jul 13 13:22:36.321: INFO: Pod "downwardapi-volume-074e9c24-2a24-4d22-9eba-a14ec757918b" satisfied condition "success or failure"
Jul 13 13:22:36.333: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod downwardapi-volume-074e9c24-2a24-4d22-9eba-a14ec757918b container client-container: <nil>
STEP: delete the pod
Jul 13 13:22:36.363: INFO: Waiting for pod downwardapi-volume-074e9c24-2a24-4d22-9eba-a14ec757918b to disappear
Jul 13 13:22:36.367: INFO: Pod downwardapi-volume-074e9c24-2a24-4d22-9eba-a14ec757918b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:22:36.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5671" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":280,"completed":181,"skipped":3210,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:22:36.385: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:22:36.427: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:22:38.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-175" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":280,"completed":182,"skipped":3215,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:22:38.579: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 13 13:22:41.159: INFO: Successfully updated pod "pod-update-af4f10a1-8396-4554-bcac-77b501a2c987"
STEP: verifying the updated pod is in kubernetes
Jul 13 13:22:41.167: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:22:41.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8189" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":280,"completed":183,"skipped":3229,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:22:41.182: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Jul 13 13:22:41.229: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-680009899 proxy --unix-socket=/tmp/kubectl-proxy-unix023264634/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:22:41.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8532" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":280,"completed":184,"skipped":3237,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:22:41.310: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 13 13:22:42.034: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jul 13 13:22:44.047: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243362, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243362, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243362, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243362, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 13:22:47.073: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:22:47.078: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:22:48.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7072" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

â€¢ [SLOW TEST:7.039 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":280,"completed":185,"skipped":3249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:22:48.352: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-f21fb653-2262-4757-9470-c3f965aa0e7f
STEP: Creating a pod to test consume configMaps
Jul 13 13:22:48.429: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-27e8368f-2a57-4be7-97e7-0ab4beee2e65" in namespace "projected-9958" to be "success or failure"
Jul 13 13:22:48.444: INFO: Pod "pod-projected-configmaps-27e8368f-2a57-4be7-97e7-0ab4beee2e65": Phase="Pending", Reason="", readiness=false. Elapsed: 15.340713ms
Jul 13 13:22:50.449: INFO: Pod "pod-projected-configmaps-27e8368f-2a57-4be7-97e7-0ab4beee2e65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02052924s
STEP: Saw pod success
Jul 13 13:22:50.450: INFO: Pod "pod-projected-configmaps-27e8368f-2a57-4be7-97e7-0ab4beee2e65" satisfied condition "success or failure"
Jul 13 13:22:50.454: INFO: Trying to get logs from node ip-10-0-3-156.us-east-2.compute.internal pod pod-projected-configmaps-27e8368f-2a57-4be7-97e7-0ab4beee2e65 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 13:22:50.496: INFO: Waiting for pod pod-projected-configmaps-27e8368f-2a57-4be7-97e7-0ab4beee2e65 to disappear
Jul 13 13:22:50.501: INFO: Pod pod-projected-configmaps-27e8368f-2a57-4be7-97e7-0ab4beee2e65 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:22:50.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9958" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":186,"skipped":3285,"failed":0}

------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:22:50.520: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 13 13:22:55.110: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8637 pod-service-account-acd8e38b-0c8a-43e2-b5de-fc1bf59df940 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 13 13:22:55.315: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8637 pod-service-account-acd8e38b-0c8a-43e2-b5de-fc1bf59df940 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 13 13:22:55.505: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8637 pod-service-account-acd8e38b-0c8a-43e2-b5de-fc1bf59df940 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:22:55.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8637" for this suite.

â€¢ [SLOW TEST:5.210 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":280,"completed":187,"skipped":3285,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:22:55.732: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 13 13:22:57.864: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:22:57.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-302" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":188,"skipped":3303,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:22:57.899: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 13 13:22:57.947: INFO: Waiting up to 5m0s for pod "pod-924794aa-db34-4a3e-a6df-76e37d732cef" in namespace "emptydir-7292" to be "success or failure"
Jul 13 13:22:57.956: INFO: Pod "pod-924794aa-db34-4a3e-a6df-76e37d732cef": Phase="Pending", Reason="", readiness=false. Elapsed: 8.632118ms
Jul 13 13:22:59.960: INFO: Pod "pod-924794aa-db34-4a3e-a6df-76e37d732cef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013011728s
STEP: Saw pod success
Jul 13 13:22:59.960: INFO: Pod "pod-924794aa-db34-4a3e-a6df-76e37d732cef" satisfied condition "success or failure"
Jul 13 13:22:59.964: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-924794aa-db34-4a3e-a6df-76e37d732cef container test-container: <nil>
STEP: delete the pod
Jul 13 13:22:59.992: INFO: Waiting for pod pod-924794aa-db34-4a3e-a6df-76e37d732cef to disappear
Jul 13 13:22:59.997: INFO: Pod pod-924794aa-db34-4a3e-a6df-76e37d732cef no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:22:59.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7292" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":189,"skipped":3308,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:23:00.018: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jul 13 13:23:00.072: INFO: Waiting up to 5m0s for pod "downward-api-e45658ec-193e-4729-9a5d-76c94d55c0f7" in namespace "downward-api-9440" to be "success or failure"
Jul 13 13:23:00.079: INFO: Pod "downward-api-e45658ec-193e-4729-9a5d-76c94d55c0f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.895058ms
Jul 13 13:23:02.085: INFO: Pod "downward-api-e45658ec-193e-4729-9a5d-76c94d55c0f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012353594s
STEP: Saw pod success
Jul 13 13:23:02.085: INFO: Pod "downward-api-e45658ec-193e-4729-9a5d-76c94d55c0f7" satisfied condition "success or failure"
Jul 13 13:23:02.092: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod downward-api-e45658ec-193e-4729-9a5d-76c94d55c0f7 container dapi-container: <nil>
STEP: delete the pod
Jul 13 13:23:02.120: INFO: Waiting for pod downward-api-e45658ec-193e-4729-9a5d-76c94d55c0f7 to disappear
Jul 13 13:23:02.125: INFO: Pod downward-api-e45658ec-193e-4729-9a5d-76c94d55c0f7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:23:02.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9440" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":280,"completed":190,"skipped":3310,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:23:02.146: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:23:02.189: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 13 13:23:02.200: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 13 13:23:07.208: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 13 13:23:07.208: INFO: Creating deployment "test-rolling-update-deployment"
Jul 13 13:23:07.215: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 13 13:23:07.237: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 13 13:23:09.248: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 13 13:23:09.252: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jul 13 13:23:09.263: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4546 /apis/apps/v1/namespaces/deployment-4546/deployments/test-rolling-update-deployment c1804a27-2d70-44ad-9339-05c8bd949506 236656 1 2020-07-13 13:23:07 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0055c29c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-07-13 13:23:07 +0000 UTC,LastTransitionTime:2020-07-13 13:23:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-07-13 13:23:08 +0000 UTC,LastTransitionTime:2020-07-13 13:23:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 13 13:23:09.267: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-4546 /apis/apps/v1/namespaces/deployment-4546/replicasets/test-rolling-update-deployment-67cf4f6444 c1553a30-21d9-4f43-917f-73194463d19c 236645 1 2020-07-13 13:23:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment c1804a27-2d70-44ad-9339-05c8bd949506 0xc0055c2e67 0xc0055c2e68}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0055c2ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 13 13:23:09.267: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 13 13:23:09.267: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4546 /apis/apps/v1/namespaces/deployment-4546/replicasets/test-rolling-update-controller 1e5b78a5-cf03-42d2-9e13-f225d4d6f8fb 236654 2 2020-07-13 13:23:02 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment c1804a27-2d70-44ad-9339-05c8bd949506 0xc0055c2d97 0xc0055c2d98}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0055c2df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 13 13:23:09.272: INFO: Pod "test-rolling-update-deployment-67cf4f6444-m5vl2" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-m5vl2 test-rolling-update-deployment-67cf4f6444- deployment-4546 /api/v1/namespaces/deployment-4546/pods/test-rolling-update-deployment-67cf4f6444-m5vl2 d9cc6a0a-f6ff-4443-b6b6-3820bcf62103 236644 0 2020-07-13 13:23:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[cni.projectcalico.org/podIP:10.20.23.172/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 c1553a30-21d9-4f43-917f-73194463d19c 0xc0055c3367 0xc0055c3368}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f6wf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f6wf4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f6wf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-3-156.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:23:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:23:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:23:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:23:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.3.156,PodIP:10.20.23.172,StartTime:2020-07-13 13:23:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-13 13:23:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://31d47326ca7d00da08323ff3836b2ad4118710b4b94d811f07ad006ca5e9ba8a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.23.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:23:09.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4546" for this suite.

â€¢ [SLOW TEST:7.142 seconds]
[sig-apps] Deployment
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":191,"skipped":3355,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:23:09.288: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-ecfd7c21-a28c-4715-8ada-04f78592193a
STEP: Creating a pod to test consume configMaps
Jul 13 13:23:09.346: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7a86922a-d9df-4b70-84b6-2e2935ad10f7" in namespace "projected-7402" to be "success or failure"
Jul 13 13:23:09.355: INFO: Pod "pod-projected-configmaps-7a86922a-d9df-4b70-84b6-2e2935ad10f7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.041203ms
Jul 13 13:23:11.360: INFO: Pod "pod-projected-configmaps-7a86922a-d9df-4b70-84b6-2e2935ad10f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01424444s
Jul 13 13:23:13.365: INFO: Pod "pod-projected-configmaps-7a86922a-d9df-4b70-84b6-2e2935ad10f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01935677s
STEP: Saw pod success
Jul 13 13:23:13.365: INFO: Pod "pod-projected-configmaps-7a86922a-d9df-4b70-84b6-2e2935ad10f7" satisfied condition "success or failure"
Jul 13 13:23:13.370: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-projected-configmaps-7a86922a-d9df-4b70-84b6-2e2935ad10f7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 13:23:13.400: INFO: Waiting for pod pod-projected-configmaps-7a86922a-d9df-4b70-84b6-2e2935ad10f7 to disappear
Jul 13 13:23:13.404: INFO: Pod pod-projected-configmaps-7a86922a-d9df-4b70-84b6-2e2935ad10f7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:23:13.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7402" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":192,"skipped":3374,"failed":0}
S
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:23:13.421: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4943 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4943;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4943 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4943;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4943.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4943.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4943.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4943.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4943.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4943.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4943.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4943.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4943.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4943.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4943.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4943.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4943.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 155.215.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.215.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.215.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.215.155_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4943 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4943;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4943 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4943;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4943.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4943.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4943.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4943.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4943.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4943.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4943.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4943.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4943.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4943.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4943.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4943.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4943.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 155.215.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.215.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.215.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.215.155_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 13 13:23:17.540: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.546: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.551: INFO: Unable to read wheezy_udp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.557: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.563: INFO: Unable to read wheezy_udp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.568: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.573: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.582: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.627: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.639: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.647: INFO: Unable to read jessie_udp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.653: INFO: Unable to read jessie_tcp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.661: INFO: Unable to read jessie_udp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.666: INFO: Unable to read jessie_tcp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.671: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.676: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:17.717: INFO: Lookups using dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4943 wheezy_tcp@dns-test-service.dns-4943 wheezy_udp@dns-test-service.dns-4943.svc wheezy_tcp@dns-test-service.dns-4943.svc wheezy_udp@_http._tcp.dns-test-service.dns-4943.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4943.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4943 jessie_tcp@dns-test-service.dns-4943 jessie_udp@dns-test-service.dns-4943.svc jessie_tcp@dns-test-service.dns-4943.svc jessie_udp@_http._tcp.dns-test-service.dns-4943.svc jessie_tcp@_http._tcp.dns-test-service.dns-4943.svc]

Jul 13 13:23:22.723: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.729: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.734: INFO: Unable to read wheezy_udp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.739: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.744: INFO: Unable to read wheezy_udp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.751: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.756: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.762: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.808: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.813: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.818: INFO: Unable to read jessie_udp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.825: INFO: Unable to read jessie_tcp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.831: INFO: Unable to read jessie_udp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.836: INFO: Unable to read jessie_tcp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.842: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.847: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:22.883: INFO: Lookups using dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4943 wheezy_tcp@dns-test-service.dns-4943 wheezy_udp@dns-test-service.dns-4943.svc wheezy_tcp@dns-test-service.dns-4943.svc wheezy_udp@_http._tcp.dns-test-service.dns-4943.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4943.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4943 jessie_tcp@dns-test-service.dns-4943 jessie_udp@dns-test-service.dns-4943.svc jessie_tcp@dns-test-service.dns-4943.svc jessie_udp@_http._tcp.dns-test-service.dns-4943.svc jessie_tcp@_http._tcp.dns-test-service.dns-4943.svc]

Jul 13 13:23:27.728: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.733: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.738: INFO: Unable to read wheezy_udp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.743: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.748: INFO: Unable to read wheezy_udp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.753: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.758: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.763: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.801: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.809: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.814: INFO: Unable to read jessie_udp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.820: INFO: Unable to read jessie_tcp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.825: INFO: Unable to read jessie_udp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.832: INFO: Unable to read jessie_tcp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.837: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.842: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:27.873: INFO: Lookups using dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4943 wheezy_tcp@dns-test-service.dns-4943 wheezy_udp@dns-test-service.dns-4943.svc wheezy_tcp@dns-test-service.dns-4943.svc wheezy_udp@_http._tcp.dns-test-service.dns-4943.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4943.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4943 jessie_tcp@dns-test-service.dns-4943 jessie_udp@dns-test-service.dns-4943.svc jessie_tcp@dns-test-service.dns-4943.svc jessie_udp@_http._tcp.dns-test-service.dns-4943.svc jessie_tcp@_http._tcp.dns-test-service.dns-4943.svc]

Jul 13 13:23:32.722: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.727: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.733: INFO: Unable to read wheezy_udp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.740: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.746: INFO: Unable to read wheezy_udp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.752: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.757: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.762: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.801: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.806: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.811: INFO: Unable to read jessie_udp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.816: INFO: Unable to read jessie_tcp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.821: INFO: Unable to read jessie_udp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.826: INFO: Unable to read jessie_tcp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.832: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.838: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:32.868: INFO: Lookups using dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4943 wheezy_tcp@dns-test-service.dns-4943 wheezy_udp@dns-test-service.dns-4943.svc wheezy_tcp@dns-test-service.dns-4943.svc wheezy_udp@_http._tcp.dns-test-service.dns-4943.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4943.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4943 jessie_tcp@dns-test-service.dns-4943 jessie_udp@dns-test-service.dns-4943.svc jessie_tcp@dns-test-service.dns-4943.svc jessie_udp@_http._tcp.dns-test-service.dns-4943.svc jessie_tcp@_http._tcp.dns-test-service.dns-4943.svc]

Jul 13 13:23:37.728: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.733: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.738: INFO: Unable to read wheezy_udp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.744: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.753: INFO: Unable to read wheezy_udp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.758: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.764: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.769: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.805: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.810: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.817: INFO: Unable to read jessie_udp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.823: INFO: Unable to read jessie_tcp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.828: INFO: Unable to read jessie_udp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.833: INFO: Unable to read jessie_tcp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.838: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:37.882: INFO: Lookups using dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4943 wheezy_tcp@dns-test-service.dns-4943 wheezy_udp@dns-test-service.dns-4943.svc wheezy_tcp@dns-test-service.dns-4943.svc wheezy_udp@_http._tcp.dns-test-service.dns-4943.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4943.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4943 jessie_tcp@dns-test-service.dns-4943 jessie_udp@dns-test-service.dns-4943.svc jessie_tcp@dns-test-service.dns-4943.svc jessie_udp@_http._tcp.dns-test-service.dns-4943.svc jessie_tcp@_http._tcp.dns-test-service.dns-4943.svc]

Jul 13 13:23:42.723: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.728: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.734: INFO: Unable to read wheezy_udp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.739: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.744: INFO: Unable to read wheezy_udp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.750: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.755: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.760: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.800: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.805: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.811: INFO: Unable to read jessie_udp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.816: INFO: Unable to read jessie_tcp@dns-test-service.dns-4943 from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.821: INFO: Unable to read jessie_udp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.826: INFO: Unable to read jessie_tcp@dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.833: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.839: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4943.svc from pod dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641: the server could not find the requested resource (get pods dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641)
Jul 13 13:23:42.874: INFO: Lookups using dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4943 wheezy_tcp@dns-test-service.dns-4943 wheezy_udp@dns-test-service.dns-4943.svc wheezy_tcp@dns-test-service.dns-4943.svc wheezy_udp@_http._tcp.dns-test-service.dns-4943.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4943.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4943 jessie_tcp@dns-test-service.dns-4943 jessie_udp@dns-test-service.dns-4943.svc jessie_tcp@dns-test-service.dns-4943.svc jessie_udp@_http._tcp.dns-test-service.dns-4943.svc jessie_tcp@_http._tcp.dns-test-service.dns-4943.svc]

Jul 13 13:23:47.872: INFO: DNS probes using dns-4943/dns-test-ec087bf6-8a99-48fd-bcdd-ec2a58ad1641 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:23:48.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4943" for this suite.

â€¢ [SLOW TEST:34.600 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":280,"completed":193,"skipped":3375,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:23:48.022: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jul 13 13:23:48.103: INFO: Waiting up to 5m0s for pod "downward-api-ee3f44b4-7558-4ffc-b676-b79d657a161e" in namespace "downward-api-8588" to be "success or failure"
Jul 13 13:23:48.114: INFO: Pod "downward-api-ee3f44b4-7558-4ffc-b676-b79d657a161e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.022342ms
Jul 13 13:23:50.118: INFO: Pod "downward-api-ee3f44b4-7558-4ffc-b676-b79d657a161e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0152661s
STEP: Saw pod success
Jul 13 13:23:50.118: INFO: Pod "downward-api-ee3f44b4-7558-4ffc-b676-b79d657a161e" satisfied condition "success or failure"
Jul 13 13:23:50.122: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod downward-api-ee3f44b4-7558-4ffc-b676-b79d657a161e container dapi-container: <nil>
STEP: delete the pod
Jul 13 13:23:50.167: INFO: Waiting for pod downward-api-ee3f44b4-7558-4ffc-b676-b79d657a161e to disappear
Jul 13 13:23:50.170: INFO: Pod downward-api-ee3f44b4-7558-4ffc-b676-b79d657a161e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:23:50.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8588" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":280,"completed":194,"skipped":3384,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:23:50.190: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0713 13:24:20.298916      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 13 13:24:20.298: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:24:20.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3564" for this suite.

â€¢ [SLOW TEST:30.122 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":280,"completed":195,"skipped":3399,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:24:20.312: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Jul 13 13:24:20.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 cluster-info'
Jul 13 13:24:20.436: INFO: stderr: ""
Jul 13 13:24:20.436: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.21.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.21.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.21.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:24:20.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8988" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":280,"completed":196,"skipped":3437,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:24:20.459: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:24:20.513: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:24:23.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8574" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":280,"completed":197,"skipped":3450,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:24:23.813: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:24:41.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3748" for this suite.

â€¢ [SLOW TEST:17.461 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":280,"completed":198,"skipped":3454,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:24:41.273: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:24:41.317: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Creating first CR 
Jul 13 13:24:41.892: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-13T13:24:41Z generation:1 name:name1 resourceVersion:237316 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:f86f89a6-21e6-4b6f-bc88-d7ca1bd1f0cf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jul 13 13:24:51.899: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-13T13:24:51Z generation:1 name:name2 resourceVersion:237364 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:45ac04fc-9f71-4972-ab16-54672f89249c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jul 13 13:25:01.914: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-13T13:24:41Z generation:2 name:name1 resourceVersion:237401 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:f86f89a6-21e6-4b6f-bc88-d7ca1bd1f0cf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jul 13 13:25:11.922: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-13T13:24:51Z generation:2 name:name2 resourceVersion:237440 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:45ac04fc-9f71-4972-ab16-54672f89249c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jul 13 13:25:21.933: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-13T13:24:41Z generation:2 name:name1 resourceVersion:237469 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:f86f89a6-21e6-4b6f-bc88-d7ca1bd1f0cf] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jul 13 13:25:31.942: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-07-13T13:24:51Z generation:2 name:name2 resourceVersion:237496 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:45ac04fc-9f71-4972-ab16-54672f89249c] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:25:42.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6179" for this suite.

â€¢ [SLOW TEST:61.211 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:41
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":280,"completed":199,"skipped":3456,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:25:42.485: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 13 13:25:43.271: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jul 13 13:25:45.283: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243543, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243543, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243543, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243543, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 13:25:48.307: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:25:48.312: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:25:49.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7353" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136

â€¢ [SLOW TEST:7.163 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":280,"completed":200,"skipped":3468,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:25:49.652: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:25:49.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-52" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":280,"completed":201,"skipped":3482,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:25:49.719: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jul 13 13:25:49.765: INFO: PodSpec: initContainers in spec.initContainers
Jul 13 13:26:34.631: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7eef06f7-3b06-48ec-a440-bd18986a4c20", GenerateName:"", Namespace:"init-container-3304", SelfLink:"/api/v1/namespaces/init-container-3304/pods/pod-init-7eef06f7-3b06-48ec-a440-bd18986a4c20", UID:"66ec61ad-25a9-4151-8637-c08bda259025", ResourceVersion:"237833", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63730243549, loc:(*time.Location)(0x7925200)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"765864397"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.20.234.47/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-zgfcv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0038d8340), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zgfcv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zgfcv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-zgfcv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002eb00d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-3-16.us-east-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00226df80), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002eb0150)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002eb0170)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002eb0178), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002eb017c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243549, loc:(*time.Location)(0x7925200)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243549, loc:(*time.Location)(0x7925200)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243549, loc:(*time.Location)(0x7925200)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243549, loc:(*time.Location)(0x7925200)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.3.16", PodIP:"10.20.234.47", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.20.234.47"}}, StartTime:(*v1.Time)(0xc0027dc0c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0031967e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003196850)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://f1d878d7c89e402cf2f80e9a54379d26c21ffb041cf5d805d23fc23e826f3bda", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0027dc100), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0027dc0e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc002eb01ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:26:34.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3304" for this suite.

â€¢ [SLOW TEST:44.943 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":280,"completed":202,"skipped":3533,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:26:34.662: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-9337/secret-test-0ba8b2c5-0e1e-4688-a4ab-c0afa651653e
STEP: Creating a pod to test consume secrets
Jul 13 13:26:34.722: INFO: Waiting up to 5m0s for pod "pod-configmaps-1cce8c7b-d9cc-4c24-b141-7b04e1c669fa" in namespace "secrets-9337" to be "success or failure"
Jul 13 13:26:34.727: INFO: Pod "pod-configmaps-1cce8c7b-d9cc-4c24-b141-7b04e1c669fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.848075ms
Jul 13 13:26:36.732: INFO: Pod "pod-configmaps-1cce8c7b-d9cc-4c24-b141-7b04e1c669fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00992028s
STEP: Saw pod success
Jul 13 13:26:36.732: INFO: Pod "pod-configmaps-1cce8c7b-d9cc-4c24-b141-7b04e1c669fa" satisfied condition "success or failure"
Jul 13 13:26:36.736: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-configmaps-1cce8c7b-d9cc-4c24-b141-7b04e1c669fa container env-test: <nil>
STEP: delete the pod
Jul 13 13:26:36.803: INFO: Waiting for pod pod-configmaps-1cce8c7b-d9cc-4c24-b141-7b04e1c669fa to disappear
Jul 13 13:26:36.807: INFO: Pod pod-configmaps-1cce8c7b-d9cc-4c24-b141-7b04e1c669fa no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:26:36.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9337" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":203,"skipped":3539,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:26:36.822: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-96714cad-c239-49d7-a453-f4ab70a15d90
STEP: Creating a pod to test consume secrets
Jul 13 13:26:36.872: INFO: Waiting up to 5m0s for pod "pod-secrets-b0c5d4f9-3f31-4ad8-85ca-06c616cf2588" in namespace "secrets-9968" to be "success or failure"
Jul 13 13:26:36.885: INFO: Pod "pod-secrets-b0c5d4f9-3f31-4ad8-85ca-06c616cf2588": Phase="Pending", Reason="", readiness=false. Elapsed: 12.168821ms
Jul 13 13:26:38.889: INFO: Pod "pod-secrets-b0c5d4f9-3f31-4ad8-85ca-06c616cf2588": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016761959s
STEP: Saw pod success
Jul 13 13:26:38.889: INFO: Pod "pod-secrets-b0c5d4f9-3f31-4ad8-85ca-06c616cf2588" satisfied condition "success or failure"
Jul 13 13:26:38.893: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-secrets-b0c5d4f9-3f31-4ad8-85ca-06c616cf2588 container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 13:26:38.929: INFO: Waiting for pod pod-secrets-b0c5d4f9-3f31-4ad8-85ca-06c616cf2588 to disappear
Jul 13 13:26:38.934: INFO: Pod pod-secrets-b0c5d4f9-3f31-4ad8-85ca-06c616cf2588 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:26:38.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9968" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":204,"skipped":3571,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:26:38.954: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jul 13 13:26:43.537: INFO: Successfully updated pod "adopt-release-4zn87"
STEP: Checking that the Job readopts the Pod
Jul 13 13:26:43.538: INFO: Waiting up to 15m0s for pod "adopt-release-4zn87" in namespace "job-6440" to be "adopted"
Jul 13 13:26:43.543: INFO: Pod "adopt-release-4zn87": Phase="Running", Reason="", readiness=true. Elapsed: 5.39965ms
Jul 13 13:26:45.549: INFO: Pod "adopt-release-4zn87": Phase="Running", Reason="", readiness=true. Elapsed: 2.010739488s
Jul 13 13:26:45.549: INFO: Pod "adopt-release-4zn87" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jul 13 13:26:46.059: INFO: Successfully updated pod "adopt-release-4zn87"
STEP: Checking that the Job releases the Pod
Jul 13 13:26:46.060: INFO: Waiting up to 15m0s for pod "adopt-release-4zn87" in namespace "job-6440" to be "released"
Jul 13 13:26:46.066: INFO: Pod "adopt-release-4zn87": Phase="Running", Reason="", readiness=true. Elapsed: 5.78512ms
Jul 13 13:26:48.079: INFO: Pod "adopt-release-4zn87": Phase="Running", Reason="", readiness=true. Elapsed: 2.019413792s
Jul 13 13:26:48.079: INFO: Pod "adopt-release-4zn87" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:26:48.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6440" for this suite.

â€¢ [SLOW TEST:9.139 seconds]
[sig-apps] Job
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":280,"completed":205,"skipped":3632,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:26:48.093: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-4ad662ae-ca49-4a0f-89e2-bec6f474999a in namespace container-probe-2414
Jul 13 13:26:50.158: INFO: Started pod test-webserver-4ad662ae-ca49-4a0f-89e2-bec6f474999a in namespace container-probe-2414
STEP: checking the pod's current state and verifying that restartCount is present
Jul 13 13:26:50.162: INFO: Initial restart count of pod test-webserver-4ad662ae-ca49-4a0f-89e2-bec6f474999a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:30:50.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2414" for this suite.

â€¢ [SLOW TEST:242.784 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":206,"skipped":3647,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:30:50.879: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Jul 13 13:30:50.962: INFO: Waiting up to 5m0s for pod "var-expansion-c566658d-5ae5-46a1-b749-a8917fe8ca38" in namespace "var-expansion-8846" to be "success or failure"
Jul 13 13:30:50.972: INFO: Pod "var-expansion-c566658d-5ae5-46a1-b749-a8917fe8ca38": Phase="Pending", Reason="", readiness=false. Elapsed: 10.450974ms
Jul 13 13:30:52.981: INFO: Pod "var-expansion-c566658d-5ae5-46a1-b749-a8917fe8ca38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018643179s
STEP: Saw pod success
Jul 13 13:30:52.981: INFO: Pod "var-expansion-c566658d-5ae5-46a1-b749-a8917fe8ca38" satisfied condition "success or failure"
Jul 13 13:30:52.989: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod var-expansion-c566658d-5ae5-46a1-b749-a8917fe8ca38 container dapi-container: <nil>
STEP: delete the pod
Jul 13 13:30:53.044: INFO: Waiting for pod var-expansion-c566658d-5ae5-46a1-b749-a8917fe8ca38 to disappear
Jul 13 13:30:53.050: INFO: Pod var-expansion-c566658d-5ae5-46a1-b749-a8917fe8ca38 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:30:53.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8846" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":280,"completed":207,"skipped":3665,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:30:53.066: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-6a621f43-6cef-4ad3-8cba-0e2549832cc1
STEP: Creating a pod to test consume configMaps
Jul 13 13:30:53.125: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6062d31f-dc44-443a-8011-c9e2473481c5" in namespace "projected-8242" to be "success or failure"
Jul 13 13:30:53.137: INFO: Pod "pod-projected-configmaps-6062d31f-dc44-443a-8011-c9e2473481c5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.550758ms
Jul 13 13:30:55.142: INFO: Pod "pod-projected-configmaps-6062d31f-dc44-443a-8011-c9e2473481c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016505203s
STEP: Saw pod success
Jul 13 13:30:55.142: INFO: Pod "pod-projected-configmaps-6062d31f-dc44-443a-8011-c9e2473481c5" satisfied condition "success or failure"
Jul 13 13:30:55.145: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-projected-configmaps-6062d31f-dc44-443a-8011-c9e2473481c5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 13:30:55.208: INFO: Waiting for pod pod-projected-configmaps-6062d31f-dc44-443a-8011-c9e2473481c5 to disappear
Jul 13 13:30:55.215: INFO: Pod pod-projected-configmaps-6062d31f-dc44-443a-8011-c9e2473481c5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:30:55.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8242" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":280,"completed":208,"skipped":3667,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:30:55.237: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 13:30:56.099: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 13 13:30:58.112: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243856, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243856, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243856, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730243856, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 13:31:01.136: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:31:01.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2509" for this suite.
STEP: Destroying namespace "webhook-2509-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.290 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":280,"completed":209,"skipped":3691,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:31:01.528: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-1396d075-ab0e-4e5e-9d77-010028bbfe44
STEP: Creating configMap with name cm-test-opt-upd-b2b19cb0-9494-4b06-928f-05810b9491ec
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1396d075-ab0e-4e5e-9d77-010028bbfe44
STEP: Updating configmap cm-test-opt-upd-b2b19cb0-9494-4b06-928f-05810b9491ec
STEP: Creating configMap with name cm-test-opt-create-1e9ab9c9-bc51-4a7d-bbbe-84c47b091728
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:32:20.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4120" for this suite.

â€¢ [SLOW TEST:78.638 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":210,"skipped":3693,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:32:20.166: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-70c54dfd-2b10-48cc-9df4-3d84bc9f9a08 in namespace container-probe-2253
Jul 13 13:32:24.251: INFO: Started pod busybox-70c54dfd-2b10-48cc-9df4-3d84bc9f9a08 in namespace container-probe-2253
STEP: checking the pod's current state and verifying that restartCount is present
Jul 13 13:32:24.256: INFO: Initial restart count of pod busybox-70c54dfd-2b10-48cc-9df4-3d84bc9f9a08 is 0
Jul 13 13:33:10.397: INFO: Restart count of pod container-probe-2253/busybox-70c54dfd-2b10-48cc-9df4-3d84bc9f9a08 is now 1 (46.1408062s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:33:10.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2253" for this suite.

â€¢ [SLOW TEST:50.272 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":211,"skipped":3707,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:33:10.443: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Jul 13 13:33:10.508: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4779" to be "success or failure"
Jul 13 13:33:10.513: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 5.526898ms
Jul 13 13:33:12.523: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015596135s
STEP: Saw pod success
Jul 13 13:33:12.523: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul 13 13:33:12.535: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul 13 13:33:12.566: INFO: Waiting for pod pod-host-path-test to disappear
Jul 13 13:33:12.570: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:33:12.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4779" for this suite.
â€¢{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":212,"skipped":3724,"failed":0}
S
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:33:12.595: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:33:12.696: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-289201f9-37e1-43fd-ae5c-2ebad214b036" in namespace "security-context-test-7915" to be "success or failure"
Jul 13 13:33:12.704: INFO: Pod "alpine-nnp-false-289201f9-37e1-43fd-ae5c-2ebad214b036": Phase="Pending", Reason="", readiness=false. Elapsed: 7.778546ms
Jul 13 13:33:14.709: INFO: Pod "alpine-nnp-false-289201f9-37e1-43fd-ae5c-2ebad214b036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012911114s
Jul 13 13:33:16.714: INFO: Pod "alpine-nnp-false-289201f9-37e1-43fd-ae5c-2ebad214b036": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017871504s
Jul 13 13:33:16.714: INFO: Pod "alpine-nnp-false-289201f9-37e1-43fd-ae5c-2ebad214b036" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:33:16.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7915" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":213,"skipped":3725,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:33:16.747: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 13:33:16.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9b3b1e5-c08d-4763-9dbd-34a4647a901e" in namespace "projected-9409" to be "success or failure"
Jul 13 13:33:16.834: INFO: Pod "downwardapi-volume-d9b3b1e5-c08d-4763-9dbd-34a4647a901e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.694141ms
Jul 13 13:33:18.839: INFO: Pod "downwardapi-volume-d9b3b1e5-c08d-4763-9dbd-34a4647a901e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022954017s
Jul 13 13:33:20.844: INFO: Pod "downwardapi-volume-d9b3b1e5-c08d-4763-9dbd-34a4647a901e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027561154s
STEP: Saw pod success
Jul 13 13:33:20.844: INFO: Pod "downwardapi-volume-d9b3b1e5-c08d-4763-9dbd-34a4647a901e" satisfied condition "success or failure"
Jul 13 13:33:20.848: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod downwardapi-volume-d9b3b1e5-c08d-4763-9dbd-34a4647a901e container client-container: <nil>
STEP: delete the pod
Jul 13 13:33:20.898: INFO: Waiting for pod downwardapi-volume-d9b3b1e5-c08d-4763-9dbd-34a4647a901e to disappear
Jul 13 13:33:20.905: INFO: Pod downwardapi-volume-d9b3b1e5-c08d-4763-9dbd-34a4647a901e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:33:20.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9409" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":214,"skipped":3731,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:33:20.919: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 13:33:21.420: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 13 13:33:23.434: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730244001, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730244001, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730244001, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730244001, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 13:33:26.456: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:33:38.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3700" for this suite.
STEP: Destroying namespace "webhook-3700-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:17.810 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":280,"completed":215,"skipped":3733,"failed":0}
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:33:38.729: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-6217
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 13 13:33:38.771: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 13 13:34:04.981: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.234.53:8080/dial?request=hostname&protocol=http&host=10.20.171.251&port=8080&tries=1'] Namespace:pod-network-test-6217 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:34:04.981: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:34:05.162: INFO: Waiting for responses: map[]
Jul 13 13:34:05.167: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.234.53:8080/dial?request=hostname&protocol=http&host=10.20.92.126&port=8080&tries=1'] Namespace:pod-network-test-6217 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:34:05.167: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:34:05.310: INFO: Waiting for responses: map[]
Jul 13 13:34:05.315: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.234.53:8080/dial?request=hostname&protocol=http&host=10.20.168.187&port=8080&tries=1'] Namespace:pod-network-test-6217 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:34:05.315: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:34:05.465: INFO: Waiting for responses: map[]
Jul 13 13:34:05.470: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.234.53:8080/dial?request=hostname&protocol=http&host=10.20.23.173&port=8080&tries=1'] Namespace:pod-network-test-6217 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:34:05.470: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:34:05.616: INFO: Waiting for responses: map[]
Jul 13 13:34:05.621: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.20.234.53:8080/dial?request=hostname&protocol=http&host=10.20.234.54&port=8080&tries=1'] Namespace:pod-network-test-6217 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:34:05.621: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:34:05.764: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:34:05.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6217" for this suite.

â€¢ [SLOW TEST:27.050 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":216,"skipped":3735,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:34:05.780: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:34:21.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3204" for this suite.

â€¢ [SLOW TEST:16.121 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":280,"completed":217,"skipped":3743,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:34:21.901: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:34:27.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6361" for this suite.

â€¢ [SLOW TEST:5.709 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":280,"completed":218,"skipped":3783,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:34:27.611: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1489
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 13 13:34:27.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9405'
Jul 13 13:34:28.574: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 13 13:34:28.574: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1495
Jul 13 13:34:30.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete deployment e2e-test-httpd-deployment --namespace=kubectl-9405'
Jul 13 13:34:30.672: INFO: stderr: ""
Jul 13 13:34:30.672: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:34:30.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9405" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":280,"completed":219,"skipped":3791,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:34:30.686: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-b3be2d85-503b-4902-b11d-0fc473348fbe
STEP: Creating a pod to test consume configMaps
Jul 13 13:34:30.743: INFO: Waiting up to 5m0s for pod "pod-configmaps-a5cb7cea-087d-4e75-a7b9-09d8d543e6a2" in namespace "configmap-2750" to be "success or failure"
Jul 13 13:34:30.752: INFO: Pod "pod-configmaps-a5cb7cea-087d-4e75-a7b9-09d8d543e6a2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.737185ms
Jul 13 13:34:32.758: INFO: Pod "pod-configmaps-a5cb7cea-087d-4e75-a7b9-09d8d543e6a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015185886s
Jul 13 13:34:34.763: INFO: Pod "pod-configmaps-a5cb7cea-087d-4e75-a7b9-09d8d543e6a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020483104s
STEP: Saw pod success
Jul 13 13:34:34.763: INFO: Pod "pod-configmaps-a5cb7cea-087d-4e75-a7b9-09d8d543e6a2" satisfied condition "success or failure"
Jul 13 13:34:34.767: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-configmaps-a5cb7cea-087d-4e75-a7b9-09d8d543e6a2 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 13:34:34.793: INFO: Waiting for pod pod-configmaps-a5cb7cea-087d-4e75-a7b9-09d8d543e6a2 to disappear
Jul 13 13:34:34.799: INFO: Pod pod-configmaps-a5cb7cea-087d-4e75-a7b9-09d8d543e6a2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:34:34.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2750" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":220,"skipped":3791,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:34:34.815: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-xr8gt in namespace proxy-5591
I0713 13:34:34.893252      20 runners.go:189] Created replication controller with name: proxy-service-xr8gt, namespace: proxy-5591, replica count: 1
I0713 13:34:35.943682      20 runners.go:189] proxy-service-xr8gt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0713 13:34:36.944073      20 runners.go:189] proxy-service-xr8gt Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0713 13:34:37.944391      20 runners.go:189] proxy-service-xr8gt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0713 13:34:38.944708      20 runners.go:189] proxy-service-xr8gt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0713 13:34:39.945034      20 runners.go:189] proxy-service-xr8gt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0713 13:34:40.945313      20 runners.go:189] proxy-service-xr8gt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0713 13:34:41.945539      20 runners.go:189] proxy-service-xr8gt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0713 13:34:42.945826      20 runners.go:189] proxy-service-xr8gt Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0713 13:34:43.955765      20 runners.go:189] proxy-service-xr8gt Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 13 13:34:43.960: INFO: setup took 9.096118656s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 13 13:34:43.973: INFO: (0) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 12.407539ms)
Jul 13 13:34:43.973: INFO: (0) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 13.196248ms)
Jul 13 13:34:43.975: INFO: (0) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 15.138644ms)
Jul 13 13:34:43.975: INFO: (0) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 14.498502ms)
Jul 13 13:34:43.976: INFO: (0) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 15.280045ms)
Jul 13 13:34:43.976: INFO: (0) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 16.642829ms)
Jul 13 13:34:43.977: INFO: (0) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 17.067237ms)
Jul 13 13:34:43.978: INFO: (0) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 18.222289ms)
Jul 13 13:34:43.978: INFO: (0) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 18.223608ms)
Jul 13 13:34:43.978: INFO: (0) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 18.229392ms)
Jul 13 13:34:43.978: INFO: (0) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 17.985667ms)
Jul 13 13:34:43.983: INFO: (0) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 23.5189ms)
Jul 13 13:34:43.984: INFO: (0) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 23.537794ms)
Jul 13 13:34:43.984: INFO: (0) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 23.269916ms)
Jul 13 13:34:43.992: INFO: (0) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 32.023793ms)
Jul 13 13:34:43.992: INFO: (0) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 31.668028ms)
Jul 13 13:34:44.008: INFO: (1) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 15.705443ms)
Jul 13 13:34:44.008: INFO: (1) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 15.401661ms)
Jul 13 13:34:44.008: INFO: (1) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 15.157661ms)
Jul 13 13:34:44.009: INFO: (1) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 15.524496ms)
Jul 13 13:34:44.009: INFO: (1) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 16.345509ms)
Jul 13 13:34:44.009: INFO: (1) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 16.1324ms)
Jul 13 13:34:44.009: INFO: (1) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 16.422022ms)
Jul 13 13:34:44.009: INFO: (1) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 16.663267ms)
Jul 13 13:34:44.009: INFO: (1) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 16.016303ms)
Jul 13 13:34:44.011: INFO: (1) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 18.411406ms)
Jul 13 13:34:44.012: INFO: (1) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 19.089081ms)
Jul 13 13:34:44.012: INFO: (1) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 19.199323ms)
Jul 13 13:34:44.012: INFO: (1) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 19.150111ms)
Jul 13 13:34:44.013: INFO: (1) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 20.125501ms)
Jul 13 13:34:44.013: INFO: (1) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 20.207959ms)
Jul 13 13:34:44.013: INFO: (1) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 20.894679ms)
Jul 13 13:34:44.024: INFO: (2) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 11.076443ms)
Jul 13 13:34:44.025: INFO: (2) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 11.066479ms)
Jul 13 13:34:44.025: INFO: (2) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 11.090218ms)
Jul 13 13:34:44.025: INFO: (2) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 11.612086ms)
Jul 13 13:34:44.025: INFO: (2) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 11.875213ms)
Jul 13 13:34:44.025: INFO: (2) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 11.554587ms)
Jul 13 13:34:44.026: INFO: (2) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 12.082135ms)
Jul 13 13:34:44.025: INFO: (2) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 11.904588ms)
Jul 13 13:34:44.025: INFO: (2) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 11.985427ms)
Jul 13 13:34:44.026: INFO: (2) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 12.248975ms)
Jul 13 13:34:44.027: INFO: (2) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 13.40672ms)
Jul 13 13:34:44.028: INFO: (2) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 14.682148ms)
Jul 13 13:34:44.028: INFO: (2) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 14.639997ms)
Jul 13 13:34:44.028: INFO: (2) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 14.608937ms)
Jul 13 13:34:44.029: INFO: (2) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 15.535141ms)
Jul 13 13:34:44.029: INFO: (2) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 15.414376ms)
Jul 13 13:34:44.035: INFO: (3) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 5.784037ms)
Jul 13 13:34:44.036: INFO: (3) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 7.293804ms)
Jul 13 13:34:44.036: INFO: (3) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 7.40507ms)
Jul 13 13:34:44.037: INFO: (3) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 7.885626ms)
Jul 13 13:34:44.038: INFO: (3) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 8.561689ms)
Jul 13 13:34:44.038: INFO: (3) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 9.040266ms)
Jul 13 13:34:44.038: INFO: (3) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 9.096634ms)
Jul 13 13:34:44.039: INFO: (3) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 9.515533ms)
Jul 13 13:34:44.039: INFO: (3) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 9.592835ms)
Jul 13 13:34:44.040: INFO: (3) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 10.341241ms)
Jul 13 13:34:44.042: INFO: (3) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 12.370099ms)
Jul 13 13:34:44.042: INFO: (3) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 12.935696ms)
Jul 13 13:34:44.043: INFO: (3) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 13.105959ms)
Jul 13 13:34:44.043: INFO: (3) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 13.16184ms)
Jul 13 13:34:44.043: INFO: (3) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 13.63264ms)
Jul 13 13:34:44.043: INFO: (3) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 13.874712ms)
Jul 13 13:34:44.054: INFO: (4) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 10.87158ms)
Jul 13 13:34:44.054: INFO: (4) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 10.381892ms)
Jul 13 13:34:44.055: INFO: (4) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 10.813973ms)
Jul 13 13:34:44.055: INFO: (4) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 10.814297ms)
Jul 13 13:34:44.055: INFO: (4) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 11.433602ms)
Jul 13 13:34:44.055: INFO: (4) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 11.68783ms)
Jul 13 13:34:44.055: INFO: (4) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 12.008606ms)
Jul 13 13:34:44.056: INFO: (4) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 12.39874ms)
Jul 13 13:34:44.056: INFO: (4) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 12.070514ms)
Jul 13 13:34:44.056: INFO: (4) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 12.493867ms)
Jul 13 13:34:44.056: INFO: (4) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 11.892168ms)
Jul 13 13:34:44.057: INFO: (4) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 13.586058ms)
Jul 13 13:34:44.059: INFO: (4) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 14.769741ms)
Jul 13 13:34:44.059: INFO: (4) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 14.927669ms)
Jul 13 13:34:44.059: INFO: (4) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 15.237321ms)
Jul 13 13:34:44.059: INFO: (4) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 15.147189ms)
Jul 13 13:34:44.065: INFO: (5) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 5.963711ms)
Jul 13 13:34:44.066: INFO: (5) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 6.393045ms)
Jul 13 13:34:44.066: INFO: (5) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 6.66543ms)
Jul 13 13:34:44.066: INFO: (5) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 6.841311ms)
Jul 13 13:34:44.067: INFO: (5) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 7.247401ms)
Jul 13 13:34:44.069: INFO: (5) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 8.593687ms)
Jul 13 13:34:44.069: INFO: (5) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 9.358567ms)
Jul 13 13:34:44.070: INFO: (5) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 10.099546ms)
Jul 13 13:34:44.070: INFO: (5) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 10.298445ms)
Jul 13 13:34:44.071: INFO: (5) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 10.828098ms)
Jul 13 13:34:44.072: INFO: (5) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 11.611893ms)
Jul 13 13:34:44.072: INFO: (5) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 12.037759ms)
Jul 13 13:34:44.072: INFO: (5) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 11.980562ms)
Jul 13 13:34:44.073: INFO: (5) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 12.68811ms)
Jul 13 13:34:44.073: INFO: (5) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 12.490292ms)
Jul 13 13:34:44.073: INFO: (5) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 12.683906ms)
Jul 13 13:34:44.080: INFO: (6) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 6.89917ms)
Jul 13 13:34:44.083: INFO: (6) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 9.468411ms)
Jul 13 13:34:44.083: INFO: (6) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 9.188429ms)
Jul 13 13:34:44.086: INFO: (6) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 12.329083ms)
Jul 13 13:34:44.086: INFO: (6) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 12.921367ms)
Jul 13 13:34:44.087: INFO: (6) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 13.019802ms)
Jul 13 13:34:44.087: INFO: (6) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 12.907999ms)
Jul 13 13:34:44.087: INFO: (6) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 13.074658ms)
Jul 13 13:34:44.087: INFO: (6) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 13.372648ms)
Jul 13 13:34:44.087: INFO: (6) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 13.813666ms)
Jul 13 13:34:44.088: INFO: (6) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 14.141133ms)
Jul 13 13:34:44.088: INFO: (6) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 13.86678ms)
Jul 13 13:34:44.088: INFO: (6) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 14.797876ms)
Jul 13 13:34:44.088: INFO: (6) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 14.833585ms)
Jul 13 13:34:44.089: INFO: (6) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 15.181651ms)
Jul 13 13:34:44.089: INFO: (6) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 15.269345ms)
Jul 13 13:34:44.097: INFO: (7) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 7.20061ms)
Jul 13 13:34:44.105: INFO: (7) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 15.325843ms)
Jul 13 13:34:44.105: INFO: (7) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 15.79707ms)
Jul 13 13:34:44.105: INFO: (7) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 15.62986ms)
Jul 13 13:34:44.105: INFO: (7) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 15.39418ms)
Jul 13 13:34:44.105: INFO: (7) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 15.341221ms)
Jul 13 13:34:44.105: INFO: (7) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 16.100253ms)
Jul 13 13:34:44.107: INFO: (7) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 17.278004ms)
Jul 13 13:34:44.109: INFO: (7) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 19.005086ms)
Jul 13 13:34:44.110: INFO: (7) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 20.013272ms)
Jul 13 13:34:44.110: INFO: (7) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 20.645476ms)
Jul 13 13:34:44.110: INFO: (7) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 20.266933ms)
Jul 13 13:34:44.110: INFO: (7) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 20.885498ms)
Jul 13 13:34:44.110: INFO: (7) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 20.426407ms)
Jul 13 13:34:44.111: INFO: (7) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 21.938231ms)
Jul 13 13:34:44.112: INFO: (7) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 22.150007ms)
Jul 13 13:34:44.120: INFO: (8) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 7.939321ms)
Jul 13 13:34:44.121: INFO: (8) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 8.487118ms)
Jul 13 13:34:44.121: INFO: (8) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 8.724803ms)
Jul 13 13:34:44.121: INFO: (8) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 8.614658ms)
Jul 13 13:34:44.121: INFO: (8) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 8.855652ms)
Jul 13 13:34:44.122: INFO: (8) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 10.002122ms)
Jul 13 13:34:44.122: INFO: (8) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 9.992427ms)
Jul 13 13:34:44.123: INFO: (8) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 10.021784ms)
Jul 13 13:34:44.123: INFO: (8) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 11.049175ms)
Jul 13 13:34:44.124: INFO: (8) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 11.994791ms)
Jul 13 13:34:44.125: INFO: (8) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 12.220669ms)
Jul 13 13:34:44.125: INFO: (8) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 12.650217ms)
Jul 13 13:34:44.125: INFO: (8) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 13.034872ms)
Jul 13 13:34:44.126: INFO: (8) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 13.092133ms)
Jul 13 13:34:44.127: INFO: (8) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 14.593504ms)
Jul 13 13:34:44.129: INFO: (8) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 16.205841ms)
Jul 13 13:34:44.139: INFO: (9) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 9.784207ms)
Jul 13 13:34:44.139: INFO: (9) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 10.117382ms)
Jul 13 13:34:44.139: INFO: (9) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 9.8928ms)
Jul 13 13:34:44.139: INFO: (9) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 10.118798ms)
Jul 13 13:34:44.139: INFO: (9) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 9.987417ms)
Jul 13 13:34:44.139: INFO: (9) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 10.378483ms)
Jul 13 13:34:44.139: INFO: (9) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 10.157281ms)
Jul 13 13:34:44.140: INFO: (9) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 10.84699ms)
Jul 13 13:34:44.140: INFO: (9) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 10.900583ms)
Jul 13 13:34:44.140: INFO: (9) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 10.818597ms)
Jul 13 13:34:44.140: INFO: (9) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 11.055078ms)
Jul 13 13:34:44.142: INFO: (9) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 12.696744ms)
Jul 13 13:34:44.143: INFO: (9) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 14.453147ms)
Jul 13 13:34:44.143: INFO: (9) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 14.450712ms)
Jul 13 13:34:44.144: INFO: (9) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 14.680895ms)
Jul 13 13:34:44.144: INFO: (9) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 14.796691ms)
Jul 13 13:34:44.151: INFO: (10) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 7.311307ms)
Jul 13 13:34:44.151: INFO: (10) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 7.350532ms)
Jul 13 13:34:44.153: INFO: (10) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 8.400212ms)
Jul 13 13:34:44.153: INFO: (10) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 8.719276ms)
Jul 13 13:34:44.154: INFO: (10) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 9.7669ms)
Jul 13 13:34:44.155: INFO: (10) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 10.48495ms)
Jul 13 13:34:44.156: INFO: (10) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 11.202869ms)
Jul 13 13:34:44.156: INFO: (10) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 11.620359ms)
Jul 13 13:34:44.157: INFO: (10) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 12.417052ms)
Jul 13 13:34:44.157: INFO: (10) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 12.816461ms)
Jul 13 13:34:44.158: INFO: (10) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 13.803821ms)
Jul 13 13:34:44.159: INFO: (10) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 15.148952ms)
Jul 13 13:34:44.160: INFO: (10) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 15.209562ms)
Jul 13 13:34:44.160: INFO: (10) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 16.030482ms)
Jul 13 13:34:44.161: INFO: (10) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 16.334504ms)
Jul 13 13:34:44.162: INFO: (10) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 17.262559ms)
Jul 13 13:34:44.173: INFO: (11) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 11.755762ms)
Jul 13 13:34:44.173: INFO: (11) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 11.454902ms)
Jul 13 13:34:44.174: INFO: (11) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 10.966193ms)
Jul 13 13:34:44.174: INFO: (11) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 11.55725ms)
Jul 13 13:34:44.174: INFO: (11) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 11.598485ms)
Jul 13 13:34:44.174: INFO: (11) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 11.688029ms)
Jul 13 13:34:44.174: INFO: (11) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 11.739107ms)
Jul 13 13:34:44.174: INFO: (11) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 11.482488ms)
Jul 13 13:34:44.174: INFO: (11) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 11.071535ms)
Jul 13 13:34:44.174: INFO: (11) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 11.813642ms)
Jul 13 13:34:44.176: INFO: (11) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 13.083375ms)
Jul 13 13:34:44.177: INFO: (11) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 14.469479ms)
Jul 13 13:34:44.179: INFO: (11) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 15.970582ms)
Jul 13 13:34:44.180: INFO: (11) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 16.772969ms)
Jul 13 13:34:44.181: INFO: (11) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 17.589876ms)
Jul 13 13:34:44.181: INFO: (11) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 18.296695ms)
Jul 13 13:34:44.191: INFO: (12) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 8.589759ms)
Jul 13 13:34:44.192: INFO: (12) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 9.442271ms)
Jul 13 13:34:44.192: INFO: (12) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 10.181078ms)
Jul 13 13:34:44.192: INFO: (12) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 10.144ms)
Jul 13 13:34:44.192: INFO: (12) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 9.881844ms)
Jul 13 13:34:44.192: INFO: (12) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 10.737489ms)
Jul 13 13:34:44.193: INFO: (12) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 10.305012ms)
Jul 13 13:34:44.193: INFO: (12) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 11.227124ms)
Jul 13 13:34:44.193: INFO: (12) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 11.518851ms)
Jul 13 13:34:44.194: INFO: (12) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 12.257779ms)
Jul 13 13:34:44.195: INFO: (12) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 12.886517ms)
Jul 13 13:34:44.195: INFO: (12) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 13.195622ms)
Jul 13 13:34:44.195: INFO: (12) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 13.873231ms)
Jul 13 13:34:44.196: INFO: (12) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 13.407888ms)
Jul 13 13:34:44.196: INFO: (12) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 14.364937ms)
Jul 13 13:34:44.199: INFO: (12) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 16.89195ms)
Jul 13 13:34:44.210: INFO: (13) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 10.62969ms)
Jul 13 13:34:44.210: INFO: (13) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 10.470437ms)
Jul 13 13:34:44.211: INFO: (13) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 10.594785ms)
Jul 13 13:34:44.211: INFO: (13) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 10.790949ms)
Jul 13 13:34:44.211: INFO: (13) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 11.246753ms)
Jul 13 13:34:44.211: INFO: (13) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 11.395148ms)
Jul 13 13:34:44.211: INFO: (13) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 11.907996ms)
Jul 13 13:34:44.211: INFO: (13) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 11.559134ms)
Jul 13 13:34:44.212: INFO: (13) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 11.891927ms)
Jul 13 13:34:44.212: INFO: (13) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 12.453101ms)
Jul 13 13:34:44.213: INFO: (13) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 13.81785ms)
Jul 13 13:34:44.213: INFO: (13) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 13.517694ms)
Jul 13 13:34:44.214: INFO: (13) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 14.819084ms)
Jul 13 13:34:44.215: INFO: (13) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 14.932738ms)
Jul 13 13:34:44.215: INFO: (13) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 15.297895ms)
Jul 13 13:34:44.215: INFO: (13) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 15.549702ms)
Jul 13 13:34:44.221: INFO: (14) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 5.997267ms)
Jul 13 13:34:44.223: INFO: (14) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 7.941109ms)
Jul 13 13:34:44.223: INFO: (14) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 8.146699ms)
Jul 13 13:34:44.224: INFO: (14) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 8.408054ms)
Jul 13 13:34:44.224: INFO: (14) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 8.872129ms)
Jul 13 13:34:44.226: INFO: (14) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 9.809908ms)
Jul 13 13:34:44.226: INFO: (14) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 10.448017ms)
Jul 13 13:34:44.226: INFO: (14) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 10.709613ms)
Jul 13 13:34:44.226: INFO: (14) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 10.665323ms)
Jul 13 13:34:44.227: INFO: (14) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 11.251423ms)
Jul 13 13:34:44.227: INFO: (14) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 11.538208ms)
Jul 13 13:34:44.228: INFO: (14) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 12.347968ms)
Jul 13 13:34:44.230: INFO: (14) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 13.806912ms)
Jul 13 13:34:44.230: INFO: (14) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 14.213586ms)
Jul 13 13:34:44.230: INFO: (14) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 14.545418ms)
Jul 13 13:34:44.230: INFO: (14) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 14.813583ms)
Jul 13 13:34:44.237: INFO: (15) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 5.961996ms)
Jul 13 13:34:44.239: INFO: (15) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 8.272421ms)
Jul 13 13:34:44.240: INFO: (15) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 9.774187ms)
Jul 13 13:34:44.240: INFO: (15) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 9.699366ms)
Jul 13 13:34:44.241: INFO: (15) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 10.214019ms)
Jul 13 13:34:44.242: INFO: (15) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 10.868174ms)
Jul 13 13:34:44.242: INFO: (15) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 11.371161ms)
Jul 13 13:34:44.243: INFO: (15) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 11.571953ms)
Jul 13 13:34:44.244: INFO: (15) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 13.248801ms)
Jul 13 13:34:44.245: INFO: (15) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 14.038187ms)
Jul 13 13:34:44.245: INFO: (15) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 14.398306ms)
Jul 13 13:34:44.246: INFO: (15) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 15.288571ms)
Jul 13 13:34:44.247: INFO: (15) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 16.04831ms)
Jul 13 13:34:44.249: INFO: (15) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 18.211404ms)
Jul 13 13:34:44.250: INFO: (15) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 18.926211ms)
Jul 13 13:34:44.251: INFO: (15) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 19.938675ms)
Jul 13 13:34:44.261: INFO: (16) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 9.70069ms)
Jul 13 13:34:44.261: INFO: (16) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 9.558365ms)
Jul 13 13:34:44.263: INFO: (16) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 11.719839ms)
Jul 13 13:34:44.264: INFO: (16) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 12.213352ms)
Jul 13 13:34:44.264: INFO: (16) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 12.714153ms)
Jul 13 13:34:44.264: INFO: (16) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 13.118229ms)
Jul 13 13:34:44.264: INFO: (16) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 12.330911ms)
Jul 13 13:34:44.265: INFO: (16) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 12.849461ms)
Jul 13 13:34:44.265: INFO: (16) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 12.981989ms)
Jul 13 13:34:44.265: INFO: (16) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 13.206228ms)
Jul 13 13:34:44.265: INFO: (16) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 14.234676ms)
Jul 13 13:34:44.269: INFO: (16) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 17.228575ms)
Jul 13 13:34:44.269: INFO: (16) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 17.765878ms)
Jul 13 13:34:44.271: INFO: (16) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 18.962952ms)
Jul 13 13:34:44.271: INFO: (16) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 18.825089ms)
Jul 13 13:34:44.271: INFO: (16) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 19.232286ms)
Jul 13 13:34:44.279: INFO: (17) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 8.327205ms)
Jul 13 13:34:44.280: INFO: (17) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 8.78621ms)
Jul 13 13:34:44.280: INFO: (17) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 9.082761ms)
Jul 13 13:34:44.280: INFO: (17) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 9.437465ms)
Jul 13 13:34:44.281: INFO: (17) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 9.848982ms)
Jul 13 13:34:44.284: INFO: (17) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 12.301938ms)
Jul 13 13:34:44.284: INFO: (17) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 12.471653ms)
Jul 13 13:34:44.285: INFO: (17) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 13.976474ms)
Jul 13 13:34:44.285: INFO: (17) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 14.181807ms)
Jul 13 13:34:44.285: INFO: (17) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 14.03311ms)
Jul 13 13:34:44.285: INFO: (17) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 14.106377ms)
Jul 13 13:34:44.285: INFO: (17) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 14.093766ms)
Jul 13 13:34:44.286: INFO: (17) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 14.855264ms)
Jul 13 13:34:44.286: INFO: (17) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 15.318651ms)
Jul 13 13:34:44.287: INFO: (17) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 15.915476ms)
Jul 13 13:34:44.287: INFO: (17) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 16.368632ms)
Jul 13 13:34:44.297: INFO: (18) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 9.000358ms)
Jul 13 13:34:44.297: INFO: (18) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 8.973784ms)
Jul 13 13:34:44.297: INFO: (18) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 9.168493ms)
Jul 13 13:34:44.298: INFO: (18) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 10.45704ms)
Jul 13 13:34:44.299: INFO: (18) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 10.683298ms)
Jul 13 13:34:44.299: INFO: (18) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 11.212726ms)
Jul 13 13:34:44.299: INFO: (18) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 11.377435ms)
Jul 13 13:34:44.301: INFO: (18) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 13.093578ms)
Jul 13 13:34:44.303: INFO: (18) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 14.455185ms)
Jul 13 13:34:44.303: INFO: (18) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 15.207679ms)
Jul 13 13:34:44.303: INFO: (18) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 15.417706ms)
Jul 13 13:34:44.303: INFO: (18) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 15.417175ms)
Jul 13 13:34:44.304: INFO: (18) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 15.655071ms)
Jul 13 13:34:44.306: INFO: (18) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 17.881035ms)
Jul 13 13:34:44.306: INFO: (18) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 18.256035ms)
Jul 13 13:34:44.306: INFO: (18) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 18.233393ms)
Jul 13 13:34:44.313: INFO: (19) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:1080/proxy/rewriteme">test<... (200; 6.048462ms)
Jul 13 13:34:44.316: INFO: (19) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:462/proxy/: tls qux (200; 9.687925ms)
Jul 13 13:34:44.317: INFO: (19) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:162/proxy/: bar (200; 10.251999ms)
Jul 13 13:34:44.320: INFO: (19) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml/proxy/rewriteme">test</a> (200; 12.675023ms)
Jul 13 13:34:44.320: INFO: (19) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:443/proxy/tlsrewritem... (200; 12.932381ms)
Jul 13 13:34:44.320: INFO: (19) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:160/proxy/: foo (200; 13.064064ms)
Jul 13 13:34:44.320: INFO: (19) /api/v1/namespaces/proxy-5591/pods/https:proxy-service-xr8gt-rddml:460/proxy/: tls baz (200; 13.374419ms)
Jul 13 13:34:44.321: INFO: (19) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:162/proxy/: bar (200; 13.318217ms)
Jul 13 13:34:44.321: INFO: (19) /api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/: <a href="/api/v1/namespaces/proxy-5591/pods/http:proxy-service-xr8gt-rddml:1080/proxy/rewriteme">... (200; 13.962757ms)
Jul 13 13:34:44.321: INFO: (19) /api/v1/namespaces/proxy-5591/pods/proxy-service-xr8gt-rddml:160/proxy/: foo (200; 13.607621ms)
Jul 13 13:34:44.323: INFO: (19) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname1/proxy/: tls baz (200; 16.067917ms)
Jul 13 13:34:44.324: INFO: (19) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname2/proxy/: bar (200; 16.223796ms)
Jul 13 13:34:44.325: INFO: (19) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname1/proxy/: foo (200; 17.325951ms)
Jul 13 13:34:44.325: INFO: (19) /api/v1/namespaces/proxy-5591/services/https:proxy-service-xr8gt:tlsportname2/proxy/: tls qux (200; 17.931776ms)
Jul 13 13:34:44.326: INFO: (19) /api/v1/namespaces/proxy-5591/services/http:proxy-service-xr8gt:portname1/proxy/: foo (200; 18.656585ms)
Jul 13 13:34:44.326: INFO: (19) /api/v1/namespaces/proxy-5591/services/proxy-service-xr8gt:portname2/proxy/: bar (200; 18.413599ms)
STEP: deleting ReplicationController proxy-service-xr8gt in namespace proxy-5591, will wait for the garbage collector to delete the pods
Jul 13 13:34:44.389: INFO: Deleting ReplicationController proxy-service-xr8gt took: 9.070835ms
Jul 13 13:34:44.490: INFO: Terminating ReplicationController proxy-service-xr8gt pods took: 100.484115ms
[AfterEach] version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:34:51.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5591" for this suite.

â€¢ [SLOW TEST:16.493 seconds]
[sig-network] Proxy
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":280,"completed":221,"skipped":3801,"failed":0}
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:34:51.310: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:35:02.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1817" for this suite.

â€¢ [SLOW TEST:11.115 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":280,"completed":222,"skipped":3801,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:35:02.425: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 13 13:35:03.344: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 13 13:35:05.358: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730244103, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730244103, loc:(*time.Location)(0x7925200)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730244103, loc:(*time.Location)(0x7925200)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730244103, loc:(*time.Location)(0x7925200)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 13 13:35:08.381: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:35:08.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2102" for this suite.
STEP: Destroying namespace "webhook-2102-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:6.143 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":280,"completed":223,"skipped":3806,"failed":0}
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:35:08.568: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:35:30.638: INFO: Container started at 2020-07-13 13:35:09 +0000 UTC, pod became ready at 2020-07-13 13:35:30 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:35:30.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2541" for this suite.

â€¢ [SLOW TEST:22.085 seconds]
[k8s.io] Probing container
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":280,"completed":224,"skipped":3806,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:35:30.655: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 13:35:30.718: INFO: Waiting up to 5m0s for pod "downwardapi-volume-256a2c43-b35c-435f-a61c-04414b7b1f91" in namespace "projected-7919" to be "success or failure"
Jul 13 13:35:30.734: INFO: Pod "downwardapi-volume-256a2c43-b35c-435f-a61c-04414b7b1f91": Phase="Pending", Reason="", readiness=false. Elapsed: 16.042267ms
Jul 13 13:35:32.739: INFO: Pod "downwardapi-volume-256a2c43-b35c-435f-a61c-04414b7b1f91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020940011s
STEP: Saw pod success
Jul 13 13:35:32.739: INFO: Pod "downwardapi-volume-256a2c43-b35c-435f-a61c-04414b7b1f91" satisfied condition "success or failure"
Jul 13 13:35:32.743: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod downwardapi-volume-256a2c43-b35c-435f-a61c-04414b7b1f91 container client-container: <nil>
STEP: delete the pod
Jul 13 13:35:32.791: INFO: Waiting for pod downwardapi-volume-256a2c43-b35c-435f-a61c-04414b7b1f91 to disappear
Jul 13 13:35:32.795: INFO: Pod downwardapi-volume-256a2c43-b35c-435f-a61c-04414b7b1f91 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:35:32.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7919" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":225,"skipped":3814,"failed":0}

------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:35:32.810: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jul 13 13:35:32.869: INFO: Created pod &Pod{ObjectMeta:{dns-4841  dns-4841 /api/v1/namespaces/dns-4841/pods/dns-4841 28e4e573-0d0f-4abb-9d50-701896aa4a00 240804 0 2020-07-13 13:35:32 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dv8v8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dv8v8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dv8v8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Jul 13 13:35:36.880: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4841 PodName:dns-4841 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:35:36.880: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Verifying customized DNS server is configured on pod...
Jul 13 13:35:37.019: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4841 PodName:dns-4841 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:35:37.019: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:35:37.144: INFO: Deleting pod dns-4841...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:35:37.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4841" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":280,"completed":226,"skipped":3814,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:35:37.179: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:35:37.223: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:35:38.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9607" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":280,"completed":227,"skipped":3816,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:35:38.447: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:35:45.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2613" for this suite.

â€¢ [SLOW TEST:7.147 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":280,"completed":228,"skipped":3832,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:35:45.594: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 13 13:35:48.684: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:35:49.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1322" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":280,"completed":229,"skipped":3835,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:35:49.723: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-eeaa5d71-d007-42a7-8ea5-f6dca6993f28
STEP: Creating a pod to test consume secrets
Jul 13 13:35:49.788: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6101762f-93b0-4376-b539-61950ca0ddea" in namespace "projected-9849" to be "success or failure"
Jul 13 13:35:49.796: INFO: Pod "pod-projected-secrets-6101762f-93b0-4376-b539-61950ca0ddea": Phase="Pending", Reason="", readiness=false. Elapsed: 8.612879ms
Jul 13 13:35:51.802: INFO: Pod "pod-projected-secrets-6101762f-93b0-4376-b539-61950ca0ddea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014484882s
STEP: Saw pod success
Jul 13 13:35:51.802: INFO: Pod "pod-projected-secrets-6101762f-93b0-4376-b539-61950ca0ddea" satisfied condition "success or failure"
Jul 13 13:35:51.807: INFO: Trying to get logs from node ip-10-0-3-156.us-east-2.compute.internal pod pod-projected-secrets-6101762f-93b0-4376-b539-61950ca0ddea container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 13 13:35:51.860: INFO: Waiting for pod pod-projected-secrets-6101762f-93b0-4376-b539-61950ca0ddea to disappear
Jul 13 13:35:51.863: INFO: Pod pod-projected-secrets-6101762f-93b0-4376-b539-61950ca0ddea no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:35:51.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9849" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":230,"skipped":3845,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:35:51.894: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-465p
STEP: Creating a pod to test atomic-volume-subpath
Jul 13 13:35:51.964: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-465p" in namespace "subpath-8117" to be "success or failure"
Jul 13 13:35:51.970: INFO: Pod "pod-subpath-test-downwardapi-465p": Phase="Pending", Reason="", readiness=false. Elapsed: 6.547974ms
Jul 13 13:35:53.976: INFO: Pod "pod-subpath-test-downwardapi-465p": Phase="Running", Reason="", readiness=true. Elapsed: 2.011678404s
Jul 13 13:35:55.981: INFO: Pod "pod-subpath-test-downwardapi-465p": Phase="Running", Reason="", readiness=true. Elapsed: 4.016884557s
Jul 13 13:35:57.986: INFO: Pod "pod-subpath-test-downwardapi-465p": Phase="Running", Reason="", readiness=true. Elapsed: 6.02179514s
Jul 13 13:35:59.991: INFO: Pod "pod-subpath-test-downwardapi-465p": Phase="Running", Reason="", readiness=true. Elapsed: 8.026641146s
Jul 13 13:36:01.996: INFO: Pod "pod-subpath-test-downwardapi-465p": Phase="Running", Reason="", readiness=true. Elapsed: 10.032068456s
Jul 13 13:36:04.001: INFO: Pod "pod-subpath-test-downwardapi-465p": Phase="Running", Reason="", readiness=true. Elapsed: 12.037306283s
Jul 13 13:36:06.006: INFO: Pod "pod-subpath-test-downwardapi-465p": Phase="Running", Reason="", readiness=true. Elapsed: 14.042511702s
Jul 13 13:36:08.011: INFO: Pod "pod-subpath-test-downwardapi-465p": Phase="Running", Reason="", readiness=true. Elapsed: 16.047221767s
Jul 13 13:36:10.017: INFO: Pod "pod-subpath-test-downwardapi-465p": Phase="Running", Reason="", readiness=true. Elapsed: 18.052644708s
Jul 13 13:36:12.022: INFO: Pod "pod-subpath-test-downwardapi-465p": Phase="Running", Reason="", readiness=true. Elapsed: 20.058064735s
Jul 13 13:36:14.027: INFO: Pod "pod-subpath-test-downwardapi-465p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.062867604s
STEP: Saw pod success
Jul 13 13:36:14.027: INFO: Pod "pod-subpath-test-downwardapi-465p" satisfied condition "success or failure"
Jul 13 13:36:14.031: INFO: Trying to get logs from node ip-10-0-3-156.us-east-2.compute.internal pod pod-subpath-test-downwardapi-465p container test-container-subpath-downwardapi-465p: <nil>
STEP: delete the pod
Jul 13 13:36:14.083: INFO: Waiting for pod pod-subpath-test-downwardapi-465p to disappear
Jul 13 13:36:14.089: INFO: Pod pod-subpath-test-downwardapi-465p no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-465p
Jul 13 13:36:14.090: INFO: Deleting pod "pod-subpath-test-downwardapi-465p" in namespace "subpath-8117"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:36:14.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8117" for this suite.

â€¢ [SLOW TEST:22.214 seconds]
[sig-storage] Subpath
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":280,"completed":231,"skipped":3851,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:36:14.111: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-701b6365-71e0-4920-b715-15b107635a38
STEP: Creating a pod to test consume secrets
Jul 13 13:36:14.181: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-637fe419-5894-499f-bb6b-9fb7898d99b9" in namespace "projected-7667" to be "success or failure"
Jul 13 13:36:14.193: INFO: Pod "pod-projected-secrets-637fe419-5894-499f-bb6b-9fb7898d99b9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.078998ms
Jul 13 13:36:16.199: INFO: Pod "pod-projected-secrets-637fe419-5894-499f-bb6b-9fb7898d99b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018005749s
Jul 13 13:36:18.204: INFO: Pod "pod-projected-secrets-637fe419-5894-499f-bb6b-9fb7898d99b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023209067s
STEP: Saw pod success
Jul 13 13:36:18.204: INFO: Pod "pod-projected-secrets-637fe419-5894-499f-bb6b-9fb7898d99b9" satisfied condition "success or failure"
Jul 13 13:36:18.208: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-projected-secrets-637fe419-5894-499f-bb6b-9fb7898d99b9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 13 13:36:18.274: INFO: Waiting for pod pod-projected-secrets-637fe419-5894-499f-bb6b-9fb7898d99b9 to disappear
Jul 13 13:36:18.278: INFO: Pod pod-projected-secrets-637fe419-5894-499f-bb6b-9fb7898d99b9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:36:18.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7667" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":232,"skipped":3869,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:36:18.294: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 13 13:36:18.350: INFO: Waiting up to 5m0s for pod "pod-143de656-0d16-41b5-9177-5a142e769a14" in namespace "emptydir-1134" to be "success or failure"
Jul 13 13:36:18.359: INFO: Pod "pod-143de656-0d16-41b5-9177-5a142e769a14": Phase="Pending", Reason="", readiness=false. Elapsed: 8.509926ms
Jul 13 13:36:20.364: INFO: Pod "pod-143de656-0d16-41b5-9177-5a142e769a14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01362934s
STEP: Saw pod success
Jul 13 13:36:20.364: INFO: Pod "pod-143de656-0d16-41b5-9177-5a142e769a14" satisfied condition "success or failure"
Jul 13 13:36:20.367: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-143de656-0d16-41b5-9177-5a142e769a14 container test-container: <nil>
STEP: delete the pod
Jul 13 13:36:20.396: INFO: Waiting for pod pod-143de656-0d16-41b5-9177-5a142e769a14 to disappear
Jul 13 13:36:20.403: INFO: Pod pod-143de656-0d16-41b5-9177-5a142e769a14 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:36:20.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1134" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":233,"skipped":3876,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:36:20.419: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-9626ddcc-41e6-4e43-9ade-545147900606
STEP: Creating a pod to test consume configMaps
Jul 13 13:36:20.488: INFO: Waiting up to 5m0s for pod "pod-configmaps-0123b895-2f29-4338-978f-dc4742bd0012" in namespace "configmap-5532" to be "success or failure"
Jul 13 13:36:20.497: INFO: Pod "pod-configmaps-0123b895-2f29-4338-978f-dc4742bd0012": Phase="Pending", Reason="", readiness=false. Elapsed: 9.441184ms
Jul 13 13:36:22.502: INFO: Pod "pod-configmaps-0123b895-2f29-4338-978f-dc4742bd0012": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014509348s
Jul 13 13:36:24.507: INFO: Pod "pod-configmaps-0123b895-2f29-4338-978f-dc4742bd0012": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01979136s
STEP: Saw pod success
Jul 13 13:36:24.508: INFO: Pod "pod-configmaps-0123b895-2f29-4338-978f-dc4742bd0012" satisfied condition "success or failure"
Jul 13 13:36:24.517: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-configmaps-0123b895-2f29-4338-978f-dc4742bd0012 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 13:36:24.557: INFO: Waiting for pod pod-configmaps-0123b895-2f29-4338-978f-dc4742bd0012 to disappear
Jul 13 13:36:24.561: INFO: Pod pod-configmaps-0123b895-2f29-4338-978f-dc4742bd0012 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:36:24.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5532" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":234,"skipped":3887,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:36:24.576: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 13:36:24.641: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab37a9b3-36dd-4603-af3d-1620e987fad9" in namespace "projected-7806" to be "success or failure"
Jul 13 13:36:24.648: INFO: Pod "downwardapi-volume-ab37a9b3-36dd-4603-af3d-1620e987fad9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.316871ms
Jul 13 13:36:26.656: INFO: Pod "downwardapi-volume-ab37a9b3-36dd-4603-af3d-1620e987fad9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014127335s
Jul 13 13:36:28.660: INFO: Pod "downwardapi-volume-ab37a9b3-36dd-4603-af3d-1620e987fad9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019027286s
STEP: Saw pod success
Jul 13 13:36:28.660: INFO: Pod "downwardapi-volume-ab37a9b3-36dd-4603-af3d-1620e987fad9" satisfied condition "success or failure"
Jul 13 13:36:28.665: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod downwardapi-volume-ab37a9b3-36dd-4603-af3d-1620e987fad9 container client-container: <nil>
STEP: delete the pod
Jul 13 13:36:28.696: INFO: Waiting for pod downwardapi-volume-ab37a9b3-36dd-4603-af3d-1620e987fad9 to disappear
Jul 13 13:36:28.700: INFO: Pod downwardapi-volume-ab37a9b3-36dd-4603-af3d-1620e987fad9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:36:28.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7806" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":235,"skipped":3904,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:36:28.717: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jul 13 13:36:28.765: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:36:32.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3478" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":280,"completed":236,"skipped":3946,"failed":0}
SSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:36:32.700: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 13 13:36:36.776: INFO: &Pod{ObjectMeta:{send-events-7d711774-4c11-4072-ba19-e8f72772fec3  events-8478 /api/v1/namespaces/events-8478/pods/send-events-7d711774-4c11-4072-ba19-e8f72772fec3 1cd4cbea-9869-4acb-b8ea-fb18abec3abc 241410 0 2020-07-13 13:36:32 +0000 UTC <nil> <nil> map[name:foo time:749097883] map[cni.projectcalico.org/podIP:10.20.168.143/32] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xsw6z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xsw6z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xsw6z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-2-22.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:36:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:36:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-07-13 13:36:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.2.22,PodIP:10.20.168.143,StartTime:2020-07-13 13:36:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-07-13 13:36:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://cf618c4c1fa16309795fef953a1c92950372db8840564ad47f5af4762f7624f2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.20.168.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jul 13 13:36:38.781: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 13 13:36:40.786: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:36:40.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8478" for this suite.

â€¢ [SLOW TEST:8.139 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":280,"completed":237,"skipped":3952,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:36:40.839: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:36:54.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9201" for this suite.
STEP: Destroying namespace "nsdeletetest-1479" for this suite.
Jul 13 13:36:54.030: INFO: Namespace nsdeletetest-1479 was already deleted
STEP: Destroying namespace "nsdeletetest-8949" for this suite.

â€¢ [SLOW TEST:13.199 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":280,"completed":238,"skipped":3959,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:36:54.038: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-82ce00ef-f72d-48b1-92d7-82f67f97f26f
STEP: Creating a pod to test consume secrets
Jul 13 13:36:54.191: INFO: Waiting up to 5m0s for pod "pod-secrets-9d22f8d9-3db5-474e-ab3c-0167a3a9e139" in namespace "secrets-3501" to be "success or failure"
Jul 13 13:36:54.201: INFO: Pod "pod-secrets-9d22f8d9-3db5-474e-ab3c-0167a3a9e139": Phase="Pending", Reason="", readiness=false. Elapsed: 9.788631ms
Jul 13 13:36:56.206: INFO: Pod "pod-secrets-9d22f8d9-3db5-474e-ab3c-0167a3a9e139": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014769681s
STEP: Saw pod success
Jul 13 13:36:56.206: INFO: Pod "pod-secrets-9d22f8d9-3db5-474e-ab3c-0167a3a9e139" satisfied condition "success or failure"
Jul 13 13:36:56.209: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-secrets-9d22f8d9-3db5-474e-ab3c-0167a3a9e139 container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 13:36:56.241: INFO: Waiting for pod pod-secrets-9d22f8d9-3db5-474e-ab3c-0167a3a9e139 to disappear
Jul 13 13:36:56.245: INFO: Pod pod-secrets-9d22f8d9-3db5-474e-ab3c-0167a3a9e139 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:36:56.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3501" for this suite.
STEP: Destroying namespace "secret-namespace-7756" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":280,"completed":239,"skipped":3976,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:36:56.267: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1275
STEP: creating the pod
Jul 13 13:36:56.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 create -f - --namespace=kubectl-2564'
Jul 13 13:36:56.563: INFO: stderr: ""
Jul 13 13:36:56.563: INFO: stdout: "pod/pause created\n"
Jul 13 13:36:56.563: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 13 13:36:56.563: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2564" to be "running and ready"
Jul 13 13:36:56.567: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.227798ms
Jul 13 13:36:58.573: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009522815s
Jul 13 13:36:58.573: INFO: Pod "pause" satisfied condition "running and ready"
Jul 13 13:36:58.573: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 13 13:36:58.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 label pods pause testing-label=testing-label-value --namespace=kubectl-2564'
Jul 13 13:36:58.658: INFO: stderr: ""
Jul 13 13:36:58.658: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 13 13:36:58.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pod pause -L testing-label --namespace=kubectl-2564'
Jul 13 13:36:58.732: INFO: stderr: ""
Jul 13 13:36:58.732: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 13 13:36:58.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 label pods pause testing-label- --namespace=kubectl-2564'
Jul 13 13:36:58.819: INFO: stderr: ""
Jul 13 13:36:58.819: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 13 13:36:58.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pod pause -L testing-label --namespace=kubectl-2564'
Jul 13 13:36:58.898: INFO: stderr: ""
Jul 13 13:36:58.898: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1282
STEP: using delete to clean up resources
Jul 13 13:36:58.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete --grace-period=0 --force -f - --namespace=kubectl-2564'
Jul 13 13:36:59.012: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 13 13:36:59.012: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 13 13:36:59.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get rc,svc -l name=pause --no-headers --namespace=kubectl-2564'
Jul 13 13:36:59.170: INFO: stderr: "No resources found in kubectl-2564 namespace.\n"
Jul 13 13:36:59.170: INFO: stdout: ""
Jul 13 13:36:59.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -l name=pause --namespace=kubectl-2564 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 13 13:36:59.276: INFO: stderr: ""
Jul 13 13:36:59.276: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:36:59.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2564" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":280,"completed":240,"skipped":4030,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:36:59.294: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:36:59.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4718" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":280,"completed":241,"skipped":4039,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:36:59.439: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:36:59.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8862" for this suite.
â€¢{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":280,"completed":242,"skipped":4049,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:36:59.517: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1585
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jul 13 13:36:59.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-4596'
Jul 13 13:36:59.683: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 13 13:36:59.683: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: rolling-update to same image controller
Jul 13 13:36:59.705: INFO: scanned /root for discovery docs: <nil>
Jul 13 13:36:59.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-4596'
Jul 13 13:37:15.594: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 13 13:37:15.594: INFO: stdout: "Created e2e-test-httpd-rc-9b5af7ecafd989c0a5b412eb1280e89b\nScaling up e2e-test-httpd-rc-9b5af7ecafd989c0a5b412eb1280e89b from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-9b5af7ecafd989c0a5b412eb1280e89b up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-9b5af7ecafd989c0a5b412eb1280e89b to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Jul 13 13:37:15.594: INFO: stdout: "Created e2e-test-httpd-rc-9b5af7ecafd989c0a5b412eb1280e89b\nScaling up e2e-test-httpd-rc-9b5af7ecafd989c0a5b412eb1280e89b from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-9b5af7ecafd989c0a5b412eb1280e89b up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-9b5af7ecafd989c0a5b412eb1280e89b to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Jul 13 13:37:15.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-4596'
Jul 13 13:37:15.670: INFO: stderr: ""
Jul 13 13:37:15.670: INFO: stdout: "e2e-test-httpd-rc-9b5af7ecafd989c0a5b412eb1280e89b-wdvhk "
Jul 13 13:37:15.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods e2e-test-httpd-rc-9b5af7ecafd989c0a5b412eb1280e89b-wdvhk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4596'
Jul 13 13:37:15.742: INFO: stderr: ""
Jul 13 13:37:15.742: INFO: stdout: "true"
Jul 13 13:37:15.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 get pods e2e-test-httpd-rc-9b5af7ecafd989c0a5b412eb1280e89b-wdvhk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4596'
Jul 13 13:37:15.823: INFO: stderr: ""
Jul 13 13:37:15.823: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Jul 13 13:37:15.823: INFO: e2e-test-httpd-rc-9b5af7ecafd989c0a5b412eb1280e89b-wdvhk is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
Jul 13 13:37:15.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 delete rc e2e-test-httpd-rc --namespace=kubectl-4596'
Jul 13 13:37:15.912: INFO: stderr: ""
Jul 13 13:37:15.912: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:37:15.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4596" for this suite.

â€¢ [SLOW TEST:16.421 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1580
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":280,"completed":243,"skipped":4054,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:37:15.938: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-7843
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7843 to expose endpoints map[]
Jul 13 13:37:16.008: INFO: Get endpoints failed (5.894778ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jul 13 13:37:17.013: INFO: successfully validated that service endpoint-test2 in namespace services-7843 exposes endpoints map[] (1.011222895s elapsed)
STEP: Creating pod pod1 in namespace services-7843
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7843 to expose endpoints map[pod1:[80]]
Jul 13 13:37:19.059: INFO: successfully validated that service endpoint-test2 in namespace services-7843 exposes endpoints map[pod1:[80]] (2.036402389s elapsed)
STEP: Creating pod pod2 in namespace services-7843
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7843 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 13 13:37:21.122: INFO: successfully validated that service endpoint-test2 in namespace services-7843 exposes endpoints map[pod1:[80] pod2:[80]] (2.05694994s elapsed)
STEP: Deleting pod pod1 in namespace services-7843
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7843 to expose endpoints map[pod2:[80]]
Jul 13 13:37:22.176: INFO: successfully validated that service endpoint-test2 in namespace services-7843 exposes endpoints map[pod2:[80]] (1.043208758s elapsed)
STEP: Deleting pod pod2 in namespace services-7843
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7843 to expose endpoints map[]
Jul 13 13:37:22.195: INFO: successfully validated that service endpoint-test2 in namespace services-7843 exposes endpoints map[] (10.350984ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:37:22.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7843" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143

â€¢ [SLOW TEST:6.307 seconds]
[sig-network] Services
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":280,"completed":244,"skipped":4087,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:37:22.245: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 13 13:37:22.295: INFO: Waiting up to 5m0s for pod "pod-ed14b2ef-26b8-465b-bf10-ebaa48e52ec7" in namespace "emptydir-7225" to be "success or failure"
Jul 13 13:37:22.303: INFO: Pod "pod-ed14b2ef-26b8-465b-bf10-ebaa48e52ec7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.78324ms
Jul 13 13:37:24.308: INFO: Pod "pod-ed14b2ef-26b8-465b-bf10-ebaa48e52ec7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013072031s
STEP: Saw pod success
Jul 13 13:37:24.308: INFO: Pod "pod-ed14b2ef-26b8-465b-bf10-ebaa48e52ec7" satisfied condition "success or failure"
Jul 13 13:37:24.316: INFO: Trying to get logs from node ip-10-0-3-156.us-east-2.compute.internal pod pod-ed14b2ef-26b8-465b-bf10-ebaa48e52ec7 container test-container: <nil>
STEP: delete the pod
Jul 13 13:37:24.344: INFO: Waiting for pod pod-ed14b2ef-26b8-465b-bf10-ebaa48e52ec7 to disappear
Jul 13 13:37:24.348: INFO: Pod pod-ed14b2ef-26b8-465b-bf10-ebaa48e52ec7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:37:24.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7225" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":245,"skipped":4113,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:37:24.364: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-c2f317aa-e9d1-4c37-8d04-23a06d43ed9a
STEP: Creating secret with name secret-projected-all-test-volume-2a38edb7-959c-4c10-b5c0-6747b0a80f4a
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 13 13:37:24.438: INFO: Waiting up to 5m0s for pod "projected-volume-906566d6-c5fb-4cd7-a575-7b883c035e08" in namespace "projected-1610" to be "success or failure"
Jul 13 13:37:24.447: INFO: Pod "projected-volume-906566d6-c5fb-4cd7-a575-7b883c035e08": Phase="Pending", Reason="", readiness=false. Elapsed: 7.835879ms
Jul 13 13:37:26.451: INFO: Pod "projected-volume-906566d6-c5fb-4cd7-a575-7b883c035e08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012510127s
Jul 13 13:37:28.456: INFO: Pod "projected-volume-906566d6-c5fb-4cd7-a575-7b883c035e08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017086283s
STEP: Saw pod success
Jul 13 13:37:28.456: INFO: Pod "projected-volume-906566d6-c5fb-4cd7-a575-7b883c035e08" satisfied condition "success or failure"
Jul 13 13:37:28.460: INFO: Trying to get logs from node ip-10-0-3-156.us-east-2.compute.internal pod projected-volume-906566d6-c5fb-4cd7-a575-7b883c035e08 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 13 13:37:28.490: INFO: Waiting for pod projected-volume-906566d6-c5fb-4cd7-a575-7b883c035e08 to disappear
Jul 13 13:37:28.494: INFO: Pod projected-volume-906566d6-c5fb-4cd7-a575-7b883c035e08 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:37:28.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1610" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":280,"completed":246,"skipped":4119,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:37:28.509: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 13:37:28.561: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c874f6fa-9422-47d9-86e5-b46ff3e7c0c7" in namespace "downward-api-7114" to be "success or failure"
Jul 13 13:37:28.571: INFO: Pod "downwardapi-volume-c874f6fa-9422-47d9-86e5-b46ff3e7c0c7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.757793ms
Jul 13 13:37:30.576: INFO: Pod "downwardapi-volume-c874f6fa-9422-47d9-86e5-b46ff3e7c0c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015067366s
STEP: Saw pod success
Jul 13 13:37:30.576: INFO: Pod "downwardapi-volume-c874f6fa-9422-47d9-86e5-b46ff3e7c0c7" satisfied condition "success or failure"
Jul 13 13:37:30.580: INFO: Trying to get logs from node ip-10-0-3-156.us-east-2.compute.internal pod downwardapi-volume-c874f6fa-9422-47d9-86e5-b46ff3e7c0c7 container client-container: <nil>
STEP: delete the pod
Jul 13 13:37:30.608: INFO: Waiting for pod downwardapi-volume-c874f6fa-9422-47d9-86e5-b46ff3e7c0c7 to disappear
Jul 13 13:37:30.612: INFO: Pod downwardapi-volume-c874f6fa-9422-47d9-86e5-b46ff3e7c0c7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:37:30.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7114" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":247,"skipped":4156,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:37:30.636: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0713 13:37:40.719099      20 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 13 13:37:40.719: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:37:40.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5297" for this suite.

â€¢ [SLOW TEST:10.105 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":280,"completed":248,"skipped":4167,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:37:40.744: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jul 13 13:37:40.803: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:37:44.566: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:37:59.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4420" for this suite.

â€¢ [SLOW TEST:19.150 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":280,"completed":249,"skipped":4200,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:37:59.894: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-8daead36-5a16-46f6-9f0c-ab450a4e15c2
STEP: Creating secret with name s-test-opt-upd-3521ce61-3600-4f4b-ab93-68f767f7db7b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8daead36-5a16-46f6-9f0c-ab450a4e15c2
STEP: Updating secret s-test-opt-upd-3521ce61-3600-4f4b-ab93-68f767f7db7b
STEP: Creating secret with name s-test-opt-create-15fb550c-5614-4641-b3b0-cc3b60d9850a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:38:06.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4703" for this suite.

â€¢ [SLOW TEST:6.234 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":250,"skipped":4206,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:38:06.129: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-387
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jul 13 13:38:06.208: INFO: Found 0 stateful pods, waiting for 3
Jul 13 13:38:16.213: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 13:38:16.213: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 13:38:16.213: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jul 13 13:38:16.258: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 13 13:38:26.301: INFO: Updating stateful set ss2
Jul 13 13:38:26.335: INFO: Waiting for Pod statefulset-387/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jul 13 13:38:36.516: INFO: Found 2 stateful pods, waiting for 3
Jul 13 13:38:46.523: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 13:38:46.523: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 13 13:38:46.523: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 13 13:38:46.558: INFO: Updating stateful set ss2
Jul 13 13:38:46.577: INFO: Waiting for Pod statefulset-387/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jul 13 13:38:56.609: INFO: Updating stateful set ss2
Jul 13 13:38:56.620: INFO: Waiting for StatefulSet statefulset-387/ss2 to complete update
Jul 13 13:38:56.620: INFO: Waiting for Pod statefulset-387/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jul 13 13:39:06.630: INFO: Deleting all statefulset in ns statefulset-387
Jul 13 13:39:06.634: INFO: Scaling statefulset ss2 to 0
Jul 13 13:39:36.663: INFO: Waiting for statefulset status.replicas updated to 0
Jul 13 13:39:36.667: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:39:36.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-387" for this suite.

â€¢ [SLOW TEST:90.569 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":280,"completed":251,"skipped":4218,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:39:36.698: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jul 13 13:39:36.737: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:39:55.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6149" for this suite.

â€¢ [SLOW TEST:19.311 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":280,"completed":252,"skipped":4219,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:39:56.010: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8972.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8972.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8972.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8972.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8972.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8972.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 13 13:40:00.147: INFO: DNS probes using dns-8972/dns-test-24f1e997-bf42-44db-9fd0-99cfbdacb44b succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:40:00.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8972" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":280,"completed":253,"skipped":4237,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:40:00.205: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-5a4f6ab1-b860-48ad-b236-fcad083c10f4
STEP: Creating a pod to test consume configMaps
Jul 13 13:40:00.261: INFO: Waiting up to 5m0s for pod "pod-configmaps-5b3b2444-3e9f-4ff1-816f-2988ae935f24" in namespace "configmap-8270" to be "success or failure"
Jul 13 13:40:00.267: INFO: Pod "pod-configmaps-5b3b2444-3e9f-4ff1-816f-2988ae935f24": Phase="Pending", Reason="", readiness=false. Elapsed: 6.241237ms
Jul 13 13:40:02.272: INFO: Pod "pod-configmaps-5b3b2444-3e9f-4ff1-816f-2988ae935f24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011543861s
STEP: Saw pod success
Jul 13 13:40:02.272: INFO: Pod "pod-configmaps-5b3b2444-3e9f-4ff1-816f-2988ae935f24" satisfied condition "success or failure"
Jul 13 13:40:02.277: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-configmaps-5b3b2444-3e9f-4ff1-816f-2988ae935f24 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 13:40:02.320: INFO: Waiting for pod pod-configmaps-5b3b2444-3e9f-4ff1-816f-2988ae935f24 to disappear
Jul 13 13:40:02.324: INFO: Pod pod-configmaps-5b3b2444-3e9f-4ff1-816f-2988ae935f24 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:40:02.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8270" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":254,"skipped":4239,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:40:02.339: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:40:02.444: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 13 13:40:02.458: INFO: Number of nodes with available pods: 0
Jul 13 13:40:02.458: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 13 13:40:02.510: INFO: Number of nodes with available pods: 0
Jul 13 13:40:02.510: INFO: Node ip-10-0-2-22.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:40:03.515: INFO: Number of nodes with available pods: 0
Jul 13 13:40:03.515: INFO: Node ip-10-0-2-22.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:40:04.515: INFO: Number of nodes with available pods: 1
Jul 13 13:40:04.515: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 13 13:40:04.542: INFO: Number of nodes with available pods: 1
Jul 13 13:40:04.542: INFO: Number of running nodes: 0, number of available pods: 1
Jul 13 13:40:05.546: INFO: Number of nodes with available pods: 0
Jul 13 13:40:05.546: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 13 13:40:05.563: INFO: Number of nodes with available pods: 0
Jul 13 13:40:05.563: INFO: Node ip-10-0-2-22.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:40:06.569: INFO: Number of nodes with available pods: 0
Jul 13 13:40:06.569: INFO: Node ip-10-0-2-22.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:40:07.568: INFO: Number of nodes with available pods: 0
Jul 13 13:40:07.568: INFO: Node ip-10-0-2-22.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:40:08.569: INFO: Number of nodes with available pods: 0
Jul 13 13:40:08.569: INFO: Node ip-10-0-2-22.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:40:09.568: INFO: Number of nodes with available pods: 0
Jul 13 13:40:09.568: INFO: Node ip-10-0-2-22.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:40:10.569: INFO: Number of nodes with available pods: 1
Jul 13 13:40:10.569: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4589, will wait for the garbage collector to delete the pods
Jul 13 13:40:10.641: INFO: Deleting DaemonSet.extensions daemon-set took: 8.965588ms
Jul 13 13:40:11.341: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.402272ms
Jul 13 13:40:23.646: INFO: Number of nodes with available pods: 0
Jul 13 13:40:23.646: INFO: Number of running nodes: 0, number of available pods: 0
Jul 13 13:40:23.650: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4589/daemonsets","resourceVersion":"243301"},"items":null}

Jul 13 13:40:23.654: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4589/pods","resourceVersion":"243301"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:40:23.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4589" for this suite.

â€¢ [SLOW TEST:21.368 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":280,"completed":255,"skipped":4246,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:40:23.707: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Jul 13 13:40:23.746: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-680009899 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:40:23.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3770" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":280,"completed":256,"skipped":4257,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:40:23.860: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 13 13:40:23.922: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul 13 13:40:30.973: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:40:30.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7320" for this suite.

â€¢ [SLOW TEST:7.132 seconds]
[k8s.io] Pods
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":280,"completed":257,"skipped":4280,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:40:30.993: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:40:31.035: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 13 13:40:34.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-1052 create -f -'
Jul 13 13:40:36.057: INFO: stderr: ""
Jul 13 13:40:36.057: INFO: stdout: "e2e-test-crd-publish-openapi-5016-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 13 13:40:36.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-1052 delete e2e-test-crd-publish-openapi-5016-crds test-cr'
Jul 13 13:40:36.177: INFO: stderr: ""
Jul 13 13:40:36.177: INFO: stdout: "e2e-test-crd-publish-openapi-5016-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jul 13 13:40:36.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-1052 apply -f -'
Jul 13 13:40:36.446: INFO: stderr: ""
Jul 13 13:40:36.446: INFO: stdout: "e2e-test-crd-publish-openapi-5016-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 13 13:40:36.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-1052 delete e2e-test-crd-publish-openapi-5016-crds test-cr'
Jul 13 13:40:36.545: INFO: stderr: ""
Jul 13 13:40:36.545: INFO: stdout: "e2e-test-crd-publish-openapi-5016-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 13 13:40:36.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 explain e2e-test-crd-publish-openapi-5016-crds'
Jul 13 13:40:36.722: INFO: stderr: ""
Jul 13 13:40:36.722: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5016-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:40:40.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1052" for this suite.

â€¢ [SLOW TEST:9.383 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":280,"completed":258,"skipped":4281,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:40:40.377: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 13 13:40:40.428: INFO: Waiting up to 5m0s for pod "pod-4933b6dc-0e8b-4cb0-aeb2-9952630c603a" in namespace "emptydir-135" to be "success or failure"
Jul 13 13:40:40.437: INFO: Pod "pod-4933b6dc-0e8b-4cb0-aeb2-9952630c603a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.793071ms
Jul 13 13:40:42.443: INFO: Pod "pod-4933b6dc-0e8b-4cb0-aeb2-9952630c603a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014512417s
Jul 13 13:40:44.447: INFO: Pod "pod-4933b6dc-0e8b-4cb0-aeb2-9952630c603a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019037217s
STEP: Saw pod success
Jul 13 13:40:44.447: INFO: Pod "pod-4933b6dc-0e8b-4cb0-aeb2-9952630c603a" satisfied condition "success or failure"
Jul 13 13:40:44.451: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-4933b6dc-0e8b-4cb0-aeb2-9952630c603a container test-container: <nil>
STEP: delete the pod
Jul 13 13:40:44.517: INFO: Waiting for pod pod-4933b6dc-0e8b-4cb0-aeb2-9952630c603a to disappear
Jul 13 13:40:44.521: INFO: Pod pod-4933b6dc-0e8b-4cb0-aeb2-9952630c603a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:40:44.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-135" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":259,"skipped":4286,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:40:44.535: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jul 13 13:40:44.584: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b3c8101-9e11-4aa7-82d2-027f06b9203e" in namespace "projected-6500" to be "success or failure"
Jul 13 13:40:44.593: INFO: Pod "downwardapi-volume-1b3c8101-9e11-4aa7-82d2-027f06b9203e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.349979ms
Jul 13 13:40:46.598: INFO: Pod "downwardapi-volume-1b3c8101-9e11-4aa7-82d2-027f06b9203e": Phase="Running", Reason="", readiness=true. Elapsed: 2.014546531s
Jul 13 13:40:48.604: INFO: Pod "downwardapi-volume-1b3c8101-9e11-4aa7-82d2-027f06b9203e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019797242s
STEP: Saw pod success
Jul 13 13:40:48.604: INFO: Pod "downwardapi-volume-1b3c8101-9e11-4aa7-82d2-027f06b9203e" satisfied condition "success or failure"
Jul 13 13:40:48.608: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod downwardapi-volume-1b3c8101-9e11-4aa7-82d2-027f06b9203e container client-container: <nil>
STEP: delete the pod
Jul 13 13:40:48.637: INFO: Waiting for pod downwardapi-volume-1b3c8101-9e11-4aa7-82d2-027f06b9203e to disappear
Jul 13 13:40:48.642: INFO: Pod downwardapi-volume-1b3c8101-9e11-4aa7-82d2-027f06b9203e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:40:48.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6500" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":260,"skipped":4292,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:40:48.657: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 13 13:40:48.701: INFO: Waiting up to 5m0s for pod "pod-38b16973-3be9-4a71-95f5-6e770ed04abb" in namespace "emptydir-8514" to be "success or failure"
Jul 13 13:40:48.707: INFO: Pod "pod-38b16973-3be9-4a71-95f5-6e770ed04abb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084007ms
Jul 13 13:40:50.712: INFO: Pod "pod-38b16973-3be9-4a71-95f5-6e770ed04abb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010627898s
Jul 13 13:40:52.717: INFO: Pod "pod-38b16973-3be9-4a71-95f5-6e770ed04abb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015513387s
STEP: Saw pod success
Jul 13 13:40:52.717: INFO: Pod "pod-38b16973-3be9-4a71-95f5-6e770ed04abb" satisfied condition "success or failure"
Jul 13 13:40:52.721: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-38b16973-3be9-4a71-95f5-6e770ed04abb container test-container: <nil>
STEP: delete the pod
Jul 13 13:40:52.752: INFO: Waiting for pod pod-38b16973-3be9-4a71-95f5-6e770ed04abb to disappear
Jul 13 13:40:52.756: INFO: Pod pod-38b16973-3be9-4a71-95f5-6e770ed04abb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:40:52.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8514" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":261,"skipped":4313,"failed":0}

------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:40:52.770: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jul 13 13:40:52.809: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:40:56.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5600" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":280,"completed":262,"skipped":4313,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:40:56.780: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-7164
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 13 13:40:56.829: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 13 13:41:19.041: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.171.255:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7164 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:41:19.042: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:41:19.215: INFO: Found all expected endpoints: [netserver-0]
Jul 13 13:41:19.224: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.92.127:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7164 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:41:19.224: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:41:19.351: INFO: Found all expected endpoints: [netserver-1]
Jul 13 13:41:19.356: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.168.149:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7164 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:41:19.356: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:41:19.474: INFO: Found all expected endpoints: [netserver-2]
Jul 13 13:41:19.479: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.23.182:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7164 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:41:19.479: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:41:19.611: INFO: Found all expected endpoints: [netserver-3]
Jul 13 13:41:19.617: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.20.234.16:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7164 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:41:19.617: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:41:19.729: INFO: Found all expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:41:19.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7164" for this suite.

â€¢ [SLOW TEST:22.966 seconds]
[sig-network] Networking
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":263,"skipped":4313,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:41:19.746: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:41:23.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9589" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":280,"completed":264,"skipped":4344,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:41:23.892: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jul 13 13:41:23.936: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jul 13 13:41:28.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-9297 create -f -'
Jul 13 13:41:29.299: INFO: stderr: ""
Jul 13 13:41:29.299: INFO: stdout: "e2e-test-crd-publish-openapi-7457-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 13 13:41:29.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-9297 delete e2e-test-crd-publish-openapi-7457-crds test-foo'
Jul 13 13:41:29.442: INFO: stderr: ""
Jul 13 13:41:29.442: INFO: stdout: "e2e-test-crd-publish-openapi-7457-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jul 13 13:41:29.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-9297 apply -f -'
Jul 13 13:41:29.695: INFO: stderr: ""
Jul 13 13:41:29.695: INFO: stdout: "e2e-test-crd-publish-openapi-7457-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 13 13:41:29.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-9297 delete e2e-test-crd-publish-openapi-7457-crds test-foo'
Jul 13 13:41:29.784: INFO: stderr: ""
Jul 13 13:41:29.784: INFO: stdout: "e2e-test-crd-publish-openapi-7457-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jul 13 13:41:29.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-9297 create -f -'
Jul 13 13:41:29.991: INFO: rc: 1
Jul 13 13:41:29.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-9297 apply -f -'
Jul 13 13:41:30.157: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jul 13 13:41:30.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-9297 create -f -'
Jul 13 13:41:30.366: INFO: rc: 1
Jul 13 13:41:30.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 --namespace=crd-publish-openapi-9297 apply -f -'
Jul 13 13:41:30.580: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jul 13 13:41:30.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 explain e2e-test-crd-publish-openapi-7457-crds'
Jul 13 13:41:30.736: INFO: stderr: ""
Jul 13 13:41:30.736: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7457-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jul 13 13:41:30.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 explain e2e-test-crd-publish-openapi-7457-crds.metadata'
Jul 13 13:41:30.895: INFO: stderr: ""
Jul 13 13:41:30.895: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7457-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jul 13 13:41:30.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 explain e2e-test-crd-publish-openapi-7457-crds.spec'
Jul 13 13:41:31.123: INFO: stderr: ""
Jul 13 13:41:31.123: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7457-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jul 13 13:41:31.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 explain e2e-test-crd-publish-openapi-7457-crds.spec.bars'
Jul 13 13:41:31.377: INFO: stderr: ""
Jul 13 13:41:31.377: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7457-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jul 13 13:41:31.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-680009899 explain e2e-test-crd-publish-openapi-7457-crds.spec.bars2'
Jul 13 13:41:31.609: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:41:35.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9297" for this suite.

â€¢ [SLOW TEST:11.380 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":280,"completed":265,"skipped":4344,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:41:35.272: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:41:39.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9873" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":280,"completed":266,"skipped":4351,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:41:39.361: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-e6a77627-c497-48a1-b873-ab11f0165ec9
STEP: Creating a pod to test consume secrets
Jul 13 13:41:39.419: INFO: Waiting up to 5m0s for pod "pod-secrets-21a94bdb-0540-47bb-ac06-67a52ab452ec" in namespace "secrets-2127" to be "success or failure"
Jul 13 13:41:39.426: INFO: Pod "pod-secrets-21a94bdb-0540-47bb-ac06-67a52ab452ec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.983455ms
Jul 13 13:41:41.431: INFO: Pod "pod-secrets-21a94bdb-0540-47bb-ac06-67a52ab452ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011660285s
STEP: Saw pod success
Jul 13 13:41:41.431: INFO: Pod "pod-secrets-21a94bdb-0540-47bb-ac06-67a52ab452ec" satisfied condition "success or failure"
Jul 13 13:41:41.435: INFO: Trying to get logs from node ip-10-0-2-22.us-east-2.compute.internal pod pod-secrets-21a94bdb-0540-47bb-ac06-67a52ab452ec container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 13:41:41.463: INFO: Waiting for pod pod-secrets-21a94bdb-0540-47bb-ac06-67a52ab452ec to disappear
Jul 13 13:41:41.467: INFO: Pod pod-secrets-21a94bdb-0540-47bb-ac06-67a52ab452ec no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:41:41.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2127" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":267,"skipped":4378,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:41:41.482: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 13 13:41:45.594: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 13 13:41:45.600: INFO: Pod pod-with-poststart-http-hook still exists
Jul 13 13:41:47.600: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 13 13:41:47.605: INFO: Pod pod-with-poststart-http-hook still exists
Jul 13 13:41:49.600: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 13 13:41:49.605: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:41:49.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1976" for this suite.

â€¢ [SLOW TEST:8.137 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":280,"completed":268,"skipped":4411,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:41:49.620: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5124.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5124.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5124.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5124.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 13 13:41:53.718: INFO: DNS probes using dns-test-aa3238eb-1b2f-46e0-a657-b0e1d9e444e0 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5124.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5124.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5124.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5124.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 13 13:41:57.814: INFO: File wheezy_udp@dns-test-service-3.dns-5124.svc.cluster.local from pod  dns-5124/dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 13 13:41:57.820: INFO: File jessie_udp@dns-test-service-3.dns-5124.svc.cluster.local from pod  dns-5124/dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 13 13:41:57.820: INFO: Lookups using dns-5124/dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 failed for: [wheezy_udp@dns-test-service-3.dns-5124.svc.cluster.local jessie_udp@dns-test-service-3.dns-5124.svc.cluster.local]

Jul 13 13:42:02.830: INFO: File wheezy_udp@dns-test-service-3.dns-5124.svc.cluster.local from pod  dns-5124/dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 13 13:42:02.843: INFO: File jessie_udp@dns-test-service-3.dns-5124.svc.cluster.local from pod  dns-5124/dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 13 13:42:02.843: INFO: Lookups using dns-5124/dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 failed for: [wheezy_udp@dns-test-service-3.dns-5124.svc.cluster.local jessie_udp@dns-test-service-3.dns-5124.svc.cluster.local]

Jul 13 13:42:07.827: INFO: File wheezy_udp@dns-test-service-3.dns-5124.svc.cluster.local from pod  dns-5124/dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 13 13:42:07.835: INFO: File jessie_udp@dns-test-service-3.dns-5124.svc.cluster.local from pod  dns-5124/dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 13 13:42:07.835: INFO: Lookups using dns-5124/dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 failed for: [wheezy_udp@dns-test-service-3.dns-5124.svc.cluster.local jessie_udp@dns-test-service-3.dns-5124.svc.cluster.local]

Jul 13 13:42:12.827: INFO: File wheezy_udp@dns-test-service-3.dns-5124.svc.cluster.local from pod  dns-5124/dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 13 13:42:12.833: INFO: File jessie_udp@dns-test-service-3.dns-5124.svc.cluster.local from pod  dns-5124/dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 13 13:42:12.833: INFO: Lookups using dns-5124/dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 failed for: [wheezy_udp@dns-test-service-3.dns-5124.svc.cluster.local jessie_udp@dns-test-service-3.dns-5124.svc.cluster.local]

Jul 13 13:42:17.829: INFO: File wheezy_udp@dns-test-service-3.dns-5124.svc.cluster.local from pod  dns-5124/dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 13 13:42:17.835: INFO: File jessie_udp@dns-test-service-3.dns-5124.svc.cluster.local from pod  dns-5124/dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 13 13:42:17.835: INFO: Lookups using dns-5124/dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 failed for: [wheezy_udp@dns-test-service-3.dns-5124.svc.cluster.local jessie_udp@dns-test-service-3.dns-5124.svc.cluster.local]

Jul 13 13:42:22.834: INFO: DNS probes using dns-test-1783b5c8-6674-47d1-a527-0068c2a900d2 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5124.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5124.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5124.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5124.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 13 13:42:26.959: INFO: DNS probes using dns-test-bbf210de-1d19-463b-982e-c37a324e0c32 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:42:27.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5124" for this suite.

â€¢ [SLOW TEST:37.406 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":280,"completed":269,"skipped":4414,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:42:27.026: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 13 13:42:33.135: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 13 13:42:33.140: INFO: Pod pod-with-prestop-http-hook still exists
Jul 13 13:42:35.140: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 13 13:42:35.145: INFO: Pod pod-with-prestop-http-hook still exists
Jul 13 13:42:37.140: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 13 13:42:37.145: INFO: Pod pod-with-prestop-http-hook still exists
Jul 13 13:42:39.140: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 13 13:42:39.145: INFO: Pod pod-with-prestop-http-hook still exists
Jul 13 13:42:41.140: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 13 13:42:41.145: INFO: Pod pod-with-prestop-http-hook still exists
Jul 13 13:42:43.140: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 13 13:42:43.145: INFO: Pod pod-with-prestop-http-hook still exists
Jul 13 13:42:45.140: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 13 13:42:45.147: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:42:45.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6280" for this suite.

â€¢ [SLOW TEST:18.162 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":280,"completed":270,"skipped":4426,"failed":0}
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:42:45.188: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:42:49.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8230" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":271,"skipped":4432,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:42:49.295: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jul 13 13:42:51.367: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8446 PodName:pod-sharedvolume-49aa6073-2f2c-4bff-aa29-a4e6eb1a3b19 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 13 13:42:51.367: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
Jul 13 13:42:51.478: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:42:51.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8446" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":280,"completed":272,"skipped":4437,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:42:51.494: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jul 13 13:42:51.540: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:43:13.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-844" for this suite.

â€¢ [SLOW TEST:21.820 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":280,"completed":273,"skipped":4492,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:43:13.315: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-ff59cb4c-9342-480e-a7b5-57b4f2902f47
STEP: Creating a pod to test consume secrets
Jul 13 13:43:13.382: INFO: Waiting up to 5m0s for pod "pod-secrets-0f2cc44f-f224-43eb-883b-a8a0a6b4676d" in namespace "secrets-1900" to be "success or failure"
Jul 13 13:43:13.389: INFO: Pod "pod-secrets-0f2cc44f-f224-43eb-883b-a8a0a6b4676d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.132453ms
Jul 13 13:43:15.395: INFO: Pod "pod-secrets-0f2cc44f-f224-43eb-883b-a8a0a6b4676d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012492161s
Jul 13 13:43:17.400: INFO: Pod "pod-secrets-0f2cc44f-f224-43eb-883b-a8a0a6b4676d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017259197s
STEP: Saw pod success
Jul 13 13:43:17.400: INFO: Pod "pod-secrets-0f2cc44f-f224-43eb-883b-a8a0a6b4676d" satisfied condition "success or failure"
Jul 13 13:43:17.404: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-secrets-0f2cc44f-f224-43eb-883b-a8a0a6b4676d container secret-volume-test: <nil>
STEP: delete the pod
Jul 13 13:43:17.434: INFO: Waiting for pod pod-secrets-0f2cc44f-f224-43eb-883b-a8a0a6b4676d to disappear
Jul 13 13:43:17.438: INFO: Pod pod-secrets-0f2cc44f-f224-43eb-883b-a8a0a6b4676d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:43:17.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1900" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":274,"skipped":4497,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:43:17.452: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 13 13:43:19.546: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:43:19.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7265" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":280,"completed":275,"skipped":4504,"failed":0}

------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:43:19.595: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 13 13:43:19.709: INFO: Number of nodes with available pods: 0
Jul 13 13:43:19.709: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:43:20.723: INFO: Number of nodes with available pods: 0
Jul 13 13:43:20.723: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:43:21.722: INFO: Number of nodes with available pods: 3
Jul 13 13:43:21.722: INFO: Node ip-10-0-2-177.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:43:22.724: INFO: Number of nodes with available pods: 5
Jul 13 13:43:22.724: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 13 13:43:22.828: INFO: Number of nodes with available pods: 4
Jul 13 13:43:22.828: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:43:23.841: INFO: Number of nodes with available pods: 4
Jul 13 13:43:23.841: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:43:24.850: INFO: Number of nodes with available pods: 4
Jul 13 13:43:24.850: INFO: Node ip-10-0-1-82.us-east-2.compute.internal is running more than one daemon pod
Jul 13 13:43:25.841: INFO: Number of nodes with available pods: 5
Jul 13 13:43:25.841: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-500, will wait for the garbage collector to delete the pods
Jul 13 13:43:25.911: INFO: Deleting DaemonSet.extensions daemon-set took: 9.215453ms
Jul 13 13:43:26.711: INFO: Terminating DaemonSet.extensions daemon-set pods took: 800.296779ms
Jul 13 13:43:40.016: INFO: Number of nodes with available pods: 0
Jul 13 13:43:40.016: INFO: Number of running nodes: 0, number of available pods: 0
Jul 13 13:43:40.020: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-500/daemonsets","resourceVersion":"245041"},"items":null}

Jul 13 13:43:40.024: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-500/pods","resourceVersion":"245041"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:43:40.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-500" for this suite.

â€¢ [SLOW TEST:20.497 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":280,"completed":276,"skipped":4504,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:43:40.094: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 13 13:43:46.207: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 13 13:43:46.217: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 13 13:43:48.217: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 13 13:43:48.221: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 13 13:43:50.217: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 13 13:43:50.222: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:43:50.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5506" for this suite.

â€¢ [SLOW TEST:10.141 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:716
  when create a pod with lifecycle hook
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":280,"completed":277,"skipped":4518,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:43:50.236: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-e3a6de6d-9868-42b7-9228-9ae94ef6d57e
STEP: Creating a pod to test consume secrets
Jul 13 13:43:50.287: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-98759585-96c3-439b-8ad7-27b5044c1052" in namespace "projected-7950" to be "success or failure"
Jul 13 13:43:50.292: INFO: Pod "pod-projected-secrets-98759585-96c3-439b-8ad7-27b5044c1052": Phase="Pending", Reason="", readiness=false. Elapsed: 5.288837ms
Jul 13 13:43:52.297: INFO: Pod "pod-projected-secrets-98759585-96c3-439b-8ad7-27b5044c1052": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010141588s
STEP: Saw pod success
Jul 13 13:43:52.297: INFO: Pod "pod-projected-secrets-98759585-96c3-439b-8ad7-27b5044c1052" satisfied condition "success or failure"
Jul 13 13:43:52.301: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-projected-secrets-98759585-96c3-439b-8ad7-27b5044c1052 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 13 13:43:52.325: INFO: Waiting for pod pod-projected-secrets-98759585-96c3-439b-8ad7-27b5044c1052 to disappear
Jul 13 13:43:52.329: INFO: Pod pod-projected-secrets-98759585-96c3-439b-8ad7-27b5044c1052 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:43:52.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7950" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":278,"skipped":4524,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:43:52.346: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5165.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5165.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5165.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5165.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5165.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5165.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5165.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 124.97.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.97.124_udp@PTR;check="$$(dig +tcp +noall +answer +search 124.97.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.97.124_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5165.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5165.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5165.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5165.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5165.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5165.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5165.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5165.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 124.97.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.97.124_udp@PTR;check="$$(dig +tcp +noall +answer +search 124.97.21.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.21.97.124_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 13 13:43:56.471: INFO: Unable to read wheezy_udp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:43:56.477: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:43:56.482: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:43:56.488: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:43:56.529: INFO: Unable to read jessie_udp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:43:56.534: INFO: Unable to read jessie_tcp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:43:56.539: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:43:56.545: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:43:56.577: INFO: Lookups using dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe failed for: [wheezy_udp@dns-test-service.dns-5165.svc.cluster.local wheezy_tcp@dns-test-service.dns-5165.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local jessie_udp@dns-test-service.dns-5165.svc.cluster.local jessie_tcp@dns-test-service.dns-5165.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local]

Jul 13 13:44:01.584: INFO: Unable to read wheezy_udp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:01.590: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:01.596: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:01.602: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:01.647: INFO: Unable to read jessie_udp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:01.654: INFO: Unable to read jessie_tcp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:01.660: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:01.667: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:01.701: INFO: Lookups using dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe failed for: [wheezy_udp@dns-test-service.dns-5165.svc.cluster.local wheezy_tcp@dns-test-service.dns-5165.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local jessie_udp@dns-test-service.dns-5165.svc.cluster.local jessie_tcp@dns-test-service.dns-5165.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local]

Jul 13 13:44:06.584: INFO: Unable to read wheezy_udp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:06.591: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:06.596: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:06.602: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:06.641: INFO: Unable to read jessie_udp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:06.646: INFO: Unable to read jessie_tcp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:06.652: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:06.657: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:06.692: INFO: Lookups using dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe failed for: [wheezy_udp@dns-test-service.dns-5165.svc.cluster.local wheezy_tcp@dns-test-service.dns-5165.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local jessie_udp@dns-test-service.dns-5165.svc.cluster.local jessie_tcp@dns-test-service.dns-5165.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local]

Jul 13 13:44:11.584: INFO: Unable to read wheezy_udp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:11.590: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:11.595: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:11.601: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:11.642: INFO: Unable to read jessie_udp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:11.648: INFO: Unable to read jessie_tcp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:11.653: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:11.659: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:11.693: INFO: Lookups using dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe failed for: [wheezy_udp@dns-test-service.dns-5165.svc.cluster.local wheezy_tcp@dns-test-service.dns-5165.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local jessie_udp@dns-test-service.dns-5165.svc.cluster.local jessie_tcp@dns-test-service.dns-5165.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local]

Jul 13 13:44:16.584: INFO: Unable to read wheezy_udp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:16.595: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:16.602: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:16.608: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:16.647: INFO: Unable to read jessie_udp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:16.653: INFO: Unable to read jessie_tcp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:16.662: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:16.668: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:16.702: INFO: Lookups using dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe failed for: [wheezy_udp@dns-test-service.dns-5165.svc.cluster.local wheezy_tcp@dns-test-service.dns-5165.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local jessie_udp@dns-test-service.dns-5165.svc.cluster.local jessie_tcp@dns-test-service.dns-5165.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local]

Jul 13 13:44:21.583: INFO: Unable to read wheezy_udp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:21.595: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:21.600: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:21.606: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:21.646: INFO: Unable to read jessie_udp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:21.653: INFO: Unable to read jessie_tcp@dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:21.659: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:21.665: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local from pod dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe: the server could not find the requested resource (get pods dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe)
Jul 13 13:44:21.699: INFO: Lookups using dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe failed for: [wheezy_udp@dns-test-service.dns-5165.svc.cluster.local wheezy_tcp@dns-test-service.dns-5165.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local jessie_udp@dns-test-service.dns-5165.svc.cluster.local jessie_tcp@dns-test-service.dns-5165.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5165.svc.cluster.local]

Jul 13 13:44:26.689: INFO: DNS probes using dns-5165/dns-test-4d46bdb0-8de6-4626-b9a1-58e3e1da2dfe succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:44:26.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5165" for this suite.

â€¢ [SLOW TEST:34.486 seconds]
[sig-network] DNS
/workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":280,"completed":279,"skipped":4537,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jul 13 13:44:26.833: INFO: >>> kubeConfig: /tmp/kubeconfig-680009899
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-3e0bf47e-b031-4ead-a315-708d3cca900f
STEP: Creating a pod to test consume configMaps
Jul 13 13:44:26.896: INFO: Waiting up to 5m0s for pod "pod-configmaps-c8d9be30-c968-4d9a-a05c-638d20812e2f" in namespace "configmap-9085" to be "success or failure"
Jul 13 13:44:26.901: INFO: Pod "pod-configmaps-c8d9be30-c968-4d9a-a05c-638d20812e2f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.587617ms
Jul 13 13:44:28.912: INFO: Pod "pod-configmaps-c8d9be30-c968-4d9a-a05c-638d20812e2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016125663s
Jul 13 13:44:30.919: INFO: Pod "pod-configmaps-c8d9be30-c968-4d9a-a05c-638d20812e2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022903492s
STEP: Saw pod success
Jul 13 13:44:30.919: INFO: Pod "pod-configmaps-c8d9be30-c968-4d9a-a05c-638d20812e2f" satisfied condition "success or failure"
Jul 13 13:44:30.923: INFO: Trying to get logs from node ip-10-0-3-16.us-east-2.compute.internal pod pod-configmaps-c8d9be30-c968-4d9a-a05c-638d20812e2f container configmap-volume-test: <nil>
STEP: delete the pod
Jul 13 13:44:30.951: INFO: Waiting for pod pod-configmaps-c8d9be30-c968-4d9a-a05c-638d20812e2f to disappear
Jul 13 13:44:30.955: INFO: Pod pod-configmaps-c8d9be30-c968-4d9a-a05c-638d20812e2f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.6-beta.0.42+fd4285294a6370/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jul 13 13:44:30.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9085" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":280,"skipped":4541,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSJul 13 13:44:30.973: INFO: Running AfterSuite actions on all nodes
Jul 13 13:44:30.976: INFO: Running AfterSuite actions on node 1
Jul 13 13:44:30.976: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":280,"completed":280,"skipped":4563,"failed":0}

Ran 280 of 4843 Specs in 4480.364 seconds
SUCCESS! -- 280 Passed | 0 Failed | 0 Pending | 4563 Skipped
PASS

Ginkgo ran 1 suite in 1h14m42.323214508s
Test Suite Passed
