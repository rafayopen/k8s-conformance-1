Conformance test: not doing test setup.
I0119 16:34:57.865675    7438 test_context.go:419] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0119 16:34:57.865816    7438 e2e.go:109] Starting e2e run "69bb3140-26bb-4142-8604-19d5106cf4fe" on Ginkgo node 1
{"msg":"Test Suite starting","total":280,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1579451696 - Will randomize all specs
Will run 280 of 4843 specs

Jan 19 16:34:58.223: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Deleting namespaces
Jan 19 16:34:58.302: INFO: namespace : kubernetes-dashboard api call to delete is complete 
STEP: Waiting for namespaces to vanish
I0119 16:34:58.302969    7438 e2e.go:233] Waiting for deletion of the following namespaces: [kubernetes-dashboard]
Jan 19 16:35:26.320: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 19 16:35:26.371: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 19 16:35:26.463: INFO: 19 / 19 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 19 16:35:26.463: INFO: expected 11 pod replicas in namespace 'kube-system', 11 are Running and Ready.
Jan 19 16:35:26.463: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 19 16:35:26.493: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 19 16:35:26.493: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 19 16:35:26.493: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Jan 19 16:35:26.493: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Jan 19 16:35:26.493: INFO: e2e test version: v1.17.1
Jan 19 16:35:26.508: INFO: kube-apiserver version: v1.17.1
Jan 19 16:35:26.508: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 16:35:26.532: INFO: Cluster IP family: ipv4
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:35:26.532: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
Jan 19 16:35:26.617: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jan 19 16:35:26.676: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5008
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 16:35:26.930: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 19 16:35:26.996: INFO: Number of nodes with available pods: 0
Jan 19 16:35:26.996: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:28.046: INFO: Number of nodes with available pods: 0
Jan 19 16:35:28.828: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:29.054: INFO: Number of nodes with available pods: 0
Jan 19 16:35:29.054: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:30.047: INFO: Number of nodes with available pods: 0
Jan 19 16:35:30.047: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:31.046: INFO: Number of nodes with available pods: 0
Jan 19 16:35:31.046: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:32.048: INFO: Number of nodes with available pods: 0
Jan 19 16:35:32.048: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:33.045: INFO: Number of nodes with available pods: 0
Jan 19 16:35:33.045: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:34.045: INFO: Number of nodes with available pods: 0
Jan 19 16:35:34.045: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:35.045: INFO: Number of nodes with available pods: 0
Jan 19 16:35:35.504: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:36.045: INFO: Number of nodes with available pods: 0
Jan 19 16:35:36.045: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:37.045: INFO: Number of nodes with available pods: 0
Jan 19 16:35:37.046: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:38.086: INFO: Number of nodes with available pods: 0
Jan 19 16:35:38.086: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:39.045: INFO: Number of nodes with available pods: 0
Jan 19 16:35:39.045: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:40.047: INFO: Number of nodes with available pods: 0
Jan 19 16:35:40.047: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:41.045: INFO: Number of nodes with available pods: 0
Jan 19 16:35:41.045: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:42.050: INFO: Number of nodes with available pods: 1
Jan 19 16:35:42.213: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:43.045: INFO: Number of nodes with available pods: 1
Jan 19 16:35:43.045: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 16:35:44.045: INFO: Number of nodes with available pods: 2
Jan 19 16:35:44.045: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 19 16:35:44.169: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:44.169: INFO: Wrong image for pod: daemon-set-pksnf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:45.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:45.204: INFO: Wrong image for pod: daemon-set-pksnf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:46.203: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:46.203: INFO: Wrong image for pod: daemon-set-pksnf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:47.203: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:47.203: INFO: Wrong image for pod: daemon-set-pksnf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:48.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:50.617: INFO: Wrong image for pod: daemon-set-pksnf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:51.203: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:51.203: INFO: Wrong image for pod: daemon-set-pksnf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:52.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:52.204: INFO: Wrong image for pod: daemon-set-pksnf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:52.204: INFO: Pod daemon-set-pksnf is not available
Jan 19 16:35:53.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:53.204: INFO: Wrong image for pod: daemon-set-pksnf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:53.204: INFO: Pod daemon-set-pksnf is not available
Jan 19 16:35:54.203: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:54.203: INFO: Wrong image for pod: daemon-set-pksnf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:54.203: INFO: Pod daemon-set-pksnf is not available
Jan 19 16:35:55.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:55.608: INFO: Wrong image for pod: daemon-set-pksnf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:55.608: INFO: Pod daemon-set-pksnf is not available
Jan 19 16:35:56.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:56.204: INFO: Wrong image for pod: daemon-set-pksnf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:56.204: INFO: Pod daemon-set-pksnf is not available
Jan 19 16:35:57.203: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:57.203: INFO: Wrong image for pod: daemon-set-pksnf. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:57.203: INFO: Pod daemon-set-pksnf is not available
Jan 19 16:35:58.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:58.204: INFO: Pod daemon-set-tsfhs is not available
Jan 19 16:35:59.205: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:35:59.205: INFO: Pod daemon-set-tsfhs is not available
Jan 19 16:36:00.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:36:00.204: INFO: Pod daemon-set-tsfhs is not available
Jan 19 16:36:01.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:36:01.204: INFO: Pod daemon-set-tsfhs is not available
Jan 19 16:36:02.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:36:02.279: INFO: Pod daemon-set-tsfhs is not available
Jan 19 16:36:03.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:36:03.204: INFO: Pod daemon-set-tsfhs is not available
Jan 19 16:36:04.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:36:04.614: INFO: Pod daemon-set-tsfhs is not available
Jan 19 16:36:05.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:36:05.204: INFO: Pod daemon-set-tsfhs is not available
Jan 19 16:36:06.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:36:07.203: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:36:08.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:36:08.204: INFO: Pod daemon-set-6t8xz is not available
Jan 19 16:36:09.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:36:09.204: INFO: Pod daemon-set-6t8xz is not available
Jan 19 16:36:10.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:36:10.204: INFO: Pod daemon-set-6t8xz is not available
Jan 19 16:36:11.203: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:36:11.249: INFO: Pod daemon-set-6t8xz is not available
Jan 19 16:36:12.204: INFO: Wrong image for pod: daemon-set-6t8xz. Expected: gcr.io/kubernetes-e2e-test-images/agnhost:2.8, got: docker.io/library/httpd:2.4.38-alpine.
Jan 19 16:36:12.204: INFO: Pod daemon-set-6t8xz is not available
Jan 19 16:36:13.204: INFO: Pod daemon-set-vlzwd is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 19 16:36:13.269: INFO: Number of nodes with available pods: 1
Jan 19 16:36:13.269: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 16:36:14.319: INFO: Number of nodes with available pods: 1
Jan 19 16:36:14.484: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 16:36:15.321: INFO: Number of nodes with available pods: 1
Jan 19 16:36:15.321: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 16:36:16.318: INFO: Number of nodes with available pods: 1
Jan 19 16:36:16.318: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 16:36:17.318: INFO: Number of nodes with available pods: 1
Jan 19 16:36:17.318: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 16:36:18.318: INFO: Number of nodes with available pods: 1
Jan 19 16:36:18.318: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 16:36:19.320: INFO: Number of nodes with available pods: 1
Jan 19 16:36:19.320: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 16:36:20.319: INFO: Number of nodes with available pods: 1
Jan 19 16:36:22.780: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 16:36:23.318: INFO: Number of nodes with available pods: 2
Jan 19 16:36:23.318: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5008, will wait for the garbage collector to delete the pods
Jan 19 16:36:23.505: INFO: Deleting DaemonSet.extensions daemon-set took: 28.100862ms
Jan 19 16:36:24.005: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.314622ms
Jan 19 16:36:37.334: INFO: Number of nodes with available pods: 0
Jan 19 16:36:37.334: INFO: Number of running nodes: 0, number of available pods: 0
Jan 19 16:36:37.351: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5008/daemonsets","resourceVersion":"8649"},"items":null}

Jan 19 16:36:37.368: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5008/pods","resourceVersion":"8649"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:36:37.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5008" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":280,"completed":1,"skipped":11,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:36:37.476: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3511
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:36:37.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3511" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":280,"completed":2,"skipped":22,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:36:37.788: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-5482
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:36:52.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5482" for this suite.
•{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":280,"completed":3,"skipped":25,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:36:52.104: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8123
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 19 16:37:03.494: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 19 16:37:03.516: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 19 16:37:05.516: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 19 16:37:05.534: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 19 16:37:07.516: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 19 16:37:07.534: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:37:07.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8123" for this suite.
•{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":280,"completed":4,"skipped":34,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:37:07.585: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3889
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 16:37:10.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715048629, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715048629, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715048629, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715048629, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:37:12.665: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715048629, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715048629, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715048629, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715048629, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 16:37:15.696: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 16:37:15.713: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5264-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:37:16.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3889" for this suite.
STEP: Destroying namespace "webhook-3889-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":280,"completed":5,"skipped":64,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:37:16.761: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7863
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:37:21.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7863" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":280,"completed":6,"skipped":83,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:37:21.791: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6968
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 19 16:37:22.058: INFO: Waiting up to 5m0s for pod "pod-0d4455c4-d12a-4ada-a49e-7ef4e745dbaa" in namespace "emptydir-6968" to be "success or failure"
Jan 19 16:37:22.075: INFO: Pod "pod-0d4455c4-d12a-4ada-a49e-7ef4e745dbaa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.472159ms
Jan 19 16:37:24.092: INFO: Pod "pod-0d4455c4-d12a-4ada-a49e-7ef4e745dbaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033945486s
Jan 19 16:37:26.110: INFO: Pod "pod-0d4455c4-d12a-4ada-a49e-7ef4e745dbaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051282466s
STEP: Saw pod success
Jan 19 16:37:26.110: INFO: Pod "pod-0d4455c4-d12a-4ada-a49e-7ef4e745dbaa" satisfied condition "success or failure"
Jan 19 16:37:26.126: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-0d4455c4-d12a-4ada-a49e-7ef4e745dbaa container test-container: <nil>
STEP: delete the pod
Jan 19 16:37:26.179: INFO: Waiting for pod pod-0d4455c4-d12a-4ada-a49e-7ef4e745dbaa to disappear
Jan 19 16:37:26.196: INFO: Pod pod-0d4455c4-d12a-4ada-a49e-7ef4e745dbaa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:37:26.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6968" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":7,"skipped":85,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:37:26.246: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2852
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 19 16:37:26.470: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:37:33.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2852" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":280,"completed":8,"skipped":156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:37:33.972: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8576
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 16:37:34.240: INFO: Waiting up to 5m0s for pod "downwardapi-volume-65f23135-469d-4990-9d02-1ecfade34e76" in namespace "downward-api-8576" to be "success or failure"
Jan 19 16:37:34.258: INFO: Pod "downwardapi-volume-65f23135-469d-4990-9d02-1ecfade34e76": Phase="Pending", Reason="", readiness=false. Elapsed: 17.682275ms
Jan 19 16:37:36.275: INFO: Pod "downwardapi-volume-65f23135-469d-4990-9d02-1ecfade34e76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035162918s
Jan 19 16:37:38.293: INFO: Pod "downwardapi-volume-65f23135-469d-4990-9d02-1ecfade34e76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052366885s
STEP: Saw pod success
Jan 19 16:37:38.293: INFO: Pod "downwardapi-volume-65f23135-469d-4990-9d02-1ecfade34e76" satisfied condition "success or failure"
Jan 19 16:37:38.309: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-65f23135-469d-4990-9d02-1ecfade34e76 container client-container: <nil>
STEP: delete the pod
Jan 19 16:37:38.354: INFO: Waiting for pod downwardapi-volume-65f23135-469d-4990-9d02-1ecfade34e76 to disappear
Jan 19 16:37:38.371: INFO: Pod downwardapi-volume-65f23135-469d-4990-9d02-1ecfade34e76 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:37:38.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8576" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":9,"skipped":209,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:37:38.420: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9775
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 19 16:37:38.676: INFO: Waiting up to 5m0s for pod "pod-f45f893e-08e7-4548-95af-1c0182ef0ff1" in namespace "emptydir-9775" to be "success or failure"
Jan 19 16:37:38.692: INFO: Pod "pod-f45f893e-08e7-4548-95af-1c0182ef0ff1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.539563ms
Jan 19 16:37:40.710: INFO: Pod "pod-f45f893e-08e7-4548-95af-1c0182ef0ff1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034448101s
Jan 19 16:37:42.727: INFO: Pod "pod-f45f893e-08e7-4548-95af-1c0182ef0ff1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051589003s
STEP: Saw pod success
Jan 19 16:37:42.728: INFO: Pod "pod-f45f893e-08e7-4548-95af-1c0182ef0ff1" satisfied condition "success or failure"
Jan 19 16:37:42.746: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-f45f893e-08e7-4548-95af-1c0182ef0ff1 container test-container: <nil>
STEP: delete the pod
Jan 19 16:37:42.791: INFO: Waiting for pod pod-f45f893e-08e7-4548-95af-1c0182ef0ff1 to disappear
Jan 19 16:37:42.807: INFO: Pod pod-f45f893e-08e7-4548-95af-1c0182ef0ff1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:37:42.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9775" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":10,"skipped":215,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:37:42.857: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9753
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jan 19 16:37:49.209: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0119 16:37:49.209035    7438 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:37:49.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9753" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":280,"completed":11,"skipped":242,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:37:49.243: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1098
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 19 16:37:49.590: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1098 /api/v1/namespaces/watch-1098/configmaps/e2e-watch-test-resource-version 24dfeb5c-f2fa-44bf-9216-3fd2dc937103 9262 0 2020-01-19 16:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 19 16:37:49.590: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1098 /api/v1/namespaces/watch-1098/configmaps/e2e-watch-test-resource-version 24dfeb5c-f2fa-44bf-9216-3fd2dc937103 9263 0 2020-01-19 16:37:49 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:37:49.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1098" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":280,"completed":12,"skipped":243,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:37:49.629: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2569
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 19 16:37:49.872: INFO: Waiting up to 5m0s for pod "pod-a29f5263-ca0c-4695-867c-cfbcb77a656a" in namespace "emptydir-2569" to be "success or failure"
Jan 19 16:37:49.888: INFO: Pod "pod-a29f5263-ca0c-4695-867c-cfbcb77a656a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.23552ms
Jan 19 16:37:51.905: INFO: Pod "pod-a29f5263-ca0c-4695-867c-cfbcb77a656a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03312867s
Jan 19 16:37:57.490: INFO: Pod "pod-a29f5263-ca0c-4695-867c-cfbcb77a656a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.617839091s
STEP: Saw pod success
Jan 19 16:37:57.490: INFO: Pod "pod-a29f5263-ca0c-4695-867c-cfbcb77a656a" satisfied condition "success or failure"
Jan 19 16:37:57.506: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-a29f5263-ca0c-4695-867c-cfbcb77a656a container test-container: <nil>
STEP: delete the pod
Jan 19 16:37:57.552: INFO: Waiting for pod pod-a29f5263-ca0c-4695-867c-cfbcb77a656a to disappear
Jan 19 16:37:57.569: INFO: Pod pod-a29f5263-ca0c-4695-867c-cfbcb77a656a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:37:57.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2569" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":13,"skipped":295,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:37:57.630: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3514
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-b67e5d1b-c105-440c-b3a4-50fbbaa462fd
STEP: Creating configMap with name cm-test-opt-upd-b4923525-1e69-49c4-a55b-cd5df34fbeff
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b67e5d1b-c105-440c-b3a4-50fbbaa462fd
STEP: Updating configmap cm-test-opt-upd-b4923525-1e69-49c4-a55b-cd5df34fbeff
STEP: Creating configMap with name cm-test-opt-create-08eaa77a-b639-405d-84dd-57b02cafaec5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:38:04.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3514" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":14,"skipped":298,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:38:04.505: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1661
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jan 19 16:38:04.732: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1661'
Jan 19 16:38:08.450: INFO: stderr: ""
Jan 19 16:38:08.450: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 19 16:38:08.450: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1661'
Jan 19 16:38:08.557: INFO: stderr: ""
Jan 19 16:38:08.557: INFO: stdout: "update-demo-nautilus-bkd8c update-demo-nautilus-ngk46 "
Jan 19 16:38:08.557: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-bkd8c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1661'
Jan 19 16:38:08.684: INFO: stderr: ""
Jan 19 16:38:08.684: INFO: stdout: ""
Jan 19 16:38:08.684: INFO: update-demo-nautilus-bkd8c is created but not running
Jan 19 16:38:13.684: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1661'
Jan 19 16:38:13.803: INFO: stderr: ""
Jan 19 16:38:13.803: INFO: stdout: "update-demo-nautilus-bkd8c update-demo-nautilus-ngk46 "
Jan 19 16:38:13.803: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-bkd8c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1661'
Jan 19 16:38:13.920: INFO: stderr: ""
Jan 19 16:38:13.920: INFO: stdout: "true"
Jan 19 16:38:13.921: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-bkd8c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1661'
Jan 19 16:38:14.039: INFO: stderr: ""
Jan 19 16:38:14.039: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 19 16:38:14.039: INFO: validating pod update-demo-nautilus-bkd8c
Jan 19 16:38:14.143: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 16:38:14.143: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 16:38:14.143: INFO: update-demo-nautilus-bkd8c is verified up and running
Jan 19 16:38:14.143: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-ngk46 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1661'
Jan 19 16:38:14.244: INFO: stderr: ""
Jan 19 16:38:14.244: INFO: stdout: "true"
Jan 19 16:38:14.244: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-ngk46 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1661'
Jan 19 16:38:14.371: INFO: stderr: ""
Jan 19 16:38:14.371: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 19 16:38:14.371: INFO: validating pod update-demo-nautilus-ngk46
Jan 19 16:38:14.471: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 16:38:14.471: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 16:38:14.471: INFO: update-demo-nautilus-ngk46 is verified up and running
STEP: scaling down the replication controller
Jan 19 16:38:14.473: INFO: scanned /root for discovery docs: <nil>
Jan 19 16:38:14.473: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1661'
Jan 19 16:38:15.637: INFO: stderr: ""
Jan 19 16:38:15.637: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 19 16:38:15.637: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1661'
Jan 19 16:38:15.758: INFO: stderr: ""
Jan 19 16:38:15.758: INFO: stdout: "update-demo-nautilus-bkd8c update-demo-nautilus-ngk46 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 19 16:38:20.758: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1661'
Jan 19 16:38:20.878: INFO: stderr: ""
Jan 19 16:38:20.878: INFO: stdout: "update-demo-nautilus-bkd8c update-demo-nautilus-ngk46 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 19 16:38:25.878: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1661'
Jan 19 16:38:25.999: INFO: stderr: ""
Jan 19 16:38:25.999: INFO: stdout: "update-demo-nautilus-bkd8c update-demo-nautilus-ngk46 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 19 16:38:30.999: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1661'
Jan 19 16:38:31.099: INFO: stderr: ""
Jan 19 16:38:31.099: INFO: stdout: "update-demo-nautilus-ngk46 "
Jan 19 16:38:31.099: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-ngk46 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1661'
Jan 19 16:38:31.214: INFO: stderr: ""
Jan 19 16:38:31.215: INFO: stdout: "true"
Jan 19 16:38:31.215: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-ngk46 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1661'
Jan 19 16:38:31.319: INFO: stderr: ""
Jan 19 16:38:31.319: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 19 16:38:31.319: INFO: validating pod update-demo-nautilus-ngk46
Jan 19 16:38:31.338: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 16:38:31.338: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 16:38:31.338: INFO: update-demo-nautilus-ngk46 is verified up and running
STEP: scaling up the replication controller
Jan 19 16:38:31.340: INFO: scanned /root for discovery docs: <nil>
Jan 19 16:38:31.340: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1661'
Jan 19 16:38:32.522: INFO: stderr: ""
Jan 19 16:38:32.522: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 19 16:38:32.523: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1661'
Jan 19 16:38:32.643: INFO: stderr: ""
Jan 19 16:38:32.643: INFO: stdout: "update-demo-nautilus-ngk46 update-demo-nautilus-wf6fg "
Jan 19 16:38:32.643: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-ngk46 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1661'
Jan 19 16:38:32.761: INFO: stderr: ""
Jan 19 16:38:32.762: INFO: stdout: "true"
Jan 19 16:38:32.762: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-ngk46 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1661'
Jan 19 16:38:32.881: INFO: stderr: ""
Jan 19 16:38:32.881: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 19 16:38:32.881: INFO: validating pod update-demo-nautilus-ngk46
Jan 19 16:38:32.901: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 16:38:32.901: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 16:38:32.901: INFO: update-demo-nautilus-ngk46 is verified up and running
Jan 19 16:38:32.902: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wf6fg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1661'
Jan 19 16:38:33.040: INFO: stderr: ""
Jan 19 16:38:33.040: INFO: stdout: ""
Jan 19 16:38:33.040: INFO: update-demo-nautilus-wf6fg is created but not running
Jan 19 16:38:38.040: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1661'
Jan 19 16:38:38.175: INFO: stderr: ""
Jan 19 16:38:38.175: INFO: stdout: "update-demo-nautilus-ngk46 update-demo-nautilus-wf6fg "
Jan 19 16:38:38.175: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-ngk46 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1661'
Jan 19 16:38:38.278: INFO: stderr: ""
Jan 19 16:38:38.278: INFO: stdout: "true"
Jan 19 16:38:38.278: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-ngk46 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1661'
Jan 19 16:38:38.390: INFO: stderr: ""
Jan 19 16:38:38.390: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 19 16:38:38.390: INFO: validating pod update-demo-nautilus-ngk46
Jan 19 16:38:38.416: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 16:38:38.416: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 16:38:38.416: INFO: update-demo-nautilus-ngk46 is verified up and running
Jan 19 16:38:38.416: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wf6fg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1661'
Jan 19 16:38:38.518: INFO: stderr: ""
Jan 19 16:38:38.518: INFO: stdout: "true"
Jan 19 16:38:38.518: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wf6fg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1661'
Jan 19 16:38:38.648: INFO: stderr: ""
Jan 19 16:38:38.648: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 19 16:38:38.648: INFO: validating pod update-demo-nautilus-wf6fg
Jan 19 16:38:38.752: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 16:38:38.752: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 16:38:38.752: INFO: update-demo-nautilus-wf6fg is verified up and running
STEP: using delete to clean up resources
Jan 19 16:38:38.752: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-1661'
Jan 19 16:38:38.898: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 16:38:38.898: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 19 16:38:38.898: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1661'
Jan 19 16:38:39.047: INFO: stderr: "No resources found in kubectl-1661 namespace.\n"
Jan 19 16:38:39.047: INFO: stdout: ""
Jan 19 16:38:39.047: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-1661 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 19 16:38:39.175: INFO: stderr: ""
Jan 19 16:38:39.175: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:38:39.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1661" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":280,"completed":15,"skipped":299,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:38:39.226: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-797
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 16:38:39.525: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"00182a93-f734-4638-a1fe-489c7e6cc7f9", Controller:(*bool)(0xc001cce6ea), BlockOwnerDeletion:(*bool)(0xc001cce6eb)}}
Jan 19 16:38:39.542: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8db00cec-be8d-46ed-919f-da4d040a4467", Controller:(*bool)(0xc001d01b76), BlockOwnerDeletion:(*bool)(0xc001d01b77)}}
Jan 19 16:38:39.559: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ca91113c-4c62-4d78-bc0f-3cf07616db7a", Controller:(*bool)(0xc002e428c6), BlockOwnerDeletion:(*bool)(0xc002e428c7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:38:44.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-797" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":280,"completed":16,"skipped":301,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:38:44.643: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5182
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-0fb652ff-7a33-4da7-b506-569fcc904d5b
STEP: Creating a pod to test consume secrets
Jan 19 16:38:44.962: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-06f5c6d8-ecb8-4c73-b6bd-b2c2576672df" in namespace "projected-5182" to be "success or failure"
Jan 19 16:38:44.979: INFO: Pod "pod-projected-secrets-06f5c6d8-ecb8-4c73-b6bd-b2c2576672df": Phase="Pending", Reason="", readiness=false. Elapsed: 16.101458ms
Jan 19 16:38:47.056: INFO: Pod "pod-projected-secrets-06f5c6d8-ecb8-4c73-b6bd-b2c2576672df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.093872195s
Jan 19 16:38:49.074: INFO: Pod "pod-projected-secrets-06f5c6d8-ecb8-4c73-b6bd-b2c2576672df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.111623414s
STEP: Saw pod success
Jan 19 16:38:49.074: INFO: Pod "pod-projected-secrets-06f5c6d8-ecb8-4c73-b6bd-b2c2576672df" satisfied condition "success or failure"
Jan 19 16:38:49.091: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-projected-secrets-06f5c6d8-ecb8-4c73-b6bd-b2c2576672df container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 19 16:38:49.136: INFO: Waiting for pod pod-projected-secrets-06f5c6d8-ecb8-4c73-b6bd-b2c2576672df to disappear
Jan 19 16:38:49.152: INFO: Pod pod-projected-secrets-06f5c6d8-ecb8-4c73-b6bd-b2c2576672df no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:38:49.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5182" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":17,"skipped":336,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:38:49.202: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2532
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 19 16:38:54.040: INFO: Successfully updated pod "pod-update-22a2600a-44a7-46ba-8af1-d75f6d38b749"
STEP: verifying the updated pod is in kubernetes
Jan 19 16:38:54.157: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:38:54.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2532" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":280,"completed":18,"skipped":341,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:38:54.206: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6096
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-91cd615c-0357-42e2-8cd8-3a7fb03fc29e
STEP: Creating a pod to test consume configMaps
Jan 19 16:38:54.485: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5caf0c54-d0fa-4a47-b51a-73ad746effe5" in namespace "projected-6096" to be "success or failure"
Jan 19 16:38:54.501: INFO: Pod "pod-projected-configmaps-5caf0c54-d0fa-4a47-b51a-73ad746effe5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.243803ms
Jan 19 16:38:56.518: INFO: Pod "pod-projected-configmaps-5caf0c54-d0fa-4a47-b51a-73ad746effe5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033612662s
Jan 19 16:38:58.536: INFO: Pod "pod-projected-configmaps-5caf0c54-d0fa-4a47-b51a-73ad746effe5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051272205s
STEP: Saw pod success
Jan 19 16:38:58.536: INFO: Pod "pod-projected-configmaps-5caf0c54-d0fa-4a47-b51a-73ad746effe5" satisfied condition "success or failure"
Jan 19 16:38:58.552: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-projected-configmaps-5caf0c54-d0fa-4a47-b51a-73ad746effe5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 19 16:38:58.637: INFO: Waiting for pod pod-projected-configmaps-5caf0c54-d0fa-4a47-b51a-73ad746effe5 to disappear
Jan 19 16:38:58.654: INFO: Pod pod-projected-configmaps-5caf0c54-d0fa-4a47-b51a-73ad746effe5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:38:58.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6096" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":19,"skipped":348,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:38:58.811: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8939
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-8939
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jan 19 16:38:59.097: INFO: Found 1 stateful pods, waiting for 3
Jan 19 16:39:09.121: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 16:39:09.121: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 16:39:09.121: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jan 19 16:39:09.216: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 19 16:39:09.297: INFO: Updating stateful set ss2
Jan 19 16:39:09.334: INFO: Waiting for Pod statefulset-8939/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jan 19 16:39:19.442: INFO: Found 2 stateful pods, waiting for 3
Jan 19 16:39:29.460: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 16:39:29.460: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 16:39:29.460: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan 19 16:39:39.461: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 16:39:39.461: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 16:39:39.461: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 19 16:39:39.540: INFO: Updating stateful set ss2
Jan 19 16:39:39.578: INFO: Waiting for Pod statefulset-8939/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 19 16:39:49.612: INFO: Waiting for Pod statefulset-8939/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 19 16:39:59.656: INFO: Updating stateful set ss2
Jan 19 16:39:59.723: INFO: Waiting for StatefulSet statefulset-8939/ss2 to complete update
Jan 19 16:39:59.723: INFO: Waiting for Pod statefulset-8939/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 19 16:40:09.758: INFO: Waiting for StatefulSet statefulset-8939/ss2 to complete update
Jan 19 16:40:09.814: INFO: Waiting for Pod statefulset-8939/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jan 19 16:40:19.759: INFO: Waiting for StatefulSet statefulset-8939/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 19 16:40:29.759: INFO: Deleting all statefulset in ns statefulset-8939
Jan 19 16:40:29.776: INFO: Scaling statefulset ss2 to 0
Jan 19 16:40:49.848: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 16:40:49.864: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:40:49.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8939" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":280,"completed":20,"skipped":352,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:40:49.967: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8862
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 19 16:40:50.186: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:40:59.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8862" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":280,"completed":21,"skipped":365,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:40:59.470: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2067
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating api versions
Jan 19 16:40:59.696: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config api-versions'
Jan 19 16:41:04.139: INFO: stderr: ""
Jan 19 16:41:04.139: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\ndns.gardener.cloud/v1alpha1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:41:04.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2067" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":280,"completed":22,"skipped":370,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:41:04.190: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7728
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test env composition
Jan 19 16:41:04.441: INFO: Waiting up to 5m0s for pod "var-expansion-3d8d15a2-7fe8-43fe-8efc-5e91c6567c41" in namespace "var-expansion-7728" to be "success or failure"
Jan 19 16:41:04.457: INFO: Pod "var-expansion-3d8d15a2-7fe8-43fe-8efc-5e91c6567c41": Phase="Pending", Reason="", readiness=false. Elapsed: 16.402787ms
Jan 19 16:41:06.475: INFO: Pod "var-expansion-3d8d15a2-7fe8-43fe-8efc-5e91c6567c41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033916116s
Jan 19 16:41:08.706: INFO: Pod "var-expansion-3d8d15a2-7fe8-43fe-8efc-5e91c6567c41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.264847735s
STEP: Saw pod success
Jan 19 16:41:08.706: INFO: Pod "var-expansion-3d8d15a2-7fe8-43fe-8efc-5e91c6567c41" satisfied condition "success or failure"
Jan 19 16:41:08.722: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod var-expansion-3d8d15a2-7fe8-43fe-8efc-5e91c6567c41 container dapi-container: <nil>
STEP: delete the pod
Jan 19 16:41:08.828: INFO: Waiting for pod var-expansion-3d8d15a2-7fe8-43fe-8efc-5e91c6567c41 to disappear
Jan 19 16:41:08.844: INFO: Pod var-expansion-3d8d15a2-7fe8-43fe-8efc-5e91c6567c41 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:41:08.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7728" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":280,"completed":23,"skipped":382,"failed":0}
SSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:41:08.894: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:41:13.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4390" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":280,"completed":24,"skipped":386,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:41:13.808: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7690
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7690.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7690.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 19 16:41:44.668: INFO: DNS probes using dns-7690/dns-test-7bb8adef-c28d-45d2-8a8b-51a92b418496 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:41:44.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7690" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":280,"completed":25,"skipped":391,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:41:44.751: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2285
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 16:41:44.992: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2f572d4-e13a-4ad2-a78c-d77b7ee28286" in namespace "projected-2285" to be "success or failure"
Jan 19 16:41:45.009: INFO: Pod "downwardapi-volume-c2f572d4-e13a-4ad2-a78c-d77b7ee28286": Phase="Pending", Reason="", readiness=false. Elapsed: 16.530187ms
Jan 19 16:41:47.026: INFO: Pod "downwardapi-volume-c2f572d4-e13a-4ad2-a78c-d77b7ee28286": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03432237s
Jan 19 16:41:49.044: INFO: Pod "downwardapi-volume-c2f572d4-e13a-4ad2-a78c-d77b7ee28286": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051621513s
STEP: Saw pod success
Jan 19 16:41:49.044: INFO: Pod "downwardapi-volume-c2f572d4-e13a-4ad2-a78c-d77b7ee28286" satisfied condition "success or failure"
Jan 19 16:41:49.060: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-c2f572d4-e13a-4ad2-a78c-d77b7ee28286 container client-container: <nil>
STEP: delete the pod
Jan 19 16:41:49.112: INFO: Waiting for pod downwardapi-volume-c2f572d4-e13a-4ad2-a78c-d77b7ee28286 to disappear
Jan 19 16:41:49.128: INFO: Pod downwardapi-volume-c2f572d4-e13a-4ad2-a78c-d77b7ee28286 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:41:49.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2285" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":26,"skipped":398,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:41:49.179: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:42:49.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-897" for this suite.
•{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":280,"completed":27,"skipped":411,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:42:49.486: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2391
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:43:00.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2391" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":280,"completed":28,"skipped":418,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:43:00.885: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9147
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:43:01.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9147" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":280,"completed":29,"skipped":434,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:43:01.254: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7046
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-secret-hd6b
STEP: Creating a pod to test atomic-volume-subpath
Jan 19 16:43:01.531: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-hd6b" in namespace "subpath-7046" to be "success or failure"
Jan 19 16:43:01.548: INFO: Pod "pod-subpath-test-secret-hd6b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.82138ms
Jan 19 16:43:03.565: INFO: Pod "pod-subpath-test-secret-hd6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033863383s
Jan 19 16:43:05.586: INFO: Pod "pod-subpath-test-secret-hd6b": Phase="Running", Reason="", readiness=true. Elapsed: 4.054980295s
Jan 19 16:43:07.603: INFO: Pod "pod-subpath-test-secret-hd6b": Phase="Running", Reason="", readiness=true. Elapsed: 6.072299009s
Jan 19 16:43:09.621: INFO: Pod "pod-subpath-test-secret-hd6b": Phase="Running", Reason="", readiness=true. Elapsed: 8.089952566s
Jan 19 16:43:11.638: INFO: Pod "pod-subpath-test-secret-hd6b": Phase="Running", Reason="", readiness=true. Elapsed: 10.107642395s
Jan 19 16:43:13.656: INFO: Pod "pod-subpath-test-secret-hd6b": Phase="Running", Reason="", readiness=true. Elapsed: 12.12513242s
Jan 19 16:43:15.674: INFO: Pod "pod-subpath-test-secret-hd6b": Phase="Running", Reason="", readiness=true. Elapsed: 14.142830443s
Jan 19 16:43:17.691: INFO: Pod "pod-subpath-test-secret-hd6b": Phase="Running", Reason="", readiness=true. Elapsed: 16.160146045s
Jan 19 16:43:19.708: INFO: Pod "pod-subpath-test-secret-hd6b": Phase="Running", Reason="", readiness=true. Elapsed: 18.177605725s
Jan 19 16:43:21.725: INFO: Pod "pod-subpath-test-secret-hd6b": Phase="Running", Reason="", readiness=true. Elapsed: 20.194707321s
Jan 19 16:43:23.745: INFO: Pod "pod-subpath-test-secret-hd6b": Phase="Running", Reason="", readiness=true. Elapsed: 22.214305952s
Jan 19 16:43:25.763: INFO: Pod "pod-subpath-test-secret-hd6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.23236868s
STEP: Saw pod success
Jan 19 16:43:25.763: INFO: Pod "pod-subpath-test-secret-hd6b" satisfied condition "success or failure"
Jan 19 16:43:25.780: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-subpath-test-secret-hd6b container test-container-subpath-secret-hd6b: <nil>
STEP: delete the pod
Jan 19 16:43:25.895: INFO: Waiting for pod pod-subpath-test-secret-hd6b to disappear
Jan 19 16:43:25.911: INFO: Pod pod-subpath-test-secret-hd6b no longer exists
STEP: Deleting pod pod-subpath-test-secret-hd6b
Jan 19 16:43:25.911: INFO: Deleting pod "pod-subpath-test-secret-hd6b" in namespace "subpath-7046"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:43:25.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7046" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":280,"completed":30,"skipped":441,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:43:25.976: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4015
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4015, will wait for the garbage collector to delete the pods
Jan 19 16:43:30.491: INFO: Deleting Job.batch foo took: 19.649065ms
Jan 19 16:43:30.591: INFO: Terminating Job.batch foo pods took: 100.282843ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:44:07.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4015" for this suite.
•{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":280,"completed":31,"skipped":445,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:44:07.363: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7951
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7951
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7951
STEP: creating replication controller externalsvc in namespace services-7951
I0119 16:44:07.955021    7438 runners.go:189] Created replication controller with name: externalsvc, namespace: services-7951, replica count: 2
I0119 16:44:11.005486    7438 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jan 19 16:44:11.071: INFO: Creating new exec pod
Jan 19 16:44:15.123: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-7951 execpodntvmp -- /bin/sh -x -c nslookup clusterip-service'
Jan 19 16:44:27.907: INFO: stderr: "+ nslookup clusterip-service\n"
Jan 19 16:44:27.908: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nclusterip-service.services-7951.svc.cluster.local\tcanonical name = externalsvc.services-7951.svc.cluster.local.\nName:\texternalsvc.services-7951.svc.cluster.local\nAddress: 100.111.215.183\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7951, will wait for the garbage collector to delete the pods
Jan 19 16:44:27.994: INFO: Deleting ReplicationController externalsvc took: 18.6178ms
Jan 19 16:44:28.302: INFO: Terminating ReplicationController externalsvc pods took: 307.657796ms
Jan 19 16:44:37.341: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:44:37.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7951" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":280,"completed":32,"skipped":453,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:44:37.527: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jan 19 16:44:42.348: INFO: Successfully updated pod "adopt-release-ncszr"
STEP: Checking that the Job readopts the Pod
Jan 19 16:44:42.349: INFO: Waiting up to 15m0s for pod "adopt-release-ncszr" in namespace "job-8128" to be "adopted"
Jan 19 16:44:42.366: INFO: Pod "adopt-release-ncszr": Phase="Running", Reason="", readiness=true. Elapsed: 17.731422ms
Jan 19 16:44:42.367: INFO: Pod "adopt-release-ncszr" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jan 19 16:44:42.905: INFO: Successfully updated pod "adopt-release-ncszr"
STEP: Checking that the Job releases the Pod
Jan 19 16:44:42.905: INFO: Waiting up to 15m0s for pod "adopt-release-ncszr" in namespace "job-8128" to be "released"
Jan 19 16:44:42.937: INFO: Pod "adopt-release-ncszr": Phase="Running", Reason="", readiness=true. Elapsed: 31.896256ms
Jan 19 16:44:42.937: INFO: Pod "adopt-release-ncszr" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:44:42.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8128" for this suite.
•{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":280,"completed":33,"skipped":471,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:44:42.986: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7725
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a replication controller
Jan 19 16:44:43.207: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7725'
Jan 19 16:46:23.487: INFO: stderr: ""
Jan 19 16:46:23.487: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 19 16:46:23.487: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7725'
Jan 19 16:46:23.628: INFO: stderr: ""
Jan 19 16:46:23.628: INFO: stdout: "update-demo-nautilus-cqlcn update-demo-nautilus-gw7ps "
Jan 19 16:46:23.628: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-cqlcn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Jan 19 16:46:23.754: INFO: stderr: ""
Jan 19 16:46:24.469: INFO: stdout: ""
Jan 19 16:46:24.469: INFO: update-demo-nautilus-cqlcn is created but not running
Jan 19 16:46:29.470: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7725'
Jan 19 16:46:29.593: INFO: stderr: ""
Jan 19 16:46:29.593: INFO: stdout: "update-demo-nautilus-cqlcn update-demo-nautilus-gw7ps "
Jan 19 16:46:29.594: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-cqlcn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Jan 19 16:46:29.723: INFO: stderr: ""
Jan 19 16:46:29.723: INFO: stdout: "true"
Jan 19 16:46:29.723: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-cqlcn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Jan 19 16:46:29.851: INFO: stderr: ""
Jan 19 16:46:30.332: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 19 16:46:30.332: INFO: validating pod update-demo-nautilus-cqlcn
Jan 19 16:46:30.435: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 16:46:30.435: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 16:46:30.435: INFO: update-demo-nautilus-cqlcn is verified up and running
Jan 19 16:46:30.435: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-gw7ps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Jan 19 16:46:30.563: INFO: stderr: ""
Jan 19 16:46:30.563: INFO: stdout: "true"
Jan 19 16:46:30.563: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-gw7ps -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7725'
Jan 19 16:46:30.663: INFO: stderr: ""
Jan 19 16:46:30.663: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 19 16:46:30.663: INFO: validating pod update-demo-nautilus-gw7ps
Jan 19 16:46:30.766: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 16:46:30.766: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 16:46:30.766: INFO: update-demo-nautilus-gw7ps is verified up and running
STEP: using delete to clean up resources
Jan 19 16:46:30.766: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-7725'
Jan 19 16:46:30.921: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 16:46:30.922: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 19 16:46:30.922: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7725'
Jan 19 16:46:31.079: INFO: stderr: "No resources found in kubectl-7725 namespace.\n"
Jan 19 16:46:31.230: INFO: stdout: ""
Jan 19 16:46:31.230: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-7725 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 19 16:46:31.361: INFO: stderr: ""
Jan 19 16:46:31.361: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:46:31.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7725" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":280,"completed":34,"skipped":473,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:46:31.413: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7131
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-cfe619ff-8c74-480f-a496-53d6f135b495
STEP: Creating a pod to test consume secrets
Jan 19 16:46:31.678: INFO: Waiting up to 5m0s for pod "pod-secrets-084e0ab8-e677-4728-a3f4-47c790c64c89" in namespace "secrets-7131" to be "success or failure"
Jan 19 16:46:31.717: INFO: Pod "pod-secrets-084e0ab8-e677-4728-a3f4-47c790c64c89": Phase="Pending", Reason="", readiness=false. Elapsed: 38.855317ms
Jan 19 16:46:33.735: INFO: Pod "pod-secrets-084e0ab8-e677-4728-a3f4-47c790c64c89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056701099s
Jan 19 16:46:35.752: INFO: Pod "pod-secrets-084e0ab8-e677-4728-a3f4-47c790c64c89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074329799s
STEP: Saw pod success
Jan 19 16:46:35.752: INFO: Pod "pod-secrets-084e0ab8-e677-4728-a3f4-47c790c64c89" satisfied condition "success or failure"
Jan 19 16:46:35.769: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-secrets-084e0ab8-e677-4728-a3f4-47c790c64c89 container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 16:46:35.904: INFO: Waiting for pod pod-secrets-084e0ab8-e677-4728-a3f4-47c790c64c89 to disappear
Jan 19 16:46:35.921: INFO: Pod pod-secrets-084e0ab8-e677-4728-a3f4-47c790c64c89 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:46:35.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7131" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":35,"skipped":490,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:46:35.971: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override arguments
Jan 19 16:46:36.221: INFO: Waiting up to 5m0s for pod "client-containers-5f46c2dd-a511-4175-9e8e-3ba49e184013" in namespace "containers-3694" to be "success or failure"
Jan 19 16:46:36.237: INFO: Pod "client-containers-5f46c2dd-a511-4175-9e8e-3ba49e184013": Phase="Pending", Reason="", readiness=false. Elapsed: 16.247502ms
Jan 19 16:46:38.255: INFO: Pod "client-containers-5f46c2dd-a511-4175-9e8e-3ba49e184013": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034003369s
Jan 19 16:46:40.273: INFO: Pod "client-containers-5f46c2dd-a511-4175-9e8e-3ba49e184013": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051893856s
STEP: Saw pod success
Jan 19 16:46:40.273: INFO: Pod "client-containers-5f46c2dd-a511-4175-9e8e-3ba49e184013" satisfied condition "success or failure"
Jan 19 16:46:40.290: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod client-containers-5f46c2dd-a511-4175-9e8e-3ba49e184013 container test-container: <nil>
STEP: delete the pod
Jan 19 16:46:40.344: INFO: Waiting for pod client-containers-5f46c2dd-a511-4175-9e8e-3ba49e184013 to disappear
Jan 19 16:46:40.362: INFO: Pod client-containers-5f46c2dd-a511-4175-9e8e-3ba49e184013 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:46:40.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3694" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":280,"completed":36,"skipped":493,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:46:40.423: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5460
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:46:41.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5460" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":280,"completed":37,"skipped":503,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:46:41.122: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8084
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:46:57.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8084" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":280,"completed":38,"skipped":529,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:46:57.678: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7352
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 19 16:46:57.900: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 16:46:57.951: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 16:46:57.967: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc before test
Jan 19 16:46:58.004: INFO: node-exporter-md4n7 from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.004: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 16:46:58.004: INFO: calico-node-75w55 from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.004: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 16:46:58.004: INFO: kube-proxy-6m6gf from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.004: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 16:46:58.004: INFO: calico-typha-deploy-9f6b455c4-x5lp9 from kube-system started at 2020-01-19 16:08:57 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.004: INFO: 	Container calico-typha ready: true, restart count 0
Jan 19 16:46:58.004: INFO: kubernetes-dashboard-5b855874f6-nb7jc from kubernetes-dashboard started at 2020-01-19 16:35:33 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.004: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 19 16:46:58.004: INFO: dashboard-metrics-scraper-894778996-rhgpz from kubernetes-dashboard started at 2020-01-19 16:35:40 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.004: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 19 16:46:58.004: INFO: node-problem-detector-pw6fz from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.004: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 16:46:58.004: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz before test
Jan 19 16:46:58.060: INFO: blackbox-exporter-54bb5f55cc-d7k59 from kube-system started at 2020-01-19 16:04:56 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.060: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan 19 16:46:58.060: INFO: node-exporter-9w5fc from kube-system started at 2020-01-19 16:05:02 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.061: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 16:46:58.061: INFO: calico-typha-vertical-autoscaler-5769b74b58-k2rg7 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.061: INFO: 	Container autoscaler ready: true, restart count 4
Jan 19 16:46:58.061: INFO: calico-kube-controllers-79bcd784b6-t6nlg from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.061: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 19 16:46:58.061: INFO: addons-nginx-ingress-controller-7c75bb76db-82dr5 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.061: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jan 19 16:46:58.061: INFO: coredns-59c969ffb8-dckvt from kube-system started at 2020-01-19 16:05:44 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.061: INFO: 	Container coredns ready: true, restart count 0
Jan 19 16:46:58.061: INFO: kube-proxy-cpmsz from kube-system started at 2020-01-19 16:05:02 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.061: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 16:46:58.061: INFO: calico-node-6gx2z from kube-system started at 2020-01-19 16:04:56 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.061: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 16:46:58.061: INFO: node-problem-detector-q4slm from kube-system started at 2020-01-19 16:05:03 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.061: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 16:46:58.061: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-95f65778d-66xm5 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.061: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jan 19 16:46:58.061: INFO: coredns-59c969ffb8-z8twk from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.061: INFO: 	Container coredns ready: true, restart count 0
Jan 19 16:46:58.061: INFO: metrics-server-b8bffff9b-2lk2p from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.061: INFO: 	Container metrics-server ready: true, restart count 0
Jan 19 16:46:58.061: INFO: vpn-shoot-5d566c4d8b-klghn from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.061: INFO: 	Container vpn-shoot ready: true, restart count 0
Jan 19 16:46:58.061: INFO: calico-typha-horizontal-autoscaler-85c99966bb-czfm4 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 16:46:58.061: INFO: 	Container autoscaler ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1dfdfe72-845e-4fe7-9c22-fae10ec4a69a 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-1dfdfe72-845e-4fe7-9c22-fae10ec4a69a off the node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1dfdfe72-845e-4fe7-9c22-fae10ec4a69a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:52:09.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7352" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77

• [SLOW TEST:312.077 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":280,"completed":39,"skipped":574,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:52:09.755: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6442
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1632
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 19 16:52:09.987: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-6442'
Jan 19 16:52:48.763: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 19 16:52:48.763: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Jan 19 16:52:48.799: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-zh9tg]
Jan 19 16:52:48.799: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-zh9tg" in namespace "kubectl-6442" to be "running and ready"
Jan 19 16:52:48.815: INFO: Pod "e2e-test-httpd-rc-zh9tg": Phase="Pending", Reason="", readiness=false. Elapsed: 16.172747ms
Jan 19 16:52:50.832: INFO: Pod "e2e-test-httpd-rc-zh9tg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032917493s
Jan 19 16:52:52.850: INFO: Pod "e2e-test-httpd-rc-zh9tg": Phase="Running", Reason="", readiness=true. Elapsed: 4.051152508s
Jan 19 16:52:52.850: INFO: Pod "e2e-test-httpd-rc-zh9tg" satisfied condition "running and ready"
Jan 19 16:52:52.850: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-zh9tg]
Jan 19 16:52:52.850: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs rc/e2e-test-httpd-rc --namespace=kubectl-6442'
Jan 19 16:52:53.058: INFO: stderr: ""
Jan 19 16:52:53.058: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 100.64.1.59. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 100.64.1.59. Set the 'ServerName' directive globally to suppress this message\n[Sun Jan 19 16:52:50.696319 2020] [mpm_event:notice] [pid 1:tid 140261285546856] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Sun Jan 19 16:52:50.696388 2020] [core:notice] [pid 1:tid 140261285546856] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1637
Jan 19 16:52:53.058: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-httpd-rc --namespace=kubectl-6442'
Jan 19 16:52:53.198: INFO: stderr: ""
Jan 19 16:52:53.198: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:52:53.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6442" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]","total":280,"completed":40,"skipped":585,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:52:53.247: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 16:52:53.484: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dafea639-4e27-47ba-887c-97ceb7a93d2b" in namespace "downward-api-4300" to be "success or failure"
Jan 19 16:52:53.501: INFO: Pod "downwardapi-volume-dafea639-4e27-47ba-887c-97ceb7a93d2b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.476268ms
Jan 19 16:52:55.518: INFO: Pod "downwardapi-volume-dafea639-4e27-47ba-887c-97ceb7a93d2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033468719s
Jan 19 16:52:57.535: INFO: Pod "downwardapi-volume-dafea639-4e27-47ba-887c-97ceb7a93d2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050482369s
STEP: Saw pod success
Jan 19 16:52:57.535: INFO: Pod "downwardapi-volume-dafea639-4e27-47ba-887c-97ceb7a93d2b" satisfied condition "success or failure"
Jan 19 16:52:57.552: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-dafea639-4e27-47ba-887c-97ceb7a93d2b container client-container: <nil>
STEP: delete the pod
Jan 19 16:52:58.261: INFO: Waiting for pod downwardapi-volume-dafea639-4e27-47ba-887c-97ceb7a93d2b to disappear
Jan 19 16:52:58.277: INFO: Pod downwardapi-volume-dafea639-4e27-47ba-887c-97ceb7a93d2b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:52:58.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4300" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":41,"skipped":592,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:52:58.335: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2206
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 19 16:52:58.562: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:53:03.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2206" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":280,"completed":42,"skipped":600,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:53:03.426: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7389
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:53:03.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7389" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":280,"completed":43,"skipped":628,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:53:03.697: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2368
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with configMap that has name projected-configmap-test-upd-21a7a78c-e8ce-4656-97ab-20f612c6f537
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-21a7a78c-e8ce-4656-97ab-20f612c6f537
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:54:33.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2368" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":44,"skipped":631,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:54:33.251: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 16:54:33.472: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config version'
Jan 19 16:54:33.647: INFO: stderr: ""
Jan 19 16:54:33.647: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.1\", GitCommit:\"d224476cd0730baca2b6e357d144171ed74192d6\", GitTreeState:\"clean\", BuildDate:\"2020-01-14T21:04:32Z\", GoVersion:\"go1.13.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.1\", GitCommit:\"d224476cd0730baca2b6e357d144171ed74192d6\", GitTreeState:\"clean\", BuildDate:\"2020-01-14T20:56:50Z\", GoVersion:\"go1.13.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:54:33.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1865" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":280,"completed":45,"skipped":648,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:54:33.683: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4712
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 16:54:33.981: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ad9b7f8-b28c-4b6d-9909-1cef21fc0974" in namespace "projected-4712" to be "success or failure"
Jan 19 16:54:33.998: INFO: Pod "downwardapi-volume-4ad9b7f8-b28c-4b6d-9909-1cef21fc0974": Phase="Pending", Reason="", readiness=false. Elapsed: 16.176268ms
Jan 19 16:54:36.015: INFO: Pod "downwardapi-volume-4ad9b7f8-b28c-4b6d-9909-1cef21fc0974": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033652944s
Jan 19 16:54:38.035: INFO: Pod "downwardapi-volume-4ad9b7f8-b28c-4b6d-9909-1cef21fc0974": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053357714s
Jan 19 16:54:40.053: INFO: Pod "downwardapi-volume-4ad9b7f8-b28c-4b6d-9909-1cef21fc0974": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071478025s
STEP: Saw pod success
Jan 19 16:54:40.053: INFO: Pod "downwardapi-volume-4ad9b7f8-b28c-4b6d-9909-1cef21fc0974" satisfied condition "success or failure"
Jan 19 16:54:40.069: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-4ad9b7f8-b28c-4b6d-9909-1cef21fc0974 container client-container: <nil>
STEP: delete the pod
Jan 19 16:54:40.115: INFO: Waiting for pod downwardapi-volume-4ad9b7f8-b28c-4b6d-9909-1cef21fc0974 to disappear
Jan 19 16:54:40.131: INFO: Pod downwardapi-volume-4ad9b7f8-b28c-4b6d-9909-1cef21fc0974 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:54:40.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4712" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":46,"skipped":659,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:54:40.181: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9030
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-8d53690e-0101-442f-8885-1413bafe6955
STEP: Creating a pod to test consume configMaps
Jan 19 16:54:40.442: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4772c70d-5055-4dd9-bb65-aa3d16bdb5ca" in namespace "projected-9030" to be "success or failure"
Jan 19 16:54:40.458: INFO: Pod "pod-projected-configmaps-4772c70d-5055-4dd9-bb65-aa3d16bdb5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 16.260428ms
Jan 19 16:54:42.475: INFO: Pod "pod-projected-configmaps-4772c70d-5055-4dd9-bb65-aa3d16bdb5ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033504413s
Jan 19 16:54:44.493: INFO: Pod "pod-projected-configmaps-4772c70d-5055-4dd9-bb65-aa3d16bdb5ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05118676s
STEP: Saw pod success
Jan 19 16:54:44.493: INFO: Pod "pod-projected-configmaps-4772c70d-5055-4dd9-bb65-aa3d16bdb5ca" satisfied condition "success or failure"
Jan 19 16:54:44.509: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-projected-configmaps-4772c70d-5055-4dd9-bb65-aa3d16bdb5ca container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 19 16:54:44.562: INFO: Waiting for pod pod-projected-configmaps-4772c70d-5055-4dd9-bb65-aa3d16bdb5ca to disappear
Jan 19 16:54:44.578: INFO: Pod pod-projected-configmaps-4772c70d-5055-4dd9-bb65-aa3d16bdb5ca no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:54:44.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9030" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":47,"skipped":667,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:54:44.627: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2385
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 19 16:54:45.419: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049684, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049684, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049684, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049684, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:54:47.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049684, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049684, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049684, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049684, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 16:54:50.465: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 16:54:50.483: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:54:51.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2385" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":280,"completed":48,"skipped":681,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:54:51.841: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl label
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1382
STEP: creating the pod
Jan 19 16:54:52.109: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-8897'
Jan 19 16:54:52.540: INFO: stderr: ""
Jan 19 16:54:52.540: INFO: stdout: "pod/pause created\n"
Jan 19 16:54:52.540: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 19 16:54:52.541: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8897" to be "running and ready"
Jan 19 16:54:52.557: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 16.487764ms
Jan 19 16:54:54.574: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033608962s
Jan 19 16:54:56.591: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.050956587s
Jan 19 16:54:56.592: INFO: Pod "pause" satisfied condition "running and ready"
Jan 19 16:54:56.592: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 19 16:54:56.592: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label=testing-label-value --namespace=kubectl-8897'
Jan 19 16:54:56.810: INFO: stderr: ""
Jan 19 16:54:56.810: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 19 16:54:56.810: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-8897'
Jan 19 16:54:57.004: INFO: stderr: ""
Jan 19 16:54:57.004: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 19 16:54:57.004: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label- --namespace=kubectl-8897'
Jan 19 16:54:57.193: INFO: stderr: ""
Jan 19 16:54:57.193: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 19 16:54:57.193: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-8897'
Jan 19 16:54:57.356: INFO: stderr: ""
Jan 19 16:54:57.356: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
STEP: using delete to clean up resources
Jan 19 16:54:57.356: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-8897'
Jan 19 16:54:57.523: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 16:54:57.523: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 19 16:54:57.523: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=pause --no-headers --namespace=kubectl-8897'
Jan 19 16:54:57.708: INFO: stderr: "No resources found in kubectl-8897 namespace.\n"
Jan 19 16:54:57.708: INFO: stdout: ""
Jan 19 16:54:57.708: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=pause --namespace=kubectl-8897 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 19 16:54:57.865: INFO: stderr: ""
Jan 19 16:54:57.865: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:54:57.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8897" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":280,"completed":49,"skipped":728,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:54:57.915: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8453
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 16:54:58.165: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd2e4d32-5023-4e8f-b72d-c790b45a991c" in namespace "projected-8453" to be "success or failure"
Jan 19 16:54:58.181: INFO: Pod "downwardapi-volume-cd2e4d32-5023-4e8f-b72d-c790b45a991c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.256801ms
Jan 19 16:55:00.199: INFO: Pod "downwardapi-volume-cd2e4d32-5023-4e8f-b72d-c790b45a991c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034065177s
Jan 19 16:55:02.216: INFO: Pod "downwardapi-volume-cd2e4d32-5023-4e8f-b72d-c790b45a991c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051405586s
STEP: Saw pod success
Jan 19 16:55:02.216: INFO: Pod "downwardapi-volume-cd2e4d32-5023-4e8f-b72d-c790b45a991c" satisfied condition "success or failure"
Jan 19 16:55:02.233: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-cd2e4d32-5023-4e8f-b72d-c790b45a991c container client-container: <nil>
STEP: delete the pod
Jan 19 16:55:02.286: INFO: Waiting for pod downwardapi-volume-cd2e4d32-5023-4e8f-b72d-c790b45a991c to disappear
Jan 19 16:55:02.302: INFO: Pod downwardapi-volume-cd2e4d32-5023-4e8f-b72d-c790b45a991c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:55:02.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8453" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":280,"completed":50,"skipped":728,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:55:02.352: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9693
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 16:55:02.607: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 19 16:55:06.641: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 19 16:55:08.659: INFO: Creating deployment "test-rollover-deployment"
Jan 19 16:55:08.703: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 19 16:55:10.741: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 19 16:55:10.776: INFO: Ensure that both replica sets have 1 created replica
Jan 19 16:55:10.810: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 19 16:55:10.845: INFO: Updating deployment test-rollover-deployment
Jan 19 16:55:10.845: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 19 16:55:12.880: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 19 16:55:12.919: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 19 16:55:12.955: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 16:55:12.955: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049710, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:55:14.990: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 16:55:14.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049712, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:55:16.979: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 16:55:16.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049712, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:55:18.979: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 16:55:18.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049712, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:55:20.978: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 16:55:20.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049712, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:55:22.979: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 16:55:22.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049712, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:55:24.991: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 16:55:24.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049712, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:55:27.072: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 16:55:27.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049712, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:55:28.990: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 16:55:28.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049712, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:55:30.988: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 16:55:30.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049712, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:55:32.990: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 16:55:32.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049712, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049707, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-574d6dfbff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:55:34.992: INFO: 
Jan 19 16:55:34.992: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 19 16:55:35.044: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9693 /apis/apps/v1/namespaces/deployment-9693/deployments/test-rollover-deployment 5cc4644d-8793-462c-8cca-71ec9ccf2050 14333 2 2020-01-19 16:55:08 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000e40c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-01-19 16:55:07 +0000 UTC,LastTransitionTime:2020-01-19 16:55:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-574d6dfbff" has successfully progressed.,LastUpdateTime:2020-01-19 16:55:32 +0000 UTC,LastTransitionTime:2020-01-19 16:55:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 19 16:55:35.064: INFO: New ReplicaSet "test-rollover-deployment-574d6dfbff" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-574d6dfbff  deployment-9693 /apis/apps/v1/namespaces/deployment-9693/replicasets/test-rollover-deployment-574d6dfbff e0bed7fa-2be5-4f8f-aea5-989d3aa7b74b 14326 2 2020-01-19 16:55:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 5cc4644d-8793-462c-8cca-71ec9ccf2050 0xc000e415a7 0xc000e415a8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 574d6dfbff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000e41618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 16:55:35.064: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 19 16:55:35.065: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9693 /apis/apps/v1/namespaces/deployment-9693/replicasets/test-rollover-controller b8225ee6-42b2-42c9-9b67-c3ba4c778857 14332 2 2020-01-19 16:55:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 5cc4644d-8793-462c-8cca-71ec9ccf2050 0xc000e414c7 0xc000e414c8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000e41528 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 16:55:35.065: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-9693 /apis/apps/v1/namespaces/deployment-9693/replicasets/test-rollover-deployment-f6c94f66c 18efe5d6-9afc-4c74-8950-bb8060139a1a 14236 2 2020-01-19 16:55:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 5cc4644d-8793-462c-8cca-71ec9ccf2050 0xc000e41690 0xc000e41691}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000e41708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 16:55:35.082: INFO: Pod "test-rollover-deployment-574d6dfbff-q5tmc" is available:
&Pod{ObjectMeta:{test-rollover-deployment-574d6dfbff-q5tmc test-rollover-deployment-574d6dfbff- deployment-9693 /api/v1/namespaces/deployment-9693/pods/test-rollover-deployment-574d6dfbff-q5tmc 8587f7d9-9e83-43fe-b6ab-141ed1d82746 14249 0 2020-01-19 16:55:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:574d6dfbff] map[cni.projectcalico.org/podIP:100.64.1.69/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-574d6dfbff e0bed7fa-2be5-4f8f-aea5-989d3aa7b74b 0xc000e41c67 0xc000e41c68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vw79x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vw79x,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vw79x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 16:55:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 16:55:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 16:55:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 16:55:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:100.64.1.69,StartTime:2020-01-19 16:55:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-19 16:55:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://9165c1727296f0183c9261ef14e2fe5c1eae284f6008a4e4f5410049ad9271f5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:55:35.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9693" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":280,"completed":51,"skipped":765,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:55:35.133: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-271
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 19 16:55:35.391: INFO: Waiting up to 5m0s for pod "pod-59d7ba22-62a9-47e1-857c-35f3538fb291" in namespace "emptydir-271" to be "success or failure"
Jan 19 16:55:35.407: INFO: Pod "pod-59d7ba22-62a9-47e1-857c-35f3538fb291": Phase="Pending", Reason="", readiness=false. Elapsed: 16.469842ms
Jan 19 16:55:37.425: INFO: Pod "pod-59d7ba22-62a9-47e1-857c-35f3538fb291": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033906963s
Jan 19 16:55:39.444: INFO: Pod "pod-59d7ba22-62a9-47e1-857c-35f3538fb291": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053487249s
STEP: Saw pod success
Jan 19 16:55:39.445: INFO: Pod "pod-59d7ba22-62a9-47e1-857c-35f3538fb291" satisfied condition "success or failure"
Jan 19 16:55:39.461: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-59d7ba22-62a9-47e1-857c-35f3538fb291 container test-container: <nil>
STEP: delete the pod
Jan 19 16:55:39.506: INFO: Waiting for pod pod-59d7ba22-62a9-47e1-857c-35f3538fb291 to disappear
Jan 19 16:55:39.523: INFO: Pod pod-59d7ba22-62a9-47e1-857c-35f3538fb291 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:55:39.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-271" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":52,"skipped":770,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:55:39.571: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 16:55:39.805: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00ed32f6-409b-4d40-b973-bd4b70a02802" in namespace "projected-6957" to be "success or failure"
Jan 19 16:55:40.038: INFO: Pod "downwardapi-volume-00ed32f6-409b-4d40-b973-bd4b70a02802": Phase="Pending", Reason="", readiness=false. Elapsed: 232.970684ms
Jan 19 16:55:42.055: INFO: Pod "downwardapi-volume-00ed32f6-409b-4d40-b973-bd4b70a02802": Phase="Pending", Reason="", readiness=false. Elapsed: 2.250734594s
Jan 19 16:55:44.073: INFO: Pod "downwardapi-volume-00ed32f6-409b-4d40-b973-bd4b70a02802": Phase="Pending", Reason="", readiness=false. Elapsed: 4.268376346s
Jan 19 16:55:46.091: INFO: Pod "downwardapi-volume-00ed32f6-409b-4d40-b973-bd4b70a02802": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.285884457s
STEP: Saw pod success
Jan 19 16:55:46.091: INFO: Pod "downwardapi-volume-00ed32f6-409b-4d40-b973-bd4b70a02802" satisfied condition "success or failure"
Jan 19 16:55:46.107: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-00ed32f6-409b-4d40-b973-bd4b70a02802 container client-container: <nil>
STEP: delete the pod
Jan 19 16:55:46.158: INFO: Waiting for pod downwardapi-volume-00ed32f6-409b-4d40-b973-bd4b70a02802 to disappear
Jan 19 16:55:46.176: INFO: Pod downwardapi-volume-00ed32f6-409b-4d40-b973-bd4b70a02802 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:55:46.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6957" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":53,"skipped":771,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:55:46.226: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9883
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 16:55:46.452: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 19 16:55:50.795: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9883 create -f -'
Jan 19 16:55:51.854: INFO: stderr: ""
Jan 19 16:55:51.854: INFO: stdout: "e2e-test-crd-publish-openapi-6321-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 19 16:55:51.854: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9883 delete e2e-test-crd-publish-openapi-6321-crds test-cr'
Jan 19 16:55:52.054: INFO: stderr: ""
Jan 19 16:55:52.054: INFO: stdout: "e2e-test-crd-publish-openapi-6321-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 19 16:55:52.054: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9883 apply -f -'
Jan 19 16:55:52.544: INFO: stderr: ""
Jan 19 16:55:52.544: INFO: stdout: "e2e-test-crd-publish-openapi-6321-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 19 16:55:52.544: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9883 delete e2e-test-crd-publish-openapi-6321-crds test-cr'
Jan 19 16:55:52.761: INFO: stderr: ""
Jan 19 16:55:52.761: INFO: stdout: "e2e-test-crd-publish-openapi-6321-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 19 16:55:52.761: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-6321-crds'
Jan 19 16:55:59.976: INFO: stderr: ""
Jan 19 16:55:59.976: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6321-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:56:04.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9883" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":280,"completed":54,"skipped":780,"failed":0}
SSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:56:04.081: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3178
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name secret-emptykey-test-1276a3cd-ce63-4d59-957c-22fa70b2393a
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:56:04.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3178" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":280,"completed":55,"skipped":783,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:56:04.380: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9359
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 16:56:04.629: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cedeccf5-b3a1-420d-b20a-75dd81cfe054" in namespace "projected-9359" to be "success or failure"
Jan 19 16:56:04.645: INFO: Pod "downwardapi-volume-cedeccf5-b3a1-420d-b20a-75dd81cfe054": Phase="Pending", Reason="", readiness=false. Elapsed: 16.05022ms
Jan 19 16:56:06.663: INFO: Pod "downwardapi-volume-cedeccf5-b3a1-420d-b20a-75dd81cfe054": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033867704s
Jan 19 16:56:08.680: INFO: Pod "downwardapi-volume-cedeccf5-b3a1-420d-b20a-75dd81cfe054": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051086827s
STEP: Saw pod success
Jan 19 16:56:08.680: INFO: Pod "downwardapi-volume-cedeccf5-b3a1-420d-b20a-75dd81cfe054" satisfied condition "success or failure"
Jan 19 16:56:08.697: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-cedeccf5-b3a1-420d-b20a-75dd81cfe054 container client-container: <nil>
STEP: delete the pod
Jan 19 16:56:08.793: INFO: Waiting for pod downwardapi-volume-cedeccf5-b3a1-420d-b20a-75dd81cfe054 to disappear
Jan 19 16:56:08.817: INFO: Pod downwardapi-volume-cedeccf5-b3a1-420d-b20a-75dd81cfe054 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:56:08.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9359" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":56,"skipped":839,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:56:08.867: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 19 16:56:09.107: INFO: Waiting up to 5m0s for pod "downward-api-b6b0e522-a672-47b4-936b-ae33428e966a" in namespace "downward-api-8021" to be "success or failure"
Jan 19 16:56:09.124: INFO: Pod "downward-api-b6b0e522-a672-47b4-936b-ae33428e966a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.287415ms
Jan 19 16:56:11.142: INFO: Pod "downward-api-b6b0e522-a672-47b4-936b-ae33428e966a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034517455s
Jan 19 16:56:13.160: INFO: Pod "downward-api-b6b0e522-a672-47b4-936b-ae33428e966a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052753869s
STEP: Saw pod success
Jan 19 16:56:13.160: INFO: Pod "downward-api-b6b0e522-a672-47b4-936b-ae33428e966a" satisfied condition "success or failure"
Jan 19 16:56:13.179: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downward-api-b6b0e522-a672-47b4-936b-ae33428e966a container dapi-container: <nil>
STEP: delete the pod
Jan 19 16:56:13.237: INFO: Waiting for pod downward-api-b6b0e522-a672-47b4-936b-ae33428e966a to disappear
Jan 19 16:56:13.253: INFO: Pod downward-api-b6b0e522-a672-47b4-936b-ae33428e966a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:56:13.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8021" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":280,"completed":57,"skipped":849,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:56:13.304: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5008
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-gqkq
STEP: Creating a pod to test atomic-volume-subpath
Jan 19 16:56:13.594: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gqkq" in namespace "subpath-5008" to be "success or failure"
Jan 19 16:56:13.611: INFO: Pod "pod-subpath-test-configmap-gqkq": Phase="Pending", Reason="", readiness=false. Elapsed: 16.520394ms
Jan 19 16:56:15.628: INFO: Pod "pod-subpath-test-configmap-gqkq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034047627s
Jan 19 16:56:17.645: INFO: Pod "pod-subpath-test-configmap-gqkq": Phase="Running", Reason="", readiness=true. Elapsed: 4.051398189s
Jan 19 16:56:19.663: INFO: Pod "pod-subpath-test-configmap-gqkq": Phase="Running", Reason="", readiness=true. Elapsed: 6.068981281s
Jan 19 16:56:21.681: INFO: Pod "pod-subpath-test-configmap-gqkq": Phase="Running", Reason="", readiness=true. Elapsed: 8.086580409s
Jan 19 16:56:23.698: INFO: Pod "pod-subpath-test-configmap-gqkq": Phase="Running", Reason="", readiness=true. Elapsed: 10.1037149s
Jan 19 16:56:25.715: INFO: Pod "pod-subpath-test-configmap-gqkq": Phase="Running", Reason="", readiness=true. Elapsed: 12.120929885s
Jan 19 16:56:27.732: INFO: Pod "pod-subpath-test-configmap-gqkq": Phase="Running", Reason="", readiness=true. Elapsed: 14.137817s
Jan 19 16:56:29.751: INFO: Pod "pod-subpath-test-configmap-gqkq": Phase="Running", Reason="", readiness=true. Elapsed: 16.156829844s
Jan 19 16:56:31.769: INFO: Pod "pod-subpath-test-configmap-gqkq": Phase="Running", Reason="", readiness=true. Elapsed: 18.174792923s
Jan 19 16:56:33.787: INFO: Pod "pod-subpath-test-configmap-gqkq": Phase="Running", Reason="", readiness=true. Elapsed: 20.192811253s
Jan 19 16:56:35.804: INFO: Pod "pod-subpath-test-configmap-gqkq": Phase="Running", Reason="", readiness=true. Elapsed: 22.210502019s
Jan 19 16:56:37.822: INFO: Pod "pod-subpath-test-configmap-gqkq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.227804025s
STEP: Saw pod success
Jan 19 16:56:37.822: INFO: Pod "pod-subpath-test-configmap-gqkq" satisfied condition "success or failure"
Jan 19 16:56:37.839: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-subpath-test-configmap-gqkq container test-container-subpath-configmap-gqkq: <nil>
STEP: delete the pod
Jan 19 16:56:37.886: INFO: Waiting for pod pod-subpath-test-configmap-gqkq to disappear
Jan 19 16:56:37.903: INFO: Pod pod-subpath-test-configmap-gqkq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-gqkq
Jan 19 16:56:37.903: INFO: Deleting pod "pod-subpath-test-configmap-gqkq" in namespace "subpath-5008"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:56:37.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5008" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":280,"completed":58,"skipped":890,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:56:37.975: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5466
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5466
STEP: creating replication controller externalsvc in namespace services-5466
I0119 16:56:38.357529    7438 runners.go:189] Created replication controller with name: externalsvc, namespace: services-5466, replica count: 2
I0119 16:56:41.408009    7438 runners.go:189] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 16:56:44.408223    7438 runners.go:189] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jan 19 16:56:44.484: INFO: Creating new exec pod
Jan 19 16:56:48.535: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-5466 execpodb2rfv -- /bin/sh -x -c nslookup nodeport-service'
Jan 19 16:56:49.185: INFO: stderr: "+ nslookup nodeport-service\n"
Jan 19 16:56:49.185: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nnodeport-service.services-5466.svc.cluster.local\tcanonical name = externalsvc.services-5466.svc.cluster.local.\nName:\texternalsvc.services-5466.svc.cluster.local\nAddress: 100.108.98.111\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5466, will wait for the garbage collector to delete the pods
Jan 19 16:56:49.271: INFO: Deleting ReplicationController externalsvc took: 18.826385ms
Jan 19 16:56:49.771: INFO: Terminating ReplicationController externalsvc pods took: 500.318089ms
Jan 19 16:57:02.717: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:57:02.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5466" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":280,"completed":59,"skipped":901,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:57:02.810: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5886
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-03e28f6c-09c2-457f-a4ef-18c77b36bfca
STEP: Creating a pod to test consume configMaps
Jan 19 16:57:03.070: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-48cc88b4-a7a8-4d66-ba15-dace2bb84558" in namespace "projected-5886" to be "success or failure"
Jan 19 16:57:03.087: INFO: Pod "pod-projected-configmaps-48cc88b4-a7a8-4d66-ba15-dace2bb84558": Phase="Pending", Reason="", readiness=false. Elapsed: 16.430965ms
Jan 19 16:57:05.105: INFO: Pod "pod-projected-configmaps-48cc88b4-a7a8-4d66-ba15-dace2bb84558": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034664231s
Jan 19 16:57:07.123: INFO: Pod "pod-projected-configmaps-48cc88b4-a7a8-4d66-ba15-dace2bb84558": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052388686s
STEP: Saw pod success
Jan 19 16:57:07.123: INFO: Pod "pod-projected-configmaps-48cc88b4-a7a8-4d66-ba15-dace2bb84558" satisfied condition "success or failure"
Jan 19 16:57:07.154: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-projected-configmaps-48cc88b4-a7a8-4d66-ba15-dace2bb84558 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 19 16:57:07.209: INFO: Waiting for pod pod-projected-configmaps-48cc88b4-a7a8-4d66-ba15-dace2bb84558 to disappear
Jan 19 16:57:07.225: INFO: Pod pod-projected-configmaps-48cc88b4-a7a8-4d66-ba15-dace2bb84558 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:57:07.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5886" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":60,"skipped":929,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:57:07.308: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8949
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: executing a command with run --rm and attach with stdin
Jan 19 16:57:07.529: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-8949 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jan 19 16:57:11.386: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jan 19 16:57:11.386: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:57:13.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8949" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run --rm job should create a job from an image, then delete the job  [Conformance]","total":280,"completed":61,"skipped":941,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:57:13.473: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2629
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 19 16:57:16.799: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:57:16.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2629" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":62,"skipped":960,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:57:16.904: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3499
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 16:57:17.160: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c8aa9951-e3fe-4e71-9068-26c7f6578549" in namespace "downward-api-3499" to be "success or failure"
Jan 19 16:57:17.192: INFO: Pod "downwardapi-volume-c8aa9951-e3fe-4e71-9068-26c7f6578549": Phase="Pending", Reason="", readiness=false. Elapsed: 31.42572ms
Jan 19 16:57:19.209: INFO: Pod "downwardapi-volume-c8aa9951-e3fe-4e71-9068-26c7f6578549": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048859894s
Jan 19 16:57:21.227: INFO: Pod "downwardapi-volume-c8aa9951-e3fe-4e71-9068-26c7f6578549": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066564235s
STEP: Saw pod success
Jan 19 16:57:21.227: INFO: Pod "downwardapi-volume-c8aa9951-e3fe-4e71-9068-26c7f6578549" satisfied condition "success or failure"
Jan 19 16:57:21.246: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-c8aa9951-e3fe-4e71-9068-26c7f6578549 container client-container: <nil>
STEP: delete the pod
Jan 19 16:57:21.296: INFO: Waiting for pod downwardapi-volume-c8aa9951-e3fe-4e71-9068-26c7f6578549 to disappear
Jan 19 16:57:21.312: INFO: Pod downwardapi-volume-c8aa9951-e3fe-4e71-9068-26c7f6578549 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:57:21.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3499" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":280,"completed":63,"skipped":966,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:57:21.363: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6936
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jan 19 16:57:21.581: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:57:42.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6936" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":280,"completed":64,"skipped":996,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:57:42.922: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8600
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8600.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8600.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 210.241.108.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.108.241.210_udp@PTR;check="$$(dig +tcp +noall +answer +search 210.241.108.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.108.241.210_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8600.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8600.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 210.241.108.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.108.241.210_udp@PTR;check="$$(dig +tcp +noall +answer +search 210.241.108.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.108.241.210_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 19 16:57:49.381: INFO: Unable to read wheezy_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:49.424: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:49.442: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:49.460: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:49.858: INFO: Unable to read jessie_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:49.880: INFO: Unable to read jessie_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:49.898: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:49.915: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:50.293: INFO: Lookups using dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609 failed for: [wheezy_udp@dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_udp@dns-test-service.dns-8600.svc.cluster.local jessie_tcp@dns-test-service.dns-8600.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local]

Jan 19 16:57:55.313: INFO: Unable to read wheezy_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:55.356: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:55.375: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:55.393: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:55.791: INFO: Unable to read jessie_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:55.809: INFO: Unable to read jessie_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:55.827: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:55.845: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:57:56.243: INFO: Lookups using dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609 failed for: [wheezy_udp@dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_udp@dns-test-service.dns-8600.svc.cluster.local jessie_tcp@dns-test-service.dns-8600.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local]

Jan 19 16:58:00.313: INFO: Unable to read wheezy_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:00.355: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:00.374: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:00.392: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:00.833: INFO: Unable to read jessie_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:00.852: INFO: Unable to read jessie_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:00.875: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:00.893: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:01.290: INFO: Lookups using dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609 failed for: [wheezy_udp@dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_udp@dns-test-service.dns-8600.svc.cluster.local jessie_tcp@dns-test-service.dns-8600.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local]

Jan 19 16:58:05.312: INFO: Unable to read wheezy_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:05.355: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:05.374: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:05.393: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:05.794: INFO: Unable to read jessie_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:05.813: INFO: Unable to read jessie_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:05.831: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:05.850: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:06.205: INFO: Lookups using dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609 failed for: [wheezy_udp@dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_udp@dns-test-service.dns-8600.svc.cluster.local jessie_tcp@dns-test-service.dns-8600.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local]

Jan 19 16:58:10.312: INFO: Unable to read wheezy_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:10.331: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:10.349: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:10.367: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:10.805: INFO: Unable to read jessie_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:10.824: INFO: Unable to read jessie_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:10.843: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:10.862: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:11.259: INFO: Lookups using dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609 failed for: [wheezy_udp@dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_udp@dns-test-service.dns-8600.svc.cluster.local jessie_tcp@dns-test-service.dns-8600.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local]

Jan 19 16:58:15.307: INFO: Unable to read wheezy_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:15.349: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:15.363: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:15.376: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:15.809: INFO: Unable to read jessie_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:15.822: INFO: Unable to read jessie_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:15.835: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:15.848: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609: the server could not find the requested resource (get pods dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609)
Jan 19 16:58:16.199: INFO: Lookups using dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609 failed for: [wheezy_udp@dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_udp@dns-test-service.dns-8600.svc.cluster.local jessie_tcp@dns-test-service.dns-8600.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local]

Jan 19 16:58:21.601: INFO: DNS probes using dns-8600/dns-test-881ac35d-b36a-46aa-a24a-e85f9ec68609 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:58:21.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8600" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":280,"completed":65,"skipped":1009,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:58:21.739: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9846
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:58:26.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9846" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":280,"completed":66,"skipped":1027,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:58:26.062: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 16:58:26.293: INFO: Waiting up to 5m0s for pod "downwardapi-volume-33a120b2-fca6-4582-bd42-5374f7749a6b" in namespace "downward-api-2781" to be "success or failure"
Jan 19 16:58:26.319: INFO: Pod "downwardapi-volume-33a120b2-fca6-4582-bd42-5374f7749a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.172598ms
Jan 19 16:58:28.337: INFO: Pod "downwardapi-volume-33a120b2-fca6-4582-bd42-5374f7749a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043336952s
Jan 19 16:58:30.355: INFO: Pod "downwardapi-volume-33a120b2-fca6-4582-bd42-5374f7749a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061288319s
STEP: Saw pod success
Jan 19 16:58:30.355: INFO: Pod "downwardapi-volume-33a120b2-fca6-4582-bd42-5374f7749a6b" satisfied condition "success or failure"
Jan 19 16:58:30.371: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-33a120b2-fca6-4582-bd42-5374f7749a6b container client-container: <nil>
STEP: delete the pod
Jan 19 16:58:30.427: INFO: Waiting for pod downwardapi-volume-33a120b2-fca6-4582-bd42-5374f7749a6b to disappear
Jan 19 16:58:30.446: INFO: Pod downwardapi-volume-33a120b2-fca6-4582-bd42-5374f7749a6b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:58:30.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2781" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":280,"completed":67,"skipped":1028,"failed":0}
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:58:30.496: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9739
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:58:36.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9739" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":68,"skipped":1032,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:58:36.869: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3564
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 19 16:58:37.108: INFO: Waiting up to 5m0s for pod "pod-20ec3c5e-7aec-47a8-9fec-45f2cd8563ec" in namespace "emptydir-3564" to be "success or failure"
Jan 19 16:58:37.125: INFO: Pod "pod-20ec3c5e-7aec-47a8-9fec-45f2cd8563ec": Phase="Pending", Reason="", readiness=false. Elapsed: 16.930325ms
Jan 19 16:58:39.143: INFO: Pod "pod-20ec3c5e-7aec-47a8-9fec-45f2cd8563ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034277497s
Jan 19 16:58:41.166: INFO: Pod "pod-20ec3c5e-7aec-47a8-9fec-45f2cd8563ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057162996s
STEP: Saw pod success
Jan 19 16:58:41.166: INFO: Pod "pod-20ec3c5e-7aec-47a8-9fec-45f2cd8563ec" satisfied condition "success or failure"
Jan 19 16:58:41.182: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-20ec3c5e-7aec-47a8-9fec-45f2cd8563ec container test-container: <nil>
STEP: delete the pod
Jan 19 16:58:41.233: INFO: Waiting for pod pod-20ec3c5e-7aec-47a8-9fec-45f2cd8563ec to disappear
Jan 19 16:58:41.249: INFO: Pod pod-20ec3c5e-7aec-47a8-9fec-45f2cd8563ec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:58:41.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3564" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":69,"skipped":1033,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:58:41.299: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7024
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override all
Jan 19 16:58:41.540: INFO: Waiting up to 5m0s for pod "client-containers-3e343bdb-986a-4805-a74f-83859fbd2a4e" in namespace "containers-7024" to be "success or failure"
Jan 19 16:58:41.557: INFO: Pod "client-containers-3e343bdb-986a-4805-a74f-83859fbd2a4e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.953561ms
Jan 19 16:58:43.574: INFO: Pod "client-containers-3e343bdb-986a-4805-a74f-83859fbd2a4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034651971s
Jan 19 16:58:45.592: INFO: Pod "client-containers-3e343bdb-986a-4805-a74f-83859fbd2a4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052722245s
STEP: Saw pod success
Jan 19 16:58:45.592: INFO: Pod "client-containers-3e343bdb-986a-4805-a74f-83859fbd2a4e" satisfied condition "success or failure"
Jan 19 16:58:45.609: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod client-containers-3e343bdb-986a-4805-a74f-83859fbd2a4e container test-container: <nil>
STEP: delete the pod
Jan 19 16:58:45.657: INFO: Waiting for pod client-containers-3e343bdb-986a-4805-a74f-83859fbd2a4e to disappear
Jan 19 16:58:45.674: INFO: Pod client-containers-3e343bdb-986a-4805-a74f-83859fbd2a4e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:58:45.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7024" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":280,"completed":70,"skipped":1067,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:58:45.724: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1597
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1597.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1597.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1597.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1597.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1597.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1597.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 19 16:58:52.580: INFO: DNS probes using dns-1597/dns-test-656c708f-54eb-4f57-b89f-88337cbd9757 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:58:52.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1597" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":280,"completed":71,"skipped":1077,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:58:52.663: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5855
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl logs
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1464
STEP: creating an pod
Jan 19 16:58:52.882: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.8 --namespace=kubectl-5855 -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 19 16:58:53.029: INFO: stderr: ""
Jan 19 16:58:53.029: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Waiting for log generator to start.
Jan 19 16:58:53.029: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 19 16:58:53.029: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5855" to be "running and ready, or succeeded"
Jan 19 16:58:53.045: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.085565ms
Jan 19 16:58:55.064: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034927344s
Jan 19 16:58:57.081: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.052386914s
Jan 19 16:58:57.081: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 19 16:58:57.081: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jan 19 16:58:57.081: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs logs-generator logs-generator --namespace=kubectl-5855'
Jan 19 16:58:57.260: INFO: stderr: ""
Jan 19 16:58:57.260: INFO: stdout: "I0119 16:58:54.920866       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/9n4t 206\nI0119 16:58:55.121004       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/hsl 338\nI0119 16:58:55.321117       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/gbgk 202\nI0119 16:58:55.521013       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/9tb 510\nI0119 16:58:55.721031       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/k6j5 224\nI0119 16:58:55.921136       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/24d 296\nI0119 16:58:56.121121       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/p6z 283\nI0119 16:58:56.321164       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/69bl 599\nI0119 16:58:56.521022       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/nk5 205\nI0119 16:58:56.721162       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/z6bc 451\nI0119 16:58:56.921187       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/n4w 299\nI0119 16:58:57.121205       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/72c 223\n"
STEP: limiting log lines
Jan 19 16:58:57.260: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs logs-generator logs-generator --namespace=kubectl-5855 --tail=1'
Jan 19 16:58:57.435: INFO: stderr: ""
Jan 19 16:58:57.435: INFO: stdout: "I0119 16:58:57.321027       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/l65 389\n"
Jan 19 16:58:57.435: INFO: got output "I0119 16:58:57.321027       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/l65 389\n"
STEP: limiting log bytes
Jan 19 16:58:57.435: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs logs-generator logs-generator --namespace=kubectl-5855 --limit-bytes=1'
Jan 19 16:58:57.585: INFO: stderr: ""
Jan 19 16:58:57.585: INFO: stdout: "I"
Jan 19 16:58:57.585: INFO: got output "I"
STEP: exposing timestamps
Jan 19 16:58:57.585: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs logs-generator logs-generator --namespace=kubectl-5855 --tail=1 --timestamps'
Jan 19 16:58:57.763: INFO: stderr: ""
Jan 19 16:58:57.763: INFO: stdout: "2020-01-19T16:58:57.721185737Z I0119 16:58:57.721006       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/nptz 447\n"
Jan 19 16:58:57.763: INFO: got output "2020-01-19T16:58:57.721185737Z I0119 16:58:57.721006       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/nptz 447\n"
STEP: restricting to a time range
Jan 19 16:59:00.263: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs logs-generator logs-generator --namespace=kubectl-5855 --since=1s'
Jan 19 16:59:00.445: INFO: stderr: ""
Jan 19 16:59:00.446: INFO: stdout: "I0119 16:58:59.521005       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/zjd 429\nI0119 16:58:59.721004       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/2nxw 314\nI0119 16:58:59.920995       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/lfw 552\nI0119 16:59:00.121007       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/k4q9 256\nI0119 16:59:00.321047       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/bfv 548\n"
Jan 19 16:59:00.446: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs logs-generator logs-generator --namespace=kubectl-5855 --since=24h'
Jan 19 16:59:00.584: INFO: stderr: ""
Jan 19 16:59:00.584: INFO: stdout: "I0119 16:58:54.920866       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/9n4t 206\nI0119 16:58:55.121004       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/hsl 338\nI0119 16:58:55.321117       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/gbgk 202\nI0119 16:58:55.521013       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/9tb 510\nI0119 16:58:55.721031       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/k6j5 224\nI0119 16:58:55.921136       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/24d 296\nI0119 16:58:56.121121       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/p6z 283\nI0119 16:58:56.321164       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/69bl 599\nI0119 16:58:56.521022       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/nk5 205\nI0119 16:58:56.721162       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/z6bc 451\nI0119 16:58:56.921187       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/n4w 299\nI0119 16:58:57.121205       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/72c 223\nI0119 16:58:57.321027       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/l65 389\nI0119 16:58:57.521088       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/gbj 325\nI0119 16:58:57.721006       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/nptz 447\nI0119 16:58:57.921020       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/qv4g 547\nI0119 16:58:58.121001       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/kbvh 369\nI0119 16:58:58.321007       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/9bvl 222\nI0119 16:58:58.521016       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/tkw7 254\nI0119 16:58:58.721015       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/qs6 245\nI0119 16:58:58.921004       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/9wnl 582\nI0119 16:58:59.121016       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/z7z 541\nI0119 16:58:59.321010       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/9nf 316\nI0119 16:58:59.521005       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/zjd 429\nI0119 16:58:59.721004       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/kube-system/pods/2nxw 314\nI0119 16:58:59.920995       1 logs_generator.go:76] 25 POST /api/v1/namespaces/kube-system/pods/lfw 552\nI0119 16:59:00.121007       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/k4q9 256\nI0119 16:59:00.321047       1 logs_generator.go:76] 27 POST /api/v1/namespaces/default/pods/bfv 548\nI0119 16:59:00.521005       1 logs_generator.go:76] 28 GET /api/v1/namespaces/kube-system/pods/zh5x 453\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1470
Jan 19 16:59:00.584: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pod logs-generator --namespace=kubectl-5855'
Jan 19 16:59:07.211: INFO: stderr: ""
Jan 19 16:59:07.211: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:59:07.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5855" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":280,"completed":72,"skipped":1082,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:59:07.262: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-568
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-f9704605-c84f-4547-b59b-b6eadcfca52d
STEP: Creating a pod to test consume configMaps
Jan 19 16:59:07.521: INFO: Waiting up to 5m0s for pod "pod-configmaps-a28eb2ef-b842-4ed5-baa4-f51ab20fe865" in namespace "configmap-568" to be "success or failure"
Jan 19 16:59:07.537: INFO: Pod "pod-configmaps-a28eb2ef-b842-4ed5-baa4-f51ab20fe865": Phase="Pending", Reason="", readiness=false. Elapsed: 16.275945ms
Jan 19 16:59:09.555: INFO: Pod "pod-configmaps-a28eb2ef-b842-4ed5-baa4-f51ab20fe865": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034357999s
Jan 19 16:59:11.577: INFO: Pod "pod-configmaps-a28eb2ef-b842-4ed5-baa4-f51ab20fe865": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055754284s
STEP: Saw pod success
Jan 19 16:59:11.577: INFO: Pod "pod-configmaps-a28eb2ef-b842-4ed5-baa4-f51ab20fe865" satisfied condition "success or failure"
Jan 19 16:59:11.593: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-configmaps-a28eb2ef-b842-4ed5-baa4-f51ab20fe865 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 19 16:59:11.642: INFO: Waiting for pod pod-configmaps-a28eb2ef-b842-4ed5-baa4-f51ab20fe865 to disappear
Jan 19 16:59:11.658: INFO: Pod pod-configmaps-a28eb2ef-b842-4ed5-baa4-f51ab20fe865 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:59:11.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-568" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":73,"skipped":1125,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:59:11.710: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 16:59:11.937: INFO: Creating ReplicaSet my-hostname-basic-111d21f9-8484-460b-8de7-37aa8b3a3a11
Jan 19 16:59:11.971: INFO: Pod name my-hostname-basic-111d21f9-8484-460b-8de7-37aa8b3a3a11: Found 1 pods out of 1
Jan 19 16:59:11.971: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-111d21f9-8484-460b-8de7-37aa8b3a3a11" is running
Jan 19 16:59:16.011: INFO: Pod "my-hostname-basic-111d21f9-8484-460b-8de7-37aa8b3a3a11-5srdf" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-19 16:59:11 +0000 UTC Reason: Message:}])
Jan 19 16:59:16.011: INFO: Trying to dial the pod
Jan 19 16:59:21.151: INFO: Controller my-hostname-basic-111d21f9-8484-460b-8de7-37aa8b3a3a11: Got expected result from replica 1 [my-hostname-basic-111d21f9-8484-460b-8de7-37aa8b3a3a11-5srdf]: "my-hostname-basic-111d21f9-8484-460b-8de7-37aa8b3a3a11-5srdf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:59:21.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2576" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":74,"skipped":1135,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:59:21.201: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-6823
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jan 19 16:59:21.423: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the sample API server.
Jan 19 16:59:22.284: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:24.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:26.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:28.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:30.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:32.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:34.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:36.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:38.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:40.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:42.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:44.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:46.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:48.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:50.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:52.300: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049961, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-867766ffc6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:55.242: INFO: Waited 921.651351ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 16:59:56.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6823" for this suite.
•{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]","total":280,"completed":75,"skipped":1157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 16:59:56.284: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 16:59:57.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049996, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049996, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049996, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049996, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 16:59:59.061: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049996, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049996, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049996, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715049996, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:00:02.092: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jan 19 17:00:02.237: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:00:02.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6969" for this suite.
STEP: Destroying namespace "webhook-6969-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":280,"completed":76,"skipped":1192,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:00:02.487: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4238
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4238
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating stateful set ss in namespace statefulset-4238
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4238
Jan 19 17:00:02.777: INFO: Found 0 stateful pods, waiting for 1
Jan 19 17:00:12.795: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 19 17:00:12.811: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4238 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 17:00:13.409: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 17:00:13.409: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 17:00:13.409: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 17:00:13.426: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 19 17:00:23.444: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 17:00:23.444: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 17:00:23.512: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999503s
Jan 19 17:00:24.530: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981316656s
Jan 19 17:00:25.547: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.96305565s
Jan 19 17:00:26.564: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.94579631s
Jan 19 17:00:27.583: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.928548449s
Jan 19 17:00:28.602: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.910262977s
Jan 19 17:00:29.620: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.891321773s
Jan 19 17:00:30.638: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.872508367s
Jan 19 17:00:31.657: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.855187714s
Jan 19 17:00:32.675: INFO: Verifying statefulset ss doesn't scale past 3 for another 836.218236ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4238
Jan 19 17:00:33.693: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4238 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 17:00:34.222: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 17:00:34.222: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 17:00:34.222: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 17:00:34.222: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4238 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 17:00:34.785: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 19 17:00:34.785: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 17:00:34.785: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 17:00:34.785: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4238 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 17:00:35.341: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 19 17:00:35.341: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 17:00:35.341: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 17:00:35.358: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 17:00:35.358: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 17:00:35.358: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 19 17:00:35.375: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4238 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 17:00:35.946: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 17:00:35.946: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 17:00:35.946: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 17:00:35.946: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4238 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 17:00:36.738: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 17:00:36.738: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 17:00:36.738: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 17:00:36.738: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4238 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 17:00:37.322: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 17:00:37.322: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 17:00:37.322: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 17:00:37.322: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 17:00:37.340: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jan 19 17:00:47.374: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 17:00:47.374: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 17:00:47.374: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 17:00:47.426: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jan 19 17:00:47.426: INFO: ss-0  shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:02 +0000 UTC  }]
Jan 19 17:00:47.426: INFO: ss-1  shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:47.426: INFO: ss-2  shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:47.426: INFO: 
Jan 19 17:00:47.426: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 19 17:00:48.443: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jan 19 17:00:48.443: INFO: ss-0  shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:02 +0000 UTC  }]
Jan 19 17:00:48.443: INFO: ss-1  shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:48.443: INFO: ss-2  shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:48.443: INFO: 
Jan 19 17:00:48.443: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 19 17:00:49.461: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jan 19 17:00:49.461: INFO: ss-0  shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:02 +0000 UTC  }]
Jan 19 17:00:49.462: INFO: ss-1  shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:49.462: INFO: ss-2  shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:49.462: INFO: 
Jan 19 17:00:49.462: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 19 17:00:50.479: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jan 19 17:00:50.479: INFO: ss-1  shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:50.479: INFO: ss-2  shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:50.479: INFO: 
Jan 19 17:00:50.479: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 19 17:00:51.497: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jan 19 17:00:51.497: INFO: ss-1  shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:51.497: INFO: ss-2  shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:51.497: INFO: 
Jan 19 17:00:51.497: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 19 17:00:52.514: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jan 19 17:00:52.514: INFO: ss-1  shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:52.515: INFO: ss-2  shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:52.515: INFO: 
Jan 19 17:00:52.515: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 19 17:00:53.532: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jan 19 17:00:53.532: INFO: ss-2  shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:53.532: INFO: 
Jan 19 17:00:53.532: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 19 17:00:54.550: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jan 19 17:00:54.550: INFO: ss-2  shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:54.550: INFO: 
Jan 19 17:00:54.550: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 19 17:00:55.568: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jan 19 17:00:55.568: INFO: ss-2  shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:55.568: INFO: 
Jan 19 17:00:55.568: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 19 17:00:56.586: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jan 19 17:00:56.586: INFO: ss-2  shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-19 17:00:23 +0000 UTC  }]
Jan 19 17:00:56.586: INFO: 
Jan 19 17:00:56.587: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4238
Jan 19 17:00:57.603: INFO: Scaling statefulset ss to 0
Jan 19 17:00:57.654: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 19 17:00:57.671: INFO: Deleting all statefulset in ns statefulset-4238
Jan 19 17:00:57.688: INFO: Scaling statefulset ss to 0
Jan 19 17:00:57.738: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 17:00:57.754: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:00:57.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4238" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":280,"completed":77,"skipped":1216,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:00:57.878: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-2854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test hostPath mode
Jan 19 17:00:58.118: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-2854" to be "success or failure"
Jan 19 17:00:58.135: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 16.282597ms
Jan 19 17:01:00.152: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033751367s
Jan 19 17:01:02.170: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0510488s
Jan 19 17:01:04.187: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.068679913s
STEP: Saw pod success
Jan 19 17:01:04.187: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jan 19 17:01:04.204: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jan 19 17:01:04.343: INFO: Waiting for pod pod-host-path-test to disappear
Jan 19 17:01:04.359: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:01:04.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-2854" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":78,"skipped":1238,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:01:04.408: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9902
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating secret secrets-9902/secret-test-7454d049-0b8c-48f4-827c-267368812b93
STEP: Creating a pod to test consume secrets
Jan 19 17:01:04.670: INFO: Waiting up to 5m0s for pod "pod-configmaps-319a483e-cbc0-4d1b-b6b5-2cffbd02160d" in namespace "secrets-9902" to be "success or failure"
Jan 19 17:01:04.690: INFO: Pod "pod-configmaps-319a483e-cbc0-4d1b-b6b5-2cffbd02160d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.354765ms
Jan 19 17:01:06.707: INFO: Pod "pod-configmaps-319a483e-cbc0-4d1b-b6b5-2cffbd02160d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036662549s
Jan 19 17:01:08.738: INFO: Pod "pod-configmaps-319a483e-cbc0-4d1b-b6b5-2cffbd02160d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067499595s
STEP: Saw pod success
Jan 19 17:01:08.738: INFO: Pod "pod-configmaps-319a483e-cbc0-4d1b-b6b5-2cffbd02160d" satisfied condition "success or failure"
Jan 19 17:01:08.755: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-configmaps-319a483e-cbc0-4d1b-b6b5-2cffbd02160d container env-test: <nil>
STEP: delete the pod
Jan 19 17:01:08.803: INFO: Waiting for pod pod-configmaps-319a483e-cbc0-4d1b-b6b5-2cffbd02160d to disappear
Jan 19 17:01:08.819: INFO: Pod pod-configmaps-319a483e-cbc0-4d1b-b6b5-2cffbd02160d no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:01:08.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9902" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":79,"skipped":1243,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:01:08.869: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3318
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-ff95ff90-59b9-411d-a2b0-2f9b67a4c823
STEP: Creating a pod to test consume secrets
Jan 19 17:01:09.130: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a3d79a9a-9ce6-4d44-82d3-9c0823d3ffd4" in namespace "projected-3318" to be "success or failure"
Jan 19 17:01:09.147: INFO: Pod "pod-projected-secrets-a3d79a9a-9ce6-4d44-82d3-9c0823d3ffd4": Phase="Pending", Reason="", readiness=false. Elapsed: 17.394034ms
Jan 19 17:01:11.165: INFO: Pod "pod-projected-secrets-a3d79a9a-9ce6-4d44-82d3-9c0823d3ffd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034773325s
Jan 19 17:01:13.182: INFO: Pod "pod-projected-secrets-a3d79a9a-9ce6-4d44-82d3-9c0823d3ffd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052210691s
STEP: Saw pod success
Jan 19 17:01:13.182: INFO: Pod "pod-projected-secrets-a3d79a9a-9ce6-4d44-82d3-9c0823d3ffd4" satisfied condition "success or failure"
Jan 19 17:01:13.199: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-projected-secrets-a3d79a9a-9ce6-4d44-82d3-9c0823d3ffd4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 19 17:01:13.246: INFO: Waiting for pod pod-projected-secrets-a3d79a9a-9ce6-4d44-82d3-9c0823d3ffd4 to disappear
Jan 19 17:01:13.262: INFO: Pod pod-projected-secrets-a3d79a9a-9ce6-4d44-82d3-9c0823d3ffd4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:01:13.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3318" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":80,"skipped":1251,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:01:13.312: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5064
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
Jan 19 17:01:13.687: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0119 17:01:13.687550    7438 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:01:13.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5064" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":280,"completed":81,"skipped":1259,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:01:13.723: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7970
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:01:13.983: INFO: (0) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 20.638763ms)
Jan 19 17:01:14.026: INFO: (1) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 42.985966ms)
Jan 19 17:01:14.044: INFO: (2) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 17.721442ms)
Jan 19 17:01:14.062: INFO: (3) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.060036ms)
Jan 19 17:01:14.080: INFO: (4) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 17.910795ms)
Jan 19 17:01:14.098: INFO: (5) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 17.923216ms)
Jan 19 17:01:14.116: INFO: (6) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.140097ms)
Jan 19 17:01:14.134: INFO: (7) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 17.866267ms)
Jan 19 17:01:14.152: INFO: (8) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.475754ms)
Jan 19 17:01:14.171: INFO: (9) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.192214ms)
Jan 19 17:01:14.189: INFO: (10) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.108064ms)
Jan 19 17:01:14.215: INFO: (11) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.064227ms)
Jan 19 17:01:14.233: INFO: (12) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 17.679643ms)
Jan 19 17:01:14.251: INFO: (13) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.378905ms)
Jan 19 17:01:14.269: INFO: (14) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.075295ms)
Jan 19 17:01:14.288: INFO: (15) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.366016ms)
Jan 19 17:01:14.307: INFO: (16) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.859427ms)
Jan 19 17:01:14.325: INFO: (17) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 17.808496ms)
Jan 19 17:01:14.343: INFO: (18) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.043488ms)
Jan 19 17:01:14.361: INFO: (19) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 17.96239ms)
[AfterEach] version v1
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:01:14.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7970" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":280,"completed":82,"skipped":1274,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:01:14.397: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 17:01:14.639: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f967b94-3708-40cc-8080-c849241a6169" in namespace "downward-api-3960" to be "success or failure"
Jan 19 17:01:14.655: INFO: Pod "downwardapi-volume-3f967b94-3708-40cc-8080-c849241a6169": Phase="Pending", Reason="", readiness=false. Elapsed: 16.066123ms
Jan 19 17:01:16.673: INFO: Pod "downwardapi-volume-3f967b94-3708-40cc-8080-c849241a6169": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033566755s
Jan 19 17:01:18.691: INFO: Pod "downwardapi-volume-3f967b94-3708-40cc-8080-c849241a6169": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051485586s
STEP: Saw pod success
Jan 19 17:01:18.691: INFO: Pod "downwardapi-volume-3f967b94-3708-40cc-8080-c849241a6169" satisfied condition "success or failure"
Jan 19 17:01:18.707: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-3f967b94-3708-40cc-8080-c849241a6169 container client-container: <nil>
STEP: delete the pod
Jan 19 17:01:18.759: INFO: Waiting for pod downwardapi-volume-3f967b94-3708-40cc-8080-c849241a6169 to disappear
Jan 19 17:01:18.776: INFO: Pod downwardapi-volume-3f967b94-3708-40cc-8080-c849241a6169 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:01:18.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3960" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":83,"skipped":1294,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:01:18.834: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2468
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 19 17:01:19.086: INFO: Waiting up to 5m0s for pod "downward-api-fd62718b-79d8-48e6-9ebf-aa2938a72d23" in namespace "downward-api-2468" to be "success or failure"
Jan 19 17:01:19.108: INFO: Pod "downward-api-fd62718b-79d8-48e6-9ebf-aa2938a72d23": Phase="Pending", Reason="", readiness=false. Elapsed: 22.606828ms
Jan 19 17:01:21.126: INFO: Pod "downward-api-fd62718b-79d8-48e6-9ebf-aa2938a72d23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040417721s
Jan 19 17:01:23.151: INFO: Pod "downward-api-fd62718b-79d8-48e6-9ebf-aa2938a72d23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065167836s
STEP: Saw pod success
Jan 19 17:01:23.151: INFO: Pod "downward-api-fd62718b-79d8-48e6-9ebf-aa2938a72d23" satisfied condition "success or failure"
Jan 19 17:01:23.167: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downward-api-fd62718b-79d8-48e6-9ebf-aa2938a72d23 container dapi-container: <nil>
STEP: delete the pod
Jan 19 17:01:23.214: INFO: Waiting for pod downward-api-fd62718b-79d8-48e6-9ebf-aa2938a72d23 to disappear
Jan 19 17:01:23.230: INFO: Pod downward-api-fd62718b-79d8-48e6-9ebf-aa2938a72d23 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:01:23.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2468" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":280,"completed":84,"skipped":1309,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:01:23.280: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-128
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:01:24.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050083, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050083, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050083, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050083, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:01:26.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050083, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050083, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050083, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050083, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:01:29.215: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:01:29.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-128" for this suite.
STEP: Destroying namespace "webhook-128-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":280,"completed":85,"skipped":1329,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:01:29.450: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-247
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-7aa9cc86-afd9-4a30-b8f4-3ad784493e66
STEP: Creating a pod to test consume secrets
Jan 19 17:01:29.711: INFO: Waiting up to 5m0s for pod "pod-secrets-a9231f0d-da52-493e-b38a-91bdce46d282" in namespace "secrets-247" to be "success or failure"
Jan 19 17:01:29.727: INFO: Pod "pod-secrets-a9231f0d-da52-493e-b38a-91bdce46d282": Phase="Pending", Reason="", readiness=false. Elapsed: 16.115422ms
Jan 19 17:01:31.744: INFO: Pod "pod-secrets-a9231f0d-da52-493e-b38a-91bdce46d282": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033350515s
Jan 19 17:01:33.762: INFO: Pod "pod-secrets-a9231f0d-da52-493e-b38a-91bdce46d282": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050600617s
STEP: Saw pod success
Jan 19 17:01:33.762: INFO: Pod "pod-secrets-a9231f0d-da52-493e-b38a-91bdce46d282" satisfied condition "success or failure"
Jan 19 17:01:33.779: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-secrets-a9231f0d-da52-493e-b38a-91bdce46d282 container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 17:01:33.825: INFO: Waiting for pod pod-secrets-a9231f0d-da52-493e-b38a-91bdce46d282 to disappear
Jan 19 17:01:33.841: INFO: Pod pod-secrets-a9231f0d-da52-493e-b38a-91bdce46d282 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:01:33.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-247" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":86,"skipped":1336,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:01:33.891: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9286
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 19 17:01:35.024: INFO: Pod name wrapped-volume-race-c91cca6c-bb83-4d97-a3f8-a739a98570f2: Found 3 pods out of 5
Jan 19 17:01:40.059: INFO: Pod name wrapped-volume-race-c91cca6c-bb83-4d97-a3f8-a739a98570f2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c91cca6c-bb83-4d97-a3f8-a739a98570f2 in namespace emptydir-wrapper-9286, will wait for the garbage collector to delete the pods
Jan 19 17:01:44.273: INFO: Deleting ReplicationController wrapped-volume-race-c91cca6c-bb83-4d97-a3f8-a739a98570f2 took: 19.090186ms
Jan 19 17:01:44.373: INFO: Terminating ReplicationController wrapped-volume-race-c91cca6c-bb83-4d97-a3f8-a739a98570f2 pods took: 100.255777ms
STEP: Creating RC which spawns configmap-volume pods
Jan 19 17:01:57.437: INFO: Pod name wrapped-volume-race-58576600-64de-4c6c-b54d-48bd800a6c31: Found 3 pods out of 5
Jan 19 17:02:02.472: INFO: Pod name wrapped-volume-race-58576600-64de-4c6c-b54d-48bd800a6c31: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-58576600-64de-4c6c-b54d-48bd800a6c31 in namespace emptydir-wrapper-9286, will wait for the garbage collector to delete the pods
Jan 19 17:02:06.668: INFO: Deleting ReplicationController wrapped-volume-race-58576600-64de-4c6c-b54d-48bd800a6c31 took: 21.247293ms
Jan 19 17:02:06.768: INFO: Terminating ReplicationController wrapped-volume-race-58576600-64de-4c6c-b54d-48bd800a6c31 pods took: 100.276896ms
STEP: Creating RC which spawns configmap-volume pods
Jan 19 17:02:17.430: INFO: Pod name wrapped-volume-race-8757598d-1058-4a35-89a3-28e62b06408c: Found 1 pods out of 5
Jan 19 17:02:22.466: INFO: Pod name wrapped-volume-race-8757598d-1058-4a35-89a3-28e62b06408c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8757598d-1058-4a35-89a3-28e62b06408c in namespace emptydir-wrapper-9286, will wait for the garbage collector to delete the pods
Jan 19 17:02:28.657: INFO: Deleting ReplicationController wrapped-volume-race-8757598d-1058-4a35-89a3-28e62b06408c took: 19.061427ms
Jan 19 17:02:28.758: INFO: Terminating ReplicationController wrapped-volume-race-8757598d-1058-4a35-89a3-28e62b06408c pods took: 100.280656ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:02:43.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9286" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":280,"completed":87,"skipped":1346,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:02:43.855: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2975
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:02:44.838: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050164, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050164, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050164, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050164, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:02:46.855: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050164, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050164, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050164, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050164, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:02:48.855: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050164, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050164, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050164, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050164, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:02:51.894: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:02:52.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2975" for this suite.
STEP: Destroying namespace "webhook-2975-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":280,"completed":88,"skipped":1359,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:02:52.372: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2523
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 19 17:03:00.776: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 19 17:03:00.793: INFO: Pod pod-with-poststart-http-hook still exists
Jan 19 17:03:02.793: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 19 17:03:02.811: INFO: Pod pod-with-poststart-http-hook still exists
Jan 19 17:03:04.793: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 19 17:03:04.811: INFO: Pod pod-with-poststart-http-hook still exists
Jan 19 17:03:06.793: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 19 17:03:06.811: INFO: Pod pod-with-poststart-http-hook still exists
Jan 19 17:03:08.793: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 19 17:03:08.811: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:03:08.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2523" for this suite.
•{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":280,"completed":89,"skipped":1398,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:03:08.861: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-1674
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:125
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:03:09.443: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 19 17:03:11.494: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050188, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050188, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050188, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050188, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-78dcf5dd84\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:03:14.542: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:03:14.560: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:03:15.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1674" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:136
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":280,"completed":90,"skipped":1398,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:03:15.677: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4031
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 19 17:03:15.927: INFO: Waiting up to 5m0s for pod "downward-api-0fbe71d9-8019-4024-bede-4ce761f8c2d2" in namespace "downward-api-4031" to be "success or failure"
Jan 19 17:03:15.948: INFO: Pod "downward-api-0fbe71d9-8019-4024-bede-4ce761f8c2d2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.984118ms
Jan 19 17:03:17.966: INFO: Pod "downward-api-0fbe71d9-8019-4024-bede-4ce761f8c2d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038832061s
Jan 19 17:03:19.988: INFO: Pod "downward-api-0fbe71d9-8019-4024-bede-4ce761f8c2d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060747849s
STEP: Saw pod success
Jan 19 17:03:19.988: INFO: Pod "downward-api-0fbe71d9-8019-4024-bede-4ce761f8c2d2" satisfied condition "success or failure"
Jan 19 17:03:20.006: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downward-api-0fbe71d9-8019-4024-bede-4ce761f8c2d2 container dapi-container: <nil>
STEP: delete the pod
Jan 19 17:03:20.057: INFO: Waiting for pod downward-api-0fbe71d9-8019-4024-bede-4ce761f8c2d2 to disappear
Jan 19 17:03:20.074: INFO: Pod downward-api-0fbe71d9-8019-4024-bede-4ce761f8c2d2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:03:20.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4031" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":280,"completed":91,"skipped":1403,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:03:20.129: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2260
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 17:03:20.376: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb5fa585-5263-4b4c-a393-d9ad5b31e3bb" in namespace "projected-2260" to be "success or failure"
Jan 19 17:03:20.392: INFO: Pod "downwardapi-volume-bb5fa585-5263-4b4c-a393-d9ad5b31e3bb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.209773ms
Jan 19 17:03:22.409: INFO: Pod "downwardapi-volume-bb5fa585-5263-4b4c-a393-d9ad5b31e3bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033544802s
Jan 19 17:03:24.427: INFO: Pod "downwardapi-volume-bb5fa585-5263-4b4c-a393-d9ad5b31e3bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051073662s
STEP: Saw pod success
Jan 19 17:03:24.427: INFO: Pod "downwardapi-volume-bb5fa585-5263-4b4c-a393-d9ad5b31e3bb" satisfied condition "success or failure"
Jan 19 17:03:24.443: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-bb5fa585-5263-4b4c-a393-d9ad5b31e3bb container client-container: <nil>
STEP: delete the pod
Jan 19 17:03:24.490: INFO: Waiting for pod downwardapi-volume-bb5fa585-5263-4b4c-a393-d9ad5b31e3bb to disappear
Jan 19 17:03:24.507: INFO: Pod downwardapi-volume-bb5fa585-5263-4b4c-a393-d9ad5b31e3bb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:03:24.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2260" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":280,"completed":92,"skipped":1409,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:03:24.558: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7448
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jan 19 17:03:34.905: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0119 17:03:34.904876    7438 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:03:34.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7448" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":280,"completed":93,"skipped":1418,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:03:34.940: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1150
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jan 19 17:03:39.270: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1150 PodName:pod-sharedvolume-f6f18db0-89d7-4285-8fde-48dfdda92c26 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:03:39.270: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:03:39.750: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:03:39.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1150" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":280,"completed":94,"skipped":1428,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:03:39.806: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:03:40.131: INFO: Create a RollingUpdate DaemonSet
Jan 19 17:03:40.151: INFO: Check that daemon pods launch on every node of the cluster
Jan 19 17:03:40.198: INFO: Number of nodes with available pods: 0
Jan 19 17:03:40.199: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:03:41.249: INFO: Number of nodes with available pods: 0
Jan 19 17:03:41.249: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:03:42.250: INFO: Number of nodes with available pods: 0
Jan 19 17:03:42.250: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:03:43.251: INFO: Number of nodes with available pods: 2
Jan 19 17:03:43.251: INFO: Number of running nodes: 2, number of available pods: 2
Jan 19 17:03:43.251: INFO: Update the DaemonSet to trigger a rollout
Jan 19 17:03:43.288: INFO: Updating DaemonSet daemon-set
Jan 19 17:03:53.374: INFO: Roll back the DaemonSet before rollout is complete
Jan 19 17:03:53.408: INFO: Updating DaemonSet daemon-set
Jan 19 17:03:53.408: INFO: Make sure DaemonSet rollback is complete
Jan 19 17:03:53.432: INFO: Wrong image for pod: daemon-set-5vkp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 19 17:03:53.432: INFO: Pod daemon-set-5vkp5 is not available
Jan 19 17:03:54.466: INFO: Wrong image for pod: daemon-set-5vkp5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jan 19 17:03:54.466: INFO: Pod daemon-set-5vkp5 is not available
Jan 19 17:03:55.466: INFO: Pod daemon-set-w6hpx is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6186, will wait for the garbage collector to delete the pods
Jan 19 17:03:55.623: INFO: Deleting DaemonSet.extensions daemon-set took: 19.177421ms
Jan 19 17:03:55.723: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.317716ms
Jan 19 17:04:07.240: INFO: Number of nodes with available pods: 0
Jan 19 17:04:07.240: INFO: Number of running nodes: 0, number of available pods: 0
Jan 19 17:04:07.256: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6186/daemonsets","resourceVersion":"17722"},"items":null}

Jan 19 17:04:07.274: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6186/pods","resourceVersion":"17722"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:04:07.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6186" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":280,"completed":95,"skipped":1437,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:04:07.394: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3463
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 19 17:04:07.642: INFO: Waiting up to 5m0s for pod "pod-dd2d3639-fe24-4df2-ae82-43f08f20a740" in namespace "emptydir-3463" to be "success or failure"
Jan 19 17:04:07.658: INFO: Pod "pod-dd2d3639-fe24-4df2-ae82-43f08f20a740": Phase="Pending", Reason="", readiness=false. Elapsed: 16.284956ms
Jan 19 17:04:09.675: INFO: Pod "pod-dd2d3639-fe24-4df2-ae82-43f08f20a740": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03342389s
Jan 19 17:04:11.693: INFO: Pod "pod-dd2d3639-fe24-4df2-ae82-43f08f20a740": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05116791s
STEP: Saw pod success
Jan 19 17:04:11.693: INFO: Pod "pod-dd2d3639-fe24-4df2-ae82-43f08f20a740" satisfied condition "success or failure"
Jan 19 17:04:11.710: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-dd2d3639-fe24-4df2-ae82-43f08f20a740 container test-container: <nil>
STEP: delete the pod
Jan 19 17:04:11.769: INFO: Waiting for pod pod-dd2d3639-fe24-4df2-ae82-43f08f20a740 to disappear
Jan 19 17:04:11.786: INFO: Pod pod-dd2d3639-fe24-4df2-ae82-43f08f20a740 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:04:11.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3463" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":96,"skipped":1442,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:04:11.836: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6457
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-6457
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating statefulset ss in namespace statefulset-6457
Jan 19 17:04:12.127: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jan 19 17:04:22.145: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 19 17:04:22.230: INFO: Deleting all statefulset in ns statefulset-6457
Jan 19 17:04:22.246: INFO: Scaling statefulset ss to 0
Jan 19 17:04:42.367: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 17:04:42.383: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:04:42.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6457" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":280,"completed":97,"skipped":1455,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:04:42.486: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9516
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1692
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 19 17:04:42.707: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-9516'
Jan 19 17:04:42.831: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 19 17:04:42.831: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: rolling-update to same image controller
Jan 19 17:04:42.866: INFO: scanned /root for discovery docs: <nil>
Jan 19 17:04:42.866: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-9516'
Jan 19 17:04:58.094: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 19 17:04:58.094: INFO: stdout: "Created e2e-test-httpd-rc-ca60e0d55376d081f62dca93bab6cfe4\nScaling up e2e-test-httpd-rc-ca60e0d55376d081f62dca93bab6cfe4 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-ca60e0d55376d081f62dca93bab6cfe4 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-ca60e0d55376d081f62dca93bab6cfe4 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Jan 19 17:04:58.094: INFO: stdout: "Created e2e-test-httpd-rc-ca60e0d55376d081f62dca93bab6cfe4\nScaling up e2e-test-httpd-rc-ca60e0d55376d081f62dca93bab6cfe4 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-ca60e0d55376d081f62dca93bab6cfe4 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-ca60e0d55376d081f62dca93bab6cfe4 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Jan 19 17:04:58.094: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-9516'
Jan 19 17:04:58.213: INFO: stderr: ""
Jan 19 17:04:58.213: INFO: stdout: "e2e-test-httpd-rc-ca60e0d55376d081f62dca93bab6cfe4-s4mzl "
Jan 19 17:04:58.214: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-httpd-rc-ca60e0d55376d081f62dca93bab6cfe4-s4mzl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9516'
Jan 19 17:04:58.331: INFO: stderr: ""
Jan 19 17:04:58.331: INFO: stdout: "true"
Jan 19 17:04:58.331: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-httpd-rc-ca60e0d55376d081f62dca93bab6cfe4-s4mzl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9516'
Jan 19 17:04:58.430: INFO: stderr: ""
Jan 19 17:04:58.430: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Jan 19 17:04:58.431: INFO: e2e-test-httpd-rc-ca60e0d55376d081f62dca93bab6cfe4-s4mzl is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1698
Jan 19 17:04:58.431: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-httpd-rc --namespace=kubectl-9516'
Jan 19 17:04:58.569: INFO: stderr: ""
Jan 19 17:04:58.569: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:04:58.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9516" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]","total":280,"completed":98,"skipped":1459,"failed":0}

------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:04:58.626: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8688
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-projected-all-test-volume-e5f7c591-4203-40b5-acd4-63476f917431
STEP: Creating secret with name secret-projected-all-test-volume-04958e16-99fd-4a05-bab6-85244686a40a
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 19 17:04:58.908: INFO: Waiting up to 5m0s for pod "projected-volume-6713a79f-eb4c-41bb-af15-eb6fcb91e5db" in namespace "projected-8688" to be "success or failure"
Jan 19 17:04:58.931: INFO: Pod "projected-volume-6713a79f-eb4c-41bb-af15-eb6fcb91e5db": Phase="Pending", Reason="", readiness=false. Elapsed: 22.963352ms
Jan 19 17:05:00.948: INFO: Pod "projected-volume-6713a79f-eb4c-41bb-af15-eb6fcb91e5db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040556675s
Jan 19 17:05:02.965: INFO: Pod "projected-volume-6713a79f-eb4c-41bb-af15-eb6fcb91e5db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057250309s
Jan 19 17:05:04.984: INFO: Pod "projected-volume-6713a79f-eb4c-41bb-af15-eb6fcb91e5db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07627166s
STEP: Saw pod success
Jan 19 17:05:04.984: INFO: Pod "projected-volume-6713a79f-eb4c-41bb-af15-eb6fcb91e5db" satisfied condition "success or failure"
Jan 19 17:05:05.000: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod projected-volume-6713a79f-eb4c-41bb-af15-eb6fcb91e5db container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 19 17:05:05.128: INFO: Waiting for pod projected-volume-6713a79f-eb4c-41bb-af15-eb6fcb91e5db to disappear
Jan 19 17:05:05.160: INFO: Pod projected-volume-6713a79f-eb4c-41bb-af15-eb6fcb91e5db no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:05:05.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8688" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":280,"completed":99,"skipped":1459,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:05:05.210: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-5967/configmap-test-34c734df-635d-4f78-8c95-f795fb25e92a
STEP: Creating a pod to test consume configMaps
Jan 19 17:05:05.469: INFO: Waiting up to 5m0s for pod "pod-configmaps-87763afa-a392-49f2-a56d-31662ecba016" in namespace "configmap-5967" to be "success or failure"
Jan 19 17:05:05.486: INFO: Pod "pod-configmaps-87763afa-a392-49f2-a56d-31662ecba016": Phase="Pending", Reason="", readiness=false. Elapsed: 17.130147ms
Jan 19 17:05:07.503: INFO: Pod "pod-configmaps-87763afa-a392-49f2-a56d-31662ecba016": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034496866s
Jan 19 17:05:09.521: INFO: Pod "pod-configmaps-87763afa-a392-49f2-a56d-31662ecba016": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052032097s
STEP: Saw pod success
Jan 19 17:05:09.521: INFO: Pod "pod-configmaps-87763afa-a392-49f2-a56d-31662ecba016" satisfied condition "success or failure"
Jan 19 17:05:09.538: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-configmaps-87763afa-a392-49f2-a56d-31662ecba016 container env-test: <nil>
STEP: delete the pod
Jan 19 17:05:09.588: INFO: Waiting for pod pod-configmaps-87763afa-a392-49f2-a56d-31662ecba016 to disappear
Jan 19 17:05:09.606: INFO: Pod pod-configmaps-87763afa-a392-49f2-a56d-31662ecba016 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:05:09.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5967" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":280,"completed":100,"skipped":1480,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:05:09.655: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-692
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-f584b60d-9263-4262-a91f-edade7bdd89b
STEP: Creating a pod to test consume secrets
Jan 19 17:05:09.914: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cf152214-0820-4099-8d19-ccae2e0773c4" in namespace "projected-692" to be "success or failure"
Jan 19 17:05:09.931: INFO: Pod "pod-projected-secrets-cf152214-0820-4099-8d19-ccae2e0773c4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.452413ms
Jan 19 17:05:11.948: INFO: Pod "pod-projected-secrets-cf152214-0820-4099-8d19-ccae2e0773c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033729761s
Jan 19 17:05:13.966: INFO: Pod "pod-projected-secrets-cf152214-0820-4099-8d19-ccae2e0773c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051492485s
STEP: Saw pod success
Jan 19 17:05:13.966: INFO: Pod "pod-projected-secrets-cf152214-0820-4099-8d19-ccae2e0773c4" satisfied condition "success or failure"
Jan 19 17:05:13.983: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-projected-secrets-cf152214-0820-4099-8d19-ccae2e0773c4 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 19 17:05:14.198: INFO: Waiting for pod pod-projected-secrets-cf152214-0820-4099-8d19-ccae2e0773c4 to disappear
Jan 19 17:05:14.230: INFO: Pod pod-projected-secrets-cf152214-0820-4099-8d19-ccae2e0773c4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:05:14.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-692" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":101,"skipped":1484,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:05:14.282: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7489
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-kfmwg in namespace proxy-7489
I0119 17:05:14.567329    7438 runners.go:189] Created replication controller with name: proxy-service-kfmwg, namespace: proxy-7489, replica count: 1
I0119 17:05:15.617769    7438 runners.go:189] proxy-service-kfmwg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 17:05:16.618071    7438 runners.go:189] proxy-service-kfmwg Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 17:05:17.618278    7438 runners.go:189] proxy-service-kfmwg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0119 17:05:18.618466    7438 runners.go:189] proxy-service-kfmwg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0119 17:05:19.618648    7438 runners.go:189] proxy-service-kfmwg Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0119 17:05:20.618974    7438 runners.go:189] proxy-service-kfmwg Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 17:05:20.636: INFO: setup took 6.11868568s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 19 17:05:20.699: INFO: (0) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 62.65845ms)
Jan 19 17:05:20.699: INFO: (0) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 62.688651ms)
Jan 19 17:05:20.699: INFO: (0) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 62.670637ms)
Jan 19 17:05:20.699: INFO: (0) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 62.658679ms)
Jan 19 17:05:20.699: INFO: (0) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 62.873521ms)
Jan 19 17:05:20.707: INFO: (0) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 70.860468ms)
Jan 19 17:05:20.707: INFO: (0) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 70.764589ms)
Jan 19 17:05:20.707: INFO: (0) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 70.860426ms)
Jan 19 17:05:20.707: INFO: (0) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 70.834805ms)
Jan 19 17:05:20.713: INFO: (0) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 76.933438ms)
Jan 19 17:05:20.713: INFO: (0) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 76.980887ms)
Jan 19 17:05:20.713: INFO: (0) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 76.972381ms)
Jan 19 17:05:20.713: INFO: (0) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 77.22068ms)
Jan 19 17:05:20.713: INFO: (0) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 77.030458ms)
Jan 19 17:05:20.713: INFO: (0) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 77.038971ms)
Jan 19 17:05:20.713: INFO: (0) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 77.110608ms)
Jan 19 17:05:20.733: INFO: (1) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.801798ms)
Jan 19 17:05:20.734: INFO: (1) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.999195ms)
Jan 19 17:05:20.734: INFO: (1) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 20.056045ms)
Jan 19 17:05:20.734: INFO: (1) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 20.220819ms)
Jan 19 17:05:20.734: INFO: (1) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 19.99759ms)
Jan 19 17:05:20.734: INFO: (1) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 20.347552ms)
Jan 19 17:05:20.734: INFO: (1) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 20.228475ms)
Jan 19 17:05:20.735: INFO: (1) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 21.725162ms)
Jan 19 17:05:20.735: INFO: (1) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 21.890635ms)
Jan 19 17:05:20.735: INFO: (1) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 21.801767ms)
Jan 19 17:05:20.735: INFO: (1) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 21.759761ms)
Jan 19 17:05:20.735: INFO: (1) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 21.997919ms)
Jan 19 17:05:20.735: INFO: (1) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 21.786323ms)
Jan 19 17:05:20.735: INFO: (1) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 21.800653ms)
Jan 19 17:05:20.736: INFO: (1) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 21.998004ms)
Jan 19 17:05:20.736: INFO: (1) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 22.705844ms)
Jan 19 17:05:20.756: INFO: (2) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 19.310196ms)
Jan 19 17:05:20.756: INFO: (2) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 19.19698ms)
Jan 19 17:05:20.756: INFO: (2) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.413185ms)
Jan 19 17:05:20.756: INFO: (2) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.315354ms)
Jan 19 17:05:20.756: INFO: (2) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 19.378213ms)
Jan 19 17:05:20.756: INFO: (2) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.407684ms)
Jan 19 17:05:20.756: INFO: (2) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 19.451037ms)
Jan 19 17:05:20.756: INFO: (2) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 19.541377ms)
Jan 19 17:05:20.756: INFO: (2) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 19.523361ms)
Jan 19 17:05:20.756: INFO: (2) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 19.474722ms)
Jan 19 17:05:20.756: INFO: (2) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.559367ms)
Jan 19 17:05:20.758: INFO: (2) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 22.196729ms)
Jan 19 17:05:20.758: INFO: (2) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 22.086906ms)
Jan 19 17:05:20.758: INFO: (2) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 22.143645ms)
Jan 19 17:05:20.760: INFO: (2) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 23.372189ms)
Jan 19 17:05:20.761: INFO: (2) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 24.581525ms)
Jan 19 17:05:20.781: INFO: (3) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 20.306407ms)
Jan 19 17:05:20.782: INFO: (3) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 20.706594ms)
Jan 19 17:05:20.782: INFO: (3) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 20.596915ms)
Jan 19 17:05:20.782: INFO: (3) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 20.65932ms)
Jan 19 17:05:20.782: INFO: (3) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 20.713507ms)
Jan 19 17:05:20.782: INFO: (3) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 20.688484ms)
Jan 19 17:05:20.782: INFO: (3) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 20.713288ms)
Jan 19 17:05:20.782: INFO: (3) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 21.096284ms)
Jan 19 17:05:20.782: INFO: (3) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 21.079267ms)
Jan 19 17:05:20.782: INFO: (3) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 21.122512ms)
Jan 19 17:05:20.783: INFO: (3) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 22.272937ms)
Jan 19 17:05:20.783: INFO: (3) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 22.173794ms)
Jan 19 17:05:20.784: INFO: (3) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 22.379148ms)
Jan 19 17:05:20.784: INFO: (3) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 22.543368ms)
Jan 19 17:05:20.784: INFO: (3) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 22.553753ms)
Jan 19 17:05:20.784: INFO: (3) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 23.307425ms)
Jan 19 17:05:20.803: INFO: (4) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 18.749168ms)
Jan 19 17:05:20.803: INFO: (4) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 18.640028ms)
Jan 19 17:05:20.803: INFO: (4) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 18.720683ms)
Jan 19 17:05:20.803: INFO: (4) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 18.711209ms)
Jan 19 17:05:20.804: INFO: (4) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 18.912763ms)
Jan 19 17:05:20.804: INFO: (4) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 18.81222ms)
Jan 19 17:05:20.804: INFO: (4) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 19.022088ms)
Jan 19 17:05:20.804: INFO: (4) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.155125ms)
Jan 19 17:05:20.804: INFO: (4) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 19.039241ms)
Jan 19 17:05:20.804: INFO: (4) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 19.053326ms)
Jan 19 17:05:20.804: INFO: (4) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 19.806805ms)
Jan 19 17:05:20.805: INFO: (4) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 19.979141ms)
Jan 19 17:05:20.807: INFO: (4) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 21.924384ms)
Jan 19 17:05:20.807: INFO: (4) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 22.00183ms)
Jan 19 17:05:20.807: INFO: (4) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 21.956439ms)
Jan 19 17:05:20.807: INFO: (4) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 22.565471ms)
Jan 19 17:05:20.827: INFO: (5) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 19.332897ms)
Jan 19 17:05:20.827: INFO: (5) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.409237ms)
Jan 19 17:05:20.827: INFO: (5) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 19.644141ms)
Jan 19 17:05:20.827: INFO: (5) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 19.413093ms)
Jan 19 17:05:20.827: INFO: (5) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.477613ms)
Jan 19 17:05:20.827: INFO: (5) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.415876ms)
Jan 19 17:05:20.827: INFO: (5) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.394583ms)
Jan 19 17:05:20.827: INFO: (5) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 19.521595ms)
Jan 19 17:05:20.827: INFO: (5) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 19.491013ms)
Jan 19 17:05:20.827: INFO: (5) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 19.500732ms)
Jan 19 17:05:20.827: INFO: (5) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 19.447183ms)
Jan 19 17:05:20.827: INFO: (5) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 19.508277ms)
Jan 19 17:05:20.829: INFO: (5) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 21.186381ms)
Jan 19 17:05:20.829: INFO: (5) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 21.811742ms)
Jan 19 17:05:20.829: INFO: (5) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 21.992212ms)
Jan 19 17:05:20.829: INFO: (5) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 22.013848ms)
Jan 19 17:05:20.849: INFO: (6) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 19.503764ms)
Jan 19 17:05:20.849: INFO: (6) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.793023ms)
Jan 19 17:05:20.849: INFO: (6) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 19.607385ms)
Jan 19 17:05:20.849: INFO: (6) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 19.702107ms)
Jan 19 17:05:20.849: INFO: (6) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 19.641579ms)
Jan 19 17:05:20.849: INFO: (6) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.780258ms)
Jan 19 17:05:20.849: INFO: (6) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 19.587754ms)
Jan 19 17:05:20.849: INFO: (6) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.675483ms)
Jan 19 17:05:20.849: INFO: (6) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 19.703119ms)
Jan 19 17:05:20.849: INFO: (6) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 19.686896ms)
Jan 19 17:05:20.849: INFO: (6) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 19.654208ms)
Jan 19 17:05:20.850: INFO: (6) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 20.569189ms)
Jan 19 17:05:20.851: INFO: (6) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 20.803355ms)
Jan 19 17:05:20.851: INFO: (6) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 21.378422ms)
Jan 19 17:05:20.852: INFO: (6) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 21.791219ms)
Jan 19 17:05:20.852: INFO: (6) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 22.019276ms)
Jan 19 17:05:20.871: INFO: (7) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 18.936906ms)
Jan 19 17:05:20.871: INFO: (7) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 19.000968ms)
Jan 19 17:05:20.871: INFO: (7) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.161588ms)
Jan 19 17:05:20.871: INFO: (7) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.047611ms)
Jan 19 17:05:20.871: INFO: (7) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 18.963184ms)
Jan 19 17:05:20.871: INFO: (7) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 19.124421ms)
Jan 19 17:05:20.871: INFO: (7) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 19.061733ms)
Jan 19 17:05:20.871: INFO: (7) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 19.042959ms)
Jan 19 17:05:20.871: INFO: (7) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 18.97387ms)
Jan 19 17:05:20.871: INFO: (7) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 19.055728ms)
Jan 19 17:05:20.871: INFO: (7) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 19.089325ms)
Jan 19 17:05:20.874: INFO: (7) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 21.915857ms)
Jan 19 17:05:20.874: INFO: (7) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 22.796629ms)
Jan 19 17:05:20.875: INFO: (7) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 22.875221ms)
Jan 19 17:05:20.875: INFO: (7) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 22.716923ms)
Jan 19 17:05:20.876: INFO: (7) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 23.726466ms)
Jan 19 17:05:20.895: INFO: (8) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.27265ms)
Jan 19 17:05:20.895: INFO: (8) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 19.232261ms)
Jan 19 17:05:20.895: INFO: (8) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 19.313767ms)
Jan 19 17:05:20.895: INFO: (8) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.199275ms)
Jan 19 17:05:20.895: INFO: (8) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 19.237164ms)
Jan 19 17:05:20.895: INFO: (8) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.497335ms)
Jan 19 17:05:20.895: INFO: (8) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 19.470604ms)
Jan 19 17:05:20.895: INFO: (8) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 19.562314ms)
Jan 19 17:05:20.895: INFO: (8) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.549414ms)
Jan 19 17:05:20.895: INFO: (8) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 19.50176ms)
Jan 19 17:05:20.895: INFO: (8) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 19.614546ms)
Jan 19 17:05:20.896: INFO: (8) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 20.484976ms)
Jan 19 17:05:20.897: INFO: (8) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 20.874579ms)
Jan 19 17:05:20.897: INFO: (8) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 21.007371ms)
Jan 19 17:05:20.897: INFO: (8) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 21.088026ms)
Jan 19 17:05:20.898: INFO: (8) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 21.943782ms)
Jan 19 17:05:20.917: INFO: (9) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 18.677275ms)
Jan 19 17:05:20.917: INFO: (9) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 18.813666ms)
Jan 19 17:05:20.917: INFO: (9) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 18.668366ms)
Jan 19 17:05:20.917: INFO: (9) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 18.750876ms)
Jan 19 17:05:20.917: INFO: (9) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 18.880132ms)
Jan 19 17:05:20.917: INFO: (9) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 18.727309ms)
Jan 19 17:05:20.917: INFO: (9) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.020301ms)
Jan 19 17:05:20.917: INFO: (9) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 18.997784ms)
Jan 19 17:05:20.917: INFO: (9) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 19.051213ms)
Jan 19 17:05:20.917: INFO: (9) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 19.204633ms)
Jan 19 17:05:20.918: INFO: (9) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 19.808119ms)
Jan 19 17:05:20.946: INFO: (9) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 48.517568ms)
Jan 19 17:05:20.946: INFO: (9) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 48.52833ms)
Jan 19 17:05:20.946: INFO: (9) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 48.487888ms)
Jan 19 17:05:20.947: INFO: (9) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 48.662064ms)
Jan 19 17:05:20.947: INFO: (9) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 48.797581ms)
Jan 19 17:05:20.967: INFO: (10) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 19.669397ms)
Jan 19 17:05:20.967: INFO: (10) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.792617ms)
Jan 19 17:05:20.967: INFO: (10) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.640258ms)
Jan 19 17:05:20.967: INFO: (10) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.825703ms)
Jan 19 17:05:20.967: INFO: (10) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 19.786641ms)
Jan 19 17:05:20.967: INFO: (10) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 19.70779ms)
Jan 19 17:05:20.967: INFO: (10) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 20.173627ms)
Jan 19 17:05:20.967: INFO: (10) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 19.969766ms)
Jan 19 17:05:20.967: INFO: (10) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 20.116642ms)
Jan 19 17:05:20.967: INFO: (10) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 20.18987ms)
Jan 19 17:05:20.967: INFO: (10) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 20.140583ms)
Jan 19 17:05:20.967: INFO: (10) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 20.492395ms)
Jan 19 17:05:20.968: INFO: (10) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 20.959232ms)
Jan 19 17:05:20.968: INFO: (10) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 20.999676ms)
Jan 19 17:05:20.968: INFO: (10) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 21.390568ms)
Jan 19 17:05:20.969: INFO: (10) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 21.838964ms)
Jan 19 17:05:20.988: INFO: (11) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 18.740207ms)
Jan 19 17:05:20.988: INFO: (11) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 18.76653ms)
Jan 19 17:05:20.988: INFO: (11) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 18.958352ms)
Jan 19 17:05:20.988: INFO: (11) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 18.886409ms)
Jan 19 17:05:20.988: INFO: (11) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 18.933367ms)
Jan 19 17:05:20.988: INFO: (11) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 18.846523ms)
Jan 19 17:05:20.988: INFO: (11) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 19.09463ms)
Jan 19 17:05:20.988: INFO: (11) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 19.057604ms)
Jan 19 17:05:20.988: INFO: (11) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 19.128182ms)
Jan 19 17:05:20.988: INFO: (11) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 19.143528ms)
Jan 19 17:05:20.989: INFO: (11) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 20.370579ms)
Jan 19 17:05:20.989: INFO: (11) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 20.425651ms)
Jan 19 17:05:20.989: INFO: (11) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 20.523582ms)
Jan 19 17:05:20.989: INFO: (11) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 20.479194ms)
Jan 19 17:05:20.990: INFO: (11) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 21.137144ms)
Jan 19 17:05:20.990: INFO: (11) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 21.172771ms)
Jan 19 17:05:21.009: INFO: (12) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 18.670764ms)
Jan 19 17:05:21.009: INFO: (12) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 18.862518ms)
Jan 19 17:05:21.009: INFO: (12) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 18.763392ms)
Jan 19 17:05:21.009: INFO: (12) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 18.784601ms)
Jan 19 17:05:21.009: INFO: (12) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 18.775516ms)
Jan 19 17:05:21.009: INFO: (12) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 18.839398ms)
Jan 19 17:05:21.009: INFO: (12) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 18.71394ms)
Jan 19 17:05:21.009: INFO: (12) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 18.822858ms)
Jan 19 17:05:21.009: INFO: (12) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 18.788349ms)
Jan 19 17:05:21.009: INFO: (12) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 18.73596ms)
Jan 19 17:05:21.009: INFO: (12) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 18.936884ms)
Jan 19 17:05:21.011: INFO: (12) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 20.2933ms)
Jan 19 17:05:21.011: INFO: (12) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 20.336658ms)
Jan 19 17:05:21.011: INFO: (12) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 20.466404ms)
Jan 19 17:05:21.011: INFO: (12) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 21.017026ms)
Jan 19 17:05:21.011: INFO: (12) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 20.976601ms)
Jan 19 17:05:21.030: INFO: (13) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 18.83666ms)
Jan 19 17:05:21.030: INFO: (13) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 18.572558ms)
Jan 19 17:05:21.030: INFO: (13) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 18.689408ms)
Jan 19 17:05:21.030: INFO: (13) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 18.684615ms)
Jan 19 17:05:21.030: INFO: (13) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 18.63227ms)
Jan 19 17:05:21.031: INFO: (13) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 19.038035ms)
Jan 19 17:05:21.031: INFO: (13) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 19.030409ms)
Jan 19 17:05:21.031: INFO: (13) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.091213ms)
Jan 19 17:05:21.031: INFO: (13) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 19.039373ms)
Jan 19 17:05:21.031: INFO: (13) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.132109ms)
Jan 19 17:05:21.031: INFO: (13) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 19.099001ms)
Jan 19 17:05:21.032: INFO: (13) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 20.452078ms)
Jan 19 17:05:21.032: INFO: (13) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 20.337112ms)
Jan 19 17:05:21.034: INFO: (13) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 22.308798ms)
Jan 19 17:05:21.034: INFO: (13) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 22.405271ms)
Jan 19 17:05:21.034: INFO: (13) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 22.954369ms)
Jan 19 17:05:21.054: INFO: (14) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 19.107234ms)
Jan 19 17:05:21.054: INFO: (14) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 19.259036ms)
Jan 19 17:05:21.054: INFO: (14) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 19.238632ms)
Jan 19 17:05:21.054: INFO: (14) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 19.129857ms)
Jan 19 17:05:21.054: INFO: (14) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.155327ms)
Jan 19 17:05:21.054: INFO: (14) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 19.151651ms)
Jan 19 17:05:21.054: INFO: (14) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.188038ms)
Jan 19 17:05:21.054: INFO: (14) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 19.275896ms)
Jan 19 17:05:21.055: INFO: (14) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 20.187123ms)
Jan 19 17:05:21.055: INFO: (14) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 20.350924ms)
Jan 19 17:05:21.062: INFO: (14) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 27.819535ms)
Jan 19 17:05:21.062: INFO: (14) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 27.759694ms)
Jan 19 17:05:21.062: INFO: (14) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 27.96591ms)
Jan 19 17:05:21.062: INFO: (14) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 27.810342ms)
Jan 19 17:05:21.067: INFO: (14) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 32.549019ms)
Jan 19 17:05:21.067: INFO: (14) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 32.566688ms)
Jan 19 17:05:21.086: INFO: (15) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.0894ms)
Jan 19 17:05:21.087: INFO: (15) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 19.37419ms)
Jan 19 17:05:21.087: INFO: (15) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 19.33277ms)
Jan 19 17:05:21.087: INFO: (15) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 19.411855ms)
Jan 19 17:05:21.087: INFO: (15) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.286871ms)
Jan 19 17:05:21.087: INFO: (15) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 19.301598ms)
Jan 19 17:05:21.087: INFO: (15) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 19.336227ms)
Jan 19 17:05:21.087: INFO: (15) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.30896ms)
Jan 19 17:05:21.087: INFO: (15) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 19.638662ms)
Jan 19 17:05:21.087: INFO: (15) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 19.769279ms)
Jan 19 17:05:21.088: INFO: (15) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 21.121536ms)
Jan 19 17:05:21.088: INFO: (15) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 21.201327ms)
Jan 19 17:05:21.088: INFO: (15) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 21.252466ms)
Jan 19 17:05:21.089: INFO: (15) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 21.266525ms)
Jan 19 17:05:21.091: INFO: (15) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 23.25364ms)
Jan 19 17:05:21.091: INFO: (15) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 24.13455ms)
Jan 19 17:05:21.110: INFO: (16) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 18.915428ms)
Jan 19 17:05:21.111: INFO: (16) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 18.948756ms)
Jan 19 17:05:21.110: INFO: (16) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 18.890484ms)
Jan 19 17:05:21.110: INFO: (16) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 18.80365ms)
Jan 19 17:05:21.110: INFO: (16) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 18.995439ms)
Jan 19 17:05:21.110: INFO: (16) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 18.818887ms)
Jan 19 17:05:21.111: INFO: (16) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 18.883303ms)
Jan 19 17:05:21.111: INFO: (16) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 18.896268ms)
Jan 19 17:05:21.111: INFO: (16) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 19.550871ms)
Jan 19 17:05:21.111: INFO: (16) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 19.564214ms)
Jan 19 17:05:21.111: INFO: (16) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 19.64019ms)
Jan 19 17:05:21.111: INFO: (16) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 19.644775ms)
Jan 19 17:05:21.112: INFO: (16) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 20.477167ms)
Jan 19 17:05:21.112: INFO: (16) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 20.544318ms)
Jan 19 17:05:21.112: INFO: (16) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 20.496102ms)
Jan 19 17:05:21.113: INFO: (16) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 21.118001ms)
Jan 19 17:05:21.132: INFO: (17) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 18.958751ms)
Jan 19 17:05:21.132: INFO: (17) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.046933ms)
Jan 19 17:05:21.132: INFO: (17) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 18.983335ms)
Jan 19 17:05:21.132: INFO: (17) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.12715ms)
Jan 19 17:05:21.132: INFO: (17) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 19.014619ms)
Jan 19 17:05:21.132: INFO: (17) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 19.271562ms)
Jan 19 17:05:21.132: INFO: (17) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 19.357816ms)
Jan 19 17:05:21.132: INFO: (17) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 19.281065ms)
Jan 19 17:05:21.132: INFO: (17) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.316235ms)
Jan 19 17:05:21.133: INFO: (17) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 19.897015ms)
Jan 19 17:05:21.133: INFO: (17) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 19.910537ms)
Jan 19 17:05:21.133: INFO: (17) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 20.509373ms)
Jan 19 17:05:21.133: INFO: (17) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 20.510822ms)
Jan 19 17:05:21.134: INFO: (17) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 20.906919ms)
Jan 19 17:05:21.134: INFO: (17) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 21.443485ms)
Jan 19 17:05:21.135: INFO: (17) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 21.813711ms)
Jan 19 17:05:21.154: INFO: (18) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 19.153141ms)
Jan 19 17:05:21.154: INFO: (18) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 19.181006ms)
Jan 19 17:05:21.154: INFO: (18) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 19.122467ms)
Jan 19 17:05:21.154: INFO: (18) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.18855ms)
Jan 19 17:05:21.154: INFO: (18) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.192405ms)
Jan 19 17:05:21.154: INFO: (18) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 19.109873ms)
Jan 19 17:05:21.154: INFO: (18) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.360447ms)
Jan 19 17:05:21.154: INFO: (18) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 19.419638ms)
Jan 19 17:05:21.154: INFO: (18) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 19.314673ms)
Jan 19 17:05:21.154: INFO: (18) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 19.348987ms)
Jan 19 17:05:21.154: INFO: (18) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 19.462006ms)
Jan 19 17:05:21.197: INFO: (18) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 61.986826ms)
Jan 19 17:05:21.197: INFO: (18) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 62.074528ms)
Jan 19 17:05:21.197: INFO: (18) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 61.988581ms)
Jan 19 17:05:21.197: INFO: (18) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 62.02552ms)
Jan 19 17:05:21.197: INFO: (18) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 62.055877ms)
Jan 19 17:05:21.217: INFO: (19) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:460/proxy/: tls baz (200; 19.714107ms)
Jan 19 17:05:21.217: INFO: (19) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">... (200; 19.735781ms)
Jan 19 17:05:21.217: INFO: (19) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 19.748961ms)
Jan 19 17:05:21.217: INFO: (19) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:443/proxy/tlsrewritem... (200; 19.87506ms)
Jan 19 17:05:21.217: INFO: (19) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname1/proxy/: tls baz (200; 19.854333ms)
Jan 19 17:05:21.217: INFO: (19) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:1080/proxy/rewriteme">test<... (200; 19.783502ms)
Jan 19 17:05:21.217: INFO: (19) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.957376ms)
Jan 19 17:05:21.217: INFO: (19) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname1/proxy/: foo (200; 19.876188ms)
Jan 19 17:05:21.217: INFO: (19) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/: <a href="/api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w/proxy/rewriteme">test</a> (200; 19.857388ms)
Jan 19 17:05:21.217: INFO: (19) /api/v1/namespaces/proxy-7489/pods/https:proxy-service-kfmwg-8qx8w:462/proxy/: tls qux (200; 19.811579ms)
Jan 19 17:05:21.217: INFO: (19) /api/v1/namespaces/proxy-7489/pods/http:proxy-service-kfmwg-8qx8w:162/proxy/: bar (200; 19.824407ms)
Jan 19 17:05:21.217: INFO: (19) /api/v1/namespaces/proxy-7489/services/https:proxy-service-kfmwg:tlsportname2/proxy/: tls qux (200; 19.904502ms)
Jan 19 17:05:21.260: INFO: (19) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname1/proxy/: foo (200; 62.380517ms)
Jan 19 17:05:21.260: INFO: (19) /api/v1/namespaces/proxy-7489/services/proxy-service-kfmwg:portname2/proxy/: bar (200; 62.409909ms)
Jan 19 17:05:21.260: INFO: (19) /api/v1/namespaces/proxy-7489/pods/proxy-service-kfmwg-8qx8w:160/proxy/: foo (200; 62.435705ms)
Jan 19 17:05:21.260: INFO: (19) /api/v1/namespaces/proxy-7489/services/http:proxy-service-kfmwg:portname2/proxy/: bar (200; 62.541837ms)
STEP: deleting ReplicationController proxy-service-kfmwg in namespace proxy-7489, will wait for the garbage collector to delete the pods
Jan 19 17:05:21.345: INFO: Deleting ReplicationController proxy-service-kfmwg took: 18.715586ms
Jan 19 17:05:21.445: INFO: Terminating ReplicationController proxy-service-kfmwg pods took: 100.315478ms
[AfterEach] version v1
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:05:27.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7489" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":280,"completed":102,"skipped":1518,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:05:27.298: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9113
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1733
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 19 17:05:27.526: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-9113'
Jan 19 17:05:27.657: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 19 17:05:27.657: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1738
Jan 19 17:05:29.692: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-httpd-deployment --namespace=kubectl-9113'
Jan 19 17:05:29.820: INFO: stderr: ""
Jan 19 17:05:29.820: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:05:29.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9113" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]","total":280,"completed":103,"skipped":1548,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:05:29.882: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1669
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:05:43.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1669" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":280,"completed":104,"skipped":1556,"failed":0}

------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:05:43.395: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1717
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Starting the proxy
Jan 19 17:05:43.620: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy --unix-socket=/tmp/kubectl-proxy-unix797654454/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:05:43.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1717" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":280,"completed":105,"skipped":1556,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:05:43.712: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-9256
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:05:43.937: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating first CR 
Jan 19 17:05:44.153: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-19T17:05:43Z generation:1 name:name1 resourceVersion:18388 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:1967aabc-66c3-4da2-8423-8324019f7e3e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jan 19 17:05:54.172: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-19T17:05:53Z generation:1 name:name2 resourceVersion:18440 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:81abe573-a3f3-4874-b17e-178659646a3b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jan 19 17:06:04.191: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-19T17:05:43Z generation:2 name:name1 resourceVersion:18477 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:1967aabc-66c3-4da2-8423-8324019f7e3e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jan 19 17:06:14.209: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-19T17:05:53Z generation:2 name:name2 resourceVersion:18516 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:81abe573-a3f3-4874-b17e-178659646a3b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jan 19 17:06:24.229: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-19T17:05:43Z generation:2 name:name1 resourceVersion:18553 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:1967aabc-66c3-4da2-8423-8324019f7e3e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jan 19 17:06:34.249: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-01-19T17:05:53Z generation:2 name:name2 resourceVersion:18591 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:81abe573-a3f3-4874-b17e-178659646a3b] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:06:44.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9256" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":280,"completed":106,"skipped":1558,"failed":0}

------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:06:44.852: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5488
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:06:45.098: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-5c11791f-8eed-453d-bd23-f2c4a51887b1" in namespace "security-context-test-5488" to be "success or failure"
Jan 19 17:06:45.115: INFO: Pod "busybox-privileged-false-5c11791f-8eed-453d-bd23-f2c4a51887b1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.713549ms
Jan 19 17:06:47.132: INFO: Pod "busybox-privileged-false-5c11791f-8eed-453d-bd23-f2c4a51887b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034052988s
Jan 19 17:06:49.149: INFO: Pod "busybox-privileged-false-5c11791f-8eed-453d-bd23-f2c4a51887b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051219386s
Jan 19 17:06:49.149: INFO: Pod "busybox-privileged-false-5c11791f-8eed-453d-bd23-f2c4a51887b1" satisfied condition "success or failure"
Jan 19 17:06:49.273: INFO: Got logs for pod "busybox-privileged-false-5c11791f-8eed-453d-bd23-f2c4a51887b1": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:06:49.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5488" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":107,"skipped":1558,"failed":0}
S
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:06:49.324: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-53
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:06:49.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-53" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":280,"completed":108,"skipped":1559,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:06:49.808: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4892
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:07:06.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4892" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":280,"completed":109,"skipped":1560,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:07:06.374: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7630
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-7630
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7630
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7630
Jan 19 17:07:06.661: INFO: Found 0 stateful pods, waiting for 1
Jan 19 17:07:16.679: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 19 17:07:16.696: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7630 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 17:07:18.034: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 17:07:18.035: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 17:07:18.035: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 17:07:18.051: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 19 17:07:28.069: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 17:07:28.069: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 17:07:28.136: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999371s
Jan 19 17:07:29.153: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.983238669s
Jan 19 17:07:30.171: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.96530836s
Jan 19 17:07:31.189: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.94767441s
Jan 19 17:07:32.207: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.929501648s
Jan 19 17:07:33.226: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.91132984s
Jan 19 17:07:34.244: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.893218969s
Jan 19 17:07:35.263: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.874765675s
Jan 19 17:07:36.280: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.856302278s
Jan 19 17:07:37.299: INFO: Verifying statefulset ss doesn't scale past 1 for another 838.275209ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7630
Jan 19 17:07:38.318: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7630 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 17:07:38.931: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 17:07:38.931: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 17:07:38.931: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 17:07:38.948: INFO: Found 1 stateful pods, waiting for 3
Jan 19 17:07:48.965: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 17:07:48.965: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 17:07:48.965: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 19 17:07:48.999: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7630 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 17:07:49.564: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 17:07:49.564: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 17:07:49.564: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 17:07:49.564: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7630 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 17:07:50.295: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 17:07:50.295: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 17:07:50.295: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 17:07:50.295: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7630 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 17:07:51.807: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 17:07:51.807: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 17:07:51.807: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 17:07:51.807: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 17:07:51.823: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jan 19 17:08:01.859: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 17:08:01.859: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 17:08:01.859: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 17:08:01.910: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999448s
Jan 19 17:08:02.929: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.982976585s
Jan 19 17:08:03.946: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.964598388s
Jan 19 17:08:04.964: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.946936524s
Jan 19 17:08:05.983: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.928936769s
Jan 19 17:08:07.003: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.910435362s
Jan 19 17:08:08.021: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.890279805s
Jan 19 17:08:09.040: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.87265211s
Jan 19 17:08:10.058: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.853307242s
Jan 19 17:08:11.076: INFO: Verifying statefulset ss doesn't scale past 3 for another 835.247267ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7630
Jan 19 17:08:12.093: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7630 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 17:08:12.782: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 17:08:12.782: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 17:08:12.782: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 17:08:12.782: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7630 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 17:08:13.410: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 17:08:13.410: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 17:08:13.410: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 17:08:13.410: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7630 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 17:08:14.059: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 17:08:14.059: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 17:08:14.059: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 17:08:14.059: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 19 17:08:44.132: INFO: Deleting all statefulset in ns statefulset-7630
Jan 19 17:08:44.149: INFO: Scaling statefulset ss to 0
Jan 19 17:08:44.200: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 17:08:44.217: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:08:44.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7630" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":280,"completed":110,"skipped":1580,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:08:44.327: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5628
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 19 17:08:44.624: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5628 /api/v1/namespaces/watch-5628/configmaps/e2e-watch-test-watch-closed af49e489-5e05-41af-9a14-0cd2c08109d9 19256 0 2020-01-19 17:08:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 19 17:08:44.624: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5628 /api/v1/namespaces/watch-5628/configmaps/e2e-watch-test-watch-closed af49e489-5e05-41af-9a14-0cd2c08109d9 19257 0 2020-01-19 17:08:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 19 17:08:44.698: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5628 /api/v1/namespaces/watch-5628/configmaps/e2e-watch-test-watch-closed af49e489-5e05-41af-9a14-0cd2c08109d9 19260 0 2020-01-19 17:08:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 19 17:08:44.699: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5628 /api/v1/namespaces/watch-5628/configmaps/e2e-watch-test-watch-closed af49e489-5e05-41af-9a14-0cd2c08109d9 19261 0 2020-01-19 17:08:44 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:08:44.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5628" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":280,"completed":111,"skipped":1589,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:08:44.734: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8334
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run default
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1596
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 19 17:08:44.957: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8334'
Jan 19 17:08:45.097: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 19 17:08:45.098: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1602
Jan 19 17:08:45.153: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-httpd-deployment --namespace=kubectl-8334'
Jan 19 17:08:45.279: INFO: stderr: ""
Jan 19 17:08:45.279: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:08:45.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8334" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]","total":280,"completed":112,"skipped":1610,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:08:45.318: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4573
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-4573
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4573
STEP: Creating statefulset with conflicting port in namespace statefulset-4573
STEP: Waiting until pod test-pod will start running in namespace statefulset-4573
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4573
Jan 19 17:08:51.677: INFO: Observed stateful pod in namespace: statefulset-4573, name: ss-0, uid: ec013496-c7ed-4f46-b129-08cafc0be06b, status phase: Failed. Waiting for statefulset controller to delete.
Jan 19 17:08:51.678: INFO: Observed stateful pod in namespace: statefulset-4573, name: ss-0, uid: ec013496-c7ed-4f46-b129-08cafc0be06b, status phase: Failed. Waiting for statefulset controller to delete.
Jan 19 17:08:51.681: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4573
STEP: Removing pod with conflicting port in namespace statefulset-4573
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4573 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 19 17:08:55.761: INFO: Deleting all statefulset in ns statefulset-4573
Jan 19 17:08:55.778: INFO: Scaling statefulset ss to 0
Jan 19 17:09:05.847: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 17:09:05.864: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:09:05.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4573" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":280,"completed":113,"skipped":1630,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:09:05.969: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-map-79df5682-0391-4c88-b097-cee1ba3b29f9
STEP: Creating a pod to test consume secrets
Jan 19 17:09:06.231: INFO: Waiting up to 5m0s for pod "pod-secrets-f460cbc4-e393-4d2a-b65e-f69a4481c4e7" in namespace "secrets-9730" to be "success or failure"
Jan 19 17:09:06.248: INFO: Pod "pod-secrets-f460cbc4-e393-4d2a-b65e-f69a4481c4e7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.418779ms
Jan 19 17:09:08.265: INFO: Pod "pod-secrets-f460cbc4-e393-4d2a-b65e-f69a4481c4e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033801412s
Jan 19 17:09:10.283: INFO: Pod "pod-secrets-f460cbc4-e393-4d2a-b65e-f69a4481c4e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052000271s
STEP: Saw pod success
Jan 19 17:09:10.283: INFO: Pod "pod-secrets-f460cbc4-e393-4d2a-b65e-f69a4481c4e7" satisfied condition "success or failure"
Jan 19 17:09:10.301: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-secrets-f460cbc4-e393-4d2a-b65e-f69a4481c4e7 container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 17:09:10.444: INFO: Waiting for pod pod-secrets-f460cbc4-e393-4d2a-b65e-f69a4481c4e7 to disappear
Jan 19 17:09:10.461: INFO: Pod pod-secrets-f460cbc4-e393-4d2a-b65e-f69a4481c4e7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:09:10.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9730" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":114,"skipped":1643,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:09:10.511: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-4788
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 19 17:09:18.870: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4788 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:09:18.870: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:09:19.315: INFO: Exec stderr: ""
Jan 19 17:09:19.315: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4788 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:09:19.315: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:09:19.838: INFO: Exec stderr: ""
Jan 19 17:09:19.838: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4788 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:09:19.838: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:09:20.322: INFO: Exec stderr: ""
Jan 19 17:09:20.323: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4788 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:09:20.323: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:09:20.775: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 19 17:09:20.775: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4788 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:09:20.775: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:09:21.222: INFO: Exec stderr: ""
Jan 19 17:09:21.222: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4788 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:09:21.222: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:09:21.617: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 19 17:09:21.617: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4788 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:09:21.617: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:09:22.046: INFO: Exec stderr: ""
Jan 19 17:09:22.047: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4788 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:09:22.047: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:09:22.562: INFO: Exec stderr: ""
Jan 19 17:09:22.562: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4788 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:09:22.562: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:09:23.032: INFO: Exec stderr: ""
Jan 19 17:09:23.032: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4788 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:09:23.032: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:09:23.468: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:09:23.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4788" for this suite.
•{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":115,"skipped":1672,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:09:23.523: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8030
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:09:23.752: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 19 17:09:23.787: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 19 17:09:27.824: INFO: Creating deployment "test-rolling-update-deployment"
Jan 19 17:09:27.842: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 19 17:09:27.884: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 19 17:09:27.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050567, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050567, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050567, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050567, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:09:29.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050567, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050567, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050567, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050567, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67cf4f6444\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:09:31.930: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 19 17:09:31.982: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8030 /apis/apps/v1/namespaces/deployment-8030/deployments/test-rolling-update-deployment d0bdeb87-eec9-4608-9a44-29a569e6b10a 19807 1 2020-01-19 17:09:27 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003246ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-01-19 17:09:27 +0000 UTC,LastTransitionTime:2020-01-19 17:09:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67cf4f6444" has successfully progressed.,LastUpdateTime:2020-01-19 17:09:30 +0000 UTC,LastTransitionTime:2020-01-19 17:09:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 19 17:09:31.999: INFO: New ReplicaSet "test-rolling-update-deployment-67cf4f6444" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67cf4f6444  deployment-8030 /apis/apps/v1/namespaces/deployment-8030/replicasets/test-rolling-update-deployment-67cf4f6444 8387c1cc-a565-4d16-8b06-b3531f038999 19800 1 2020-01-19 17:09:27 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d0bdeb87-eec9-4608-9a44-29a569e6b10a 0xc003247387 0xc003247388}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67cf4f6444,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0032473f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 17:09:31.999: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 19 17:09:31.999: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8030 /apis/apps/v1/namespaces/deployment-8030/replicasets/test-rolling-update-controller be81cf49-1c7f-408e-81f4-3b6bbd0623d3 19806 2 2020-01-19 17:09:23 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d0bdeb87-eec9-4608-9a44-29a569e6b10a 0xc0032472b7 0xc0032472b8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003247318 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 17:09:32.017: INFO: Pod "test-rolling-update-deployment-67cf4f6444-bjlml" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67cf4f6444-bjlml test-rolling-update-deployment-67cf4f6444- deployment-8030 /api/v1/namespaces/deployment-8030/pods/test-rolling-update-deployment-67cf4f6444-bjlml 0184e629-34ce-4ccb-912a-ff0999584a61 19799 0 2020-01-19 17:09:27 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67cf4f6444] map[cni.projectcalico.org/podIP:100.64.1.140/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-67cf4f6444 8387c1cc-a565-4d16-8b06-b3531f038999 0xc003247867 0xc003247868}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jjqfk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jjqfk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jjqfk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:09:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:09:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:09:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:09:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:100.64.1.140,StartTime:2020-01-19 17:09:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-19 17:09:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://eae5526418475b33f6667d6ed00fcf975766a691e51b145052f668235f075a09,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:09:32.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8030" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":116,"skipped":1683,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:09:32.068: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5807
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 17:09:32.310: INFO: Waiting up to 5m0s for pod "downwardapi-volume-28606e8e-7df3-4e2e-a63e-97d46a248b6b" in namespace "projected-5807" to be "success or failure"
Jan 19 17:09:32.327: INFO: Pod "downwardapi-volume-28606e8e-7df3-4e2e-a63e-97d46a248b6b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.464976ms
Jan 19 17:09:34.345: INFO: Pod "downwardapi-volume-28606e8e-7df3-4e2e-a63e-97d46a248b6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03456297s
Jan 19 17:09:36.363: INFO: Pod "downwardapi-volume-28606e8e-7df3-4e2e-a63e-97d46a248b6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053137426s
Jan 19 17:09:38.381: INFO: Pod "downwardapi-volume-28606e8e-7df3-4e2e-a63e-97d46a248b6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071098943s
STEP: Saw pod success
Jan 19 17:09:38.381: INFO: Pod "downwardapi-volume-28606e8e-7df3-4e2e-a63e-97d46a248b6b" satisfied condition "success or failure"
Jan 19 17:09:38.398: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz pod downwardapi-volume-28606e8e-7df3-4e2e-a63e-97d46a248b6b container client-container: <nil>
STEP: delete the pod
Jan 19 17:09:38.452: INFO: Waiting for pod downwardapi-volume-28606e8e-7df3-4e2e-a63e-97d46a248b6b to disappear
Jan 19 17:09:38.483: INFO: Pod downwardapi-volume-28606e8e-7df3-4e2e-a63e-97d46a248b6b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:09:38.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5807" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":117,"skipped":1685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:09:38.536: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9337
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 19 17:09:42.903: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:09:42.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9337" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":280,"completed":118,"skipped":1713,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:09:43.013: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 17:09:43.258: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54d1fab2-ab56-41db-aeb3-d5a992f321b9" in namespace "downward-api-4109" to be "success or failure"
Jan 19 17:09:43.275: INFO: Pod "downwardapi-volume-54d1fab2-ab56-41db-aeb3-d5a992f321b9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.530887ms
Jan 19 17:09:45.292: INFO: Pod "downwardapi-volume-54d1fab2-ab56-41db-aeb3-d5a992f321b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034005113s
Jan 19 17:09:47.309: INFO: Pod "downwardapi-volume-54d1fab2-ab56-41db-aeb3-d5a992f321b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051015872s
STEP: Saw pod success
Jan 19 17:09:47.309: INFO: Pod "downwardapi-volume-54d1fab2-ab56-41db-aeb3-d5a992f321b9" satisfied condition "success or failure"
Jan 19 17:09:47.326: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-54d1fab2-ab56-41db-aeb3-d5a992f321b9 container client-container: <nil>
STEP: delete the pod
Jan 19 17:09:47.383: INFO: Waiting for pod downwardapi-volume-54d1fab2-ab56-41db-aeb3-d5a992f321b9 to disappear
Jan 19 17:09:47.410: INFO: Pod downwardapi-volume-54d1fab2-ab56-41db-aeb3-d5a992f321b9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:09:47.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4109" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":119,"skipped":1759,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:09:47.461: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4313
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:09:54.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4313" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":280,"completed":120,"skipped":1805,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:09:54.791: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5258
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:09:55.015: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 19 17:09:58.496: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5258 create -f -'
Jan 19 17:09:59.167: INFO: stderr: ""
Jan 19 17:09:59.167: INFO: stdout: "e2e-test-crd-publish-openapi-5477-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 19 17:09:59.167: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5258 delete e2e-test-crd-publish-openapi-5477-crds test-cr'
Jan 19 17:09:59.366: INFO: stderr: ""
Jan 19 17:09:59.366: INFO: stdout: "e2e-test-crd-publish-openapi-5477-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 19 17:09:59.366: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5258 apply -f -'
Jan 19 17:09:59.697: INFO: stderr: ""
Jan 19 17:09:59.698: INFO: stdout: "e2e-test-crd-publish-openapi-5477-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 19 17:09:59.698: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5258 delete e2e-test-crd-publish-openapi-5477-crds test-cr'
Jan 19 17:09:59.906: INFO: stderr: ""
Jan 19 17:09:59.906: INFO: stdout: "e2e-test-crd-publish-openapi-5477-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 19 17:09:59.906: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-5477-crds'
Jan 19 17:10:00.261: INFO: stderr: ""
Jan 19 17:10:00.261: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5477-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:10:04.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5258" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":280,"completed":121,"skipped":1831,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:10:04.512: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1485
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jan 19 17:10:15.037: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:10:15.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0119 17:10:15.037820    7438 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-1485" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":280,"completed":122,"skipped":1858,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:10:15.074: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-636
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:46
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jan 19 17:10:19.416: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jan 19 17:10:29.577: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:10:29.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-636" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]","total":280,"completed":123,"skipped":1862,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:10:29.629: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8392
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:10:29.856: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jan 19 17:10:33.255: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8392 create -f -'
Jan 19 17:10:33.910: INFO: stderr: ""
Jan 19 17:10:33.910: INFO: stdout: "e2e-test-crd-publish-openapi-6989-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 19 17:10:33.910: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8392 delete e2e-test-crd-publish-openapi-6989-crds test-cr'
Jan 19 17:10:34.073: INFO: stderr: ""
Jan 19 17:10:34.073: INFO: stdout: "e2e-test-crd-publish-openapi-6989-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 19 17:10:34.073: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8392 apply -f -'
Jan 19 17:10:34.425: INFO: stderr: ""
Jan 19 17:10:34.425: INFO: stdout: "e2e-test-crd-publish-openapi-6989-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 19 17:10:34.426: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8392 delete e2e-test-crd-publish-openapi-6989-crds test-cr'
Jan 19 17:10:34.552: INFO: stderr: ""
Jan 19 17:10:34.553: INFO: stdout: "e2e-test-crd-publish-openapi-6989-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jan 19 17:10:34.553: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-6989-crds'
Jan 19 17:10:34.780: INFO: stderr: ""
Jan 19 17:10:34.780: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6989-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:10:38.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8392" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":280,"completed":124,"skipped":1884,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:10:38.670: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7060
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-88869656-dd34-4fbf-94bc-f30a4c359fcc in namespace container-probe-7060
Jan 19 17:10:44.947: INFO: Started pod busybox-88869656-dd34-4fbf-94bc-f30a4c359fcc in namespace container-probe-7060
STEP: checking the pod's current state and verifying that restartCount is present
Jan 19 17:10:44.964: INFO: Initial restart count of pod busybox-88869656-dd34-4fbf-94bc-f30a4c359fcc is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:14:45.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7060" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":125,"skipped":1888,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:14:45.378: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1713
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:14:45.592: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:14:46.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1713" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":280,"completed":126,"skipped":1891,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:14:46.241: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2788
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:14:46.461: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-2788'
Jan 19 17:14:46.746: INFO: stderr: ""
Jan 19 17:14:46.746: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Jan 19 17:14:46.747: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-2788'
Jan 19 17:14:46.978: INFO: stderr: ""
Jan 19 17:14:46.978: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jan 19 17:14:47.996: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 17:14:47.996: INFO: Found 0 / 1
Jan 19 17:14:48.996: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 17:14:48.996: INFO: Found 0 / 1
Jan 19 17:14:49.996: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 17:14:49.996: INFO: Found 1 / 1
Jan 19 17:14:49.996: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 19 17:14:50.015: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 17:14:50.015: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 19 17:14:50.015: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe pod agnhost-master-2nxrb --namespace=kubectl-2788'
Jan 19 17:14:50.177: INFO: stderr: ""
Jan 19 17:14:50.178: INFO: stdout: "Name:         agnhost-master-2nxrb\nNamespace:    kubectl-2788\nPriority:     0\nNode:         shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc/10.250.0.5\nStart Time:   Sun, 19 Jan 2020 17:14:46 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 100.64.1.148/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           100.64.1.148\nIPs:\n  IP:           100.64.1.148\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://b544e09390d3370d3ada301f30a7d2eb53d19cadb155208cff5301be7fef4b45\n    Image:          gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 19 Jan 2020 17:14:49 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-944gq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-944gq:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-944gq\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                                  Message\n  ----    ------     ----       ----                                                  -------\n  Normal  Scheduled  <unknown>  default-scheduler                                     Successfully assigned kubectl-2788/agnhost-master-2nxrb to shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc\n  Normal  Pulled     2s         kubelet, shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Container image \"gcr.io/kubernetes-e2e-test-images/agnhost:2.8\" already present on machine\n  Normal  Created    2s         kubelet, shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Created container agnhost-master\n  Normal  Started    1s         kubelet, shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc  Started container agnhost-master\n"
Jan 19 17:14:50.178: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe rc agnhost-master --namespace=kubectl-2788'
Jan 19 17:14:50.339: INFO: stderr: ""
Jan 19 17:14:50.340: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-2788\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/agnhost:2.8\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-master-2nxrb\n"
Jan 19 17:14:50.340: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe service agnhost-master --namespace=kubectl-2788'
Jan 19 17:14:50.472: INFO: stderr: ""
Jan 19 17:14:50.472: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-2788\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                100.111.174.128\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.64.1.148:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 19 17:14:50.504: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc'
Jan 19 17:14:50.736: INFO: stderr: ""
Jan 19 17:14:50.736: INFO: stdout: "Name:               shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_D2_v3\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=westeurope\n                    failure-domain.beta.kubernetes.io/zone=1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=Standard_D2_v3\n                    node.kubernetes.io/role=node\n                    topology.kubernetes.io/region=westeurope\n                    topology.kubernetes.io/zone=1\n                    worker.garden.sapcloud.io/group=worker-1\n                    worker.gardener.cloud/pool=worker-1\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.64.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 19 Jan 2020 16:05:21 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 19 Jan 2020 17:14:49 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  KernelDeadlock                False   Sun, 19 Jan 2020 17:14:15 +0000   Sun, 19 Jan 2020 16:06:34 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Sun, 19 Jan 2020 17:14:15 +0000   Sun, 19 Jan 2020 16:06:34 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  CorruptDockerOverlay2         False   Sun, 19 Jan 2020 17:14:15 +0000   Sun, 19 Jan 2020 16:06:34 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  FrequentUnregisterNetDevice   False   Sun, 19 Jan 2020 17:14:15 +0000   Sun, 19 Jan 2020 16:06:34 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  FrequentKubeletRestart        False   Sun, 19 Jan 2020 17:14:15 +0000   Sun, 19 Jan 2020 16:06:34 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Sun, 19 Jan 2020 17:14:15 +0000   Sun, 19 Jan 2020 16:06:34 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Sun, 19 Jan 2020 17:14:15 +0000   Sun, 19 Jan 2020 16:06:34 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  NetworkUnavailable            False   Sun, 19 Jan 2020 16:05:24 +0000   Sun, 19 Jan 2020 16:05:24 +0000   RouteCreated                    RouteController created a route\n  MemoryPressure                False   Sun, 19 Jan 2020 17:14:42 +0000   Sun, 19 Jan 2020 16:05:21 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Sun, 19 Jan 2020 17:14:42 +0000   Sun, 19 Jan 2020 16:05:21 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Sun, 19 Jan 2020 17:14:42 +0000   Sun, 19 Jan 2020 16:05:21 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Sun, 19 Jan 2020 17:14:42 +0000   Sun, 19 Jan 2020 16:06:07 +0000   KubeletReady                    kubelet is posting ready status\nAddresses:\n  Hostname:    shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc\n  InternalIP:  10.250.0.5\nCapacity:\n  attachable-volumes-azure-disk:  4\n  cpu:                            2\n  ephemeral-storage:              33136428Ki\n  hugepages-1Gi:                  0\n  hugepages-2Mi:                  0\n  memory:                         8145312Ki\n  pods:                           110\nAllocatable:\n  attachable-volumes-azure-disk:  4\n  cpu:                            1920m\n  ephemeral-storage:              32235117134\n  hugepages-1Gi:                  0\n  hugepages-2Mi:                  0\n  memory:                         6850017684\n  pods:                           110\nSystem Info:\n  Machine ID:                 64b74194c1b04d0398416cb1450928f2\n  System UUID:                b89c4734-61d0-5c4c-99a2-f43db6e7ea4e\n  Boot ID:                    e77c7e05-c7d2-490a-ad2b-96fe83cf3327\n  Kernel Version:             4.19.86-coreos\n  OS Image:                   Container Linux by CoreOS 2303.3.0 (Rhyolite)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://18.6.3\n  Kubelet Version:            v1.17.1\n  Kube-Proxy Version:         v1.17.1\nPodCIDR:                      100.64.1.0/24\nPodCIDRs:                     100.64.1.0/24\nProviderID:                   azure:///subscriptions/0b9904be-2a50-4fda-a947-c5f1b1d07666/resourceGroups/shoot--it--tmbjf-zbh/providers/Microsoft.Compute/virtualMachines/shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-75w55                            100m (5%)     500m (26%)  100Mi (1%)       700Mi (10%)    69m\n  kube-system                 calico-typha-deploy-9f6b455c4-x5lp9          0 (0%)        0 (0%)      0 (0%)           0 (0%)         65m\n  kube-system                 kube-proxy-6m6gf                             20m (1%)      0 (0%)      64Mi (0%)        0 (0%)         69m\n  kube-system                 node-exporter-md4n7                          5m (0%)       25m (1%)    10Mi (0%)        100Mi (1%)     69m\n  kube-system                 node-problem-detector-pw6fz                  20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)     69m\n  kubectl-2788                agnhost-master-2nxrb                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  kubernetes-dashboard        dashboard-metrics-scraper-894778996-rhgpz    0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m\n  kubernetes-dashboard        kubernetes-dashboard-5b855874f6-nb7jc        50m (2%)      100m (5%)   50Mi (0%)        256Mi (3%)     39m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests    Limits\n  --------                       --------    ------\n  cpu                            195m (10%)  825m (42%)\n  memory                         244Mi (3%)  1156Mi (17%)\n  ephemeral-storage              0 (0%)      0 (0%)\n  attachable-volumes-azure-disk  0           0\nEvents:                          <none>\n"
Jan 19 17:14:50.736: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe namespace kubectl-2788'
Jan 19 17:14:50.914: INFO: stderr: ""
Jan 19 17:14:50.914: INFO: stdout: "Name:         kubectl-2788\nLabels:       e2e-framework=kubectl\n              e2e-run=69bb3140-26bb-4142-8604-19d5106cf4fe\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:14:50.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2788" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":280,"completed":127,"skipped":1893,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:14:50.963: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-809
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:14:51.576: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050890, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050890, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050890, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050890, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:14:53.593: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050890, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050890, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050890, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050890, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:14:56.624: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:14:56.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-809" for this suite.
STEP: Destroying namespace "webhook-809-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":280,"completed":128,"skipped":1893,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:14:57.043: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7656
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating pod
Jan 19 17:15:01.351: INFO: Pod pod-hostip-8f90ba66-9af1-4b10-b3e2-74d12aa9b34a has hostIP: 10.250.0.5
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:15:01.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7656" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":280,"completed":129,"skipped":1909,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:15:01.401: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:15:01.625: INFO: Creating deployment "test-recreate-deployment"
Jan 19 17:15:01.641: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 19 17:15:01.687: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 19 17:15:01.710: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050900, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050900, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050900, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050900, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:15:03.727: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050900, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050900, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050900, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715050900, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-799c574856\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:15:05.729: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 19 17:15:05.763: INFO: Updating deployment test-recreate-deployment
Jan 19 17:15:05.764: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 19 17:15:05.855: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3865 /apis/apps/v1/namespaces/deployment-3865/deployments/test-recreate-deployment d51ca154-625c-45cb-90ef-632d3ac43cd7 21497 2 2020-01-19 17:15:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003fee908 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-01-19 17:15:05 +0000 UTC,LastTransitionTime:2020-01-19 17:15:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-01-19 17:15:05 +0000 UTC,LastTransitionTime:2020-01-19 17:15:00 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 19 17:15:05.872: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-3865 /apis/apps/v1/namespaces/deployment-3865/replicasets/test-recreate-deployment-5f94c574ff fd27153c-cce8-41bd-9efe-d3b9b1440e4b 21496 1 2020-01-19 17:15:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d51ca154-625c-45cb-90ef-632d3ac43cd7 0xc003feec97 0xc003feec98}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003feecf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 17:15:05.872: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 19 17:15:05.872: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-799c574856  deployment-3865 /apis/apps/v1/namespaces/deployment-3865/replicasets/test-recreate-deployment-799c574856 48290c23-c255-4c72-bdeb-629b7d6a4e86 21490 2 2020-01-19 17:15:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d51ca154-625c-45cb-90ef-632d3ac43cd7 0xc003feed67 0xc003feed68}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 799c574856,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:799c574856] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003feedd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 17:15:05.890: INFO: Pod "test-recreate-deployment-5f94c574ff-nlmtr" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-nlmtr test-recreate-deployment-5f94c574ff- deployment-3865 /api/v1/namespaces/deployment-3865/pods/test-recreate-deployment-5f94c574ff-nlmtr aa3998a6-fdbe-4fa0-b296-28eea76ce981 21495 0 2020-01-19 17:15:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff fd27153c-cce8-41bd-9efe-d3b9b1440e4b 0xc003fef227 0xc003fef228}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zjv4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zjv4d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zjv4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:15:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:15:05.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3865" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":280,"completed":130,"skipped":1974,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:15:05.942: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6491
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-0021d9a5-10ed-4afe-83e9-64326ae0f7fd
STEP: Creating a pod to test consume configMaps
Jan 19 17:15:06.214: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-be9353db-b187-40ef-a035-ea8fc0381c44" in namespace "projected-6491" to be "success or failure"
Jan 19 17:15:06.230: INFO: Pod "pod-projected-configmaps-be9353db-b187-40ef-a035-ea8fc0381c44": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005458ms
Jan 19 17:15:08.248: INFO: Pod "pod-projected-configmaps-be9353db-b187-40ef-a035-ea8fc0381c44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033908062s
Jan 19 17:15:10.265: INFO: Pod "pod-projected-configmaps-be9353db-b187-40ef-a035-ea8fc0381c44": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051204517s
Jan 19 17:15:12.283: INFO: Pod "pod-projected-configmaps-be9353db-b187-40ef-a035-ea8fc0381c44": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069205663s
Jan 19 17:15:14.301: INFO: Pod "pod-projected-configmaps-be9353db-b187-40ef-a035-ea8fc0381c44": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086975831s
Jan 19 17:15:16.319: INFO: Pod "pod-projected-configmaps-be9353db-b187-40ef-a035-ea8fc0381c44": Phase="Pending", Reason="", readiness=false. Elapsed: 10.105026882s
Jan 19 17:15:18.337: INFO: Pod "pod-projected-configmaps-be9353db-b187-40ef-a035-ea8fc0381c44": Phase="Pending", Reason="", readiness=false. Elapsed: 12.122876298s
Jan 19 17:15:20.353: INFO: Pod "pod-projected-configmaps-be9353db-b187-40ef-a035-ea8fc0381c44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.139737342s
STEP: Saw pod success
Jan 19 17:15:20.354: INFO: Pod "pod-projected-configmaps-be9353db-b187-40ef-a035-ea8fc0381c44" satisfied condition "success or failure"
Jan 19 17:15:20.370: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-projected-configmaps-be9353db-b187-40ef-a035-ea8fc0381c44 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 19 17:15:20.507: INFO: Waiting for pod pod-projected-configmaps-be9353db-b187-40ef-a035-ea8fc0381c44 to disappear
Jan 19 17:15:20.524: INFO: Pod pod-projected-configmaps-be9353db-b187-40ef-a035-ea8fc0381c44 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:15:20.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6491" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":131,"skipped":2013,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:15:20.574: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7861
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 19 17:15:20.818: INFO: Waiting up to 5m0s for pod "pod-2d2d063a-9c1a-44d9-9b0e-29f06335658d" in namespace "emptydir-7861" to be "success or failure"
Jan 19 17:15:20.834: INFO: Pod "pod-2d2d063a-9c1a-44d9-9b0e-29f06335658d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.313912ms
Jan 19 17:15:22.853: INFO: Pod "pod-2d2d063a-9c1a-44d9-9b0e-29f06335658d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034437856s
Jan 19 17:15:24.870: INFO: Pod "pod-2d2d063a-9c1a-44d9-9b0e-29f06335658d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052173253s
STEP: Saw pod success
Jan 19 17:15:24.870: INFO: Pod "pod-2d2d063a-9c1a-44d9-9b0e-29f06335658d" satisfied condition "success or failure"
Jan 19 17:15:24.887: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-2d2d063a-9c1a-44d9-9b0e-29f06335658d container test-container: <nil>
STEP: delete the pod
Jan 19 17:15:24.937: INFO: Waiting for pod pod-2d2d063a-9c1a-44d9-9b0e-29f06335658d to disappear
Jan 19 17:15:24.953: INFO: Pod pod-2d2d063a-9c1a-44d9-9b0e-29f06335658d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:15:24.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7861" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":132,"skipped":2055,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:15:25.003: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-149
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5213
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3452
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:15:31.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-149" for this suite.
STEP: Destroying namespace "nsdeletetest-5213" for this suite.
Jan 19 17:15:31.787: INFO: Namespace nsdeletetest-5213 was already deleted
STEP: Destroying namespace "nsdeletetest-3452" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":280,"completed":133,"skipped":2096,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:15:31.805: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4434
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-f84ef31b-6b82-4263-ba9c-32da2fec1453
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:15:36.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4434" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":134,"skipped":2103,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:15:36.336: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5533
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-376f09fa-33cd-40ef-82a1-ffbc91b64428
STEP: Creating a pod to test consume configMaps
Jan 19 17:15:36.599: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-52061084-1c53-48da-b297-18884f01a7b4" in namespace "projected-5533" to be "success or failure"
Jan 19 17:15:36.616: INFO: Pod "pod-projected-configmaps-52061084-1c53-48da-b297-18884f01a7b4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.894654ms
Jan 19 17:15:38.635: INFO: Pod "pod-projected-configmaps-52061084-1c53-48da-b297-18884f01a7b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035541047s
Jan 19 17:15:40.652: INFO: Pod "pod-projected-configmaps-52061084-1c53-48da-b297-18884f01a7b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053219795s
STEP: Saw pod success
Jan 19 17:15:40.653: INFO: Pod "pod-projected-configmaps-52061084-1c53-48da-b297-18884f01a7b4" satisfied condition "success or failure"
Jan 19 17:15:40.672: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-projected-configmaps-52061084-1c53-48da-b297-18884f01a7b4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 19 17:15:40.716: INFO: Waiting for pod pod-projected-configmaps-52061084-1c53-48da-b297-18884f01a7b4 to disappear
Jan 19 17:15:40.733: INFO: Pod pod-projected-configmaps-52061084-1c53-48da-b297-18884f01a7b4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:15:40.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5533" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":135,"skipped":2124,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:15:40.785: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3795
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-map-3c739e71-dc9e-4ece-bff9-3a20dac92cb1
STEP: Creating a pod to test consume configMaps
Jan 19 17:15:41.044: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6378a2ea-ee56-453f-b156-f2532889e582" in namespace "projected-3795" to be "success or failure"
Jan 19 17:15:41.060: INFO: Pod "pod-projected-configmaps-6378a2ea-ee56-453f-b156-f2532889e582": Phase="Pending", Reason="", readiness=false. Elapsed: 16.076995ms
Jan 19 17:15:43.078: INFO: Pod "pod-projected-configmaps-6378a2ea-ee56-453f-b156-f2532889e582": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033776057s
Jan 19 17:15:45.096: INFO: Pod "pod-projected-configmaps-6378a2ea-ee56-453f-b156-f2532889e582": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051656059s
STEP: Saw pod success
Jan 19 17:15:45.096: INFO: Pod "pod-projected-configmaps-6378a2ea-ee56-453f-b156-f2532889e582" satisfied condition "success or failure"
Jan 19 17:15:45.112: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-projected-configmaps-6378a2ea-ee56-453f-b156-f2532889e582 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 19 17:15:45.159: INFO: Waiting for pod pod-projected-configmaps-6378a2ea-ee56-453f-b156-f2532889e582 to disappear
Jan 19 17:15:45.176: INFO: Pod pod-projected-configmaps-6378a2ea-ee56-453f-b156-f2532889e582 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:15:45.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3795" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":280,"completed":136,"skipped":2145,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:15:45.226: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service multi-endpoint-test in namespace services-9108
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9108 to expose endpoints map[]
Jan 19 17:15:45.504: INFO: successfully validated that service multi-endpoint-test in namespace services-9108 exposes endpoints map[] (16.86289ms elapsed)
STEP: Creating pod pod1 in namespace services-9108
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9108 to expose endpoints map[pod1:[100]]
Jan 19 17:15:48.670: INFO: successfully validated that service multi-endpoint-test in namespace services-9108 exposes endpoints map[pod1:[100]] (3.139720043s elapsed)
STEP: Creating pod pod2 in namespace services-9108
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9108 to expose endpoints map[pod1:[100] pod2:[101]]
Jan 19 17:15:51.903: INFO: successfully validated that service multi-endpoint-test in namespace services-9108 exposes endpoints map[pod1:[100] pod2:[101]] (3.215007414s elapsed)
STEP: Deleting pod pod1 in namespace services-9108
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9108 to expose endpoints map[pod2:[101]]
Jan 19 17:15:51.954: INFO: successfully validated that service multi-endpoint-test in namespace services-9108 exposes endpoints map[pod2:[101]] (33.483762ms elapsed)
STEP: Deleting pod pod2 in namespace services-9108
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9108 to expose endpoints map[]
Jan 19 17:15:51.989: INFO: successfully validated that service multi-endpoint-test in namespace services-9108 exposes endpoints map[] (17.023187ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:15:52.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9108" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":280,"completed":137,"skipped":2155,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:15:52.067: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 19 17:15:56.931: INFO: Successfully updated pod "annotationupdatef515df32-3600-4181-b7a5-c37846cc263f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:15:58.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9628" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":138,"skipped":2165,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:15:59.032: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:64
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:79
STEP: Creating service test in namespace statefulset-6476
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a new StatefulSet
Jan 19 17:15:59.308: INFO: Found 1 stateful pods, waiting for 3
Jan 19 17:16:09.325: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 17:16:09.325: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 17:16:09.325: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Jan 19 17:16:19.325: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 17:16:19.325: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 17:16:19.325: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 17:16:19.377: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6476 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 17:16:19.948: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 17:16:19.948: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 17:16:19.948: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jan 19 17:16:30.060: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 19 17:16:30.131: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6476 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 17:16:30.657: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 17:16:30.657: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 17:16:30.657: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 17:16:50.762: INFO: Waiting for StatefulSet statefulset-6476/ss2 to complete update
STEP: Rolling back to a previous revision
Jan 19 17:17:00.798: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6476 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 17:17:01.431: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 17:17:01.431: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 17:17:01.431: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 17:17:11.544: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 19 17:17:11.595: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6476 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 17:17:12.119: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 17:17:12.119: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 17:17:12.119: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 17:17:42.226: INFO: Waiting for StatefulSet statefulset-6476/ss2 to complete update
Jan 19 17:17:42.226: INFO: Waiting for Pod statefulset-6476/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
Jan 19 17:17:52.260: INFO: Deleting all statefulset in ns statefulset-6476
Jan 19 17:17:52.276: INFO: Scaling statefulset ss2 to 0
Jan 19 17:18:12.344: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 17:18:12.361: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:18:12.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6476" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":280,"completed":139,"skipped":2182,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:18:12.473: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2556
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:18:13.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051093, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051093, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051093, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051093, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:18:15.833: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051093, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051093, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051093, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051093, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:18:18.872: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:18:19.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2556" for this suite.
STEP: Destroying namespace "webhook-2556-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":280,"completed":140,"skipped":2196,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:18:19.430: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5815
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 19 17:18:24.386: INFO: Successfully updated pod "labelsupdatee1b880b7-b741-4151-9630-c923035fdbb9"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:18:26.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5815" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":141,"skipped":2223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:18:26.484: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8555
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jan 19 17:18:26.707: INFO: namespace kubectl-8555
Jan 19 17:18:26.708: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-8555'
Jan 19 17:18:26.937: INFO: stderr: ""
Jan 19 17:18:26.937: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jan 19 17:18:27.955: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 17:18:27.955: INFO: Found 0 / 1
Jan 19 17:18:28.954: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 17:18:28.954: INFO: Found 0 / 1
Jan 19 17:18:29.955: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 17:18:29.955: INFO: Found 1 / 1
Jan 19 17:18:29.955: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 19 17:18:29.972: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 17:18:29.972: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 19 17:18:29.972: INFO: wait on agnhost-master startup in kubectl-8555 
Jan 19 17:18:29.972: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs agnhost-master-66vwx agnhost-master --namespace=kubectl-8555'
Jan 19 17:18:30.128: INFO: stderr: ""
Jan 19 17:18:30.128: INFO: stdout: "Paused\n"
STEP: exposing RC
Jan 19 17:18:30.128: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8555'
Jan 19 17:18:30.295: INFO: stderr: ""
Jan 19 17:18:30.295: INFO: stdout: "service/rm2 exposed\n"
Jan 19 17:18:30.312: INFO: Service rm2 in namespace kubectl-8555 found.
STEP: exposing service
Jan 19 17:18:32.347: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8555'
Jan 19 17:18:32.505: INFO: stderr: ""
Jan 19 17:18:32.505: INFO: stdout: "service/rm3 exposed\n"
Jan 19 17:18:32.522: INFO: Service rm3 in namespace kubectl-8555 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:18:34.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8555" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":280,"completed":142,"skipped":2247,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:18:34.608: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9722
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9722 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9722;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9722 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9722;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9722.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9722.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9722.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9722.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9722.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9722.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9722.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9722.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9722.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9722.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9722.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9722.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9722.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 34.224.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.224.34_udp@PTR;check="$$(dig +tcp +noall +answer +search 34.224.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.224.34_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9722 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9722;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9722 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9722;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9722.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9722.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9722.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9722.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9722.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9722.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9722.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9722.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9722.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9722.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9722.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9722.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9722.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 34.224.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.224.34_udp@PTR;check="$$(dig +tcp +noall +answer +search 34.224.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.224.34_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 19 17:18:41.242: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:41.326: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:41.345: INFO: Unable to read wheezy_udp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:41.364: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:41.383: INFO: Unable to read wheezy_udp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:41.402: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:41.422: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:41.441: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:41.845: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:41.864: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:41.882: INFO: Unable to read jessie_udp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:41.902: INFO: Unable to read jessie_tcp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:41.920: INFO: Unable to read jessie_udp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:41.938: INFO: Unable to read jessie_tcp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:41.957: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:41.976: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:42.397: INFO: Lookups using dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9722 wheezy_tcp@dns-test-service.dns-9722 wheezy_udp@dns-test-service.dns-9722.svc wheezy_tcp@dns-test-service.dns-9722.svc wheezy_udp@_http._tcp.dns-test-service.dns-9722.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9722.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9722 jessie_tcp@dns-test-service.dns-9722 jessie_udp@dns-test-service.dns-9722.svc jessie_tcp@dns-test-service.dns-9722.svc jessie_udp@_http._tcp.dns-test-service.dns-9722.svc jessie_tcp@_http._tcp.dns-test-service.dns-9722.svc]

Jan 19 17:18:47.417: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:47.435: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:47.453: INFO: Unable to read wheezy_udp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:47.471: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:47.490: INFO: Unable to read wheezy_udp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:47.509: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:47.528: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:47.546: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:47.988: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:48.006: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:48.024: INFO: Unable to read jessie_udp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:48.044: INFO: Unable to read jessie_tcp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:48.062: INFO: Unable to read jessie_udp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:48.080: INFO: Unable to read jessie_tcp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:48.099: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:48.117: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:48.516: INFO: Lookups using dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9722 wheezy_tcp@dns-test-service.dns-9722 wheezy_udp@dns-test-service.dns-9722.svc wheezy_tcp@dns-test-service.dns-9722.svc wheezy_udp@_http._tcp.dns-test-service.dns-9722.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9722.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9722 jessie_tcp@dns-test-service.dns-9722 jessie_udp@dns-test-service.dns-9722.svc jessie_tcp@dns-test-service.dns-9722.svc jessie_udp@_http._tcp.dns-test-service.dns-9722.svc jessie_tcp@_http._tcp.dns-test-service.dns-9722.svc]

Jan 19 17:18:52.422: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:52.465: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:52.483: INFO: Unable to read wheezy_udp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:52.501: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:52.520: INFO: Unable to read wheezy_udp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:52.539: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:52.557: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:52.575: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:52.912: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:52.930: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:52.948: INFO: Unable to read jessie_udp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:52.967: INFO: Unable to read jessie_tcp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:52.984: INFO: Unable to read jessie_udp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:53.003: INFO: Unable to read jessie_tcp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:53.020: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:53.039: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:53.414: INFO: Lookups using dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9722 wheezy_tcp@dns-test-service.dns-9722 wheezy_udp@dns-test-service.dns-9722.svc wheezy_tcp@dns-test-service.dns-9722.svc wheezy_udp@_http._tcp.dns-test-service.dns-9722.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9722.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9722 jessie_tcp@dns-test-service.dns-9722 jessie_udp@dns-test-service.dns-9722.svc jessie_tcp@dns-test-service.dns-9722.svc jessie_udp@_http._tcp.dns-test-service.dns-9722.svc jessie_tcp@_http._tcp.dns-test-service.dns-9722.svc]

Jan 19 17:18:57.418: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:57.437: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:57.455: INFO: Unable to read wheezy_udp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:57.473: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:57.492: INFO: Unable to read wheezy_udp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:57.510: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:57.528: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:57.547: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:57.932: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:57.951: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:57.969: INFO: Unable to read jessie_udp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:57.988: INFO: Unable to read jessie_tcp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:58.007: INFO: Unable to read jessie_udp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:58.025: INFO: Unable to read jessie_tcp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:58.043: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:58.061: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:18:58.460: INFO: Lookups using dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9722 wheezy_tcp@dns-test-service.dns-9722 wheezy_udp@dns-test-service.dns-9722.svc wheezy_tcp@dns-test-service.dns-9722.svc wheezy_udp@_http._tcp.dns-test-service.dns-9722.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9722.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9722 jessie_tcp@dns-test-service.dns-9722 jessie_udp@dns-test-service.dns-9722.svc jessie_tcp@dns-test-service.dns-9722.svc jessie_udp@_http._tcp.dns-test-service.dns-9722.svc jessie_tcp@_http._tcp.dns-test-service.dns-9722.svc]

Jan 19 17:19:02.417: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:02.437: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:02.455: INFO: Unable to read wheezy_udp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:02.475: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:02.493: INFO: Unable to read wheezy_udp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:02.511: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:02.537: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:02.555: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:02.997: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:03.015: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:03.033: INFO: Unable to read jessie_udp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:03.051: INFO: Unable to read jessie_tcp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:03.069: INFO: Unable to read jessie_udp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:03.088: INFO: Unable to read jessie_tcp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:03.107: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:03.125: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:03.488: INFO: Lookups using dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9722 wheezy_tcp@dns-test-service.dns-9722 wheezy_udp@dns-test-service.dns-9722.svc wheezy_tcp@dns-test-service.dns-9722.svc wheezy_udp@_http._tcp.dns-test-service.dns-9722.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9722.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9722 jessie_tcp@dns-test-service.dns-9722 jessie_udp@dns-test-service.dns-9722.svc jessie_tcp@dns-test-service.dns-9722.svc jessie_udp@_http._tcp.dns-test-service.dns-9722.svc jessie_tcp@_http._tcp.dns-test-service.dns-9722.svc]

Jan 19 17:19:07.426: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:07.446: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:07.480: INFO: Unable to read wheezy_udp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:07.517: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:07.536: INFO: Unable to read wheezy_udp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:07.556: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:07.575: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:07.593: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:08.036: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:08.054: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:08.073: INFO: Unable to read jessie_udp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:08.091: INFO: Unable to read jessie_tcp@dns-test-service.dns-9722 from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:08.109: INFO: Unable to read jessie_udp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:08.127: INFO: Unable to read jessie_tcp@dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:08.146: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:08.164: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9722.svc from pod dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8: the server could not find the requested resource (get pods dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8)
Jan 19 17:19:08.523: INFO: Lookups using dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9722 wheezy_tcp@dns-test-service.dns-9722 wheezy_udp@dns-test-service.dns-9722.svc wheezy_tcp@dns-test-service.dns-9722.svc wheezy_udp@_http._tcp.dns-test-service.dns-9722.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9722.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9722 jessie_tcp@dns-test-service.dns-9722 jessie_udp@dns-test-service.dns-9722.svc jessie_tcp@dns-test-service.dns-9722.svc jessie_udp@_http._tcp.dns-test-service.dns-9722.svc jessie_tcp@_http._tcp.dns-test-service.dns-9722.svc]

Jan 19 17:19:14.324: INFO: DNS probes using dns-9722/dns-test-d2b1247d-e97c-4b35-ac11-98daebba23f8 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:19:14.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9722" for this suite.
•{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":280,"completed":143,"skipped":2257,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:19:14.449: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3224
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:19:31.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3224" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":280,"completed":144,"skipped":2259,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:19:31.874: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1429
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:19:33.284: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051172, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051172, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051172, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051172, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:19:35.308: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051172, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051172, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051172, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051172, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:19:38.332: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:19:49.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1429" for this suite.
STEP: Destroying namespace "webhook-1429-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":280,"completed":145,"skipped":2260,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:19:49.177: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1826
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-fa9174de-8173-4e1f-b428-c09dfc8ac906
STEP: Creating a pod to test consume secrets
Jan 19 17:19:49.432: INFO: Waiting up to 5m0s for pod "pod-secrets-4627ce37-7e66-4e8e-be91-bea389f8d8b9" in namespace "secrets-1826" to be "success or failure"
Jan 19 17:19:49.448: INFO: Pod "pod-secrets-4627ce37-7e66-4e8e-be91-bea389f8d8b9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.270563ms
Jan 19 17:19:51.465: INFO: Pod "pod-secrets-4627ce37-7e66-4e8e-be91-bea389f8d8b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033106922s
Jan 19 17:19:53.484: INFO: Pod "pod-secrets-4627ce37-7e66-4e8e-be91-bea389f8d8b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051567185s
STEP: Saw pod success
Jan 19 17:19:53.484: INFO: Pod "pod-secrets-4627ce37-7e66-4e8e-be91-bea389f8d8b9" satisfied condition "success or failure"
Jan 19 17:19:53.510: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-secrets-4627ce37-7e66-4e8e-be91-bea389f8d8b9 container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 17:19:53.556: INFO: Waiting for pod pod-secrets-4627ce37-7e66-4e8e-be91-bea389f8d8b9 to disappear
Jan 19 17:19:53.572: INFO: Pod pod-secrets-4627ce37-7e66-4e8e-be91-bea389f8d8b9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:19:53.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1826" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":146,"skipped":2273,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:19:53.624: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5885
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 19 17:19:53.863: INFO: Waiting up to 5m0s for pod "pod-b97f8784-c3af-4d04-a68c-d7721de48696" in namespace "emptydir-5885" to be "success or failure"
Jan 19 17:19:53.879: INFO: Pod "pod-b97f8784-c3af-4d04-a68c-d7721de48696": Phase="Pending", Reason="", readiness=false. Elapsed: 16.191818ms
Jan 19 17:19:55.897: INFO: Pod "pod-b97f8784-c3af-4d04-a68c-d7721de48696": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034240453s
Jan 19 17:19:57.914: INFO: Pod "pod-b97f8784-c3af-4d04-a68c-d7721de48696": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051487949s
STEP: Saw pod success
Jan 19 17:19:57.914: INFO: Pod "pod-b97f8784-c3af-4d04-a68c-d7721de48696" satisfied condition "success or failure"
Jan 19 17:19:57.932: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-b97f8784-c3af-4d04-a68c-d7721de48696 container test-container: <nil>
STEP: delete the pod
Jan 19 17:19:57.981: INFO: Waiting for pod pod-b97f8784-c3af-4d04-a68c-d7721de48696 to disappear
Jan 19 17:19:58.003: INFO: Pod pod-b97f8784-c3af-4d04-a68c-d7721de48696 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:19:58.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5885" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":147,"skipped":2283,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:19:58.053: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-4828
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:19:58.270: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4828
I0119 17:19:58.293426    7438 runners.go:189] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4828, replica count: 1
I0119 17:19:59.343994    7438 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 17:20:00.344298    7438 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 17:20:01.344508    7438 runners.go:189] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 17:20:01.474: INFO: Created: latency-svc-kmhh8
Jan 19 17:20:01.479: INFO: Got endpoints: latency-svc-kmhh8 [35.099363ms]
Jan 19 17:20:01.510: INFO: Created: latency-svc-8szcj
Jan 19 17:20:01.513: INFO: Got endpoints: latency-svc-8szcj [33.567239ms]
Jan 19 17:20:01.524: INFO: Created: latency-svc-h56d7
Jan 19 17:20:01.534: INFO: Created: latency-svc-ppnh4
Jan 19 17:20:01.534: INFO: Got endpoints: latency-svc-h56d7 [54.518382ms]
Jan 19 17:20:01.560: INFO: Got endpoints: latency-svc-ppnh4 [80.513003ms]
Jan 19 17:20:01.569: INFO: Created: latency-svc-dxjtp
Jan 19 17:20:01.572: INFO: Got endpoints: latency-svc-dxjtp [91.937138ms]
Jan 19 17:20:01.585: INFO: Created: latency-svc-lkwj2
Jan 19 17:20:01.588: INFO: Got endpoints: latency-svc-lkwj2 [107.842777ms]
Jan 19 17:20:01.617: INFO: Created: latency-svc-qczwp
Jan 19 17:20:01.627: INFO: Got endpoints: latency-svc-qczwp [147.510186ms]
Jan 19 17:20:01.635: INFO: Created: latency-svc-545ss
Jan 19 17:20:01.673: INFO: Created: latency-svc-gsbv8
Jan 19 17:20:01.673: INFO: Got endpoints: latency-svc-545ss [193.202788ms]
Jan 19 17:20:01.720: INFO: Created: latency-svc-mjggf
Jan 19 17:20:01.722: INFO: Got endpoints: latency-svc-gsbv8 [241.684099ms]
Jan 19 17:20:01.731: INFO: Created: latency-svc-fm9nx
Jan 19 17:20:01.731: INFO: Got endpoints: latency-svc-mjggf [251.160863ms]
Jan 19 17:20:01.734: INFO: Got endpoints: latency-svc-fm9nx [254.160457ms]
Jan 19 17:20:01.743: INFO: Created: latency-svc-fsc7m
Jan 19 17:20:01.763: INFO: Created: latency-svc-hj266
Jan 19 17:20:01.763: INFO: Got endpoints: latency-svc-fsc7m [283.628346ms]
Jan 19 17:20:01.771: INFO: Got endpoints: latency-svc-hj266 [291.535545ms]
Jan 19 17:20:01.782: INFO: Created: latency-svc-k49t8
Jan 19 17:20:01.811: INFO: Got endpoints: latency-svc-k49t8 [331.212599ms]
Jan 19 17:20:01.820: INFO: Created: latency-svc-75cxz
Jan 19 17:20:01.823: INFO: Got endpoints: latency-svc-75cxz [343.059132ms]
Jan 19 17:20:01.836: INFO: Created: latency-svc-6c4sh
Jan 19 17:20:01.857: INFO: Created: latency-svc-xmjwr
Jan 19 17:20:01.857: INFO: Got endpoints: latency-svc-6c4sh [377.234764ms]
Jan 19 17:20:01.860: INFO: Got endpoints: latency-svc-xmjwr [347.213008ms]
Jan 19 17:20:01.870: INFO: Created: latency-svc-fhhdc
Jan 19 17:20:01.907: INFO: Created: latency-svc-bcpwk
Jan 19 17:20:01.907: INFO: Got endpoints: latency-svc-fhhdc [372.911821ms]
Jan 19 17:20:01.918: INFO: Created: latency-svc-br98t
Jan 19 17:20:01.918: INFO: Got endpoints: latency-svc-bcpwk [357.855019ms]
Jan 19 17:20:01.926: INFO: Got endpoints: latency-svc-br98t [354.181865ms]
Jan 19 17:20:01.951: INFO: Created: latency-svc-vrdvf
Jan 19 17:20:01.954: INFO: Got endpoints: latency-svc-vrdvf [366.260058ms]
Jan 19 17:20:01.962: INFO: Created: latency-svc-896qk
Jan 19 17:20:01.973: INFO: Got endpoints: latency-svc-896qk [345.286428ms]
Jan 19 17:20:01.973: INFO: Created: latency-svc-m7c4j
Jan 19 17:20:01.983: INFO: Created: latency-svc-h2w8t
Jan 19 17:20:01.983: INFO: Got endpoints: latency-svc-m7c4j [309.720361ms]
Jan 19 17:20:02.018: INFO: Got endpoints: latency-svc-h2w8t [296.383007ms]
Jan 19 17:20:02.023: INFO: Created: latency-svc-64k8z
Jan 19 17:20:02.026: INFO: Got endpoints: latency-svc-64k8z [294.680896ms]
Jan 19 17:20:02.069: INFO: Created: latency-svc-qmtmt
Jan 19 17:20:02.076: INFO: Got endpoints: latency-svc-qmtmt [341.884197ms]
Jan 19 17:20:02.086: INFO: Created: latency-svc-4nwbx
Jan 19 17:20:02.108: INFO: Created: latency-svc-zczgj
Jan 19 17:20:02.108: INFO: Got endpoints: latency-svc-4nwbx [344.110707ms]
Jan 19 17:20:02.111: INFO: Got endpoints: latency-svc-zczgj [339.063739ms]
Jan 19 17:20:02.156: INFO: Created: latency-svc-rgvrk
Jan 19 17:20:02.168: INFO: Created: latency-svc-xq7dp
Jan 19 17:20:02.168: INFO: Got endpoints: latency-svc-rgvrk [356.49068ms]
Jan 19 17:20:02.172: INFO: Got endpoints: latency-svc-xq7dp [349.465252ms]
Jan 19 17:20:02.216: INFO: Created: latency-svc-5b2qk
Jan 19 17:20:02.224: INFO: Got endpoints: latency-svc-5b2qk [367.023582ms]
Jan 19 17:20:02.233: INFO: Created: latency-svc-vh45h
Jan 19 17:20:02.244: INFO: Got endpoints: latency-svc-vh45h [383.14439ms]
Jan 19 17:20:02.244: INFO: Created: latency-svc-dmcgl
Jan 19 17:20:02.246: INFO: Got endpoints: latency-svc-dmcgl [338.957867ms]
Jan 19 17:20:02.273: INFO: Created: latency-svc-m4875
Jan 19 17:20:02.285: INFO: Created: latency-svc-z2njr
Jan 19 17:20:02.285: INFO: Got endpoints: latency-svc-m4875 [366.733562ms]
Jan 19 17:20:02.316: INFO: Got endpoints: latency-svc-z2njr [390.064002ms]
Jan 19 17:20:02.325: INFO: Created: latency-svc-lm8j7
Jan 19 17:20:02.340: INFO: Created: latency-svc-7nzl5
Jan 19 17:20:02.340: INFO: Got endpoints: latency-svc-lm8j7 [385.81341ms]
Jan 19 17:20:02.344: INFO: Got endpoints: latency-svc-7nzl5 [370.784316ms]
Jan 19 17:20:02.359: INFO: Created: latency-svc-fhmjt
Jan 19 17:20:02.371: INFO: Created: latency-svc-vhqtm
Jan 19 17:20:02.371: INFO: Got endpoints: latency-svc-fhmjt [388.422732ms]
Jan 19 17:20:02.411: INFO: Got endpoints: latency-svc-vhqtm [392.532495ms]
Jan 19 17:20:02.413: INFO: Created: latency-svc-fwlnb
Jan 19 17:20:02.417: INFO: Got endpoints: latency-svc-fwlnb [390.789649ms]
Jan 19 17:20:02.431: INFO: Created: latency-svc-2md4b
Jan 19 17:20:02.468: INFO: Got endpoints: latency-svc-2md4b [392.437478ms]
Jan 19 17:20:02.477: INFO: Created: latency-svc-kmqgk
Jan 19 17:20:02.480: INFO: Got endpoints: latency-svc-kmqgk [372.157537ms]
Jan 19 17:20:02.495: INFO: Created: latency-svc-z8j87
Jan 19 17:20:02.498: INFO: Got endpoints: latency-svc-z8j87 [387.332352ms]
Jan 19 17:20:02.507: INFO: Created: latency-svc-kk88d
Jan 19 17:20:02.525: INFO: Got endpoints: latency-svc-kk88d [357.429513ms]
Jan 19 17:20:02.526: INFO: Created: latency-svc-rtljt
Jan 19 17:20:02.528: INFO: Got endpoints: latency-svc-rtljt [355.872589ms]
Jan 19 17:20:02.570: INFO: Created: latency-svc-vgthv
Jan 19 17:20:02.583: INFO: Created: latency-svc-krxb4
Jan 19 17:20:02.583: INFO: Got endpoints: latency-svc-vgthv [358.543263ms]
Jan 19 17:20:02.591: INFO: Got endpoints: latency-svc-krxb4 [347.634144ms]
Jan 19 17:20:02.607: INFO: Created: latency-svc-cspw5
Jan 19 17:20:02.619: INFO: Created: latency-svc-5dk75
Jan 19 17:20:02.619: INFO: Got endpoints: latency-svc-cspw5 [373.082235ms]
Jan 19 17:20:02.622: INFO: Got endpoints: latency-svc-5dk75 [337.423633ms]
Jan 19 17:20:02.663: INFO: Created: latency-svc-6ftwt
Jan 19 17:20:02.674: INFO: Created: latency-svc-l6z5c
Jan 19 17:20:02.674: INFO: Got endpoints: latency-svc-6ftwt [358.253516ms]
Jan 19 17:20:02.717: INFO: Created: latency-svc-mbr97
Jan 19 17:20:02.717: INFO: Got endpoints: latency-svc-l6z5c [377.033911ms]
Jan 19 17:20:02.728: INFO: Created: latency-svc-f8bns
Jan 19 17:20:02.729: INFO: Got endpoints: latency-svc-mbr97 [384.95399ms]
Jan 19 17:20:02.738: INFO: Got endpoints: latency-svc-f8bns [366.251106ms]
Jan 19 17:20:02.778: INFO: Created: latency-svc-kqsxx
Jan 19 17:20:02.781: INFO: Got endpoints: latency-svc-kqsxx [369.914238ms]
Jan 19 17:20:02.817: INFO: Created: latency-svc-84rlq
Jan 19 17:20:02.820: INFO: Got endpoints: latency-svc-84rlq [403.469419ms]
Jan 19 17:20:02.831: INFO: Created: latency-svc-f5rh6
Jan 19 17:20:02.841: INFO: Got endpoints: latency-svc-f5rh6 [372.364315ms]
Jan 19 17:20:02.860: INFO: Created: latency-svc-9xgvr
Jan 19 17:20:02.863: INFO: Got endpoints: latency-svc-9xgvr [382.793392ms]
Jan 19 17:20:02.874: INFO: Created: latency-svc-82qcm
Jan 19 17:20:02.912: INFO: Got endpoints: latency-svc-82qcm [414.30481ms]
Jan 19 17:20:02.915: INFO: Created: latency-svc-v82l5
Jan 19 17:20:02.973: INFO: Created: latency-svc-hwrhq
Jan 19 17:20:02.973: INFO: Got endpoints: latency-svc-v82l5 [447.689472ms]
Jan 19 17:20:02.985: INFO: Created: latency-svc-hjxfk
Jan 19 17:20:02.985: INFO: Got endpoints: latency-svc-hwrhq [456.383736ms]
Jan 19 17:20:02.996: INFO: Created: latency-svc-tvcbm
Jan 19 17:20:03.061: INFO: Created: latency-svc-lnn2v
Jan 19 17:20:03.061: INFO: Got endpoints: latency-svc-hjxfk [477.666845ms]
Jan 19 17:20:03.071: INFO: Created: latency-svc-w98s4
Jan 19 17:20:03.083: INFO: Created: latency-svc-cpfcm
Jan 19 17:20:03.083: INFO: Got endpoints: latency-svc-tvcbm [491.896343ms]
Jan 19 17:20:03.096: INFO: Created: latency-svc-llsgt
Jan 19 17:20:03.122: INFO: Created: latency-svc-9f475
Jan 19 17:20:03.135: INFO: Created: latency-svc-9htwh
Jan 19 17:20:03.135: INFO: Got endpoints: latency-svc-lnn2v [515.829512ms]
Jan 19 17:20:03.156: INFO: Created: latency-svc-6688d
Jan 19 17:20:03.217: INFO: Created: latency-svc-22xz6
Jan 19 17:20:03.217: INFO: Got endpoints: latency-svc-w98s4 [594.245199ms]
Jan 19 17:20:03.233: INFO: Got endpoints: latency-svc-cpfcm [557.922949ms]
Jan 19 17:20:03.233: INFO: Created: latency-svc-jbdmq
Jan 19 17:20:03.244: INFO: Created: latency-svc-vwfzt
Jan 19 17:20:03.269: INFO: Created: latency-svc-fglwp
Jan 19 17:20:03.280: INFO: Created: latency-svc-xmf5p
Jan 19 17:20:03.280: INFO: Got endpoints: latency-svc-llsgt [563.383561ms]
Jan 19 17:20:03.313: INFO: Created: latency-svc-9tf72
Jan 19 17:20:03.360: INFO: Got endpoints: latency-svc-9f475 [631.357331ms]
Jan 19 17:20:03.361: INFO: Created: latency-svc-67hpq
Jan 19 17:20:03.374: INFO: Created: latency-svc-77mdv
Jan 19 17:20:03.385: INFO: Created: latency-svc-n4zh4
Jan 19 17:20:03.385: INFO: Got endpoints: latency-svc-9htwh [646.902555ms]
Jan 19 17:20:03.422: INFO: Created: latency-svc-7fch9
Jan 19 17:20:03.430: INFO: Got endpoints: latency-svc-6688d [649.344829ms]
Jan 19 17:20:03.451: INFO: Created: latency-svc-nfgvn
Jan 19 17:20:03.514: INFO: Created: latency-svc-g5nft
Jan 19 17:20:03.514: INFO: Got endpoints: latency-svc-22xz6 [694.004687ms]
Jan 19 17:20:03.527: INFO: Created: latency-svc-8z6xg
Jan 19 17:20:03.530: INFO: Got endpoints: latency-svc-jbdmq [689.420304ms]
Jan 19 17:20:03.538: INFO: Created: latency-svc-rsm48
Jan 19 17:20:03.566: INFO: Created: latency-svc-vchxk
Jan 19 17:20:03.578: INFO: Created: latency-svc-2fmkc
Jan 19 17:20:03.578: INFO: Got endpoints: latency-svc-vwfzt [715.195246ms]
Jan 19 17:20:03.608: INFO: Created: latency-svc-wqm6c
Jan 19 17:20:03.654: INFO: Created: latency-svc-kx8mt
Jan 19 17:20:03.654: INFO: Got endpoints: latency-svc-fglwp [741.928946ms]
Jan 19 17:20:03.683: INFO: Got endpoints: latency-svc-xmf5p [710.146186ms]
Jan 19 17:20:03.683: INFO: Created: latency-svc-p74bg
Jan 19 17:20:03.712: INFO: Created: latency-svc-4bkwc
Jan 19 17:20:03.731: INFO: Got endpoints: latency-svc-9tf72 [745.818706ms]
Jan 19 17:20:03.771: INFO: Created: latency-svc-c2qzh
Jan 19 17:20:03.780: INFO: Got endpoints: latency-svc-67hpq [719.526345ms]
Jan 19 17:20:03.811: INFO: Created: latency-svc-5l8zd
Jan 19 17:20:03.828: INFO: Got endpoints: latency-svc-77mdv [744.877571ms]
Jan 19 17:20:03.857: INFO: Created: latency-svc-crnhr
Jan 19 17:20:03.889: INFO: Got endpoints: latency-svc-n4zh4 [753.6387ms]
Jan 19 17:20:03.917: INFO: Created: latency-svc-qvgn6
Jan 19 17:20:03.928: INFO: Got endpoints: latency-svc-7fch9 [711.320915ms]
Jan 19 17:20:03.957: INFO: Created: latency-svc-qcm5r
Jan 19 17:20:03.978: INFO: Got endpoints: latency-svc-nfgvn [745.44834ms]
Jan 19 17:20:04.032: INFO: Created: latency-svc-f87wl
Jan 19 17:20:04.032: INFO: Got endpoints: latency-svc-g5nft [751.502007ms]
Jan 19 17:20:04.064: INFO: Created: latency-svc-t22cx
Jan 19 17:20:04.081: INFO: Got endpoints: latency-svc-8z6xg [721.211404ms]
Jan 19 17:20:04.110: INFO: Created: latency-svc-28khs
Jan 19 17:20:04.154: INFO: Got endpoints: latency-svc-rsm48 [769.73292ms]
Jan 19 17:20:04.184: INFO: Created: latency-svc-x6jwh
Jan 19 17:20:04.184: INFO: Got endpoints: latency-svc-vchxk [754.242366ms]
Jan 19 17:20:04.214: INFO: Created: latency-svc-225zr
Jan 19 17:20:04.228: INFO: Got endpoints: latency-svc-2fmkc [714.162823ms]
Jan 19 17:20:04.274: INFO: Created: latency-svc-nnpdf
Jan 19 17:20:04.286: INFO: Got endpoints: latency-svc-wqm6c [755.953383ms]
Jan 19 17:20:04.314: INFO: Created: latency-svc-f2rkc
Jan 19 17:20:04.328: INFO: Got endpoints: latency-svc-kx8mt [749.444291ms]
Jan 19 17:20:04.356: INFO: Created: latency-svc-p5x6l
Jan 19 17:20:04.389: INFO: Got endpoints: latency-svc-p74bg [734.92243ms]
Jan 19 17:20:04.418: INFO: Created: latency-svc-rr866
Jan 19 17:20:04.428: INFO: Got endpoints: latency-svc-4bkwc [744.648722ms]
Jan 19 17:20:04.457: INFO: Created: latency-svc-kt2bw
Jan 19 17:20:04.478: INFO: Got endpoints: latency-svc-c2qzh [747.484ms]
Jan 19 17:20:04.526: INFO: Created: latency-svc-nvvzl
Jan 19 17:20:04.528: INFO: Got endpoints: latency-svc-5l8zd [748.193945ms]
Jan 19 17:20:04.558: INFO: Created: latency-svc-q5hv5
Jan 19 17:20:04.578: INFO: Got endpoints: latency-svc-crnhr [749.663104ms]
Jan 19 17:20:04.620: INFO: Created: latency-svc-mf5x2
Jan 19 17:20:04.628: INFO: Got endpoints: latency-svc-qvgn6 [739.241062ms]
Jan 19 17:20:04.659: INFO: Created: latency-svc-wwnxk
Jan 19 17:20:04.678: INFO: Got endpoints: latency-svc-qcm5r [749.533678ms]
Jan 19 17:20:04.713: INFO: Created: latency-svc-kkpxb
Jan 19 17:20:04.734: INFO: Got endpoints: latency-svc-f87wl [755.56349ms]
Jan 19 17:20:04.767: INFO: Created: latency-svc-vvjhg
Jan 19 17:20:04.778: INFO: Got endpoints: latency-svc-t22cx [745.977082ms]
Jan 19 17:20:04.807: INFO: Created: latency-svc-72grl
Jan 19 17:20:04.828: INFO: Got endpoints: latency-svc-28khs [747.035452ms]
Jan 19 17:20:04.860: INFO: Created: latency-svc-9l5xr
Jan 19 17:20:04.878: INFO: Got endpoints: latency-svc-x6jwh [723.40375ms]
Jan 19 17:20:04.907: INFO: Created: latency-svc-8hp5q
Jan 19 17:20:04.928: INFO: Got endpoints: latency-svc-225zr [743.969472ms]
Jan 19 17:20:04.970: INFO: Created: latency-svc-gc6r9
Jan 19 17:20:04.978: INFO: Got endpoints: latency-svc-nnpdf [749.179563ms]
Jan 19 17:20:05.006: INFO: Created: latency-svc-q59m4
Jan 19 17:20:05.029: INFO: Got endpoints: latency-svc-f2rkc [742.422522ms]
Jan 19 17:20:05.058: INFO: Created: latency-svc-dzdsl
Jan 19 17:20:05.079: INFO: Got endpoints: latency-svc-p5x6l [751.835384ms]
Jan 19 17:20:05.108: INFO: Created: latency-svc-5v4v8
Jan 19 17:20:05.128: INFO: Got endpoints: latency-svc-rr866 [738.588045ms]
Jan 19 17:20:05.158: INFO: Created: latency-svc-cqfcp
Jan 19 17:20:05.178: INFO: Got endpoints: latency-svc-kt2bw [750.084788ms]
Jan 19 17:20:05.208: INFO: Created: latency-svc-8zj9h
Jan 19 17:20:05.228: INFO: Got endpoints: latency-svc-nvvzl [749.555817ms]
Jan 19 17:20:05.257: INFO: Created: latency-svc-x5mqm
Jan 19 17:20:05.278: INFO: Got endpoints: latency-svc-q5hv5 [749.18616ms]
Jan 19 17:20:05.322: INFO: Created: latency-svc-ln78v
Jan 19 17:20:05.328: INFO: Got endpoints: latency-svc-mf5x2 [749.972806ms]
Jan 19 17:20:05.358: INFO: Created: latency-svc-b5dtx
Jan 19 17:20:05.378: INFO: Got endpoints: latency-svc-wwnxk [749.861522ms]
Jan 19 17:20:05.407: INFO: Created: latency-svc-jgdd4
Jan 19 17:20:05.429: INFO: Got endpoints: latency-svc-kkpxb [751.639938ms]
Jan 19 17:20:05.470: INFO: Created: latency-svc-8rdgt
Jan 19 17:20:05.478: INFO: Got endpoints: latency-svc-vvjhg [744.426561ms]
Jan 19 17:20:05.507: INFO: Created: latency-svc-twmf6
Jan 19 17:20:05.528: INFO: Got endpoints: latency-svc-72grl [750.019769ms]
Jan 19 17:20:05.556: INFO: Created: latency-svc-hrr2r
Jan 19 17:20:05.578: INFO: Got endpoints: latency-svc-9l5xr [749.8615ms]
Jan 19 17:20:05.609: INFO: Created: latency-svc-5fpqb
Jan 19 17:20:05.629: INFO: Got endpoints: latency-svc-8hp5q [750.746716ms]
Jan 19 17:20:05.668: INFO: Created: latency-svc-5jrc4
Jan 19 17:20:05.678: INFO: Got endpoints: latency-svc-gc6r9 [749.421435ms]
Jan 19 17:20:05.706: INFO: Created: latency-svc-x5prg
Jan 19 17:20:05.728: INFO: Got endpoints: latency-svc-q59m4 [750.252825ms]
Jan 19 17:20:05.756: INFO: Created: latency-svc-qfbvw
Jan 19 17:20:05.781: INFO: Got endpoints: latency-svc-dzdsl [751.704744ms]
Jan 19 17:20:05.809: INFO: Created: latency-svc-4swbz
Jan 19 17:20:05.828: INFO: Got endpoints: latency-svc-5v4v8 [748.29384ms]
Jan 19 17:20:05.856: INFO: Created: latency-svc-gjpjg
Jan 19 17:20:05.890: INFO: Got endpoints: latency-svc-cqfcp [761.745251ms]
Jan 19 17:20:05.918: INFO: Created: latency-svc-jzjvm
Jan 19 17:20:05.929: INFO: Got endpoints: latency-svc-8zj9h [750.487018ms]
Jan 19 17:20:05.959: INFO: Created: latency-svc-v4qfw
Jan 19 17:20:05.979: INFO: Got endpoints: latency-svc-x5mqm [751.171261ms]
Jan 19 17:20:06.019: INFO: Created: latency-svc-9jd47
Jan 19 17:20:06.028: INFO: Got endpoints: latency-svc-ln78v [750.355092ms]
Jan 19 17:20:06.059: INFO: Created: latency-svc-f2qjg
Jan 19 17:20:06.078: INFO: Got endpoints: latency-svc-b5dtx [749.955289ms]
Jan 19 17:20:06.122: INFO: Created: latency-svc-lv987
Jan 19 17:20:06.128: INFO: Got endpoints: latency-svc-jgdd4 [749.745905ms]
Jan 19 17:20:06.157: INFO: Created: latency-svc-2kzrg
Jan 19 17:20:06.178: INFO: Got endpoints: latency-svc-8rdgt [748.351829ms]
Jan 19 17:20:06.212: INFO: Created: latency-svc-6qzb9
Jan 19 17:20:06.237: INFO: Got endpoints: latency-svc-twmf6 [758.256574ms]
Jan 19 17:20:06.266: INFO: Created: latency-svc-z8fzl
Jan 19 17:20:06.278: INFO: Got endpoints: latency-svc-hrr2r [750.157129ms]
Jan 19 17:20:06.307: INFO: Created: latency-svc-g2vrh
Jan 19 17:20:06.328: INFO: Got endpoints: latency-svc-5fpqb [749.298298ms]
Jan 19 17:20:06.362: INFO: Created: latency-svc-5frf4
Jan 19 17:20:06.378: INFO: Got endpoints: latency-svc-5jrc4 [749.225827ms]
Jan 19 17:20:06.407: INFO: Created: latency-svc-64mjx
Jan 19 17:20:06.428: INFO: Got endpoints: latency-svc-x5prg [750.019255ms]
Jan 19 17:20:06.469: INFO: Created: latency-svc-llmjm
Jan 19 17:20:06.478: INFO: Got endpoints: latency-svc-qfbvw [750.052325ms]
Jan 19 17:20:06.509: INFO: Created: latency-svc-bnwxk
Jan 19 17:20:06.528: INFO: Got endpoints: latency-svc-4swbz [747.738215ms]
Jan 19 17:20:06.558: INFO: Created: latency-svc-95kfr
Jan 19 17:20:06.587: INFO: Got endpoints: latency-svc-gjpjg [758.85548ms]
Jan 19 17:20:06.615: INFO: Created: latency-svc-7ws56
Jan 19 17:20:06.628: INFO: Got endpoints: latency-svc-jzjvm [738.277589ms]
Jan 19 17:20:06.657: INFO: Created: latency-svc-86hxx
Jan 19 17:20:06.678: INFO: Got endpoints: latency-svc-v4qfw [749.408258ms]
Jan 19 17:20:06.722: INFO: Created: latency-svc-wmw9n
Jan 19 17:20:06.728: INFO: Got endpoints: latency-svc-9jd47 [748.82781ms]
Jan 19 17:20:06.759: INFO: Created: latency-svc-5vnb7
Jan 19 17:20:06.778: INFO: Got endpoints: latency-svc-f2qjg [749.746367ms]
Jan 19 17:20:06.819: INFO: Created: latency-svc-hbg7l
Jan 19 17:20:06.828: INFO: Got endpoints: latency-svc-lv987 [750.156879ms]
Jan 19 17:20:06.855: INFO: Created: latency-svc-dh2z6
Jan 19 17:20:06.878: INFO: Got endpoints: latency-svc-2kzrg [750.245053ms]
Jan 19 17:20:06.913: INFO: Created: latency-svc-zxw2s
Jan 19 17:20:06.967: INFO: Got endpoints: latency-svc-6qzb9 [789.236342ms]
Jan 19 17:20:06.978: INFO: Got endpoints: latency-svc-z8fzl [741.305524ms]
Jan 19 17:20:06.995: INFO: Created: latency-svc-xkhgw
Jan 19 17:20:07.006: INFO: Created: latency-svc-5n7nm
Jan 19 17:20:07.028: INFO: Got endpoints: latency-svc-g2vrh [749.644474ms]
Jan 19 17:20:07.056: INFO: Created: latency-svc-2lvgj
Jan 19 17:20:07.083: INFO: Got endpoints: latency-svc-5frf4 [755.317049ms]
Jan 19 17:20:07.114: INFO: Created: latency-svc-tlvlg
Jan 19 17:20:07.128: INFO: Got endpoints: latency-svc-64mjx [750.236004ms]
Jan 19 17:20:07.158: INFO: Created: latency-svc-s8fhf
Jan 19 17:20:07.179: INFO: Got endpoints: latency-svc-llmjm [750.689806ms]
Jan 19 17:20:07.223: INFO: Created: latency-svc-64fj2
Jan 19 17:20:07.229: INFO: Got endpoints: latency-svc-bnwxk [750.478815ms]
Jan 19 17:20:07.257: INFO: Created: latency-svc-bwg9m
Jan 19 17:20:07.278: INFO: Got endpoints: latency-svc-95kfr [749.66937ms]
Jan 19 17:20:07.329: INFO: Got endpoints: latency-svc-7ws56 [742.275146ms]
Jan 19 17:20:07.329: INFO: Created: latency-svc-mxvhx
Jan 19 17:20:07.358: INFO: Created: latency-svc-wp48r
Jan 19 17:20:07.379: INFO: Got endpoints: latency-svc-86hxx [751.248256ms]
Jan 19 17:20:07.411: INFO: Created: latency-svc-nq5xj
Jan 19 17:20:07.434: INFO: Got endpoints: latency-svc-wmw9n [756.418136ms]
Jan 19 17:20:07.473: INFO: Created: latency-svc-k6c77
Jan 19 17:20:07.482: INFO: Got endpoints: latency-svc-5vnb7 [753.380246ms]
Jan 19 17:20:07.511: INFO: Created: latency-svc-kxcbj
Jan 19 17:20:07.528: INFO: Got endpoints: latency-svc-hbg7l [749.709715ms]
Jan 19 17:20:07.561: INFO: Created: latency-svc-8b2v4
Jan 19 17:20:07.579: INFO: Got endpoints: latency-svc-dh2z6 [750.124209ms]
Jan 19 17:20:07.608: INFO: Created: latency-svc-mwmfz
Jan 19 17:20:07.628: INFO: Got endpoints: latency-svc-zxw2s [749.243815ms]
Jan 19 17:20:07.662: INFO: Created: latency-svc-tm2nh
Jan 19 17:20:07.679: INFO: Got endpoints: latency-svc-xkhgw [711.427693ms]
Jan 19 17:20:07.708: INFO: Created: latency-svc-2xbjz
Jan 19 17:20:07.728: INFO: Got endpoints: latency-svc-5n7nm [749.854469ms]
Jan 19 17:20:07.757: INFO: Created: latency-svc-czw92
Jan 19 17:20:07.778: INFO: Got endpoints: latency-svc-2lvgj [750.222523ms]
Jan 19 17:20:07.811: INFO: Created: latency-svc-6c2mg
Jan 19 17:20:07.828: INFO: Got endpoints: latency-svc-tlvlg [745.041376ms]
Jan 19 17:20:07.857: INFO: Created: latency-svc-pjg65
Jan 19 17:20:07.893: INFO: Got endpoints: latency-svc-s8fhf [764.862769ms]
Jan 19 17:20:07.922: INFO: Created: latency-svc-r77mv
Jan 19 17:20:07.931: INFO: Got endpoints: latency-svc-64fj2 [752.283554ms]
Jan 19 17:20:07.960: INFO: Created: latency-svc-qtbcf
Jan 19 17:20:07.978: INFO: Got endpoints: latency-svc-bwg9m [749.387823ms]
Jan 19 17:20:08.023: INFO: Created: latency-svc-6qm66
Jan 19 17:20:08.028: INFO: Got endpoints: latency-svc-mxvhx [749.985923ms]
Jan 19 17:20:08.061: INFO: Created: latency-svc-6f8bc
Jan 19 17:20:08.078: INFO: Got endpoints: latency-svc-wp48r [749.001915ms]
Jan 19 17:20:08.108: INFO: Created: latency-svc-sqnps
Jan 19 17:20:08.128: INFO: Got endpoints: latency-svc-nq5xj [748.621105ms]
Jan 19 17:20:08.157: INFO: Created: latency-svc-4tvlc
Jan 19 17:20:08.178: INFO: Got endpoints: latency-svc-k6c77 [743.569961ms]
Jan 19 17:20:08.206: INFO: Created: latency-svc-lqlwq
Jan 19 17:20:08.244: INFO: Got endpoints: latency-svc-kxcbj [762.561412ms]
Jan 19 17:20:08.273: INFO: Created: latency-svc-zmw7k
Jan 19 17:20:08.278: INFO: Got endpoints: latency-svc-8b2v4 [749.93185ms]
Jan 19 17:20:08.307: INFO: Created: latency-svc-pnbhr
Jan 19 17:20:08.328: INFO: Got endpoints: latency-svc-mwmfz [749.325622ms]
Jan 19 17:20:08.378: INFO: Created: latency-svc-fbjnm
Jan 19 17:20:08.380: INFO: Got endpoints: latency-svc-tm2nh [752.845127ms]
Jan 19 17:20:08.409: INFO: Created: latency-svc-mwwzj
Jan 19 17:20:08.428: INFO: Got endpoints: latency-svc-2xbjz [749.223019ms]
Jan 19 17:20:08.457: INFO: Created: latency-svc-7n8mh
Jan 19 17:20:08.479: INFO: Got endpoints: latency-svc-czw92 [750.791922ms]
Jan 19 17:20:08.508: INFO: Created: latency-svc-fch68
Jan 19 17:20:08.529: INFO: Got endpoints: latency-svc-6c2mg [750.813687ms]
Jan 19 17:20:08.558: INFO: Created: latency-svc-fwsxt
Jan 19 17:20:08.580: INFO: Got endpoints: latency-svc-pjg65 [751.623838ms]
Jan 19 17:20:08.610: INFO: Created: latency-svc-p6zlq
Jan 19 17:20:08.628: INFO: Got endpoints: latency-svc-r77mv [734.52393ms]
Jan 19 17:20:08.657: INFO: Created: latency-svc-97t4c
Jan 19 17:20:08.678: INFO: Got endpoints: latency-svc-qtbcf [746.963976ms]
Jan 19 17:20:08.718: INFO: Created: latency-svc-vgmqt
Jan 19 17:20:08.728: INFO: Got endpoints: latency-svc-6qm66 [750.058942ms]
Jan 19 17:20:08.756: INFO: Created: latency-svc-7pz87
Jan 19 17:20:08.781: INFO: Got endpoints: latency-svc-6f8bc [752.562337ms]
Jan 19 17:20:08.822: INFO: Created: latency-svc-mhr8p
Jan 19 17:20:08.828: INFO: Got endpoints: latency-svc-sqnps [749.973059ms]
Jan 19 17:20:08.859: INFO: Created: latency-svc-tb4g7
Jan 19 17:20:08.879: INFO: Got endpoints: latency-svc-4tvlc [750.283813ms]
Jan 19 17:20:08.911: INFO: Created: latency-svc-rvmzk
Jan 19 17:20:08.936: INFO: Got endpoints: latency-svc-lqlwq [757.763955ms]
Jan 19 17:20:08.974: INFO: Created: latency-svc-bx5jz
Jan 19 17:20:08.979: INFO: Got endpoints: latency-svc-zmw7k [734.181083ms]
Jan 19 17:20:09.008: INFO: Created: latency-svc-gmfdw
Jan 19 17:20:09.028: INFO: Got endpoints: latency-svc-pnbhr [750.475388ms]
Jan 19 17:20:09.062: INFO: Created: latency-svc-svftn
Jan 19 17:20:09.083: INFO: Got endpoints: latency-svc-fbjnm [754.658199ms]
Jan 19 17:20:09.112: INFO: Created: latency-svc-psrn6
Jan 19 17:20:09.128: INFO: Got endpoints: latency-svc-mwwzj [747.486192ms]
Jan 19 17:20:09.166: INFO: Created: latency-svc-5xvjt
Jan 19 17:20:09.179: INFO: Got endpoints: latency-svc-7n8mh [750.893763ms]
Jan 19 17:20:09.208: INFO: Created: latency-svc-c5bv8
Jan 19 17:20:09.229: INFO: Got endpoints: latency-svc-fch68 [749.807938ms]
Jan 19 17:20:09.258: INFO: Created: latency-svc-tfh6v
Jan 19 17:20:09.280: INFO: Got endpoints: latency-svc-fwsxt [750.934617ms]
Jan 19 17:20:09.314: INFO: Created: latency-svc-lwx8f
Jan 19 17:20:09.328: INFO: Got endpoints: latency-svc-p6zlq [748.283841ms]
Jan 19 17:20:09.378: INFO: Got endpoints: latency-svc-97t4c [750.140766ms]
Jan 19 17:20:09.428: INFO: Got endpoints: latency-svc-vgmqt [750.405653ms]
Jan 19 17:20:09.479: INFO: Got endpoints: latency-svc-7pz87 [750.759084ms]
Jan 19 17:20:09.531: INFO: Got endpoints: latency-svc-mhr8p [750.012158ms]
Jan 19 17:20:09.579: INFO: Got endpoints: latency-svc-tb4g7 [750.315119ms]
Jan 19 17:20:09.628: INFO: Got endpoints: latency-svc-rvmzk [749.62129ms]
Jan 19 17:20:09.678: INFO: Got endpoints: latency-svc-bx5jz [742.241731ms]
Jan 19 17:20:09.728: INFO: Got endpoints: latency-svc-gmfdw [749.302614ms]
Jan 19 17:20:09.779: INFO: Got endpoints: latency-svc-svftn [750.293892ms]
Jan 19 17:20:09.837: INFO: Got endpoints: latency-svc-psrn6 [754.124219ms]
Jan 19 17:20:09.879: INFO: Got endpoints: latency-svc-5xvjt [750.496366ms]
Jan 19 17:20:09.929: INFO: Got endpoints: latency-svc-c5bv8 [749.865198ms]
Jan 19 17:20:09.978: INFO: Got endpoints: latency-svc-tfh6v [749.694995ms]
Jan 19 17:20:10.029: INFO: Got endpoints: latency-svc-lwx8f [748.617033ms]
Jan 19 17:20:10.029: INFO: Latencies: [33.567239ms 54.518382ms 80.513003ms 91.937138ms 107.842777ms 147.510186ms 193.202788ms 241.684099ms 251.160863ms 254.160457ms 283.628346ms 291.535545ms 294.680896ms 296.383007ms 309.720361ms 331.212599ms 337.423633ms 338.957867ms 339.063739ms 341.884197ms 343.059132ms 344.110707ms 345.286428ms 347.213008ms 347.634144ms 349.465252ms 354.181865ms 355.872589ms 356.49068ms 357.429513ms 357.855019ms 358.253516ms 358.543263ms 366.251106ms 366.260058ms 366.733562ms 367.023582ms 369.914238ms 370.784316ms 372.157537ms 372.364315ms 372.911821ms 373.082235ms 377.033911ms 377.234764ms 382.793392ms 383.14439ms 384.95399ms 385.81341ms 387.332352ms 388.422732ms 390.064002ms 390.789649ms 392.437478ms 392.532495ms 403.469419ms 414.30481ms 447.689472ms 456.383736ms 477.666845ms 491.896343ms 515.829512ms 557.922949ms 563.383561ms 594.245199ms 631.357331ms 646.902555ms 649.344829ms 689.420304ms 694.004687ms 710.146186ms 711.320915ms 711.427693ms 714.162823ms 715.195246ms 719.526345ms 721.211404ms 723.40375ms 734.181083ms 734.52393ms 734.92243ms 738.277589ms 738.588045ms 739.241062ms 741.305524ms 741.928946ms 742.241731ms 742.275146ms 742.422522ms 743.569961ms 743.969472ms 744.426561ms 744.648722ms 744.877571ms 745.041376ms 745.44834ms 745.818706ms 745.977082ms 746.963976ms 747.035452ms 747.484ms 747.486192ms 747.738215ms 748.193945ms 748.283841ms 748.29384ms 748.351829ms 748.617033ms 748.621105ms 748.82781ms 749.001915ms 749.179563ms 749.18616ms 749.223019ms 749.225827ms 749.243815ms 749.298298ms 749.302614ms 749.325622ms 749.387823ms 749.408258ms 749.421435ms 749.444291ms 749.533678ms 749.555817ms 749.62129ms 749.644474ms 749.663104ms 749.66937ms 749.694995ms 749.709715ms 749.745905ms 749.746367ms 749.807938ms 749.854469ms 749.8615ms 749.861522ms 749.865198ms 749.93185ms 749.955289ms 749.972806ms 749.973059ms 749.985923ms 750.012158ms 750.019255ms 750.019769ms 750.052325ms 750.058942ms 750.084788ms 750.124209ms 750.140766ms 750.156879ms 750.157129ms 750.222523ms 750.236004ms 750.245053ms 750.252825ms 750.283813ms 750.293892ms 750.315119ms 750.355092ms 750.405653ms 750.475388ms 750.478815ms 750.487018ms 750.496366ms 750.689806ms 750.746716ms 750.759084ms 750.791922ms 750.813687ms 750.893763ms 750.934617ms 751.171261ms 751.248256ms 751.502007ms 751.623838ms 751.639938ms 751.704744ms 751.835384ms 752.283554ms 752.562337ms 752.845127ms 753.380246ms 753.6387ms 754.124219ms 754.242366ms 754.658199ms 755.317049ms 755.56349ms 755.953383ms 756.418136ms 757.763955ms 758.256574ms 758.85548ms 761.745251ms 762.561412ms 764.862769ms 769.73292ms 789.236342ms]
Jan 19 17:20:10.029: INFO: 50 %ile: 747.484ms
Jan 19 17:20:10.029: INFO: 90 %ile: 752.283554ms
Jan 19 17:20:10.029: INFO: 99 %ile: 769.73292ms
Jan 19 17:20:10.029: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:20:10.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4828" for this suite.
•{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":280,"completed":148,"skipped":2346,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:20:10.093: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-3628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:46
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:20:10.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3628" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":280,"completed":149,"skipped":2356,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:20:10.385: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8940
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 19 17:20:10.706: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8940 /api/v1/namespaces/watch-8940/configmaps/e2e-watch-test-label-changed 4020a251-a559-48e2-a717-ccae7494dcdf 23984 0 2020-01-19 17:20:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 19 17:20:10.706: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8940 /api/v1/namespaces/watch-8940/configmaps/e2e-watch-test-label-changed 4020a251-a559-48e2-a717-ccae7494dcdf 23985 0 2020-01-19 17:20:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 19 17:20:10.706: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8940 /api/v1/namespaces/watch-8940/configmaps/e2e-watch-test-label-changed 4020a251-a559-48e2-a717-ccae7494dcdf 23986 0 2020-01-19 17:20:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 19 17:20:20.860: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8940 /api/v1/namespaces/watch-8940/configmaps/e2e-watch-test-label-changed 4020a251-a559-48e2-a717-ccae7494dcdf 24611 0 2020-01-19 17:20:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 19 17:20:20.860: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8940 /api/v1/namespaces/watch-8940/configmaps/e2e-watch-test-label-changed 4020a251-a559-48e2-a717-ccae7494dcdf 24616 0 2020-01-19 17:20:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jan 19 17:20:20.860: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8940 /api/v1/namespaces/watch-8940/configmaps/e2e-watch-test-label-changed 4020a251-a559-48e2-a717-ccae7494dcdf 24620 0 2020-01-19 17:20:10 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:20:20.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8940" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":280,"completed":150,"skipped":2426,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:20:20.926: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2423
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jan 19 17:20:21.177: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:20:24.475: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:20:37.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2423" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":280,"completed":151,"skipped":2442,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:20:37.994: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3726
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:20:38.228: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jan 19 17:20:41.452: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3726 create -f -'
Jan 19 17:20:42.150: INFO: stderr: ""
Jan 19 17:20:42.150: INFO: stdout: "e2e-test-crd-publish-openapi-9046-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 19 17:20:42.150: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3726 delete e2e-test-crd-publish-openapi-9046-crds test-foo'
Jan 19 17:20:42.285: INFO: stderr: ""
Jan 19 17:20:42.285: INFO: stdout: "e2e-test-crd-publish-openapi-9046-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 19 17:20:42.285: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3726 apply -f -'
Jan 19 17:20:42.550: INFO: stderr: ""
Jan 19 17:20:42.550: INFO: stdout: "e2e-test-crd-publish-openapi-9046-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 19 17:20:42.550: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3726 delete e2e-test-crd-publish-openapi-9046-crds test-foo'
Jan 19 17:20:42.680: INFO: stderr: ""
Jan 19 17:20:42.680: INFO: stdout: "e2e-test-crd-publish-openapi-9046-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jan 19 17:20:42.680: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3726 create -f -'
Jan 19 17:20:43.037: INFO: rc: 1
Jan 19 17:20:43.037: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3726 apply -f -'
Jan 19 17:20:43.228: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jan 19 17:20:43.228: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3726 create -f -'
Jan 19 17:20:43.419: INFO: rc: 1
Jan 19 17:20:43.419: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-3726 apply -f -'
Jan 19 17:20:43.599: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jan 19 17:20:43.599: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-9046-crds'
Jan 19 17:20:43.794: INFO: stderr: ""
Jan 19 17:20:43.794: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9046-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jan 19 17:20:43.795: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-9046-crds.metadata'
Jan 19 17:20:43.992: INFO: stderr: ""
Jan 19 17:20:43.992: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9046-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 19 17:20:43.993: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-9046-crds.spec'
Jan 19 17:20:44.188: INFO: stderr: ""
Jan 19 17:20:44.188: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9046-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 19 17:20:44.188: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-9046-crds.spec.bars'
Jan 19 17:20:44.371: INFO: stderr: ""
Jan 19 17:20:44.371: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9046-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jan 19 17:20:44.371: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-9046-crds.spec.bars2'
Jan 19 17:20:44.686: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:20:48.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3726" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":280,"completed":152,"skipped":2447,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:20:48.560: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2210
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:20:49.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051248, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051248, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051248, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051248, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:20:51.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051248, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051248, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051248, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051248, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:20:54.685: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:21:07.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2210" for this suite.
STEP: Destroying namespace "webhook-2210-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":280,"completed":153,"skipped":2470,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:21:07.522: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8743
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: set up a multi version CRD
Jan 19 17:21:07.737: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:21:26.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8743" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":280,"completed":154,"skipped":2544,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:21:26.363: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4958
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4958.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4958.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4958.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4958.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4958.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4958.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4958.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4958.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4958.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4958.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 19 17:21:34.776: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:34.818: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:34.840: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:34.862: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:35.008: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:35.027: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:35.046: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:35.065: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:35.211: INFO: Lookups using dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4958.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4958.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local jessie_udp@dns-test-service-2.dns-4958.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4958.svc.cluster.local]

Jan 19 17:21:40.231: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:40.250: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:40.268: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:40.289: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:40.435: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:40.453: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:40.472: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:40.491: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:40.593: INFO: Lookups using dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4958.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4958.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local jessie_udp@dns-test-service-2.dns-4958.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4958.svc.cluster.local]

Jan 19 17:21:45.231: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:45.249: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:45.267: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:45.285: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:45.580: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:45.598: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:45.617: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:45.636: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:45.718: INFO: Lookups using dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4958.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4958.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local jessie_udp@dns-test-service-2.dns-4958.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4958.svc.cluster.local]

Jan 19 17:21:50.231: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:50.273: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:50.292: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:50.310: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:50.454: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:50.473: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:50.491: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:50.509: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:50.615: INFO: Lookups using dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4958.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4958.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local jessie_udp@dns-test-service-2.dns-4958.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4958.svc.cluster.local]

Jan 19 17:21:55.231: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:55.250: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:55.268: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:55.286: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:55.432: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:55.450: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:55.468: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:55.487: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:21:55.589: INFO: Lookups using dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4958.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4958.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local jessie_udp@dns-test-service-2.dns-4958.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4958.svc.cluster.local]

Jan 19 17:22:00.232: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:22:00.251: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:22:00.269: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:22:00.288: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:22:00.517: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:22:00.536: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:22:00.555: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4958.svc.cluster.local from pod dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733: the server could not find the requested resource (get pods dns-test-ca650156-5a81-4b47-898b-08728934e733)
Jan 19 17:22:00.657: INFO: Lookups using dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4958.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4958.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4958.svc.cluster.local jessie_udp@dns-test-service-2.dns-4958.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4958.svc.cluster.local]

Jan 19 17:22:06.211: INFO: DNS probes using dns-4958/dns-test-ca650156-5a81-4b47-898b-08728934e733 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:22:06.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4958" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":280,"completed":155,"skipped":2574,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:22:06.301: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
Jan 19 17:22:06.523: INFO: PodSpec: initContainers in spec.initContainers
Jan 19 17:22:55.290: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-acd4f311-9558-424e-a0fe-2235e747a9b3", GenerateName:"", Namespace:"init-container-7145", SelfLink:"/api/v1/namespaces/init-container-7145/pods/pod-init-acd4f311-9558-424e-a0fe-2235e747a9b3", UID:"19c14ccd-5e8e-4d04-933d-77fdd8525d24", ResourceVersion:"25441", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63715051326, loc:(*time.Location)(0x7db5bc0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"523529358"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.64.1.177/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-9v2wz", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc003d3b1c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9v2wz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9v2wz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9v2wz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003b7d4f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00305e7e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003b7d570)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003b7d590)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003b7d598), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc003b7d59c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051326, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051326, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051326, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051326, loc:(*time.Location)(0x7db5bc0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.5", PodIP:"100.64.1.177", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.64.1.177"}}, StartTime:(*v1.Time)(0xc0045280e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002a4c0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002a4c150)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://7f58d84427e02949ef6327b291a55b8a5ac5af39527099b50056621af4b37ca6", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004528120), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004528100), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc003b7d61f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:22:55.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7145" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":280,"completed":156,"skipped":2622,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:22:55.341: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8281
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:22:55.588: INFO: Waiting up to 5m0s for pod "busybox-user-65534-f2b4e0ec-6735-4d22-a6db-5f414e3f60c0" in namespace "security-context-test-8281" to be "success or failure"
Jan 19 17:22:55.605: INFO: Pod "busybox-user-65534-f2b4e0ec-6735-4d22-a6db-5f414e3f60c0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.854314ms
Jan 19 17:22:57.622: INFO: Pod "busybox-user-65534-f2b4e0ec-6735-4d22-a6db-5f414e3f60c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033855913s
Jan 19 17:22:59.639: INFO: Pod "busybox-user-65534-f2b4e0ec-6735-4d22-a6db-5f414e3f60c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050959508s
Jan 19 17:22:59.639: INFO: Pod "busybox-user-65534-f2b4e0ec-6735-4d22-a6db-5f414e3f60c0" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:22:59.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8281" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":157,"skipped":2638,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:22:59.689: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6451
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:22:59.908: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:23:00.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6451" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":280,"completed":158,"skipped":2638,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:23:00.772: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2623
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:23:05.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2623" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":280,"completed":159,"skipped":2650,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:23:05.224: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4524
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-e2da8ee6-108b-4556-a540-d52ddcfd817f
STEP: Creating a pod to test consume secrets
Jan 19 17:23:05.488: INFO: Waiting up to 5m0s for pod "pod-secrets-3f41804c-c219-4bcd-b6fa-77da281f5a01" in namespace "secrets-4524" to be "success or failure"
Jan 19 17:23:05.504: INFO: Pod "pod-secrets-3f41804c-c219-4bcd-b6fa-77da281f5a01": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008328ms
Jan 19 17:23:07.522: INFO: Pod "pod-secrets-3f41804c-c219-4bcd-b6fa-77da281f5a01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03354659s
Jan 19 17:23:09.539: INFO: Pod "pod-secrets-3f41804c-c219-4bcd-b6fa-77da281f5a01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050985798s
STEP: Saw pod success
Jan 19 17:23:09.539: INFO: Pod "pod-secrets-3f41804c-c219-4bcd-b6fa-77da281f5a01" satisfied condition "success or failure"
Jan 19 17:23:09.555: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-secrets-3f41804c-c219-4bcd-b6fa-77da281f5a01 container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 17:23:09.610: INFO: Waiting for pod pod-secrets-3f41804c-c219-4bcd-b6fa-77da281f5a01 to disappear
Jan 19 17:23:09.628: INFO: Pod pod-secrets-3f41804c-c219-4bcd-b6fa-77da281f5a01 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:23:09.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4524" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":160,"skipped":2671,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:23:09.686: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8231
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 19 17:23:16.554: INFO: Successfully updated pod "annotationupdateff141fbc-e1fe-403d-aa0c-df63900484d5"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:23:18.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8231" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":280,"completed":161,"skipped":2675,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:23:18.651: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name projected-secret-test-dcdeb2cf-b955-4021-88f7-57f15906974f
STEP: Creating a pod to test consume secrets
Jan 19 17:23:18.903: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9a09b84b-1f74-4f3d-a1b1-86194918b21b" in namespace "projected-559" to be "success or failure"
Jan 19 17:23:18.920: INFO: Pod "pod-projected-secrets-9a09b84b-1f74-4f3d-a1b1-86194918b21b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.647089ms
Jan 19 17:23:20.938: INFO: Pod "pod-projected-secrets-9a09b84b-1f74-4f3d-a1b1-86194918b21b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034715398s
Jan 19 17:23:22.955: INFO: Pod "pod-projected-secrets-9a09b84b-1f74-4f3d-a1b1-86194918b21b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05180081s
STEP: Saw pod success
Jan 19 17:23:22.955: INFO: Pod "pod-projected-secrets-9a09b84b-1f74-4f3d-a1b1-86194918b21b" satisfied condition "success or failure"
Jan 19 17:23:22.972: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-projected-secrets-9a09b84b-1f74-4f3d-a1b1-86194918b21b container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 17:23:23.019: INFO: Waiting for pod pod-projected-secrets-9a09b84b-1f74-4f3d-a1b1-86194918b21b to disappear
Jan 19 17:23:23.035: INFO: Pod pod-projected-secrets-9a09b84b-1f74-4f3d-a1b1-86194918b21b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:23:23.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-559" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":280,"completed":162,"skipped":2678,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:23:23.092: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4143
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:23:24.034: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051403, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051403, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051403, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051403, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:23:26.052: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051403, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051403, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051403, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051403, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:23:29.081: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:23:29.097: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:23:30.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4143" for this suite.
STEP: Destroying namespace "webhook-4143-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":280,"completed":163,"skipped":2686,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:23:30.388: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7297
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting the proxy server
Jan 19 17:23:30.615: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:23:30.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7297" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":280,"completed":164,"skipped":2701,"failed":0}

------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:23:30.752: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4636
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jan 19 17:23:30.991: INFO: Created pod &Pod{ObjectMeta:{dns-4636  dns-4636 /api/v1/namespaces/dns-4636/pods/dns-4636 e355c34e-7f06-4015-bad2-1745beed68b5 25770 0 2020-01-19 17:23:30 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ctxsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ctxsq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ctxsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
STEP: Verifying customized DNS suffix list is configured on pod...
Jan 19 17:23:35.025: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4636 PodName:dns-4636 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:23:35.025: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Verifying customized DNS server is configured on pod...
Jan 19 17:23:35.420: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4636 PodName:dns-4636 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:23:35.420: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:23:35.856: INFO: Deleting pod dns-4636...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:23:35.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4636" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":280,"completed":165,"skipped":2701,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:23:35.932: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1981
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 19 17:23:36.176: INFO: Waiting up to 5m0s for pod "pod-c3c71f6e-861a-43f1-9909-802788e69f04" in namespace "emptydir-1981" to be "success or failure"
Jan 19 17:23:36.192: INFO: Pod "pod-c3c71f6e-861a-43f1-9909-802788e69f04": Phase="Pending", Reason="", readiness=false. Elapsed: 16.188248ms
Jan 19 17:23:38.209: INFO: Pod "pod-c3c71f6e-861a-43f1-9909-802788e69f04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033773564s
Jan 19 17:23:40.228: INFO: Pod "pod-c3c71f6e-861a-43f1-9909-802788e69f04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05248056s
STEP: Saw pod success
Jan 19 17:23:40.228: INFO: Pod "pod-c3c71f6e-861a-43f1-9909-802788e69f04" satisfied condition "success or failure"
Jan 19 17:23:40.245: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-c3c71f6e-861a-43f1-9909-802788e69f04 container test-container: <nil>
STEP: delete the pod
Jan 19 17:23:40.290: INFO: Waiting for pod pod-c3c71f6e-861a-43f1-9909-802788e69f04 to disappear
Jan 19 17:23:40.307: INFO: Pod pod-c3c71f6e-861a-43f1-9909-802788e69f04 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:23:40.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1981" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":166,"skipped":2706,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:23:40.356: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-936
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:23:51.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-936" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":280,"completed":167,"skipped":2720,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:23:51.771: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3495
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: validating cluster-info
Jan 19 17:23:51.994: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config cluster-info'
Jan 19 17:23:52.120: INFO: stderr: ""
Jan 19 17:23:52.120: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:23:52.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3495" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":280,"completed":168,"skipped":2747,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:23:52.157: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7488
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
STEP: reading a file in the container
Jan 19 17:23:56.981: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-7488 pod-service-account-52437c24-a5c1-426f-878d-5d94d6acb8ef -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan 19 17:23:57.536: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-7488 pod-service-account-52437c24-a5c1-426f-878d-5d94d6acb8ef -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan 19 17:23:58.127: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-7488 pod-service-account-52437c24-a5c1-426f-878d-5d94d6acb8ef -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:23:58.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7488" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":280,"completed":169,"skipped":2751,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:23:58.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9496
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating all guestbook components
Jan 19 17:23:58.975: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Jan 19 17:23:58.975: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-9496'
Jan 19 17:23:59.261: INFO: stderr: ""
Jan 19 17:23:59.261: INFO: stdout: "service/agnhost-slave created\n"
Jan 19 17:23:59.261: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Jan 19 17:23:59.261: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-9496'
Jan 19 17:23:59.493: INFO: stderr: ""
Jan 19 17:23:59.493: INFO: stdout: "service/agnhost-master created\n"
Jan 19 17:23:59.494: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 19 17:23:59.494: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-9496'
Jan 19 17:23:59.713: INFO: stderr: ""
Jan 19 17:23:59.713: INFO: stdout: "service/frontend created\n"
Jan 19 17:23:59.713: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 19 17:23:59.713: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-9496'
Jan 19 17:23:59.902: INFO: stderr: ""
Jan 19 17:23:59.902: INFO: stdout: "deployment.apps/frontend created\n"
Jan 19 17:23:59.902: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 19 17:23:59.902: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-9496'
Jan 19 17:24:00.115: INFO: stderr: ""
Jan 19 17:24:00.115: INFO: stdout: "deployment.apps/agnhost-master created\n"
Jan 19 17:24:00.115: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/kubernetes-e2e-test-images/agnhost:2.8
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 19 17:24:00.115: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-9496'
Jan 19 17:24:00.303: INFO: stderr: ""
Jan 19 17:24:00.304: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Jan 19 17:24:00.304: INFO: Waiting for all frontend pods to be Running.
Jan 19 17:24:05.354: INFO: Waiting for frontend to serve content.
Jan 19 17:24:05.458: INFO: Trying to add a new entry to the guestbook.
Jan 19 17:24:05.504: INFO: Verifying that added entry can be retrieved.
Jan 19 17:24:05.607: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Jan 19 17:24:10.738: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-9496'
Jan 19 17:24:10.890: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 17:24:10.890: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Jan 19 17:24:10.890: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-9496'
Jan 19 17:24:11.060: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 17:24:11.060: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 19 17:24:11.060: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-9496'
Jan 19 17:24:11.202: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 17:24:11.202: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 19 17:24:11.202: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-9496'
Jan 19 17:24:11.317: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 17:24:11.317: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 19 17:24:11.317: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-9496'
Jan 19 17:24:11.461: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 17:24:11.461: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 19 17:24:11.461: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-9496'
Jan 19 17:24:11.578: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 17:24:11.579: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:24:11.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9496" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":280,"completed":170,"skipped":2785,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:24:11.629: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-573
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Update Demo
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:330
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the initial replication controller
Jan 19 17:24:11.849: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-573'
Jan 19 17:24:12.043: INFO: stderr: ""
Jan 19 17:24:12.043: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 19 17:24:12.043: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-573'
Jan 19 17:24:12.163: INFO: stderr: ""
Jan 19 17:24:12.163: INFO: stdout: "update-demo-nautilus-lx7gc update-demo-nautilus-s8gft "
Jan 19 17:24:12.163: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-lx7gc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-573'
Jan 19 17:24:12.279: INFO: stderr: ""
Jan 19 17:24:12.279: INFO: stdout: ""
Jan 19 17:24:12.279: INFO: update-demo-nautilus-lx7gc is created but not running
Jan 19 17:24:17.279: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-573'
Jan 19 17:24:17.394: INFO: stderr: ""
Jan 19 17:24:17.394: INFO: stdout: "update-demo-nautilus-lx7gc update-demo-nautilus-s8gft "
Jan 19 17:24:17.394: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-lx7gc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-573'
Jan 19 17:24:17.503: INFO: stderr: ""
Jan 19 17:24:17.504: INFO: stdout: "true"
Jan 19 17:24:17.504: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-lx7gc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-573'
Jan 19 17:24:17.631: INFO: stderr: ""
Jan 19 17:24:17.631: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 19 17:24:17.631: INFO: validating pod update-demo-nautilus-lx7gc
Jan 19 17:24:17.733: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 17:24:17.733: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 17:24:17.733: INFO: update-demo-nautilus-lx7gc is verified up and running
Jan 19 17:24:17.733: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-s8gft -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-573'
Jan 19 17:24:17.858: INFO: stderr: ""
Jan 19 17:24:17.858: INFO: stdout: "true"
Jan 19 17:24:17.858: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-s8gft -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-573'
Jan 19 17:24:17.983: INFO: stderr: ""
Jan 19 17:24:17.983: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 19 17:24:17.983: INFO: validating pod update-demo-nautilus-s8gft
Jan 19 17:24:18.084: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 17:24:18.084: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 17:24:18.084: INFO: update-demo-nautilus-s8gft is verified up and running
STEP: rolling-update to new replication controller
Jan 19 17:24:18.087: INFO: scanned /root for discovery docs: <nil>
Jan 19 17:24:18.087: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-573'
Jan 19 17:24:42.283: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 19 17:24:42.283: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 19 17:24:42.283: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-573'
Jan 19 17:24:42.443: INFO: stderr: ""
Jan 19 17:24:42.443: INFO: stdout: "update-demo-kitten-9vtt5 update-demo-kitten-km9jx update-demo-nautilus-lx7gc "
STEP: Replicas for name=update-demo: expected=2 actual=3
Jan 19 17:24:47.444: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-573'
Jan 19 17:24:47.567: INFO: stderr: ""
Jan 19 17:24:47.568: INFO: stdout: "update-demo-kitten-9vtt5 update-demo-kitten-km9jx "
Jan 19 17:24:47.568: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-9vtt5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-573'
Jan 19 17:24:47.691: INFO: stderr: ""
Jan 19 17:24:47.691: INFO: stdout: "true"
Jan 19 17:24:47.691: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-9vtt5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-573'
Jan 19 17:24:47.829: INFO: stderr: ""
Jan 19 17:24:47.829: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 19 17:24:47.829: INFO: validating pod update-demo-kitten-9vtt5
Jan 19 17:24:47.933: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 19 17:24:47.933: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 19 17:24:47.933: INFO: update-demo-kitten-9vtt5 is verified up and running
Jan 19 17:24:47.933: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-km9jx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-573'
Jan 19 17:24:48.057: INFO: stderr: ""
Jan 19 17:24:48.057: INFO: stdout: "true"
Jan 19 17:24:48.057: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-km9jx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-573'
Jan 19 17:24:48.174: INFO: stderr: ""
Jan 19 17:24:48.174: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 19 17:24:48.174: INFO: validating pod update-demo-kitten-km9jx
Jan 19 17:24:48.274: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 19 17:24:48.274: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 19 17:24:48.274: INFO: update-demo-kitten-km9jx is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:24:48.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-573" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]","total":280,"completed":171,"skipped":2806,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:24:48.325: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4093
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:24:48.546: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:24:50.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4093" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":280,"completed":172,"skipped":2807,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:24:50.356: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7956
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:24:51.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051490, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051490, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051490, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051490, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:24:53.332: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051490, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051490, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051490, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715051490, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:24:56.364: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jan 19 17:25:00.574: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config attach --namespace=webhook-7956 to-be-attached-pod -i -c=container1'
Jan 19 17:25:00.773: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:25:00.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7956" for this suite.
STEP: Destroying namespace "webhook-7956-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":280,"completed":173,"skipped":2836,"failed":0}
SSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:25:00.969: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-6900
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:172
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating server pod server in namespace prestop-6900
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6900
STEP: Deleting pre-stop pod
Jan 19 17:25:14.448: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:25:14.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6900" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":280,"completed":174,"skipped":2840,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:25:14.515: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:25:18.830: INFO: Waiting up to 5m0s for pod "client-envvars-4942ddb2-c131-4c77-be01-b2dc6c12aab7" in namespace "pods-8263" to be "success or failure"
Jan 19 17:25:18.844: INFO: Pod "client-envvars-4942ddb2-c131-4c77-be01-b2dc6c12aab7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.341547ms
Jan 19 17:25:20.858: INFO: Pod "client-envvars-4942ddb2-c131-4c77-be01-b2dc6c12aab7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027611823s
Jan 19 17:25:22.873: INFO: Pod "client-envvars-4942ddb2-c131-4c77-be01-b2dc6c12aab7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042596646s
STEP: Saw pod success
Jan 19 17:25:22.873: INFO: Pod "client-envvars-4942ddb2-c131-4c77-be01-b2dc6c12aab7" satisfied condition "success or failure"
Jan 19 17:25:22.884: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod client-envvars-4942ddb2-c131-4c77-be01-b2dc6c12aab7 container env3cont: <nil>
STEP: delete the pod
Jan 19 17:25:22.975: INFO: Waiting for pod client-envvars-4942ddb2-c131-4c77-be01-b2dc6c12aab7 to disappear
Jan 19 17:25:22.986: INFO: Pod client-envvars-4942ddb2-c131-4c77-be01-b2dc6c12aab7 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:25:22.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8263" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":280,"completed":175,"skipped":2859,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:25:23.018: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6521
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-cbd1cf4f-f1fb-4181-8fbf-b9ca327b1ceb in namespace container-probe-6521
Jan 19 17:25:27.259: INFO: Started pod liveness-cbd1cf4f-f1fb-4181-8fbf-b9ca327b1ceb in namespace container-probe-6521
STEP: checking the pod's current state and verifying that restartCount is present
Jan 19 17:25:27.276: INFO: Initial restart count of pod liveness-cbd1cf4f-f1fb-4181-8fbf-b9ca327b1ceb is 0
Jan 19 17:25:39.397: INFO: Restart count of pod container-probe-6521/liveness-cbd1cf4f-f1fb-4181-8fbf-b9ca327b1ceb is now 1 (12.120531944s elapsed)
Jan 19 17:25:59.581: INFO: Restart count of pod container-probe-6521/liveness-cbd1cf4f-f1fb-4181-8fbf-b9ca327b1ceb is now 2 (32.304962585s elapsed)
Jan 19 17:26:19.756: INFO: Restart count of pod container-probe-6521/liveness-cbd1cf4f-f1fb-4181-8fbf-b9ca327b1ceb is now 3 (52.479845435s elapsed)
Jan 19 17:26:39.931: INFO: Restart count of pod container-probe-6521/liveness-cbd1cf4f-f1fb-4181-8fbf-b9ca327b1ceb is now 4 (1m12.655279738s elapsed)
Jan 19 17:27:40.453: INFO: Restart count of pod container-probe-6521/liveness-cbd1cf4f-f1fb-4181-8fbf-b9ca327b1ceb is now 5 (2m13.176825873s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:27:40.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6521" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":280,"completed":176,"skipped":2866,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:27:40.532: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8748
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-8748
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 19 17:27:40.759: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 19 17:28:03.051: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.1.200 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8748 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:28:03.051: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:28:04.467: INFO: Found all expected endpoints: [netserver-0]
Jan 19 17:28:04.484: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.0.55 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8748 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:28:04.484: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:28:05.926: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:28:05.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8748" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":177,"skipped":2875,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:28:05.977: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7888
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service endpoint-test2 in namespace services-7888
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7888 to expose endpoints map[]
Jan 19 17:28:06.251: INFO: successfully validated that service endpoint-test2 in namespace services-7888 exposes endpoints map[] (21.365674ms elapsed)
STEP: Creating pod pod1 in namespace services-7888
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7888 to expose endpoints map[pod1:[80]]
Jan 19 17:28:09.410: INFO: successfully validated that service endpoint-test2 in namespace services-7888 exposes endpoints map[pod1:[80]] (3.139054127s elapsed)
STEP: Creating pod pod2 in namespace services-7888
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7888 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 19 17:28:13.676: INFO: Unexpected endpoints: found map[88b3da8c-a971-4513-8a4a-74617cc4433b:[80]], expected map[pod1:[80] pod2:[80]] (4.248756013s elapsed, will retry)
Jan 19 17:28:15.774: INFO: successfully validated that service endpoint-test2 in namespace services-7888 exposes endpoints map[pod1:[80] pod2:[80]] (6.346799453s elapsed)
STEP: Deleting pod pod1 in namespace services-7888
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7888 to expose endpoints map[pod2:[80]]
Jan 19 17:28:15.835: INFO: successfully validated that service endpoint-test2 in namespace services-7888 exposes endpoints map[pod2:[80]] (42.638193ms elapsed)
STEP: Deleting pod pod2 in namespace services-7888
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7888 to expose endpoints map[]
Jan 19 17:28:15.869: INFO: successfully validated that service endpoint-test2 in namespace services-7888 exposes endpoints map[] (16.403869ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:28:15.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7888" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":280,"completed":178,"skipped":2913,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:28:15.961: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-4373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:163
Jan 19 17:28:16.191: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 17:29:16.334: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:29:16.350: INFO: Starting informer...
STEP: Starting pod...
Jan 19 17:29:16.387: INFO: Pod is running on shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jan 19 17:29:16.449: INFO: Pod wasn't evicted. Proceeding
Jan 19 17:29:16.449: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jan 19 17:30:31.517: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:30:31.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4373" for this suite.
•{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":280,"completed":179,"skipped":2919,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:30:31.568: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7083
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jan 19 17:30:31.790: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:30:35.129: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:30:47.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7083" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":280,"completed":180,"skipped":2929,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:30:47.557: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3587
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 19 17:30:50.884: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:30:50.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3587" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":280,"completed":181,"skipped":2935,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:30:50.983: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-787
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating replication controller my-hostname-basic-514e2d90-c7e5-4773-8c2a-184e8366aff3
Jan 19 17:30:51.249: INFO: Pod name my-hostname-basic-514e2d90-c7e5-4773-8c2a-184e8366aff3: Found 1 pods out of 1
Jan 19 17:30:51.249: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-514e2d90-c7e5-4773-8c2a-184e8366aff3" are running
Jan 19 17:30:55.283: INFO: Pod "my-hostname-basic-514e2d90-c7e5-4773-8c2a-184e8366aff3-6cfxx" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-19 17:30:50 +0000 UTC Reason: Message:}])
Jan 19 17:30:55.283: INFO: Trying to dial the pod
Jan 19 17:31:00.422: INFO: Controller my-hostname-basic-514e2d90-c7e5-4773-8c2a-184e8366aff3: Got expected result from replica 1 [my-hostname-basic-514e2d90-c7e5-4773-8c2a-184e8366aff3-6cfxx]: "my-hostname-basic-514e2d90-c7e5-4773-8c2a-184e8366aff3-6cfxx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:31:00.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-787" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":280,"completed":182,"skipped":2936,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:31:00.472: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6699
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 19 17:31:04.813: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:31:04.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6699" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":183,"skipped":2958,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:31:04.902: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4341
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 19 17:31:05.142: INFO: Waiting up to 5m0s for pod "pod-fc2400d9-f0a1-42ca-b9c5-356f956d974f" in namespace "emptydir-4341" to be "success or failure"
Jan 19 17:31:05.158: INFO: Pod "pod-fc2400d9-f0a1-42ca-b9c5-356f956d974f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.236862ms
Jan 19 17:31:07.175: INFO: Pod "pod-fc2400d9-f0a1-42ca-b9c5-356f956d974f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033772851s
Jan 19 17:31:09.195: INFO: Pod "pod-fc2400d9-f0a1-42ca-b9c5-356f956d974f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053249499s
Jan 19 17:31:11.213: INFO: Pod "pod-fc2400d9-f0a1-42ca-b9c5-356f956d974f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071144804s
STEP: Saw pod success
Jan 19 17:31:11.213: INFO: Pod "pod-fc2400d9-f0a1-42ca-b9c5-356f956d974f" satisfied condition "success or failure"
Jan 19 17:31:11.230: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-fc2400d9-f0a1-42ca-b9c5-356f956d974f container test-container: <nil>
STEP: delete the pod
Jan 19 17:31:11.372: INFO: Waiting for pod pod-fc2400d9-f0a1-42ca-b9c5-356f956d974f to disappear
Jan 19 17:31:11.389: INFO: Pod pod-fc2400d9-f0a1-42ca-b9c5-356f956d974f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:31:11.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4341" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":184,"skipped":2967,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:31:11.439: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3897
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's command
Jan 19 17:31:11.679: INFO: Waiting up to 5m0s for pod "var-expansion-1842283d-c666-40f8-bbce-199f51af8ba8" in namespace "var-expansion-3897" to be "success or failure"
Jan 19 17:31:11.695: INFO: Pod "var-expansion-1842283d-c666-40f8-bbce-199f51af8ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.376016ms
Jan 19 17:31:13.713: INFO: Pod "var-expansion-1842283d-c666-40f8-bbce-199f51af8ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033878583s
Jan 19 17:31:15.730: INFO: Pod "var-expansion-1842283d-c666-40f8-bbce-199f51af8ba8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051548259s
STEP: Saw pod success
Jan 19 17:31:15.731: INFO: Pod "var-expansion-1842283d-c666-40f8-bbce-199f51af8ba8" satisfied condition "success or failure"
Jan 19 17:31:15.748: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod var-expansion-1842283d-c666-40f8-bbce-199f51af8ba8 container dapi-container: <nil>
STEP: delete the pod
Jan 19 17:31:15.796: INFO: Waiting for pod var-expansion-1842283d-c666-40f8-bbce-199f51af8ba8 to disappear
Jan 19 17:31:15.813: INFO: Pod var-expansion-1842283d-c666-40f8-bbce-199f51af8ba8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:31:15.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3897" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":280,"completed":185,"skipped":2980,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:31:15.862: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8423
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod test-webserver-d34ff932-e778-433a-bcda-5c73c5b5f6cd in namespace container-probe-8423
Jan 19 17:31:20.137: INFO: Started pod test-webserver-d34ff932-e778-433a-bcda-5c73c5b5f6cd in namespace container-probe-8423
STEP: checking the pod's current state and verifying that restartCount is present
Jan 19 17:31:20.154: INFO: Initial restart count of pod test-webserver-d34ff932-e778-433a-bcda-5c73c5b5f6cd is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:35:20.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8423" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":186,"skipped":2994,"failed":0}
SSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:35:20.312: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1392
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan 19 17:35:20.586: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jan 19 17:35:29.764: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:35:29.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1392" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":280,"completed":187,"skipped":3000,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:35:29.816: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-556
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-556
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 19 17:35:30.034: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 19 17:35:50.347: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.212:8080/dial?request=hostname&protocol=http&host=100.64.1.211&port=8080&tries=1'] Namespace:pod-network-test-556 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:35:50.347: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:35:50.832: INFO: Waiting for responses: map[]
Jan 19 17:35:50.848: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.212:8080/dial?request=hostname&protocol=http&host=100.64.0.59&port=8080&tries=1'] Namespace:pod-network-test-556 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:35:50.848: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:35:51.306: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:35:51.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-556" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":188,"skipped":3012,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:35:51.355: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7673
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:35:52.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052151, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052151, loc:(*time.Location)(0x7db5bc0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5f65f8c764\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052151, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052151, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:35:54.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052151, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052151, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052151, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052151, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:35:57.532: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:35:58.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7673" for this suite.
STEP: Destroying namespace "webhook-7673-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":280,"completed":189,"skipped":3031,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:35:58.213: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-372
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating service nodeport-test with type=NodePort in namespace services-372
STEP: creating replication controller nodeport-test in namespace services-372
I0119 17:35:58.495496    7438 runners.go:189] Created replication controller with name: nodeport-test, namespace: services-372, replica count: 2
Jan 19 17:36:01.546: INFO: Creating new exec pod
I0119 17:36:01.546052    7438 runners.go:189] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 17:36:06.630: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-372 execpod89zxq -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jan 19 17:36:07.647: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 17:36:07.647: INFO: stdout: ""
Jan 19 17:36:07.647: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-372 execpod89zxq -- /bin/sh -x -c nc -zv -t -w 2 100.108.119.153 80'
Jan 19 17:36:08.264: INFO: stderr: "+ nc -zv -t -w 2 100.108.119.153 80\nConnection to 100.108.119.153 80 port [tcp/http] succeeded!\n"
Jan 19 17:36:08.264: INFO: stdout: ""
Jan 19 17:36:08.264: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-372 execpod89zxq -- /bin/sh -x -c nc -zv -t -w 2 10.250.0.5 30242'
Jan 19 17:36:08.833: INFO: stderr: "+ nc -zv -t -w 2 10.250.0.5 30242\nConnection to 10.250.0.5 30242 port [tcp/30242] succeeded!\n"
Jan 19 17:36:08.833: INFO: stdout: ""
Jan 19 17:36:08.833: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-372 execpod89zxq -- /bin/sh -x -c nc -zv -t -w 2 10.250.0.4 30242'
Jan 19 17:36:09.408: INFO: stderr: "+ nc -zv -t -w 2 10.250.0.4 30242\nConnection to 10.250.0.4 30242 port [tcp/30242] succeeded!\n"
Jan 19 17:36:09.408: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:36:09.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-372" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":280,"completed":190,"skipped":3043,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:36:09.459: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3667
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:36:10.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052169, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052169, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052169, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052169, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:36:12.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052169, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052169, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052169, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052169, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:36:14.441: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052169, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052169, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052169, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052169, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:36:17.474: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:36:17.492: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4948-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:36:18.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3667" for this suite.
STEP: Destroying namespace "webhook-3667-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":280,"completed":191,"skipped":3067,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:36:18.681: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-700
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:36:18.900: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:36:19.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-700" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":280,"completed":192,"skipped":3084,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:36:19.076: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9743
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-configmap-s7rh
STEP: Creating a pod to test atomic-volume-subpath
Jan 19 17:36:19.355: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-s7rh" in namespace "subpath-9743" to be "success or failure"
Jan 19 17:36:19.372: INFO: Pod "pod-subpath-test-configmap-s7rh": Phase="Pending", Reason="", readiness=false. Elapsed: 16.265488ms
Jan 19 17:36:21.390: INFO: Pod "pod-subpath-test-configmap-s7rh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034203542s
Jan 19 17:36:23.407: INFO: Pod "pod-subpath-test-configmap-s7rh": Phase="Running", Reason="", readiness=true. Elapsed: 4.05184292s
Jan 19 17:36:25.424: INFO: Pod "pod-subpath-test-configmap-s7rh": Phase="Running", Reason="", readiness=true. Elapsed: 6.068905406s
Jan 19 17:36:27.442: INFO: Pod "pod-subpath-test-configmap-s7rh": Phase="Running", Reason="", readiness=true. Elapsed: 8.08623477s
Jan 19 17:36:29.459: INFO: Pod "pod-subpath-test-configmap-s7rh": Phase="Running", Reason="", readiness=true. Elapsed: 10.103584263s
Jan 19 17:36:31.476: INFO: Pod "pod-subpath-test-configmap-s7rh": Phase="Running", Reason="", readiness=true. Elapsed: 12.120915452s
Jan 19 17:36:33.494: INFO: Pod "pod-subpath-test-configmap-s7rh": Phase="Running", Reason="", readiness=true. Elapsed: 14.13829394s
Jan 19 17:36:35.511: INFO: Pod "pod-subpath-test-configmap-s7rh": Phase="Running", Reason="", readiness=true. Elapsed: 16.155822324s
Jan 19 17:36:37.529: INFO: Pod "pod-subpath-test-configmap-s7rh": Phase="Running", Reason="", readiness=true. Elapsed: 18.173406305s
Jan 19 17:36:39.547: INFO: Pod "pod-subpath-test-configmap-s7rh": Phase="Running", Reason="", readiness=true. Elapsed: 20.191407457s
Jan 19 17:36:41.564: INFO: Pod "pod-subpath-test-configmap-s7rh": Phase="Running", Reason="", readiness=true. Elapsed: 22.208911235s
Jan 19 17:36:43.583: INFO: Pod "pod-subpath-test-configmap-s7rh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.227740685s
STEP: Saw pod success
Jan 19 17:36:43.583: INFO: Pod "pod-subpath-test-configmap-s7rh" satisfied condition "success or failure"
Jan 19 17:36:43.599: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-subpath-test-configmap-s7rh container test-container-subpath-configmap-s7rh: <nil>
STEP: delete the pod
Jan 19 17:36:43.649: INFO: Waiting for pod pod-subpath-test-configmap-s7rh to disappear
Jan 19 17:36:43.666: INFO: Pod pod-subpath-test-configmap-s7rh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-s7rh
Jan 19 17:36:43.666: INFO: Deleting pod "pod-subpath-test-configmap-s7rh" in namespace "subpath-9743"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:36:43.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9743" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":280,"completed":193,"skipped":3086,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:36:43.736: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-128
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:37:06.040: INFO: Container started at 2020-01-19 17:36:46 +0000 UTC, pod became ready at 2020-01-19 17:37:04 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:37:06.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-128" for this suite.
•{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":280,"completed":194,"skipped":3098,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:37:06.090: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3886
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:37:06.337: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-fc2ab867-9c6c-420a-a010-5da8285beee4" in namespace "security-context-test-3886" to be "success or failure"
Jan 19 17:37:06.353: INFO: Pod "busybox-readonly-false-fc2ab867-9c6c-420a-a010-5da8285beee4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.117445ms
Jan 19 17:37:08.370: INFO: Pod "busybox-readonly-false-fc2ab867-9c6c-420a-a010-5da8285beee4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033346517s
Jan 19 17:37:10.388: INFO: Pod "busybox-readonly-false-fc2ab867-9c6c-420a-a010-5da8285beee4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051290694s
Jan 19 17:37:10.388: INFO: Pod "busybox-readonly-false-fc2ab867-9c6c-420a-a010-5da8285beee4" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:37:10.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3886" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":280,"completed":195,"skipped":3106,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:37:10.440: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3257
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:37:11.497: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052230, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052230, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052230, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052230, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:37:13.514: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052230, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052230, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052230, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052230, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:37:16.581: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:37:16.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3257" for this suite.
STEP: Destroying namespace "webhook-3257-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":280,"completed":196,"skipped":3129,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:37:16.931: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9418
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating the pod
Jan 19 17:37:21.801: INFO: Successfully updated pod "labelsupdateca44c5a4-1ad3-4131-95b7-6089e54256b1"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:37:23.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9418" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":280,"completed":197,"skipped":3145,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:37:23.978: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jan 19 17:38:04.332: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0119 17:38:04.332534    7438 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:38:04.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5846" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":280,"completed":198,"skipped":3145,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:38:04.368: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3590
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3590.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3590.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 19 17:38:09.204: INFO: DNS probes using dns-3590/dns-test-0d8531ac-c8fe-4917-b8cc-ab71faf10080 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:38:09.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3590" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":280,"completed":199,"skipped":3145,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:38:09.307: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1065
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:38:09.566: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 19 17:38:15.600: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 19 17:38:19.735: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1065 /apis/apps/v1/namespaces/deployment-1065/deployments/test-cleanup-deployment 8e84ab9a-10aa-4a38-8624-abcad53818e9 30448 1 2020-01-19 17:38:15 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0034b8678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-01-19 17:38:14 +0000 UTC,LastTransitionTime:2020-01-19 17:38:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-55ffc6b7b6" has successfully progressed.,LastUpdateTime:2020-01-19 17:38:17 +0000 UTC,LastTransitionTime:2020-01-19 17:38:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 19 17:38:19.753: INFO: New ReplicaSet "test-cleanup-deployment-55ffc6b7b6" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6  deployment-1065 /apis/apps/v1/namespaces/deployment-1065/replicasets/test-cleanup-deployment-55ffc6b7b6 cbe56a98-327b-442c-8dfd-a1db023ff48c 30441 1 2020-01-19 17:38:15 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 8e84ab9a-10aa-4a38-8624-abcad53818e9 0xc0034b8a47 0xc0034b8a48}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55ffc6b7b6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[] [] []  []} {[] [] [{agnhost gcr.io/kubernetes-e2e-test-images/agnhost:2.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0034b8ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 17:38:19.770: INFO: Pod "test-cleanup-deployment-55ffc6b7b6-2cdrc" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-55ffc6b7b6-2cdrc test-cleanup-deployment-55ffc6b7b6- deployment-1065 /api/v1/namespaces/deployment-1065/pods/test-cleanup-deployment-55ffc6b7b6-2cdrc 3cb69621-2d65-4ed0-b765-26a496dffc13 30440 0 2020-01-19 17:38:15 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:55ffc6b7b6] map[cni.projectcalico.org/podIP:100.64.1.230/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-55ffc6b7b6 cbe56a98-327b-442c-8dfd-a1db023ff48c 0xc0034b9a97 0xc0034b9a98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-699ls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-699ls,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-699ls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:38:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:38:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:38:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:38:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:100.64.1.230,StartTime:2020-01-19 17:38:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-19 17:38:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://f8380b40f31ac94f7074b740d51a7151127514911889a732eca98b0bc8f4af99,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:38:19.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1065" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":280,"completed":200,"skipped":3151,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:38:19.828: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9523
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 19 17:38:20.049: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 17:38:20.100: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 17:38:20.117: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc before test
Jan 19 17:38:20.140: INFO: node-problem-detector-pw6fz from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.140: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 17:38:20.140: INFO: kube-proxy-6m6gf from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.140: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 17:38:20.140: INFO: calico-node-75w55 from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.140: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 17:38:20.140: INFO: node-exporter-md4n7 from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.140: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 17:38:20.140: INFO: test-cleanup-deployment-55ffc6b7b6-2cdrc from deployment-1065 started at 2020-01-19 17:38:15 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.140: INFO: 	Container agnhost ready: true, restart count 0
Jan 19 17:38:20.140: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz before test
Jan 19 17:38:20.213: INFO: vpn-shoot-5d566c4d8b-klghn from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container vpn-shoot ready: true, restart count 0
Jan 19 17:38:20.214: INFO: calico-typha-horizontal-autoscaler-85c99966bb-czfm4 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container autoscaler ready: true, restart count 0
Jan 19 17:38:20.214: INFO: dashboard-metrics-scraper-894778996-t484m from kubernetes-dashboard started at 2020-01-19 17:29:16 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 19 17:38:20.214: INFO: calico-kube-controllers-79bcd784b6-t6nlg from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 19 17:38:20.214: INFO: addons-nginx-ingress-controller-7c75bb76db-82dr5 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jan 19 17:38:20.214: INFO: coredns-59c969ffb8-dckvt from kube-system started at 2020-01-19 16:05:44 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container coredns ready: true, restart count 0
Jan 19 17:38:20.214: INFO: blackbox-exporter-54bb5f55cc-d7k59 from kube-system started at 2020-01-19 16:04:56 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan 19 17:38:20.214: INFO: node-exporter-9w5fc from kube-system started at 2020-01-19 16:05:02 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 17:38:20.214: INFO: calico-typha-vertical-autoscaler-5769b74b58-k2rg7 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container autoscaler ready: true, restart count 4
Jan 19 17:38:20.214: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-95f65778d-66xm5 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jan 19 17:38:20.214: INFO: coredns-59c969ffb8-z8twk from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container coredns ready: true, restart count 0
Jan 19 17:38:20.214: INFO: metrics-server-b8bffff9b-2lk2p from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container metrics-server ready: true, restart count 0
Jan 19 17:38:20.214: INFO: kubernetes-dashboard-5b855874f6-b7hp4 from kubernetes-dashboard started at 2020-01-19 17:29:16 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 19 17:38:20.214: INFO: calico-typha-deploy-9f6b455c4-7hxxh from kube-system started at 2020-01-19 17:29:16 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container calico-typha ready: true, restart count 0
Jan 19 17:38:20.214: INFO: kube-proxy-cpmsz from kube-system started at 2020-01-19 16:05:02 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 17:38:20.214: INFO: calico-node-6gx2z from kube-system started at 2020-01-19 16:04:56 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 17:38:20.214: INFO: node-problem-detector-q4slm from kube-system started at 2020-01-19 16:05:03 +0000 UTC (1 container statuses recorded)
Jan 19 17:38:20.214: INFO: 	Container node-problem-detector ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-dc5ec23c-c6e9-422f-843f-94e338121337 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-dc5ec23c-c6e9-422f-843f-94e338121337 off the node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc
STEP: verifying the node doesn't have the label kubernetes.io/e2e-dc5ec23c-c6e9-422f-843f-94e338121337
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:38:38.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9523" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":280,"completed":201,"skipped":3154,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:38:38.588: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5605
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-8fa20fa7-82a1-4fd1-abdc-0965008df078
STEP: Creating a pod to test consume secrets
Jan 19 17:38:38.837: INFO: Waiting up to 5m0s for pod "pod-secrets-69887928-1b3c-4edb-a274-b56970b07c96" in namespace "secrets-5605" to be "success or failure"
Jan 19 17:38:38.853: INFO: Pod "pod-secrets-69887928-1b3c-4edb-a274-b56970b07c96": Phase="Pending", Reason="", readiness=false. Elapsed: 16.223483ms
Jan 19 17:38:40.870: INFO: Pod "pod-secrets-69887928-1b3c-4edb-a274-b56970b07c96": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033432706s
Jan 19 17:38:42.888: INFO: Pod "pod-secrets-69887928-1b3c-4edb-a274-b56970b07c96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051010403s
STEP: Saw pod success
Jan 19 17:38:42.888: INFO: Pod "pod-secrets-69887928-1b3c-4edb-a274-b56970b07c96" satisfied condition "success or failure"
Jan 19 17:38:42.905: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-secrets-69887928-1b3c-4edb-a274-b56970b07c96 container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 17:38:42.950: INFO: Waiting for pod pod-secrets-69887928-1b3c-4edb-a274-b56970b07c96 to disappear
Jan 19 17:38:42.966: INFO: Pod pod-secrets-69887928-1b3c-4edb-a274-b56970b07c96 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:38:42.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5605" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":202,"skipped":3161,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:38:43.026: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8679
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-1536fe78-54b7-443f-9791-f2f3c12848be
STEP: Creating a pod to test consume configMaps
Jan 19 17:38:43.287: INFO: Waiting up to 5m0s for pod "pod-configmaps-4f8e809c-61ed-4559-86a2-31cb428843c8" in namespace "configmap-8679" to be "success or failure"
Jan 19 17:38:43.303: INFO: Pod "pod-configmaps-4f8e809c-61ed-4559-86a2-31cb428843c8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.974094ms
Jan 19 17:38:45.321: INFO: Pod "pod-configmaps-4f8e809c-61ed-4559-86a2-31cb428843c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033558384s
Jan 19 17:38:47.338: INFO: Pod "pod-configmaps-4f8e809c-61ed-4559-86a2-31cb428843c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051206296s
STEP: Saw pod success
Jan 19 17:38:47.338: INFO: Pod "pod-configmaps-4f8e809c-61ed-4559-86a2-31cb428843c8" satisfied condition "success or failure"
Jan 19 17:38:47.355: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-configmaps-4f8e809c-61ed-4559-86a2-31cb428843c8 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 19 17:38:47.408: INFO: Waiting for pod pod-configmaps-4f8e809c-61ed-4559-86a2-31cb428843c8 to disappear
Jan 19 17:38:47.424: INFO: Pod pod-configmaps-4f8e809c-61ed-4559-86a2-31cb428843c8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:38:47.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8679" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":203,"skipped":3182,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:38:47.474: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2755
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jan 19 17:38:47.701: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jan 19 17:39:01.278: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:39:04.488: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:39:17.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2755" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":280,"completed":204,"skipped":3191,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:39:17.994: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9091
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 19 17:39:18.278: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9091 /api/v1/namespaces/watch-9091/configmaps/e2e-watch-test-configmap-a e68e9597-dfd9-4729-b53c-75ed90331f60 30811 0 2020-01-19 17:39:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 19 17:39:18.278: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9091 /api/v1/namespaces/watch-9091/configmaps/e2e-watch-test-configmap-a e68e9597-dfd9-4729-b53c-75ed90331f60 30811 0 2020-01-19 17:39:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 19 17:39:28.312: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9091 /api/v1/namespaces/watch-9091/configmaps/e2e-watch-test-configmap-a e68e9597-dfd9-4729-b53c-75ed90331f60 30854 0 2020-01-19 17:39:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 19 17:39:28.312: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9091 /api/v1/namespaces/watch-9091/configmaps/e2e-watch-test-configmap-a e68e9597-dfd9-4729-b53c-75ed90331f60 30854 0 2020-01-19 17:39:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 19 17:39:38.347: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9091 /api/v1/namespaces/watch-9091/configmaps/e2e-watch-test-configmap-a e68e9597-dfd9-4729-b53c-75ed90331f60 30894 0 2020-01-19 17:39:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 19 17:39:38.347: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9091 /api/v1/namespaces/watch-9091/configmaps/e2e-watch-test-configmap-a e68e9597-dfd9-4729-b53c-75ed90331f60 30894 0 2020-01-19 17:39:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 19 17:39:48.366: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9091 /api/v1/namespaces/watch-9091/configmaps/e2e-watch-test-configmap-a e68e9597-dfd9-4729-b53c-75ed90331f60 30931 0 2020-01-19 17:39:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 19 17:39:48.366: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9091 /api/v1/namespaces/watch-9091/configmaps/e2e-watch-test-configmap-a e68e9597-dfd9-4729-b53c-75ed90331f60 30931 0 2020-01-19 17:39:18 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 19 17:39:58.386: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9091 /api/v1/namespaces/watch-9091/configmaps/e2e-watch-test-configmap-b c8d177f5-5c16-4ae3-ab1f-9c45f6271925 30970 0 2020-01-19 17:39:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 19 17:39:58.386: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9091 /api/v1/namespaces/watch-9091/configmaps/e2e-watch-test-configmap-b c8d177f5-5c16-4ae3-ab1f-9c45f6271925 30970 0 2020-01-19 17:39:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 19 17:40:08.407: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9091 /api/v1/namespaces/watch-9091/configmaps/e2e-watch-test-configmap-b c8d177f5-5c16-4ae3-ab1f-9c45f6271925 31008 0 2020-01-19 17:39:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 19 17:40:08.407: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9091 /api/v1/namespaces/watch-9091/configmaps/e2e-watch-test-configmap-b c8d177f5-5c16-4ae3-ab1f-9c45f6271925 31008 0 2020-01-19 17:39:58 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:40:18.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9091" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":280,"completed":205,"skipped":3220,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:40:18.457: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 17:40:18.698: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78c570b9-9c7a-4d69-8b15-028c73534969" in namespace "downward-api-2973" to be "success or failure"
Jan 19 17:40:18.715: INFO: Pod "downwardapi-volume-78c570b9-9c7a-4d69-8b15-028c73534969": Phase="Pending", Reason="", readiness=false. Elapsed: 16.437474ms
Jan 19 17:40:20.732: INFO: Pod "downwardapi-volume-78c570b9-9c7a-4d69-8b15-028c73534969": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033803309s
Jan 19 17:40:22.750: INFO: Pod "downwardapi-volume-78c570b9-9c7a-4d69-8b15-028c73534969": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051490349s
STEP: Saw pod success
Jan 19 17:40:22.750: INFO: Pod "downwardapi-volume-78c570b9-9c7a-4d69-8b15-028c73534969" satisfied condition "success or failure"
Jan 19 17:40:22.766: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-78c570b9-9c7a-4d69-8b15-028c73534969 container client-container: <nil>
STEP: delete the pod
Jan 19 17:40:22.907: INFO: Waiting for pod downwardapi-volume-78c570b9-9c7a-4d69-8b15-028c73534969 to disappear
Jan 19 17:40:22.923: INFO: Pod downwardapi-volume-78c570b9-9c7a-4d69-8b15-028c73534969 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:40:22.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2973" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":206,"skipped":3220,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:40:22.972: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6921
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jan 19 17:40:53.426: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0119 17:40:53.426582    7438 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:40:53.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6921" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":280,"completed":207,"skipped":3221,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:40:53.461: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4703
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-projected-kjzx
STEP: Creating a pod to test atomic-volume-subpath
Jan 19 17:40:53.741: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-kjzx" in namespace "subpath-4703" to be "success or failure"
Jan 19 17:40:53.758: INFO: Pod "pod-subpath-test-projected-kjzx": Phase="Pending", Reason="", readiness=false. Elapsed: 16.54714ms
Jan 19 17:40:55.775: INFO: Pod "pod-subpath-test-projected-kjzx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033564056s
Jan 19 17:40:57.793: INFO: Pod "pod-subpath-test-projected-kjzx": Phase="Running", Reason="", readiness=true. Elapsed: 4.051231958s
Jan 19 17:40:59.810: INFO: Pod "pod-subpath-test-projected-kjzx": Phase="Running", Reason="", readiness=true. Elapsed: 6.068448452s
Jan 19 17:41:01.827: INFO: Pod "pod-subpath-test-projected-kjzx": Phase="Running", Reason="", readiness=true. Elapsed: 8.086136729s
Jan 19 17:41:03.845: INFO: Pod "pod-subpath-test-projected-kjzx": Phase="Running", Reason="", readiness=true. Elapsed: 10.103573117s
Jan 19 17:41:05.863: INFO: Pod "pod-subpath-test-projected-kjzx": Phase="Running", Reason="", readiness=true. Elapsed: 12.121502673s
Jan 19 17:41:07.881: INFO: Pod "pod-subpath-test-projected-kjzx": Phase="Running", Reason="", readiness=true. Elapsed: 14.139666138s
Jan 19 17:41:09.899: INFO: Pod "pod-subpath-test-projected-kjzx": Phase="Running", Reason="", readiness=true. Elapsed: 16.157714372s
Jan 19 17:41:11.917: INFO: Pod "pod-subpath-test-projected-kjzx": Phase="Running", Reason="", readiness=true. Elapsed: 18.175542824s
Jan 19 17:41:13.935: INFO: Pod "pod-subpath-test-projected-kjzx": Phase="Running", Reason="", readiness=true. Elapsed: 20.193527599s
Jan 19 17:41:15.955: INFO: Pod "pod-subpath-test-projected-kjzx": Phase="Running", Reason="", readiness=true. Elapsed: 22.213385573s
Jan 19 17:41:17.974: INFO: Pod "pod-subpath-test-projected-kjzx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.23218254s
STEP: Saw pod success
Jan 19 17:41:17.974: INFO: Pod "pod-subpath-test-projected-kjzx" satisfied condition "success or failure"
Jan 19 17:41:17.990: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-subpath-test-projected-kjzx container test-container-subpath-projected-kjzx: <nil>
STEP: delete the pod
Jan 19 17:41:18.036: INFO: Waiting for pod pod-subpath-test-projected-kjzx to disappear
Jan 19 17:41:18.053: INFO: Pod pod-subpath-test-projected-kjzx no longer exists
STEP: Deleting pod pod-subpath-test-projected-kjzx
Jan 19 17:41:18.053: INFO: Deleting pod "pod-subpath-test-projected-kjzx" in namespace "subpath-4703"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:41:18.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4703" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":280,"completed":208,"skipped":3227,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:41:18.118: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5166
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-1cfd678f-baa9-4343-9b6f-4ee3a3ab7222
STEP: Creating a pod to test consume configMaps
Jan 19 17:41:18.388: INFO: Waiting up to 5m0s for pod "pod-configmaps-14cabde9-d323-411d-96bb-2e86d40bfa19" in namespace "configmap-5166" to be "success or failure"
Jan 19 17:41:18.405: INFO: Pod "pod-configmaps-14cabde9-d323-411d-96bb-2e86d40bfa19": Phase="Pending", Reason="", readiness=false. Elapsed: 16.126786ms
Jan 19 17:41:20.422: INFO: Pod "pod-configmaps-14cabde9-d323-411d-96bb-2e86d40bfa19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033417335s
Jan 19 17:41:22.441: INFO: Pod "pod-configmaps-14cabde9-d323-411d-96bb-2e86d40bfa19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052253632s
STEP: Saw pod success
Jan 19 17:41:22.441: INFO: Pod "pod-configmaps-14cabde9-d323-411d-96bb-2e86d40bfa19" satisfied condition "success or failure"
Jan 19 17:41:22.458: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-configmaps-14cabde9-d323-411d-96bb-2e86d40bfa19 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 19 17:41:22.508: INFO: Waiting for pod pod-configmaps-14cabde9-d323-411d-96bb-2e86d40bfa19 to disappear
Jan 19 17:41:22.531: INFO: Pod pod-configmaps-14cabde9-d323-411d-96bb-2e86d40bfa19 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:41:22.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5166" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":280,"completed":209,"skipped":3240,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:41:22.581: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2992
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 19 17:41:22.835: INFO: Waiting up to 5m0s for pod "pod-a6c8b514-a8ca-4b2e-926f-28b9d8a06539" in namespace "emptydir-2992" to be "success or failure"
Jan 19 17:41:22.852: INFO: Pod "pod-a6c8b514-a8ca-4b2e-926f-28b9d8a06539": Phase="Pending", Reason="", readiness=false. Elapsed: 16.19512ms
Jan 19 17:41:24.869: INFO: Pod "pod-a6c8b514-a8ca-4b2e-926f-28b9d8a06539": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03402579s
Jan 19 17:41:26.888: INFO: Pod "pod-a6c8b514-a8ca-4b2e-926f-28b9d8a06539": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052938084s
STEP: Saw pod success
Jan 19 17:41:26.888: INFO: Pod "pod-a6c8b514-a8ca-4b2e-926f-28b9d8a06539" satisfied condition "success or failure"
Jan 19 17:41:26.905: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-a6c8b514-a8ca-4b2e-926f-28b9d8a06539 container test-container: <nil>
STEP: delete the pod
Jan 19 17:41:26.972: INFO: Waiting for pod pod-a6c8b514-a8ca-4b2e-926f-28b9d8a06539 to disappear
Jan 19 17:41:26.997: INFO: Pod pod-a6c8b514-a8ca-4b2e-926f-28b9d8a06539 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:41:26.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2992" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":210,"skipped":3244,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:41:27.047: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:41:27.267: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:41:31.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9158" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":280,"completed":211,"skipped":3280,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:41:31.585: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2168
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:41:42.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2168" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":280,"completed":212,"skipped":3290,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:41:43.026: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9751
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 19 17:41:51.404: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 19 17:41:51.421: INFO: Pod pod-with-prestop-http-hook still exists
Jan 19 17:41:53.421: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 19 17:41:53.438: INFO: Pod pod-with-prestop-http-hook still exists
Jan 19 17:41:55.421: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 19 17:41:55.438: INFO: Pod pod-with-prestop-http-hook still exists
Jan 19 17:41:57.421: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 19 17:41:57.438: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:41:57.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9751" for this suite.
•{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":280,"completed":213,"skipped":3310,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:41:57.600: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9811
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-9811
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 19 17:41:57.869: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 19 17:42:22.167: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.1.245:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9811 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:42:22.167: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:42:22.685: INFO: Found all expected endpoints: [netserver-0]
Jan 19 17:42:22.702: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.0.66:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9811 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:42:22.702: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:42:23.188: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:42:23.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9811" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":214,"skipped":3326,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:42:23.239: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6875
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-04e6ce61-42d6-4a59-8ac8-78512e463c57
STEP: Creating a pod to test consume configMaps
Jan 19 17:42:23.502: INFO: Waiting up to 5m0s for pod "pod-configmaps-b4589200-1720-4e68-9787-6c1527e0de3e" in namespace "configmap-6875" to be "success or failure"
Jan 19 17:42:23.519: INFO: Pod "pod-configmaps-b4589200-1720-4e68-9787-6c1527e0de3e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.304004ms
Jan 19 17:42:25.539: INFO: Pod "pod-configmaps-b4589200-1720-4e68-9787-6c1527e0de3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037016243s
Jan 19 17:42:27.557: INFO: Pod "pod-configmaps-b4589200-1720-4e68-9787-6c1527e0de3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055029131s
STEP: Saw pod success
Jan 19 17:42:27.557: INFO: Pod "pod-configmaps-b4589200-1720-4e68-9787-6c1527e0de3e" satisfied condition "success or failure"
Jan 19 17:42:27.575: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-configmaps-b4589200-1720-4e68-9787-6c1527e0de3e container configmap-volume-test: <nil>
STEP: delete the pod
Jan 19 17:42:27.630: INFO: Waiting for pod pod-configmaps-b4589200-1720-4e68-9787-6c1527e0de3e to disappear
Jan 19 17:42:27.646: INFO: Pod pod-configmaps-b4589200-1720-4e68-9787-6c1527e0de3e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:42:27.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6875" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":280,"completed":215,"skipped":3345,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:42:27.710: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1721
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 19 17:42:32.070: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:42:32.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1721" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":280,"completed":216,"skipped":3437,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:42:32.163: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6844
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name cm-test-opt-del-79e04310-c46d-42c9-ab1c-b211d59ccce8
STEP: Creating configMap with name cm-test-opt-upd-e568038c-01e1-4f84-b288-43f1a14a0fbd
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-79e04310-c46d-42c9-ab1c-b211d59ccce8
STEP: Updating configmap cm-test-opt-upd-e568038c-01e1-4f84-b288-43f1a14a0fbd
STEP: Creating configMap with name cm-test-opt-create-1ad17bff-470c-49cd-a322-da2ff89c47b4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:42:40.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6844" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":217,"skipped":3452,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:42:41.035: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7548
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating Agnhost RC
Jan 19 17:42:41.262: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7548'
Jan 19 17:42:41.536: INFO: stderr: ""
Jan 19 17:42:41.536: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Jan 19 17:42:42.553: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 17:42:42.553: INFO: Found 0 / 1
Jan 19 17:42:43.553: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 17:42:43.554: INFO: Found 0 / 1
Jan 19 17:42:44.561: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 17:42:44.561: INFO: Found 0 / 1
Jan 19 17:42:45.554: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 17:42:45.554: INFO: Found 1 / 1
Jan 19 17:42:45.554: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 19 17:42:45.572: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 17:42:45.572: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 19 17:42:45.572: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config patch pod agnhost-master-wkgl2 --namespace=kubectl-7548 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 19 17:42:45.721: INFO: stderr: ""
Jan 19 17:42:45.721: INFO: stdout: "pod/agnhost-master-wkgl2 patched\n"
STEP: checking annotations
Jan 19 17:42:45.738: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 17:42:45.738: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:42:45.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7548" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":280,"completed":218,"skipped":3452,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:42:45.788: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5464
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5464.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5464.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5464.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5464.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 19 17:42:52.283: INFO: DNS probes using dns-test-54333d5c-537a-4a7e-8ea8-6ea42aa760a5 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5464.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5464.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5464.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5464.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 19 17:42:58.521: INFO: File wheezy_udp@dns-test-service-3.dns-5464.svc.cluster.local from pod  dns-5464/dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 19 17:42:58.606: INFO: File jessie_udp@dns-test-service-3.dns-5464.svc.cluster.local from pod  dns-5464/dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 19 17:42:58.606: INFO: Lookups using dns-5464/dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd failed for: [wheezy_udp@dns-test-service-3.dns-5464.svc.cluster.local jessie_udp@dns-test-service-3.dns-5464.svc.cluster.local]

Jan 19 17:43:03.626: INFO: File wheezy_udp@dns-test-service-3.dns-5464.svc.cluster.local from pod  dns-5464/dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 19 17:43:03.669: INFO: File jessie_udp@dns-test-service-3.dns-5464.svc.cluster.local from pod  dns-5464/dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd contains '' instead of 'bar.example.com.'
Jan 19 17:43:03.669: INFO: Lookups using dns-5464/dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd failed for: [wheezy_udp@dns-test-service-3.dns-5464.svc.cluster.local jessie_udp@dns-test-service-3.dns-5464.svc.cluster.local]

Jan 19 17:43:08.627: INFO: File wheezy_udp@dns-test-service-3.dns-5464.svc.cluster.local from pod  dns-5464/dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 19 17:43:08.711: INFO: File jessie_udp@dns-test-service-3.dns-5464.svc.cluster.local from pod  dns-5464/dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 19 17:43:08.711: INFO: Lookups using dns-5464/dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd failed for: [wheezy_udp@dns-test-service-3.dns-5464.svc.cluster.local jessie_udp@dns-test-service-3.dns-5464.svc.cluster.local]

Jan 19 17:43:13.626: INFO: File wheezy_udp@dns-test-service-3.dns-5464.svc.cluster.local from pod  dns-5464/dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 19 17:43:13.709: INFO: File jessie_udp@dns-test-service-3.dns-5464.svc.cluster.local from pod  dns-5464/dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 19 17:43:13.709: INFO: Lookups using dns-5464/dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd failed for: [wheezy_udp@dns-test-service-3.dns-5464.svc.cluster.local jessie_udp@dns-test-service-3.dns-5464.svc.cluster.local]

Jan 19 17:43:18.626: INFO: File wheezy_udp@dns-test-service-3.dns-5464.svc.cluster.local from pod  dns-5464/dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 19 17:43:18.670: INFO: File jessie_udp@dns-test-service-3.dns-5464.svc.cluster.local from pod  dns-5464/dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 19 17:43:18.670: INFO: Lookups using dns-5464/dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd failed for: [wheezy_udp@dns-test-service-3.dns-5464.svc.cluster.local jessie_udp@dns-test-service-3.dns-5464.svc.cluster.local]

Jan 19 17:43:23.710: INFO: DNS probes using dns-test-a9fd7a5b-b93c-4af6-920c-956205289bbd succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5464.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5464.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5464.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5464.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 19 17:43:30.112: INFO: DNS probes using dns-test-6ca211c9-725f-40e0-a99e-518eb0336ce1 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:43:30.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5464" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":280,"completed":219,"skipped":3483,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] version v1
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:43:30.217: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-162
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:43:30.481: INFO: (0) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 22.179405ms)
Jan 19 17:43:30.523: INFO: (1) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 42.292461ms)
Jan 19 17:43:30.542: INFO: (2) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.401718ms)
Jan 19 17:43:30.566: INFO: (3) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.660267ms)
Jan 19 17:43:30.588: INFO: (4) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 21.381536ms)
Jan 19 17:43:30.606: INFO: (5) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.396636ms)
Jan 19 17:43:30.625: INFO: (6) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.633645ms)
Jan 19 17:43:30.643: INFO: (7) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.094165ms)
Jan 19 17:43:30.661: INFO: (8) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 17.747156ms)
Jan 19 17:43:30.679: INFO: (9) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 17.973578ms)
Jan 19 17:43:30.697: INFO: (10) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.200669ms)
Jan 19 17:43:30.722: INFO: (11) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.24678ms)
Jan 19 17:43:30.740: INFO: (12) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.45515ms)
Jan 19 17:43:30.759: INFO: (13) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.558742ms)
Jan 19 17:43:30.777: INFO: (14) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.31359ms)
Jan 19 17:43:30.795: INFO: (15) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.001402ms)
Jan 19 17:43:30.813: INFO: (16) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 17.970948ms)
Jan 19 17:43:30.831: INFO: (17) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 17.793483ms)
Jan 19 17:43:30.849: INFO: (18) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 18.116102ms)
Jan 19 17:43:30.867: INFO: (19) /api/v1/nodes/shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 17.756817ms)
[AfterEach] version v1
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:43:30.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-162" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":280,"completed":220,"skipped":3499,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:43:30.902: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2487
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1861
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 19 17:43:31.121: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2487'
Jan 19 17:43:31.257: INFO: stderr: ""
Jan 19 17:43:31.257: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1866
Jan 19 17:43:31.274: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-httpd-pod --namespace=kubectl-2487'
Jan 19 17:43:34.661: INFO: stderr: ""
Jan 19 17:43:34.661: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:43:34.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2487" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":280,"completed":221,"skipped":3541,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:43:34.712: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-567
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3452
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8212
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:43:52.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-567" for this suite.
STEP: Destroying namespace "nsdeletetest-3452" for this suite.
Jan 19 17:43:52.537: INFO: Namespace nsdeletetest-3452 was already deleted
STEP: Destroying namespace "nsdeletetest-8212" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":280,"completed":222,"skipped":3572,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:43:52.555: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:178
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:43:52.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2189" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":280,"completed":223,"skipped":3574,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:43:52.852: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-4149
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:344
Jan 19 17:43:53.082: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 17:44:53.225: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:44:53.242: INFO: Starting informer...
STEP: Starting pods...
Jan 19 17:44:53.296: INFO: Pod1 is running on shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc. Tainting Node
Jan 19 17:44:57.378: INFO: Pod2 is running on shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jan 19 17:45:05.427: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 19 17:45:27.214: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:45:27.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-4149" for this suite.
•{"msg":"PASSED [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":280,"completed":224,"skipped":3594,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:45:27.300: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4536
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 19 17:45:27.710: INFO: Number of nodes with available pods: 0
Jan 19 17:45:27.710: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:45:28.759: INFO: Number of nodes with available pods: 0
Jan 19 17:45:28.759: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:45:29.758: INFO: Number of nodes with available pods: 1
Jan 19 17:45:29.758: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:30.759: INFO: Number of nodes with available pods: 1
Jan 19 17:45:30.759: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:31.761: INFO: Number of nodes with available pods: 2
Jan 19 17:45:31.761: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 19 17:45:31.845: INFO: Number of nodes with available pods: 1
Jan 19 17:45:31.845: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:32.895: INFO: Number of nodes with available pods: 1
Jan 19 17:45:32.895: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:33.895: INFO: Number of nodes with available pods: 1
Jan 19 17:45:33.895: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:34.895: INFO: Number of nodes with available pods: 1
Jan 19 17:45:34.895: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:35.895: INFO: Number of nodes with available pods: 1
Jan 19 17:45:35.896: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:36.895: INFO: Number of nodes with available pods: 1
Jan 19 17:45:36.895: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:37.895: INFO: Number of nodes with available pods: 1
Jan 19 17:45:37.895: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:38.894: INFO: Number of nodes with available pods: 1
Jan 19 17:45:38.894: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:39.899: INFO: Number of nodes with available pods: 1
Jan 19 17:45:39.899: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:40.895: INFO: Number of nodes with available pods: 1
Jan 19 17:45:40.895: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:41.895: INFO: Number of nodes with available pods: 1
Jan 19 17:45:41.895: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:42.895: INFO: Number of nodes with available pods: 1
Jan 19 17:45:42.895: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:43.895: INFO: Number of nodes with available pods: 1
Jan 19 17:45:43.895: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:44.894: INFO: Number of nodes with available pods: 1
Jan 19 17:45:44.894: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz is running more than one daemon pod
Jan 19 17:45:45.895: INFO: Number of nodes with available pods: 2
Jan 19 17:45:45.895: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4536, will wait for the garbage collector to delete the pods
Jan 19 17:45:45.998: INFO: Deleting DaemonSet.extensions daemon-set took: 19.591778ms
Jan 19 17:45:46.098: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.29786ms
Jan 19 17:46:02.715: INFO: Number of nodes with available pods: 0
Jan 19 17:46:02.715: INFO: Number of running nodes: 0, number of available pods: 0
Jan 19 17:46:02.732: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4536/daemonsets","resourceVersion":"32856"},"items":null}

Jan 19 17:46:02.749: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4536/pods","resourceVersion":"32856"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:46:02.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4536" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":280,"completed":225,"skipped":3625,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:46:02.855: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2548
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 19 17:46:03.227: INFO: Number of nodes with available pods: 0
Jan 19 17:46:03.227: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:46:04.277: INFO: Number of nodes with available pods: 0
Jan 19 17:46:04.277: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:46:05.282: INFO: Number of nodes with available pods: 0
Jan 19 17:46:05.282: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:46:06.276: INFO: Number of nodes with available pods: 2
Jan 19 17:46:06.277: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 19 17:46:06.372: INFO: Number of nodes with available pods: 1
Jan 19 17:46:06.372: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:46:07.428: INFO: Number of nodes with available pods: 1
Jan 19 17:46:07.428: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:46:08.422: INFO: Number of nodes with available pods: 1
Jan 19 17:46:08.422: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:46:09.422: INFO: Number of nodes with available pods: 2
Jan 19 17:46:09.422: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2548, will wait for the garbage collector to delete the pods
Jan 19 17:46:09.549: INFO: Deleting DaemonSet.extensions daemon-set took: 18.88777ms
Jan 19 17:46:09.650: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.314891ms
Jan 19 17:46:17.266: INFO: Number of nodes with available pods: 0
Jan 19 17:46:17.266: INFO: Number of running nodes: 0, number of available pods: 0
Jan 19 17:46:17.283: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2548/daemonsets","resourceVersion":"32957"},"items":null}

Jan 19 17:46:17.300: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2548/pods","resourceVersion":"32957"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:46:17.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2548" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":280,"completed":226,"skipped":3635,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:46:17.403: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod busybox-08338a02-c5a4-413e-a278-b30e30eff03a in namespace container-probe-8872
Jan 19 17:46:21.686: INFO: Started pod busybox-08338a02-c5a4-413e-a278-b30e30eff03a in namespace container-probe-8872
STEP: checking the pod's current state and verifying that restartCount is present
Jan 19 17:46:21.703: INFO: Initial restart count of pod busybox-08338a02-c5a4-413e-a278-b30e30eff03a is 0
Jan 19 17:47:12.160: INFO: Restart count of pod container-probe-8872/busybox-08338a02-c5a4-413e-a278-b30e30eff03a is now 1 (50.457195729s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:47:12.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8872" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":280,"completed":227,"skipped":3668,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:47:12.243: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2179
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test substitution in container's args
Jan 19 17:47:12.487: INFO: Waiting up to 5m0s for pod "var-expansion-e804e26c-a166-459a-97d8-b4f602a2c6fe" in namespace "var-expansion-2179" to be "success or failure"
Jan 19 17:47:12.504: INFO: Pod "var-expansion-e804e26c-a166-459a-97d8-b4f602a2c6fe": Phase="Pending", Reason="", readiness=false. Elapsed: 16.31309ms
Jan 19 17:47:14.521: INFO: Pod "var-expansion-e804e26c-a166-459a-97d8-b4f602a2c6fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033362565s
Jan 19 17:47:16.537: INFO: Pod "var-expansion-e804e26c-a166-459a-97d8-b4f602a2c6fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050233358s
STEP: Saw pod success
Jan 19 17:47:16.538: INFO: Pod "var-expansion-e804e26c-a166-459a-97d8-b4f602a2c6fe" satisfied condition "success or failure"
Jan 19 17:47:16.554: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod var-expansion-e804e26c-a166-459a-97d8-b4f602a2c6fe container dapi-container: <nil>
STEP: delete the pod
Jan 19 17:47:16.690: INFO: Waiting for pod var-expansion-e804e26c-a166-459a-97d8-b4f602a2c6fe to disappear
Jan 19 17:47:16.707: INFO: Pod var-expansion-e804e26c-a166-459a-97d8-b4f602a2c6fe no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:47:16.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2179" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":280,"completed":228,"skipped":3677,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:47:16.756: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9816
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:47:16.975: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jan 19 17:47:18.149: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:47:18.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9816" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":280,"completed":229,"skipped":3712,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:47:18.229: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-679
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl run job
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1788
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 19 17:47:18.461: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-679'
Jan 19 17:47:18.892: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 19 17:47:18.892: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1793
Jan 19 17:47:18.909: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete jobs e2e-test-httpd-job --namespace=kubectl-679'
Jan 19 17:47:19.059: INFO: stderr: ""
Jan 19 17:47:19.059: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:47:19.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-679" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]","total":280,"completed":230,"skipped":3764,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:47:19.109: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9040
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-d3b4d322-edd7-4505-a1c1-e57374d21e5f
STEP: Creating secret with name s-test-opt-upd-ed66df45-ce56-4d9c-82fc-71dd02c063c3
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d3b4d322-edd7-4505-a1c1-e57374d21e5f
STEP: Updating secret s-test-opt-upd-ed66df45-ce56-4d9c-82fc-71dd02c063c3
STEP: Creating secret with name s-test-opt-create-45cffff1-115c-4fe1-97b2-a54bac444230
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:47:27.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9040" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":231,"skipped":3775,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:47:28.024: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8013
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 19 17:47:28.271: INFO: Waiting up to 5m0s for pod "downward-api-8f051bd4-9f70-496f-b248-f305474668bd" in namespace "downward-api-8013" to be "success or failure"
Jan 19 17:47:28.290: INFO: Pod "downward-api-8f051bd4-9f70-496f-b248-f305474668bd": Phase="Pending", Reason="", readiness=false. Elapsed: 19.111645ms
Jan 19 17:47:30.308: INFO: Pod "downward-api-8f051bd4-9f70-496f-b248-f305474668bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036930915s
Jan 19 17:47:32.325: INFO: Pod "downward-api-8f051bd4-9f70-496f-b248-f305474668bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054061629s
STEP: Saw pod success
Jan 19 17:47:32.325: INFO: Pod "downward-api-8f051bd4-9f70-496f-b248-f305474668bd" satisfied condition "success or failure"
Jan 19 17:47:32.341: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downward-api-8f051bd4-9f70-496f-b248-f305474668bd container dapi-container: <nil>
STEP: delete the pod
Jan 19 17:47:32.485: INFO: Waiting for pod downward-api-8f051bd4-9f70-496f-b248-f305474668bd to disappear
Jan 19 17:47:32.501: INFO: Pod downward-api-8f051bd4-9f70-496f-b248-f305474668bd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:47:32.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8013" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":280,"completed":232,"skipped":3776,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:47:32.551: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2900
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:47:32.775: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:47:38.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2900" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":280,"completed":233,"skipped":3793,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:47:38.968: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9460
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-map-38f597fe-7b1a-4cc2-8ecc-746d172c78f5
STEP: Creating a pod to test consume secrets
Jan 19 17:47:39.226: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fae02bc0-b16d-4f71-be2b-ee3c70da2d73" in namespace "projected-9460" to be "success or failure"
Jan 19 17:47:39.242: INFO: Pod "pod-projected-secrets-fae02bc0-b16d-4f71-be2b-ee3c70da2d73": Phase="Pending", Reason="", readiness=false. Elapsed: 16.355971ms
Jan 19 17:47:41.260: INFO: Pod "pod-projected-secrets-fae02bc0-b16d-4f71-be2b-ee3c70da2d73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033834655s
Jan 19 17:47:43.277: INFO: Pod "pod-projected-secrets-fae02bc0-b16d-4f71-be2b-ee3c70da2d73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050937987s
STEP: Saw pod success
Jan 19 17:47:43.277: INFO: Pod "pod-projected-secrets-fae02bc0-b16d-4f71-be2b-ee3c70da2d73" satisfied condition "success or failure"
Jan 19 17:47:43.294: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-projected-secrets-fae02bc0-b16d-4f71-be2b-ee3c70da2d73 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 19 17:47:43.345: INFO: Waiting for pod pod-projected-secrets-fae02bc0-b16d-4f71-be2b-ee3c70da2d73 to disappear
Jan 19 17:47:43.362: INFO: Pod pod-projected-secrets-fae02bc0-b16d-4f71-be2b-ee3c70da2d73 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:47:43.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9460" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":280,"completed":234,"skipped":3797,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:47:43.414: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6244
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:47:48.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6244" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":280,"completed":235,"skipped":3837,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:47:48.721: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6478
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:47:49.555: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052868, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052868, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052868, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052868, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:47:51.581: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052868, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052868, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052868, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715052868, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:47:54.602: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:47:55.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6478" for this suite.
STEP: Destroying namespace "webhook-6478-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":280,"completed":236,"skipped":3857,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:47:55.252: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8570
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name secret-test-e8cf8a50-30e7-45b6-9d9f-92c827a291c2
STEP: Creating a pod to test consume secrets
Jan 19 17:47:55.519: INFO: Waiting up to 5m0s for pod "pod-secrets-3976f7bc-b984-4427-8a56-0cd45362f645" in namespace "secrets-8570" to be "success or failure"
Jan 19 17:47:55.536: INFO: Pod "pod-secrets-3976f7bc-b984-4427-8a56-0cd45362f645": Phase="Pending", Reason="", readiness=false. Elapsed: 16.421771ms
Jan 19 17:47:57.554: INFO: Pod "pod-secrets-3976f7bc-b984-4427-8a56-0cd45362f645": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0348882s
Jan 19 17:47:59.572: INFO: Pod "pod-secrets-3976f7bc-b984-4427-8a56-0cd45362f645": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052352457s
STEP: Saw pod success
Jan 19 17:47:59.572: INFO: Pod "pod-secrets-3976f7bc-b984-4427-8a56-0cd45362f645" satisfied condition "success or failure"
Jan 19 17:47:59.589: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-secrets-3976f7bc-b984-4427-8a56-0cd45362f645 container secret-env-test: <nil>
STEP: delete the pod
Jan 19 17:47:59.642: INFO: Waiting for pod pod-secrets-3976f7bc-b984-4427-8a56-0cd45362f645 to disappear
Jan 19 17:47:59.659: INFO: Pod pod-secrets-3976f7bc-b984-4427-8a56-0cd45362f645 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:47:59.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8570" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":280,"completed":237,"skipped":3877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:47:59.709: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 17:47:59.964: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2389073-7da6-4ce3-92d5-5660438d9156" in namespace "downward-api-2826" to be "success or failure"
Jan 19 17:47:59.980: INFO: Pod "downwardapi-volume-c2389073-7da6-4ce3-92d5-5660438d9156": Phase="Pending", Reason="", readiness=false. Elapsed: 15.927ms
Jan 19 17:48:01.998: INFO: Pod "downwardapi-volume-c2389073-7da6-4ce3-92d5-5660438d9156": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033355307s
Jan 19 17:48:04.015: INFO: Pod "downwardapi-volume-c2389073-7da6-4ce3-92d5-5660438d9156": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050414995s
STEP: Saw pod success
Jan 19 17:48:04.015: INFO: Pod "downwardapi-volume-c2389073-7da6-4ce3-92d5-5660438d9156" satisfied condition "success or failure"
Jan 19 17:48:04.031: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-c2389073-7da6-4ce3-92d5-5660438d9156 container client-container: <nil>
STEP: delete the pod
Jan 19 17:48:04.082: INFO: Waiting for pod downwardapi-volume-c2389073-7da6-4ce3-92d5-5660438d9156 to disappear
Jan 19 17:48:04.098: INFO: Pod downwardapi-volume-c2389073-7da6-4ce3-92d5-5660438d9156 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:48:04.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2826" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":280,"completed":238,"skipped":3913,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:48:04.156: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7257
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:133
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:48:04.462: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 19 17:48:04.499: INFO: Number of nodes with available pods: 0
Jan 19 17:48:04.499: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jan 19 17:48:04.569: INFO: Number of nodes with available pods: 0
Jan 19 17:48:04.569: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:48:05.586: INFO: Number of nodes with available pods: 0
Jan 19 17:48:05.586: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:48:06.586: INFO: Number of nodes with available pods: 0
Jan 19 17:48:06.586: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:48:07.587: INFO: Number of nodes with available pods: 1
Jan 19 17:48:07.587: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 19 17:48:07.661: INFO: Number of nodes with available pods: 0
Jan 19 17:48:07.661: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 19 17:48:07.694: INFO: Number of nodes with available pods: 0
Jan 19 17:48:07.694: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:48:08.712: INFO: Number of nodes with available pods: 0
Jan 19 17:48:08.712: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:48:09.712: INFO: Number of nodes with available pods: 0
Jan 19 17:48:09.712: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:48:10.712: INFO: Number of nodes with available pods: 0
Jan 19 17:48:10.712: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:48:11.712: INFO: Number of nodes with available pods: 0
Jan 19 17:48:11.712: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:48:12.712: INFO: Number of nodes with available pods: 0
Jan 19 17:48:12.712: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:48:13.712: INFO: Number of nodes with available pods: 0
Jan 19 17:48:13.712: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:48:14.711: INFO: Number of nodes with available pods: 0
Jan 19 17:48:14.711: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:48:15.717: INFO: Number of nodes with available pods: 0
Jan 19 17:48:15.718: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:48:16.712: INFO: Number of nodes with available pods: 0
Jan 19 17:48:16.712: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:48:17.713: INFO: Number of nodes with available pods: 0
Jan 19 17:48:17.713: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:48:18.712: INFO: Number of nodes with available pods: 0
Jan 19 17:48:18.712: INFO: Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc is running more than one daemon pod
Jan 19 17:48:19.712: INFO: Number of nodes with available pods: 1
Jan 19 17:48:19.712: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7257, will wait for the garbage collector to delete the pods
Jan 19 17:48:19.833: INFO: Deleting DaemonSet.extensions daemon-set took: 20.145953ms
Jan 19 17:48:19.933: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.260369ms
Jan 19 17:48:23.654: INFO: Number of nodes with available pods: 0
Jan 19 17:48:23.654: INFO: Number of running nodes: 0, number of available pods: 0
Jan 19 17:48:23.671: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7257/daemonsets","resourceVersion":"33875"},"items":null}

Jan 19 17:48:23.688: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7257/pods","resourceVersion":"33875"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:48:23.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7257" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":280,"completed":239,"skipped":3922,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:48:23.812: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1350
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 19 17:48:24.081: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:48:25.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1350" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":280,"completed":240,"skipped":3924,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:48:25.218: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9139
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:177
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 19 17:48:30.062: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c1edba3a-6b10-4c2f-93ac-ff6a90f64234"
Jan 19 17:48:30.062: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c1edba3a-6b10-4c2f-93ac-ff6a90f64234" in namespace "pods-9139" to be "terminated due to deadline exceeded"
Jan 19 17:48:30.078: INFO: Pod "pod-update-activedeadlineseconds-c1edba3a-6b10-4c2f-93ac-ff6a90f64234": Phase="Running", Reason="", readiness=true. Elapsed: 16.346954ms
Jan 19 17:48:32.097: INFO: Pod "pod-update-activedeadlineseconds-c1edba3a-6b10-4c2f-93ac-ff6a90f64234": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.034541709s
Jan 19 17:48:32.097: INFO: Pod "pod-update-activedeadlineseconds-c1edba3a-6b10-4c2f-93ac-ff6a90f64234" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:48:32.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9139" for this suite.
•{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":280,"completed":241,"skipped":3939,"failed":0}
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:48:32.149: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1930
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: getting the auto-created API token
Jan 19 17:48:32.965: INFO: created pod pod-service-account-defaultsa
Jan 19 17:48:32.965: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 19 17:48:33.179: INFO: created pod pod-service-account-mountsa
Jan 19 17:48:33.179: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 19 17:48:33.196: INFO: created pod pod-service-account-nomountsa
Jan 19 17:48:33.196: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 19 17:48:33.220: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 19 17:48:33.220: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 19 17:48:33.237: INFO: created pod pod-service-account-mountsa-mountspec
Jan 19 17:48:33.237: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 19 17:48:33.254: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 19 17:48:33.254: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 19 17:48:33.271: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 19 17:48:33.271: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 19 17:48:33.291: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 19 17:48:33.291: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 19 17:48:33.320: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 19 17:48:33.320: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:48:33.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1930" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":280,"completed":242,"skipped":3940,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:48:33.369: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-655
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 19 17:48:33.618: INFO: Waiting up to 5m0s for pod "pod-9f2d2e69-d976-4f33-9118-37d00f20ae77" in namespace "emptydir-655" to be "success or failure"
Jan 19 17:48:33.634: INFO: Pod "pod-9f2d2e69-d976-4f33-9118-37d00f20ae77": Phase="Pending", Reason="", readiness=false. Elapsed: 16.083938ms
Jan 19 17:48:35.652: INFO: Pod "pod-9f2d2e69-d976-4f33-9118-37d00f20ae77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033868129s
Jan 19 17:48:37.670: INFO: Pod "pod-9f2d2e69-d976-4f33-9118-37d00f20ae77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051931278s
Jan 19 17:48:39.687: INFO: Pod "pod-9f2d2e69-d976-4f33-9118-37d00f20ae77": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068970627s
Jan 19 17:48:41.713: INFO: Pod "pod-9f2d2e69-d976-4f33-9118-37d00f20ae77": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094928037s
Jan 19 17:48:43.730: INFO: Pod "pod-9f2d2e69-d976-4f33-9118-37d00f20ae77": Phase="Pending", Reason="", readiness=false. Elapsed: 10.112192549s
Jan 19 17:48:45.748: INFO: Pod "pod-9f2d2e69-d976-4f33-9118-37d00f20ae77": Phase="Pending", Reason="", readiness=false. Elapsed: 12.129913182s
Jan 19 17:48:47.765: INFO: Pod "pod-9f2d2e69-d976-4f33-9118-37d00f20ae77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.147477818s
STEP: Saw pod success
Jan 19 17:48:47.765: INFO: Pod "pod-9f2d2e69-d976-4f33-9118-37d00f20ae77" satisfied condition "success or failure"
Jan 19 17:48:47.782: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-9f2d2e69-d976-4f33-9118-37d00f20ae77 container test-container: <nil>
STEP: delete the pod
Jan 19 17:48:47.862: INFO: Waiting for pod pod-9f2d2e69-d976-4f33-9118-37d00f20ae77 to disappear
Jan 19 17:48:47.878: INFO: Pod pod-9f2d2e69-d976-4f33-9118-37d00f20ae77 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:48:47.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-655" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":243,"skipped":3941,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:48:47.929: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-442
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 17:48:48.177: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8fe6eb0-7c8d-49b9-ba21-b3d47071bdf9" in namespace "downward-api-442" to be "success or failure"
Jan 19 17:48:48.193: INFO: Pod "downwardapi-volume-a8fe6eb0-7c8d-49b9-ba21-b3d47071bdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.439462ms
Jan 19 17:48:50.211: INFO: Pod "downwardapi-volume-a8fe6eb0-7c8d-49b9-ba21-b3d47071bdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034319097s
Jan 19 17:48:52.229: INFO: Pod "downwardapi-volume-a8fe6eb0-7c8d-49b9-ba21-b3d47071bdf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051551565s
STEP: Saw pod success
Jan 19 17:48:52.229: INFO: Pod "downwardapi-volume-a8fe6eb0-7c8d-49b9-ba21-b3d47071bdf9" satisfied condition "success or failure"
Jan 19 17:48:52.245: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-a8fe6eb0-7c8d-49b9-ba21-b3d47071bdf9 container client-container: <nil>
STEP: delete the pod
Jan 19 17:48:52.297: INFO: Waiting for pod downwardapi-volume-a8fe6eb0-7c8d-49b9-ba21-b3d47071bdf9 to disappear
Jan 19 17:48:52.313: INFO: Pod downwardapi-volume-a8fe6eb0-7c8d-49b9-ba21-b3d47071bdf9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:48:52.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-442" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":280,"completed":244,"skipped":3947,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:48:52.362: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5839
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 17:48:52.605: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc4f9211-21fb-4949-8089-ffe3893f99c8" in namespace "projected-5839" to be "success or failure"
Jan 19 17:48:52.621: INFO: Pod "downwardapi-volume-fc4f9211-21fb-4949-8089-ffe3893f99c8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.412955ms
Jan 19 17:48:54.639: INFO: Pod "downwardapi-volume-fc4f9211-21fb-4949-8089-ffe3893f99c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034433059s
Jan 19 17:48:56.657: INFO: Pod "downwardapi-volume-fc4f9211-21fb-4949-8089-ffe3893f99c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052196135s
STEP: Saw pod success
Jan 19 17:48:56.657: INFO: Pod "downwardapi-volume-fc4f9211-21fb-4949-8089-ffe3893f99c8" satisfied condition "success or failure"
Jan 19 17:48:56.673: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-fc4f9211-21fb-4949-8089-ffe3893f99c8 container client-container: <nil>
STEP: delete the pod
Jan 19 17:48:56.727: INFO: Waiting for pod downwardapi-volume-fc4f9211-21fb-4949-8089-ffe3893f99c8 to disappear
Jan 19 17:48:56.746: INFO: Pod downwardapi-volume-fc4f9211-21fb-4949-8089-ffe3893f99c8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:48:56.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5839" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":245,"skipped":3956,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:48:56.801: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4348
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 19 17:48:57.047: INFO: Waiting up to 5m0s for pod "pod-debcae6f-2fa7-4fca-ae68-5a4f384b1f8f" in namespace "emptydir-4348" to be "success or failure"
Jan 19 17:48:57.064: INFO: Pod "pod-debcae6f-2fa7-4fca-ae68-5a4f384b1f8f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.465916ms
Jan 19 17:48:59.081: INFO: Pod "pod-debcae6f-2fa7-4fca-ae68-5a4f384b1f8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033735277s
Jan 19 17:49:01.099: INFO: Pod "pod-debcae6f-2fa7-4fca-ae68-5a4f384b1f8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051428677s
STEP: Saw pod success
Jan 19 17:49:01.099: INFO: Pod "pod-debcae6f-2fa7-4fca-ae68-5a4f384b1f8f" satisfied condition "success or failure"
Jan 19 17:49:01.115: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-debcae6f-2fa7-4fca-ae68-5a4f384b1f8f container test-container: <nil>
STEP: delete the pod
Jan 19 17:49:01.202: INFO: Waiting for pod pod-debcae6f-2fa7-4fca-ae68-5a4f384b1f8f to disappear
Jan 19 17:49:01.219: INFO: Pod pod-debcae6f-2fa7-4fca-ae68-5a4f384b1f8f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:49:01.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4348" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":246,"skipped":3987,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:49:01.271: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8378
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8378
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8378
I0119 17:49:01.581813    7438 runners.go:189] Created replication controller with name: externalname-service, namespace: services-8378, replica count: 2
Jan 19 17:49:04.632: INFO: Creating new exec pod
I0119 17:49:04.632306    7438 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 17:49:09.688: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-8378 execpod6lng8 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jan 19 17:49:10.262: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 19 17:49:10.262: INFO: stdout: ""
Jan 19 17:49:10.263: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-8378 execpod6lng8 -- /bin/sh -x -c nc -zv -t -w 2 100.111.227.149 80'
Jan 19 17:49:10.746: INFO: stderr: "+ nc -zv -t -w 2 100.111.227.149 80\nConnection to 100.111.227.149 80 port [tcp/http] succeeded!\n"
Jan 19 17:49:10.746: INFO: stdout: ""
Jan 19 17:49:10.746: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:49:10.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8378" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":280,"completed":247,"skipped":3999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:49:10.839: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 19 17:49:19.223: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 19 17:49:19.240: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 19 17:49:21.241: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 19 17:49:21.258: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 19 17:49:23.241: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 19 17:49:23.258: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 19 17:49:25.241: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 19 17:49:25.259: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 19 17:49:27.241: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 19 17:49:27.257: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:49:27.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9115" for this suite.
•{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":280,"completed":248,"skipped":4027,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:49:27.330: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-23
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating secret with name s-test-opt-del-effe2acf-6bd9-486d-ad0f-5cbf0ad095ed
STEP: Creating secret with name s-test-opt-upd-62d8e6ea-90a4-4c94-a55d-98d1224fee32
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-effe2acf-6bd9-486d-ad0f-5cbf0ad095ed
STEP: Updating secret s-test-opt-upd-62d8e6ea-90a4-4c94-a55d-98d1224fee32
STEP: Creating secret with name s-test-opt-create-12aabf49-9dfd-4291-b1c4-25aae4b4e633
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:49:36.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-23" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":249,"skipped":4032,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:49:36.191: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2759
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-map-71d50234-b22e-44b8-897f-6b5fe9a75500
STEP: Creating a pod to test consume configMaps
Jan 19 17:49:36.457: INFO: Waiting up to 5m0s for pod "pod-configmaps-2d2145ad-37de-40cc-a205-5ada8db00b80" in namespace "configmap-2759" to be "success or failure"
Jan 19 17:49:36.473: INFO: Pod "pod-configmaps-2d2145ad-37de-40cc-a205-5ada8db00b80": Phase="Pending", Reason="", readiness=false. Elapsed: 16.167707ms
Jan 19 17:49:38.491: INFO: Pod "pod-configmaps-2d2145ad-37de-40cc-a205-5ada8db00b80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034323935s
Jan 19 17:49:40.509: INFO: Pod "pod-configmaps-2d2145ad-37de-40cc-a205-5ada8db00b80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052031549s
STEP: Saw pod success
Jan 19 17:49:40.509: INFO: Pod "pod-configmaps-2d2145ad-37de-40cc-a205-5ada8db00b80" satisfied condition "success or failure"
Jan 19 17:49:40.525: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-configmaps-2d2145ad-37de-40cc-a205-5ada8db00b80 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 19 17:49:40.581: INFO: Waiting for pod pod-configmaps-2d2145ad-37de-40cc-a205-5ada8db00b80 to disappear
Jan 19 17:49:40.598: INFO: Pod pod-configmaps-2d2145ad-37de-40cc-a205-5ada8db00b80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:49:40.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2759" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":250,"skipped":4074,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:49:40.649: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6336
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:49:45.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6336" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":280,"completed":251,"skipped":4094,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:49:45.088: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-656
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Performing setup for networking test in namespace pod-network-test-656
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 19 17:49:45.370: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 19 17:50:11.671: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.44:8080/dial?request=hostname&protocol=udp&host=100.64.1.43&port=8081&tries=1'] Namespace:pod-network-test-656 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:50:11.671: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:50:12.141: INFO: Waiting for responses: map[]
Jan 19 17:50:12.157: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.44:8080/dial?request=hostname&protocol=udp&host=100.64.0.72&port=8081&tries=1'] Namespace:pod-network-test-656 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 19 17:50:12.157: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jan 19 17:50:12.612: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:50:12.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-656" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":252,"skipped":4110,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:50:12.662: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4613
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test override command
Jan 19 17:50:12.908: INFO: Waiting up to 5m0s for pod "client-containers-4114f27c-4d8a-4f1d-96bd-0c0f84a64a34" in namespace "containers-4613" to be "success or failure"
Jan 19 17:50:12.927: INFO: Pod "client-containers-4114f27c-4d8a-4f1d-96bd-0c0f84a64a34": Phase="Pending", Reason="", readiness=false. Elapsed: 18.223801ms
Jan 19 17:50:14.945: INFO: Pod "client-containers-4114f27c-4d8a-4f1d-96bd-0c0f84a64a34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036186685s
Jan 19 17:50:16.962: INFO: Pod "client-containers-4114f27c-4d8a-4f1d-96bd-0c0f84a64a34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054045986s
STEP: Saw pod success
Jan 19 17:50:16.963: INFO: Pod "client-containers-4114f27c-4d8a-4f1d-96bd-0c0f84a64a34" satisfied condition "success or failure"
Jan 19 17:50:16.980: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod client-containers-4114f27c-4d8a-4f1d-96bd-0c0f84a64a34 container test-container: <nil>
STEP: delete the pod
Jan 19 17:50:17.027: INFO: Waiting for pod client-containers-4114f27c-4d8a-4f1d-96bd-0c0f84a64a34 to disappear
Jan 19 17:50:17.044: INFO: Pod client-containers-4114f27c-4d8a-4f1d-96bd-0c0f84a64a34 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:50:17.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4613" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":280,"completed":253,"skipped":4130,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:50:17.094: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5015
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:50:18.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053017, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053017, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053017, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053017, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:50:20.365: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053017, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053017, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053017, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053017, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:50:22.364: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053017, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053017, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053017, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053017, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:50:25.397: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:50:25.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5015" for this suite.
STEP: Destroying namespace "webhook-5015-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":280,"completed":254,"skipped":4143,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:50:25.883: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-294
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-b79d0a44-2f1e-4052-8315-ab595c6d8475
STEP: Creating a pod to test consume configMaps
Jan 19 17:50:26.142: INFO: Waiting up to 5m0s for pod "pod-configmaps-5c8a6e75-fcfe-4c74-aec0-503bab9615a2" in namespace "configmap-294" to be "success or failure"
Jan 19 17:50:26.159: INFO: Pod "pod-configmaps-5c8a6e75-fcfe-4c74-aec0-503bab9615a2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.218983ms
Jan 19 17:50:28.176: INFO: Pod "pod-configmaps-5c8a6e75-fcfe-4c74-aec0-503bab9615a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033905669s
Jan 19 17:50:30.198: INFO: Pod "pod-configmaps-5c8a6e75-fcfe-4c74-aec0-503bab9615a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05514131s
Jan 19 17:50:32.216: INFO: Pod "pod-configmaps-5c8a6e75-fcfe-4c74-aec0-503bab9615a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073181253s
STEP: Saw pod success
Jan 19 17:50:32.216: INFO: Pod "pod-configmaps-5c8a6e75-fcfe-4c74-aec0-503bab9615a2" satisfied condition "success or failure"
Jan 19 17:50:32.233: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-configmaps-5c8a6e75-fcfe-4c74-aec0-503bab9615a2 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 19 17:50:32.292: INFO: Waiting for pod pod-configmaps-5c8a6e75-fcfe-4c74-aec0-503bab9615a2 to disappear
Jan 19 17:50:32.309: INFO: Pod pod-configmaps-5c8a6e75-fcfe-4c74-aec0-503bab9615a2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:50:32.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-294" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":255,"skipped":4156,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:50:32.361: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4263
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-volume-ec6c9e0d-5cb8-4fee-a1f4-a4da1c0a5b87
STEP: Creating a pod to test consume configMaps
Jan 19 17:50:32.634: INFO: Waiting up to 5m0s for pod "pod-configmaps-16288cc2-1fe7-4be7-b8d6-cee78fab891f" in namespace "configmap-4263" to be "success or failure"
Jan 19 17:50:32.650: INFO: Pod "pod-configmaps-16288cc2-1fe7-4be7-b8d6-cee78fab891f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.409443ms
Jan 19 17:50:34.668: INFO: Pod "pod-configmaps-16288cc2-1fe7-4be7-b8d6-cee78fab891f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033719606s
Jan 19 17:50:36.685: INFO: Pod "pod-configmaps-16288cc2-1fe7-4be7-b8d6-cee78fab891f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051465148s
STEP: Saw pod success
Jan 19 17:50:36.686: INFO: Pod "pod-configmaps-16288cc2-1fe7-4be7-b8d6-cee78fab891f" satisfied condition "success or failure"
Jan 19 17:50:36.702: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-configmaps-16288cc2-1fe7-4be7-b8d6-cee78fab891f container configmap-volume-test: <nil>
STEP: delete the pod
Jan 19 17:50:36.758: INFO: Waiting for pod pod-configmaps-16288cc2-1fe7-4be7-b8d6-cee78fab891f to disappear
Jan 19 17:50:36.775: INFO: Pod pod-configmaps-16288cc2-1fe7-4be7-b8d6-cee78fab891f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:50:36.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4263" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":256,"skipped":4177,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:50:36.825: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2456
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap configmap-2456/configmap-test-a16af4e1-7d37-453f-b552-0e2d348c9c3d
STEP: Creating a pod to test consume configMaps
Jan 19 17:50:37.086: INFO: Waiting up to 5m0s for pod "pod-configmaps-d07e1096-0f74-4d4a-b603-7923482c811f" in namespace "configmap-2456" to be "success or failure"
Jan 19 17:50:37.102: INFO: Pod "pod-configmaps-d07e1096-0f74-4d4a-b603-7923482c811f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.393818ms
Jan 19 17:50:39.119: INFO: Pod "pod-configmaps-d07e1096-0f74-4d4a-b603-7923482c811f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033531247s
Jan 19 17:50:41.144: INFO: Pod "pod-configmaps-d07e1096-0f74-4d4a-b603-7923482c811f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05817105s
STEP: Saw pod success
Jan 19 17:50:41.144: INFO: Pod "pod-configmaps-d07e1096-0f74-4d4a-b603-7923482c811f" satisfied condition "success or failure"
Jan 19 17:50:41.161: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-configmaps-d07e1096-0f74-4d4a-b603-7923482c811f container env-test: <nil>
STEP: delete the pod
Jan 19 17:50:41.206: INFO: Waiting for pod pod-configmaps-d07e1096-0f74-4d4a-b603-7923482c811f to disappear
Jan 19 17:50:41.222: INFO: Pod pod-configmaps-d07e1096-0f74-4d4a-b603-7923482c811f no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:50:41.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2456" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":280,"completed":257,"skipped":4189,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:50:41.277: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7085
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:278
[BeforeEach] Kubectl replace
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1897
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jan 19 17:50:41.493: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-7085'
Jan 19 17:50:41.664: INFO: stderr: ""
Jan 19 17:50:41.664: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jan 19 17:50:46.714: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod e2e-test-httpd-pod --namespace=kubectl-7085 -o json'
Jan 19 17:50:46.849: INFO: stderr: ""
Jan 19 17:50:46.849: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.64.1.50/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2020-01-19T17:50:41Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-7085\",\n        \"resourceVersion\": \"34961\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7085/pods/e2e-test-httpd-pod\",\n        \"uid\": \"b116c936-ad38-4769-82cf-caed280c3ec0\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-tmxnk\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-tmxnk\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-tmxnk\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-19T17:50:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-19T17:50:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-19T17:50:44Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-19T17:50:41Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://3944e37189fbef90d357acc50f6812683123825fabb9d74252646b9e1ea1ab44\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-01-19T17:50:43Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.64.1.50\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.64.1.50\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-01-19T17:50:41Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 19 17:50:46.849: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config replace -f - --namespace=kubectl-7085'
Jan 19 17:50:47.153: INFO: stderr: ""
Jan 19 17:50:47.153: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1902
Jan 19 17:50:47.170: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-httpd-pod --namespace=kubectl-7085'
Jan 19 17:50:49.665: INFO: stderr: ""
Jan 19 17:50:49.665: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:50:49.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7085" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":280,"completed":258,"skipped":4235,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:50:49.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7421
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 19 17:50:49.965: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 17:50:50.017: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 17:50:50.033: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc before test
Jan 19 17:50:50.056: INFO: calico-node-75w55 from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.056: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 17:50:50.056: INFO: node-exporter-md4n7 from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.056: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 17:50:50.056: INFO: node-problem-detector-pw6fz from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.056: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 17:50:50.056: INFO: kube-proxy-6m6gf from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.056: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 17:50:50.056: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz before test
Jan 19 17:50:50.136: INFO: dashboard-metrics-scraper-894778996-t484m from kubernetes-dashboard started at 2020-01-19 17:29:16 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 19 17:50:50.136: INFO: coredns-59c969ffb8-dckvt from kube-system started at 2020-01-19 16:05:44 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container coredns ready: true, restart count 0
Jan 19 17:50:50.136: INFO: blackbox-exporter-54bb5f55cc-d7k59 from kube-system started at 2020-01-19 16:04:56 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan 19 17:50:50.136: INFO: node-exporter-9w5fc from kube-system started at 2020-01-19 16:05:02 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 17:50:50.136: INFO: calico-typha-vertical-autoscaler-5769b74b58-k2rg7 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container autoscaler ready: true, restart count 4
Jan 19 17:50:50.136: INFO: calico-kube-controllers-79bcd784b6-t6nlg from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 19 17:50:50.136: INFO: addons-nginx-ingress-controller-7c75bb76db-82dr5 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jan 19 17:50:50.136: INFO: metrics-server-b8bffff9b-2lk2p from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container metrics-server ready: true, restart count 0
Jan 19 17:50:50.136: INFO: kubernetes-dashboard-5b855874f6-b7hp4 from kubernetes-dashboard started at 2020-01-19 17:29:16 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 19 17:50:50.136: INFO: calico-typha-deploy-9f6b455c4-7hxxh from kube-system started at 2020-01-19 17:29:16 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container calico-typha ready: true, restart count 0
Jan 19 17:50:50.136: INFO: kube-proxy-cpmsz from kube-system started at 2020-01-19 16:05:02 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 17:50:50.136: INFO: calico-node-6gx2z from kube-system started at 2020-01-19 16:04:56 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 17:50:50.136: INFO: node-problem-detector-q4slm from kube-system started at 2020-01-19 16:05:03 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 17:50:50.136: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-95f65778d-66xm5 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jan 19 17:50:50.136: INFO: coredns-59c969ffb8-z8twk from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container coredns ready: true, restart count 0
Jan 19 17:50:50.136: INFO: vpn-shoot-5d566c4d8b-klghn from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container vpn-shoot ready: true, restart count 0
Jan 19 17:50:50.136: INFO: calico-typha-horizontal-autoscaler-85c99966bb-czfm4 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:50:50.136: INFO: 	Container autoscaler ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: verifying the node has the label node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc
STEP: verifying the node has the label node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod addons-nginx-ingress-controller-7c75bb76db-82dr5 requesting resource cpu=100m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-95f65778d-66xm5 requesting resource cpu=0m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod blackbox-exporter-54bb5f55cc-d7k59 requesting resource cpu=5m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod calico-kube-controllers-79bcd784b6-t6nlg requesting resource cpu=0m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod calico-node-6gx2z requesting resource cpu=100m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod calico-node-75w55 requesting resource cpu=100m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc
Jan 19 17:50:50.248: INFO: Pod calico-typha-deploy-9f6b455c4-7hxxh requesting resource cpu=0m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod calico-typha-horizontal-autoscaler-85c99966bb-czfm4 requesting resource cpu=10m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod calico-typha-vertical-autoscaler-5769b74b58-k2rg7 requesting resource cpu=0m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod coredns-59c969ffb8-dckvt requesting resource cpu=50m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod coredns-59c969ffb8-z8twk requesting resource cpu=50m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod kube-proxy-6m6gf requesting resource cpu=20m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc
Jan 19 17:50:50.248: INFO: Pod kube-proxy-cpmsz requesting resource cpu=20m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod metrics-server-b8bffff9b-2lk2p requesting resource cpu=20m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod node-exporter-9w5fc requesting resource cpu=5m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod node-exporter-md4n7 requesting resource cpu=5m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc
Jan 19 17:50:50.248: INFO: Pod node-problem-detector-pw6fz requesting resource cpu=20m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc
Jan 19 17:50:50.248: INFO: Pod node-problem-detector-q4slm requesting resource cpu=20m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod vpn-shoot-5d566c4d8b-klghn requesting resource cpu=100m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod dashboard-metrics-scraper-894778996-t484m requesting resource cpu=0m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
Jan 19 17:50:50.248: INFO: Pod kubernetes-dashboard-5b855874f6-b7hp4 requesting resource cpu=50m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
STEP: Starting Pods to consume most of the cluster CPU.
Jan 19 17:50:50.248: INFO: Creating a pod which consumes cpu=1242m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc
Jan 19 17:50:50.268: INFO: Creating a pod which consumes cpu=973m on Node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4a8bc112-909f-4890-960d-e7831d4e672f.15eb5b17319e18d3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7421/filler-pod-4a8bc112-909f-4890-960d-e7831d4e672f to shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4a8bc112-909f-4890-960d-e7831d4e672f.15eb5b17a54fae3b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4a8bc112-909f-4890-960d-e7831d4e672f.15eb5b17c1085d46], Reason = [Created], Message = [Created container filler-pod-4a8bc112-909f-4890-960d-e7831d4e672f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4a8bc112-909f-4890-960d-e7831d4e672f.15eb5b17cd07eb6a], Reason = [Started], Message = [Started container filler-pod-4a8bc112-909f-4890-960d-e7831d4e672f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2dee249-b045-4407-b601-ae98ce3ea4f9.15eb5b1732a84f43], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7421/filler-pod-c2dee249-b045-4407-b601-ae98ce3ea4f9 to shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2dee249-b045-4407-b601-ae98ce3ea4f9.15eb5b17d110ac5e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2dee249-b045-4407-b601-ae98ce3ea4f9.15eb5b18022035ea], Reason = [Created], Message = [Created container filler-pod-c2dee249-b045-4407-b601-ae98ce3ea4f9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c2dee249-b045-4407-b601-ae98ce3ea4f9.15eb5b18175b3ead], Reason = [Started], Message = [Started container filler-pod-c2dee249-b045-4407-b601-ae98ce3ea4f9]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15eb5b189fa3aadc], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:50:57.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7421" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":280,"completed":259,"skipped":4258,"failed":0}
SSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:50:57.568: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:39
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:50:57.810: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-7c50a394-99fb-4d6f-8fd3-75a66a369775" in namespace "security-context-test-9327" to be "success or failure"
Jan 19 17:50:57.827: INFO: Pod "alpine-nnp-false-7c50a394-99fb-4d6f-8fd3-75a66a369775": Phase="Pending", Reason="", readiness=false. Elapsed: 16.815773ms
Jan 19 17:50:59.846: INFO: Pod "alpine-nnp-false-7c50a394-99fb-4d6f-8fd3-75a66a369775": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03578046s
Jan 19 17:51:01.863: INFO: Pod "alpine-nnp-false-7c50a394-99fb-4d6f-8fd3-75a66a369775": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052827872s
Jan 19 17:51:03.880: INFO: Pod "alpine-nnp-false-7c50a394-99fb-4d6f-8fd3-75a66a369775": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069957717s
Jan 19 17:51:05.897: INFO: Pod "alpine-nnp-false-7c50a394-99fb-4d6f-8fd3-75a66a369775": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.087364968s
Jan 19 17:51:05.897: INFO: Pod "alpine-nnp-false-7c50a394-99fb-4d6f-8fd3-75a66a369775" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:51:05.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9327" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":260,"skipped":4262,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:51:05.974: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2226
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 19 17:51:06.213: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 17:51:06.267: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 17:51:06.283: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc before test
Jan 19 17:51:06.310: INFO: calico-node-75w55 from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.310: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 17:51:06.310: INFO: node-exporter-md4n7 from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.310: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 17:51:06.310: INFO: node-problem-detector-pw6fz from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.310: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 17:51:06.310: INFO: kube-proxy-6m6gf from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.310: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 17:51:06.310: INFO: alpine-nnp-false-7c50a394-99fb-4d6f-8fd3-75a66a369775 from security-context-test-9327 started at 2020-01-19 17:50:57 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.310: INFO: 	Container alpine-nnp-false-7c50a394-99fb-4d6f-8fd3-75a66a369775 ready: false, restart count 0
Jan 19 17:51:06.310: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz before test
Jan 19 17:51:06.386: INFO: vpn-shoot-5d566c4d8b-klghn from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container vpn-shoot ready: true, restart count 0
Jan 19 17:51:06.386: INFO: calico-typha-horizontal-autoscaler-85c99966bb-czfm4 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container autoscaler ready: true, restart count 0
Jan 19 17:51:06.386: INFO: dashboard-metrics-scraper-894778996-t484m from kubernetes-dashboard started at 2020-01-19 17:29:16 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 19 17:51:06.386: INFO: blackbox-exporter-54bb5f55cc-d7k59 from kube-system started at 2020-01-19 16:04:56 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan 19 17:51:06.386: INFO: node-exporter-9w5fc from kube-system started at 2020-01-19 16:05:02 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 17:51:06.386: INFO: calico-typha-vertical-autoscaler-5769b74b58-k2rg7 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container autoscaler ready: true, restart count 4
Jan 19 17:51:06.386: INFO: calico-kube-controllers-79bcd784b6-t6nlg from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 19 17:51:06.386: INFO: addons-nginx-ingress-controller-7c75bb76db-82dr5 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jan 19 17:51:06.386: INFO: coredns-59c969ffb8-dckvt from kube-system started at 2020-01-19 16:05:44 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container coredns ready: true, restart count 0
Jan 19 17:51:06.386: INFO: calico-typha-deploy-9f6b455c4-7hxxh from kube-system started at 2020-01-19 17:29:16 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container calico-typha ready: true, restart count 0
Jan 19 17:51:06.386: INFO: kube-proxy-cpmsz from kube-system started at 2020-01-19 16:05:02 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 17:51:06.386: INFO: calico-node-6gx2z from kube-system started at 2020-01-19 16:04:56 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 17:51:06.386: INFO: node-problem-detector-q4slm from kube-system started at 2020-01-19 16:05:03 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 17:51:06.386: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-95f65778d-66xm5 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jan 19 17:51:06.386: INFO: coredns-59c969ffb8-z8twk from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container coredns ready: true, restart count 0
Jan 19 17:51:06.386: INFO: metrics-server-b8bffff9b-2lk2p from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container metrics-server ready: true, restart count 0
Jan 19 17:51:06.386: INFO: kubernetes-dashboard-5b855874f6-b7hp4 from kubernetes-dashboard started at 2020-01-19 17:29:16 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:06.386: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15eb5b1af6a5fc57], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:51:07.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2226" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":280,"completed":261,"skipped":4269,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:51:07.529: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:86
Jan 19 17:51:07.748: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 17:51:07.798: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 17:51:07.815: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc before test
Jan 19 17:51:07.840: INFO: node-exporter-md4n7 from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.840: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 17:51:07.840: INFO: calico-node-75w55 from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.840: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 17:51:07.840: INFO: kube-proxy-6m6gf from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.840: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 17:51:07.840: INFO: node-problem-detector-pw6fz from kube-system started at 2020-01-19 16:05:27 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.840: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 17:51:07.840: INFO: alpine-nnp-false-7c50a394-99fb-4d6f-8fd3-75a66a369775 from security-context-test-9327 started at 2020-01-19 17:50:57 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.840: INFO: 	Container alpine-nnp-false-7c50a394-99fb-4d6f-8fd3-75a66a369775 ready: false, restart count 0
Jan 19 17:51:07.840: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz before test
Jan 19 17:51:07.912: INFO: dashboard-metrics-scraper-894778996-t484m from kubernetes-dashboard started at 2020-01-19 17:29:16 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 19 17:51:07.912: INFO: coredns-59c969ffb8-dckvt from kube-system started at 2020-01-19 16:05:44 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container coredns ready: true, restart count 0
Jan 19 17:51:07.912: INFO: blackbox-exporter-54bb5f55cc-d7k59 from kube-system started at 2020-01-19 16:04:56 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan 19 17:51:07.912: INFO: node-exporter-9w5fc from kube-system started at 2020-01-19 16:05:02 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container node-exporter ready: true, restart count 0
Jan 19 17:51:07.912: INFO: calico-typha-vertical-autoscaler-5769b74b58-k2rg7 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container autoscaler ready: true, restart count 4
Jan 19 17:51:07.912: INFO: calico-kube-controllers-79bcd784b6-t6nlg from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 19 17:51:07.912: INFO: addons-nginx-ingress-controller-7c75bb76db-82dr5 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jan 19 17:51:07.912: INFO: metrics-server-b8bffff9b-2lk2p from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container metrics-server ready: true, restart count 0
Jan 19 17:51:07.912: INFO: kubernetes-dashboard-5b855874f6-b7hp4 from kubernetes-dashboard started at 2020-01-19 17:29:16 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 19 17:51:07.912: INFO: calico-typha-deploy-9f6b455c4-7hxxh from kube-system started at 2020-01-19 17:29:16 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container calico-typha ready: true, restart count 0
Jan 19 17:51:07.912: INFO: kube-proxy-cpmsz from kube-system started at 2020-01-19 16:05:02 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 19 17:51:07.912: INFO: calico-node-6gx2z from kube-system started at 2020-01-19 16:04:56 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container calico-node ready: true, restart count 0
Jan 19 17:51:07.912: INFO: node-problem-detector-q4slm from kube-system started at 2020-01-19 16:05:03 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 19 17:51:07.912: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-95f65778d-66xm5 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jan 19 17:51:07.912: INFO: coredns-59c969ffb8-z8twk from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container coredns ready: true, restart count 0
Jan 19 17:51:07.912: INFO: vpn-shoot-5d566c4d8b-klghn from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container vpn-shoot ready: true, restart count 0
Jan 19 17:51:07.912: INFO: calico-typha-horizontal-autoscaler-85c99966bb-czfm4 from kube-system started at 2020-01-19 16:05:42 +0000 UTC (1 container statuses recorded)
Jan 19 17:51:07.912: INFO: 	Container autoscaler ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2b5b1fe5-b823-40f6-a53b-aeba7991e0a4 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-2b5b1fe5-b823-40f6-a53b-aeba7991e0a4 off the node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2b5b1fe5-b823-40f6-a53b-aeba7991e0a4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:51:16.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3230" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:77
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":280,"completed":262,"skipped":4287,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:51:16.213: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4957
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-5530
STEP: Creating secret with name secret-test-0aa6203d-072d-43e6-aa42-6eab30e8d8d9
STEP: Creating a pod to test consume secrets
Jan 19 17:51:16.695: INFO: Waiting up to 5m0s for pod "pod-secrets-5fc50936-31b4-4201-91a5-d748b487fb07" in namespace "secrets-4957" to be "success or failure"
Jan 19 17:51:16.720: INFO: Pod "pod-secrets-5fc50936-31b4-4201-91a5-d748b487fb07": Phase="Pending", Reason="", readiness=false. Elapsed: 25.581315ms
Jan 19 17:51:18.740: INFO: Pod "pod-secrets-5fc50936-31b4-4201-91a5-d748b487fb07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045623583s
Jan 19 17:51:20.758: INFO: Pod "pod-secrets-5fc50936-31b4-4201-91a5-d748b487fb07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063019104s
STEP: Saw pod success
Jan 19 17:51:20.758: INFO: Pod "pod-secrets-5fc50936-31b4-4201-91a5-d748b487fb07" satisfied condition "success or failure"
Jan 19 17:51:20.774: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-secrets-5fc50936-31b4-4201-91a5-d748b487fb07 container secret-volume-test: <nil>
STEP: delete the pod
Jan 19 17:51:20.837: INFO: Waiting for pod pod-secrets-5fc50936-31b4-4201-91a5-d748b487fb07 to disappear
Jan 19 17:51:20.853: INFO: Pod pod-secrets-5fc50936-31b4-4201-91a5-d748b487fb07 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:51:20.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4957" for this suite.
STEP: Destroying namespace "secret-namespace-5530" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":280,"completed":263,"skipped":4306,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:51:20.924: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5004
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:51:21.833: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 17:51:23.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053081, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053081, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053081, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053081, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:51:26.943: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:51:27.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5004" for this suite.
STEP: Destroying namespace "webhook-5004-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":280,"completed":264,"skipped":4314,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:51:27.580: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1955
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward API volume plugin
Jan 19 17:51:27.824: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3fbe00a6-7d25-4192-89b8-93de84912e42" in namespace "projected-1955" to be "success or failure"
Jan 19 17:51:27.840: INFO: Pod "downwardapi-volume-3fbe00a6-7d25-4192-89b8-93de84912e42": Phase="Pending", Reason="", readiness=false. Elapsed: 16.387896ms
Jan 19 17:51:29.858: INFO: Pod "downwardapi-volume-3fbe00a6-7d25-4192-89b8-93de84912e42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033598428s
Jan 19 17:51:31.875: INFO: Pod "downwardapi-volume-3fbe00a6-7d25-4192-89b8-93de84912e42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051346329s
STEP: Saw pod success
Jan 19 17:51:31.876: INFO: Pod "downwardapi-volume-3fbe00a6-7d25-4192-89b8-93de84912e42" satisfied condition "success or failure"
Jan 19 17:51:31.892: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downwardapi-volume-3fbe00a6-7d25-4192-89b8-93de84912e42 container client-container: <nil>
STEP: delete the pod
Jan 19 17:51:31.948: INFO: Waiting for pod downwardapi-volume-3fbe00a6-7d25-4192-89b8-93de84912e42 to disappear
Jan 19 17:51:31.965: INFO: Pod downwardapi-volume-3fbe00a6-7d25-4192-89b8-93de84912e42 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:51:31.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1955" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":280,"completed":265,"skipped":4317,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:51:32.015: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7773
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 19 17:51:32.262: INFO: Waiting up to 5m0s for pod "pod-3d3d9e6d-86ba-4aaf-a33a-c434fcdff10a" in namespace "emptydir-7773" to be "success or failure"
Jan 19 17:51:32.279: INFO: Pod "pod-3d3d9e6d-86ba-4aaf-a33a-c434fcdff10a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.906899ms
Jan 19 17:51:34.297: INFO: Pod "pod-3d3d9e6d-86ba-4aaf-a33a-c434fcdff10a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035120478s
Jan 19 17:51:36.315: INFO: Pod "pod-3d3d9e6d-86ba-4aaf-a33a-c434fcdff10a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053138706s
STEP: Saw pod success
Jan 19 17:51:36.315: INFO: Pod "pod-3d3d9e6d-86ba-4aaf-a33a-c434fcdff10a" satisfied condition "success or failure"
Jan 19 17:51:36.332: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-3d3d9e6d-86ba-4aaf-a33a-c434fcdff10a container test-container: <nil>
STEP: delete the pod
Jan 19 17:51:36.377: INFO: Waiting for pod pod-3d3d9e6d-86ba-4aaf-a33a-c434fcdff10a to disappear
Jan 19 17:51:36.394: INFO: Pod pod-3d3d9e6d-86ba-4aaf-a33a-c434fcdff10a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:51:36.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7773" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":266,"skipped":4323,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:51:36.444: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9660
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jan 19 17:51:40.761: INFO: &Pod{ObjectMeta:{send-events-a0a819d5-fbc1-4c43-a358-2cd521c92e1f  events-9660 /api/v1/namespaces/events-9660/pods/send-events-a0a819d5-fbc1-4c43-a358-2cd521c92e1f d2f717ec-1f64-426e-9352-15550a295d1a 35433 0 2020-01-19 17:51:36 +0000 UTC <nil> <nil> map[name:foo time:674012985] map[cni.projectcalico.org/podIP:100.64.1.59/32 kubernetes.io/psp:e2e-test-privileged-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-r8tdt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-r8tdt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-r8tdt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:51:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:51:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:51:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:51:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:100.64.1.59,StartTime:2020-01-19 17:51:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-19 17:51:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.8,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:daf5332100521b1256d0e3c56d697a238eaec3af48897ed9167cbadd426773b5,ContainerID:docker://147199c676b792c44acdf853b7f1f950045558220f9b8f6b2c420c86f6405085,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jan 19 17:51:42.779: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jan 19 17:51:44.797: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:51:44.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9660" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":280,"completed":267,"skipped":4325,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:51:44.873: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1862
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 19 17:51:45.613: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053104, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053104, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053104, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053104, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 17:51:47.630: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053104, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053104, loc:(*time.Location)(0x7db5bc0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053104, loc:(*time.Location)(0x7db5bc0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63715053104, loc:(*time.Location)(0x7db5bc0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5f65f8c764\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 19 17:51:50.664: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:51:50.681: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5765-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:51:51.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1862" for this suite.
STEP: Destroying namespace "webhook-1862-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":280,"completed":268,"skipped":4326,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:51:51.760: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7490
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod pod-subpath-test-downwardapi-k2jc
STEP: Creating a pod to test atomic-volume-subpath
Jan 19 17:51:52.050: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-k2jc" in namespace "subpath-7490" to be "success or failure"
Jan 19 17:51:52.066: INFO: Pod "pod-subpath-test-downwardapi-k2jc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.305699ms
Jan 19 17:51:54.083: INFO: Pod "pod-subpath-test-downwardapi-k2jc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033708734s
Jan 19 17:51:56.102: INFO: Pod "pod-subpath-test-downwardapi-k2jc": Phase="Running", Reason="", readiness=true. Elapsed: 4.0522826s
Jan 19 17:51:58.119: INFO: Pod "pod-subpath-test-downwardapi-k2jc": Phase="Running", Reason="", readiness=true. Elapsed: 6.069392316s
Jan 19 17:52:00.137: INFO: Pod "pod-subpath-test-downwardapi-k2jc": Phase="Running", Reason="", readiness=true. Elapsed: 8.087271139s
Jan 19 17:52:02.154: INFO: Pod "pod-subpath-test-downwardapi-k2jc": Phase="Running", Reason="", readiness=true. Elapsed: 10.1046705s
Jan 19 17:52:04.172: INFO: Pod "pod-subpath-test-downwardapi-k2jc": Phase="Running", Reason="", readiness=true. Elapsed: 12.121876992s
Jan 19 17:52:06.189: INFO: Pod "pod-subpath-test-downwardapi-k2jc": Phase="Running", Reason="", readiness=true. Elapsed: 14.138858464s
Jan 19 17:52:08.207: INFO: Pod "pod-subpath-test-downwardapi-k2jc": Phase="Running", Reason="", readiness=true. Elapsed: 16.157100999s
Jan 19 17:52:10.226: INFO: Pod "pod-subpath-test-downwardapi-k2jc": Phase="Running", Reason="", readiness=true. Elapsed: 18.17597589s
Jan 19 17:52:12.248: INFO: Pod "pod-subpath-test-downwardapi-k2jc": Phase="Running", Reason="", readiness=true. Elapsed: 20.198044656s
Jan 19 17:52:14.265: INFO: Pod "pod-subpath-test-downwardapi-k2jc": Phase="Running", Reason="", readiness=true. Elapsed: 22.214788921s
Jan 19 17:52:16.281: INFO: Pod "pod-subpath-test-downwardapi-k2jc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.23168782s
STEP: Saw pod success
Jan 19 17:52:16.282: INFO: Pod "pod-subpath-test-downwardapi-k2jc" satisfied condition "success or failure"
Jan 19 17:52:16.298: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-subpath-test-downwardapi-k2jc container test-container-subpath-downwardapi-k2jc: <nil>
STEP: delete the pod
Jan 19 17:52:16.392: INFO: Waiting for pod pod-subpath-test-downwardapi-k2jc to disappear
Jan 19 17:52:16.408: INFO: Pod pod-subpath-test-downwardapi-k2jc no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-k2jc
Jan 19 17:52:16.408: INFO: Deleting pod "pod-subpath-test-downwardapi-k2jc" in namespace "subpath-7490"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:52:16.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7490" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":280,"completed":269,"skipped":4329,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:52:16.476: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap that has name configmap-test-emptyKey-2189d3cc-79ab-4992-97fa-5104b02c7a0d
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:52:16.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5922" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":280,"completed":270,"skipped":4341,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:52:16.755: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-831
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:69
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
Jan 19 17:52:16.990: INFO: Creating deployment "webserver-deployment"
Jan 19 17:52:17.007: INFO: Waiting for observed generation 1
Jan 19 17:52:19.043: INFO: Waiting for all required pods to come up
Jan 19 17:52:19.075: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 19 17:52:25.122: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 19 17:52:25.156: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 19 17:52:25.189: INFO: Updating deployment webserver-deployment
Jan 19 17:52:25.189: INFO: Waiting for observed generation 2
Jan 19 17:52:27.234: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 19 17:52:27.250: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 19 17:52:27.266: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 19 17:52:27.317: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 19 17:52:27.317: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 19 17:52:27.334: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 19 17:52:27.366: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 19 17:52:27.366: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 19 17:52:27.399: INFO: Updating deployment webserver-deployment
Jan 19 17:52:27.399: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 19 17:52:27.436: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 19 17:52:29.478: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:63
Jan 19 17:52:29.512: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-831 /apis/apps/v1/namespaces/deployment-831/deployments/webserver-deployment 14c058d9-6c54-48d0-b7e6-447dedffc3d0 35907 3 2020-01-19 17:52:16 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003b55958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-01-19 17:52:26 +0000 UTC,LastTransitionTime:2020-01-19 17:52:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-01-19 17:52:26 +0000 UTC,LastTransitionTime:2020-01-19 17:52:16 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 19 17:52:29.530: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-831 /apis/apps/v1/namespaces/deployment-831/replicasets/webserver-deployment-c7997dcc8 d727e0b6-0dbc-43f7-b9f2-99af8dfffebd 35906 3 2020-01-19 17:52:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 14c058d9-6c54-48d0-b7e6-447dedffc3d0 0xc0038b20c7 0xc0038b20c8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0038b2138 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 17:52:29.530: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 19 17:52:29.530: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-831 /apis/apps/v1/namespaces/deployment-831/replicasets/webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 35891 3 2020-01-19 17:52:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 14c058d9-6c54-48d0-b7e6-447dedffc3d0 0xc0038b2007 0xc0038b2008}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0038b2068 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 19 17:52:29.581: INFO: Pod "webserver-deployment-595b5b9587-26qsw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-26qsw webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-26qsw 7d82d82c-3880-4c9a-92e9-1ada175d8ad9 35902 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ad5e27 0xc003ad5e28}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.581: INFO: Pod "webserver-deployment-595b5b9587-49xpf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-49xpf webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-49xpf 243be682-5fe5-43b1-9812-1def2e1f0fcb 35914 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ad5f70 0xc003ad5f71}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.581: INFO: Pod "webserver-deployment-595b5b9587-7zt4b" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7zt4b webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-7zt4b 4f782b58-3f5f-4289-bf8b-27901c4dc7af 35759 0 2020-01-19 17:52:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.0.76/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae20c0 0xc003ae20c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:100.64.0.76,StartTime:2020-01-19 17:52:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-19 17:52:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ea02e02bd582a3fb91fd7bace271c3f5dc4b8c80516d409a859a1eb1cd77dbda,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.582: INFO: Pod "webserver-deployment-595b5b9587-crlnk" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-crlnk webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-crlnk 6428994c-fdd6-4058-9c6e-3651acfa97f8 35789 0 2020-01-19 17:52:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.1.66/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae2230 0xc003ae2231}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:100.64.1.66,StartTime:2020-01-19 17:52:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-19 17:52:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8ceb0661c0d484bf380f1688c2b7929a8197f80aed9bb32f1290f30c1173a798,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.66,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.582: INFO: Pod "webserver-deployment-595b5b9587-dxj8z" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dxj8z webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-dxj8z 74896c13-9343-46ac-87b3-08b265e7d929 35893 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae2390 0xc003ae2391}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.582: INFO: Pod "webserver-deployment-595b5b9587-f5qv4" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-f5qv4 webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-f5qv4 07814161-ff25-4357-99bf-99aa8ff45e0c 35898 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae24d0 0xc003ae24d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.582: INFO: Pod "webserver-deployment-595b5b9587-g4cz9" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-g4cz9 webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-g4cz9 38e697e7-d618-400f-8369-9b645bad7c3f 35763 0 2020-01-19 17:52:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.0.77/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae2630 0xc003ae2631}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:100.64.0.77,StartTime:2020-01-19 17:52:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-19 17:52:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c51787df2375b2cb28829712454de7630a793ced1960d097a587b8438edb9915,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.582: INFO: Pod "webserver-deployment-595b5b9587-g594x" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-g594x webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-g594x d7cbfc00-91ef-4429-b413-b07d423eceac 35903 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae2790 0xc003ae2791}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.582: INFO: Pod "webserver-deployment-595b5b9587-gwc2x" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-gwc2x webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-gwc2x 75a88d4e-3cc6-46a0-b9ff-bb2921c6b31e 35757 0 2020-01-19 17:52:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.0.75/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae28e0 0xc003ae28e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:100.64.0.75,StartTime:2020-01-19 17:52:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-19 17:52:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://46b395a11d4fd6798022f60c64699d6eb9b2384f6a769049426fc3aa36a172e1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.583: INFO: Pod "webserver-deployment-595b5b9587-jtqw2" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jtqw2 webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-jtqw2 34c4ead8-0623-4b14-9e83-83eb2b234e7d 35900 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae2a40 0xc003ae2a41}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.583: INFO: Pod "webserver-deployment-595b5b9587-kjslz" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-kjslz webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-kjslz d17aeee7-3b47-479b-9078-b7b354530ba5 35901 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae2b90 0xc003ae2b91}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.583: INFO: Pod "webserver-deployment-595b5b9587-m87xf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-m87xf webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-m87xf 5b0f98a9-ad75-40cc-a7b8-47b0a2591431 35896 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae2cd0 0xc003ae2cd1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.583: INFO: Pod "webserver-deployment-595b5b9587-mbvq8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mbvq8 webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-mbvq8 e362c7ca-2d8b-4592-aaac-c7fd4d52aa83 35904 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae2e10 0xc003ae2e11}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.583: INFO: Pod "webserver-deployment-595b5b9587-mxqjj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mxqjj webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-mxqjj f1a2697c-60f3-43b4-8be7-e2538539d1fc 35911 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae2f50 0xc003ae2f51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.583: INFO: Pod "webserver-deployment-595b5b9587-n7s8w" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-n7s8w webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-n7s8w 4c5229b9-cadd-4692-a4ec-36b6ee78cfbe 35770 0 2020-01-19 17:52:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.1.65/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae30a0 0xc003ae30a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:100.64.1.65,StartTime:2020-01-19 17:52:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-19 17:52:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://d8a0f0a13bb92353efa5a82c4b672aa24d04178ab95fde40275d91d33e3ea265,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.65,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.584: INFO: Pod "webserver-deployment-595b5b9587-szm8b" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-szm8b webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-szm8b 33bd85cb-8ebf-4a15-847e-2def97dea868 35780 0 2020-01-19 17:52:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.1.64/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae3220 0xc003ae3221}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:100.64.1.64,StartTime:2020-01-19 17:52:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-19 17:52:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e14a20099c4b13b9a60de8252a1157b38f2aaf784e84474d5df8fbfe266962e7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.584: INFO: Pod "webserver-deployment-595b5b9587-trk5c" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-trk5c webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-trk5c 4bea7de3-f181-4a1e-a605-fcb2375b44ff 35783 0 2020-01-19 17:52:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.1.62/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae3390 0xc003ae3391}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:100.64.1.62,StartTime:2020-01-19 17:52:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-19 17:52:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://08b527e42ae69e2f46e52b55c7cc8125c0badb27386242db7c16f23861cff98d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.584: INFO: Pod "webserver-deployment-595b5b9587-xwqxz" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xwqxz webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-xwqxz 77e24549-439c-471a-953b-bdc9c1d15589 35734 0 2020-01-19 17:52:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.0.74/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae3500 0xc003ae3501}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:100.64.0.74,StartTime:2020-01-19 17:52:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-01-19 17:52:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e9cd4d044b87f10249960798e39226ad006425806718be84905eac6598cf1d32,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.584: INFO: Pod "webserver-deployment-595b5b9587-z2wtr" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-z2wtr webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-z2wtr c0910ec1-6f86-475a-b7de-dbfa89f7b1a0 35908 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae3660 0xc003ae3661}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.584: INFO: Pod "webserver-deployment-595b5b9587-z9bqz" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-z9bqz webserver-deployment-595b5b9587- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-595b5b9587-z9bqz e2fbede2-f63b-42b4-abe1-50995f7d3bf9 35913 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 97ec3760-5c11-43b8-a4a4-009ea1157c5c 0xc003ae37a0 0xc003ae37a1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.584: INFO: Pod "webserver-deployment-c7997dcc8-465x6" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-465x6 webserver-deployment-c7997dcc8- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-c7997dcc8-465x6 7faeece8-2192-4d41-b90d-93595ae1e321 35899 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d727e0b6-0dbc-43f7-b9f2-99af8dfffebd 0xc003ae38e0 0xc003ae38e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.584: INFO: Pod "webserver-deployment-c7997dcc8-4p6dd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4p6dd webserver-deployment-c7997dcc8- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-c7997dcc8-4p6dd 0e00affb-813a-4296-946f-34c7619da8b7 35909 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d727e0b6-0dbc-43f7-b9f2-99af8dfffebd 0xc003ae3a40 0xc003ae3a41}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.584: INFO: Pod "webserver-deployment-c7997dcc8-9hhhv" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-9hhhv webserver-deployment-c7997dcc8- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-c7997dcc8-9hhhv c70afb7a-bcd8-41b4-9006-652e7740ef12 35919 0 2020-01-19 17:52:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.64.1.68/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d727e0b6-0dbc-43f7-b9f2-99af8dfffebd 0xc003ae3bb0 0xc003ae3bb1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2020-01-19 17:52:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.584: INFO: Pod "webserver-deployment-c7997dcc8-bkwmq" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-bkwmq webserver-deployment-c7997dcc8- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-c7997dcc8-bkwmq 39999cb3-8759-4edd-adef-25ccf3b88006 35839 0 2020-01-19 17:52:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.64.0.79/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d727e0b6-0dbc-43f7-b9f2-99af8dfffebd 0xc003ae3d30 0xc003ae3d31}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2020-01-19 17:52:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.585: INFO: Pod "webserver-deployment-c7997dcc8-cvkxt" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cvkxt webserver-deployment-c7997dcc8- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-c7997dcc8-cvkxt 56554459-7ca3-4a6e-a048-091f8953ecb6 35912 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d727e0b6-0dbc-43f7-b9f2-99af8dfffebd 0xc003ae3e90 0xc003ae3e91}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.585: INFO: Pod "webserver-deployment-c7997dcc8-dt4d5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-dt4d5 webserver-deployment-c7997dcc8- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-c7997dcc8-dt4d5 22c8c133-ede8-4ccc-b104-682a06b95794 35921 0 2020-01-19 17:52:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.64.1.70/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d727e0b6-0dbc-43f7-b9f2-99af8dfffebd 0xc003d44000 0xc003d44001}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2020-01-19 17:52:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.585: INFO: Pod "webserver-deployment-c7997dcc8-fdl5m" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-fdl5m webserver-deployment-c7997dcc8- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-c7997dcc8-fdl5m 2c68ac36-a038-4673-aa41-10a8e8ee5103 35910 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d727e0b6-0dbc-43f7-b9f2-99af8dfffebd 0xc003d443f0 0xc003d443f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.585: INFO: Pod "webserver-deployment-c7997dcc8-qlmnd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qlmnd webserver-deployment-c7997dcc8- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-c7997dcc8-qlmnd e56685fe-1a56-45ee-b48d-1dc685f18c95 35897 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d727e0b6-0dbc-43f7-b9f2-99af8dfffebd 0xc003d445e0 0xc003d445e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.585: INFO: Pod "webserver-deployment-c7997dcc8-qnxl7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qnxl7 webserver-deployment-c7997dcc8- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-c7997dcc8-qnxl7 23377303-7f4e-467c-8744-e4c540faabe9 35920 0 2020-01-19 17:52:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.64.1.69/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d727e0b6-0dbc-43f7-b9f2-99af8dfffebd 0xc003d44750 0xc003d44751}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2020-01-19 17:52:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.585: INFO: Pod "webserver-deployment-c7997dcc8-tdtrx" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-tdtrx webserver-deployment-c7997dcc8- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-c7997dcc8-tdtrx 0d58b718-f3ad-4b96-ad9e-2c53959027e6 35838 0 2020-01-19 17:52:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.64.0.78/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d727e0b6-0dbc-43f7-b9f2-99af8dfffebd 0xc003d448c0 0xc003d448c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2020-01-19 17:52:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.585: INFO: Pod "webserver-deployment-c7997dcc8-wbt22" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-wbt22 webserver-deployment-c7997dcc8- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-c7997dcc8-wbt22 6f8dbb38-2f50-47b5-967e-7c8b1855ac35 35895 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d727e0b6-0dbc-43f7-b9f2-99af8dfffebd 0xc003d44a40 0xc003d44a41}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.585: INFO: Pod "webserver-deployment-c7997dcc8-xft4k" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xft4k webserver-deployment-c7997dcc8- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-c7997dcc8-xft4k f2c1d58c-bef2-4403-93fa-378b70f4721a 35894 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d727e0b6-0dbc-43f7-b9f2-99af8dfffebd 0xc003d44ba0 0xc003d44ba1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 17:52:29.586: INFO: Pod "webserver-deployment-c7997dcc8-zbjks" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zbjks webserver-deployment-c7997dcc8- deployment-831 /api/v1/namespaces/deployment-831/pods/webserver-deployment-c7997dcc8-zbjks e578377c-1036-4b54-a86a-cd6acf0db2d4 35905 0 2020-01-19 17:52:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 d727e0b6-0dbc-43f7-b9f2-99af8dfffebd 0xc003d44d00 0xc003d44d01}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-m7ssx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-m7ssx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-m7ssx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmbjf-zbh-worker-1-875cbbc-w74fz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-01-19 17:52:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2020-01-19 17:52:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:52:29.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-831" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":280,"completed":271,"skipped":4346,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:52:29.623: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5178
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name projected-configmap-test-volume-c9e6d041-985f-4f7e-bfb0-a84dbef300b9
STEP: Creating a pod to test consume configMaps
Jan 19 17:52:29.887: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-18e5a867-3b8a-423e-97b3-0d0b0a7c3ee9" in namespace "projected-5178" to be "success or failure"
Jan 19 17:52:29.903: INFO: Pod "pod-projected-configmaps-18e5a867-3b8a-423e-97b3-0d0b0a7c3ee9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.226048ms
Jan 19 17:52:31.921: INFO: Pod "pod-projected-configmaps-18e5a867-3b8a-423e-97b3-0d0b0a7c3ee9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033573179s
Jan 19 17:52:33.938: INFO: Pod "pod-projected-configmaps-18e5a867-3b8a-423e-97b3-0d0b0a7c3ee9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050997973s
Jan 19 17:52:35.955: INFO: Pod "pod-projected-configmaps-18e5a867-3b8a-423e-97b3-0d0b0a7c3ee9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067682877s
Jan 19 17:52:37.972: INFO: Pod "pod-projected-configmaps-18e5a867-3b8a-423e-97b3-0d0b0a7c3ee9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.084984823s
Jan 19 17:52:39.989: INFO: Pod "pod-projected-configmaps-18e5a867-3b8a-423e-97b3-0d0b0a7c3ee9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.102230018s
Jan 19 17:52:42.007: INFO: Pod "pod-projected-configmaps-18e5a867-3b8a-423e-97b3-0d0b0a7c3ee9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.119515071s
Jan 19 17:52:44.024: INFO: Pod "pod-projected-configmaps-18e5a867-3b8a-423e-97b3-0d0b0a7c3ee9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.136893653s
STEP: Saw pod success
Jan 19 17:52:44.024: INFO: Pod "pod-projected-configmaps-18e5a867-3b8a-423e-97b3-0d0b0a7c3ee9" satisfied condition "success or failure"
Jan 19 17:52:44.041: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-projected-configmaps-18e5a867-3b8a-423e-97b3-0d0b0a7c3ee9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 19 17:52:44.088: INFO: Waiting for pod pod-projected-configmaps-18e5a867-3b8a-423e-97b3-0d0b0a7c3ee9 to disappear
Jan 19 17:52:44.105: INFO: Pod pod-projected-configmaps-18e5a867-3b8a-423e-97b3-0d0b0a7c3ee9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:52:44.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5178" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":280,"completed":272,"skipped":4365,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:52:44.160: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9296
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating projection with secret that has name projected-secret-test-5c4c9882-e4c6-4db5-b756-047c35cd07e7
STEP: Creating a pod to test consume secrets
Jan 19 17:52:44.413: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c9a5d413-8493-4c15-adac-babf2e8cc9b9" in namespace "projected-9296" to be "success or failure"
Jan 19 17:52:44.429: INFO: Pod "pod-projected-secrets-c9a5d413-8493-4c15-adac-babf2e8cc9b9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.2206ms
Jan 19 17:52:46.447: INFO: Pod "pod-projected-secrets-c9a5d413-8493-4c15-adac-babf2e8cc9b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03377314s
Jan 19 17:52:48.464: INFO: Pod "pod-projected-secrets-c9a5d413-8493-4c15-adac-babf2e8cc9b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050547652s
Jan 19 17:52:50.481: INFO: Pod "pod-projected-secrets-c9a5d413-8493-4c15-adac-babf2e8cc9b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.068030723s
STEP: Saw pod success
Jan 19 17:52:50.481: INFO: Pod "pod-projected-secrets-c9a5d413-8493-4c15-adac-babf2e8cc9b9" satisfied condition "success or failure"
Jan 19 17:52:50.498: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod pod-projected-secrets-c9a5d413-8493-4c15-adac-babf2e8cc9b9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 19 17:52:50.556: INFO: Waiting for pod pod-projected-secrets-c9a5d413-8493-4c15-adac-babf2e8cc9b9 to disappear
Jan 19 17:52:50.572: INFO: Pod pod-projected-secrets-c9a5d413-8493-4c15-adac-babf2e8cc9b9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:52:50.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9296" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":273,"skipped":4367,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:52:50.623: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-48
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:139
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: creating a service externalname-service with the type=ExternalName in namespace services-48
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-48
I0119 17:52:50.965854    7438 runners.go:189] Created replication controller with name: externalname-service, namespace: services-48, replica count: 2
I0119 17:52:54.016444    7438 runners.go:189] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 17:52:54.016: INFO: Creating new exec pod
Jan 19 17:52:59.107: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-48 execpodc79sd -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jan 19 17:52:59.693: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 19 17:52:59.694: INFO: stdout: ""
Jan 19 17:52:59.694: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-48 execpodc79sd -- /bin/sh -x -c nc -zv -t -w 2 100.104.198.22 80'
Jan 19 17:53:00.184: INFO: stderr: "+ nc -zv -t -w 2 100.104.198.22 80\nConnection to 100.104.198.22 80 port [tcp/http] succeeded!\n"
Jan 19 17:53:00.184: INFO: stdout: ""
Jan 19 17:53:00.185: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-48 execpodc79sd -- /bin/sh -x -c nc -zv -t -w 2 10.250.0.5 30663'
Jan 19 17:53:00.747: INFO: stderr: "+ nc -zv -t -w 2 10.250.0.5 30663\nConnection to 10.250.0.5 30663 port [tcp/30663] succeeded!\n"
Jan 19 17:53:00.747: INFO: stdout: ""
Jan 19 17:53:00.747: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmbjf-zbh.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-48 execpodc79sd -- /bin/sh -x -c nc -zv -t -w 2 10.250.0.4 30663'
Jan 19 17:53:01.315: INFO: stderr: "+ nc -zv -t -w 2 10.250.0.4 30663\nConnection to 10.250.0.4 30663 port [tcp/30663] succeeded!\n"
Jan 19 17:53:01.316: INFO: stdout: ""
Jan 19 17:53:01.316: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:53:01.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-48" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:143
•{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":280,"completed":274,"skipped":4376,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:53:01.414: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8857
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating pod liveness-65baa417-abf0-435c-a1de-42b91c56decf in namespace container-probe-8857
Jan 19 17:53:05.691: INFO: Started pod liveness-65baa417-abf0-435c-a1de-42b91c56decf in namespace container-probe-8857
STEP: checking the pod's current state and verifying that restartCount is present
Jan 19 17:53:05.707: INFO: Initial restart count of pod liveness-65baa417-abf0-435c-a1de-42b91c56decf is 0
Jan 19 17:53:23.882: INFO: Restart count of pod container-probe-8857/liveness-65baa417-abf0-435c-a1de-42b91c56decf is now 1 (18.174061579s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:53:23.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8857" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":280,"completed":275,"skipped":4385,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:53:23.963: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2011
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating a pod to test downward api env vars
Jan 19 17:53:24.210: INFO: Waiting up to 5m0s for pod "downward-api-e684a3b6-df2e-4c08-ae9d-dc8d47069342" in namespace "downward-api-2011" to be "success or failure"
Jan 19 17:53:24.231: INFO: Pod "downward-api-e684a3b6-df2e-4c08-ae9d-dc8d47069342": Phase="Pending", Reason="", readiness=false. Elapsed: 20.718166ms
Jan 19 17:53:26.248: INFO: Pod "downward-api-e684a3b6-df2e-4c08-ae9d-dc8d47069342": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037848379s
Jan 19 17:53:28.266: INFO: Pod "downward-api-e684a3b6-df2e-4c08-ae9d-dc8d47069342": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055399348s
STEP: Saw pod success
Jan 19 17:53:28.266: INFO: Pod "downward-api-e684a3b6-df2e-4c08-ae9d-dc8d47069342" satisfied condition "success or failure"
Jan 19 17:53:28.283: INFO: Trying to get logs from node shoot--it--tmbjf-zbh-worker-1-875cbbc-5hgxc pod downward-api-e684a3b6-df2e-4c08-ae9d-dc8d47069342 container dapi-container: <nil>
STEP: delete the pod
Jan 19 17:53:28.332: INFO: Waiting for pod downward-api-e684a3b6-df2e-4c08-ae9d-dc8d47069342 to disappear
Jan 19 17:53:28.349: INFO: Pod downward-api-e684a3b6-df2e-4c08-ae9d-dc8d47069342 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:53:28.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2011" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":280,"completed":276,"skipped":4385,"failed":0}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:53:28.399: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8287
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:53:32.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8287" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":280,"completed":277,"skipped":4390,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:53:32.772: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6943
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:53:49.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6943" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":280,"completed":278,"skipped":4426,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:53:49.205: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3290
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Creating configMap with name configmap-test-upd-696eae35-2ef9-43c7-a55b-8f93b29c8ee7
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-696eae35-2ef9-43c7-a55b-8f93b29c8ee7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:53:55.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3290" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":280,"completed":279,"skipped":4430,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jan 19 17:53:55.718: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2026
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:721
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.17.1-beta.0.42+d224476cd0730b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jan 19 17:54:25.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2026" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":280,"completed":280,"skipped":4540,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSJan 19 17:54:25.983: INFO: Running AfterSuite actions on all nodes
Jan 19 17:54:25.983: INFO: Running AfterSuite actions on node 1
Jan 19 17:54:25.983: INFO: Skipping dumping logs from cluster
{"msg":"Test Suite completed","total":280,"completed":280,"skipped":4563,"failed":0}

Ran 280 of 4843 Specs in 4767.947 seconds
SUCCESS! -- 280 Passed | 0 Failed | 0 Flaked | 0 Pending | 4563 Skipped
PASS

Ginkgo ran 1 suite in 1h19m29.572799405s
Test Suite Passed
