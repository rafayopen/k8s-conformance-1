I0406 22:39:34.018077      23 test_context.go:410] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-285118746
I0406 22:39:34.018092      23 test_context.go:423] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0406 22:39:34.018208      23 e2e.go:124] Starting e2e run "5a78746c-4c31-4ce8-bcd6-68eb0651b974" on Ginkgo node 1
{"msg":"Test Suite starting","total":275,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1586212772 - Will randomize all specs
Will run 275 of 4992 specs

Apr  6 22:39:34.076: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
E0406 22:39:34.077477      23 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post http://localhost:8099/progress: dial tcp 127.0.0.1:8099: connect: connection refused
Apr  6 22:39:34.079: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr  6 22:39:34.108: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr  6 22:39:34.163: INFO: 18 / 18 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr  6 22:39:34.163: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Apr  6 22:39:34.163: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr  6 22:39:34.170: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Apr  6 22:39:34.170: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Apr  6 22:39:34.170: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'metricbeat' (0 seconds elapsed)
Apr  6 22:39:34.170: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-device-plugin-daemonset' (0 seconds elapsed)
Apr  6 22:39:34.170: INFO: e2e test version: v1.18.0
Apr  6 22:39:34.171: INFO: kube-apiserver version: v1.18.0
Apr  6 22:39:34.171: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 22:39:34.174: INFO: Cluster IP family: ipv4
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:39:34.174: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
Apr  6 22:39:34.225: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Apr  6 22:39:34.234: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4770
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 22:39:34.857: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr  6 22:39:37.350: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 22:39:39.356: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 22:39:41.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 22:39:43.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809574, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 22:39:46.375: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:39:46.377: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3048-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:39:47.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4770" for this suite.
STEP: Destroying namespace "webhook-4770-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:13.666 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":275,"completed":1,"skipped":6,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:39:47.840: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-741
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 22:39:48.568: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr  6 22:39:50.572: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809588, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809588, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809588, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721809588, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 22:39:53.630: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Apr  6 22:39:55.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 attach --namespace=webhook-741 to-be-attached-pod -i -c=container1'
Apr  6 22:39:58.983: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:39:58.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-741" for this suite.
STEP: Destroying namespace "webhook-741-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:11.294 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":275,"completed":2,"skipped":20,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:39:59.135: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3691
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr  6 22:40:04.439: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:40:04.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3691" for this suite.

â€¢ [SLOW TEST:5.446 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    on terminated container
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:133
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":275,"completed":3,"skipped":48,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:40:04.581: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-839
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr  6 22:40:04.770: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:40:24.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-839" for this suite.

â€¢ [SLOW TEST:20.162 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":275,"completed":4,"skipped":82,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:40:24.743: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4722
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-7dea91df-d615-4291-b657-768d65541200
STEP: Creating a pod to test consume configMaps
Apr  6 22:40:24.926: INFO: Waiting up to 5m0s for pod "pod-configmaps-04a251a9-8920-4817-a2cf-8266dd641917" in namespace "configmap-4722" to be "Succeeded or Failed"
Apr  6 22:40:24.931: INFO: Pod "pod-configmaps-04a251a9-8920-4817-a2cf-8266dd641917": Phase="Pending", Reason="", readiness=false. Elapsed: 5.039715ms
Apr  6 22:40:26.933: INFO: Pod "pod-configmaps-04a251a9-8920-4817-a2cf-8266dd641917": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007045772s
Apr  6 22:40:28.935: INFO: Pod "pod-configmaps-04a251a9-8920-4817-a2cf-8266dd641917": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009542529s
STEP: Saw pod success
Apr  6 22:40:28.935: INFO: Pod "pod-configmaps-04a251a9-8920-4817-a2cf-8266dd641917" satisfied condition "Succeeded or Failed"
Apr  6 22:40:28.938: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-configmaps-04a251a9-8920-4817-a2cf-8266dd641917 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 22:40:29.050: INFO: Waiting for pod pod-configmaps-04a251a9-8920-4817-a2cf-8266dd641917 to disappear
Apr  6 22:40:29.079: INFO: Pod pod-configmaps-04a251a9-8920-4817-a2cf-8266dd641917 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:40:29.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4722" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":275,"completed":5,"skipped":91,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:40:29.083: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4144
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4144
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4144
STEP: creating replication controller externalsvc in namespace services-4144
I0406 22:40:29.335461      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4144, replica count: 2
I0406 22:40:32.386108      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0406 22:40:35.386250      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0406 22:40:38.386390      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Apr  6 22:40:38.475: INFO: Creating new exec pod
Apr  6 22:40:40.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=services-4144 execpodlf6k5 -- /bin/sh -x -c nslookup clusterip-service'
Apr  6 22:40:40.658: INFO: stderr: "+ nslookup clusterip-service\n"
Apr  6 22:40:40.658: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-4144.svc.cluster.local\tcanonical name = externalsvc.services-4144.svc.cluster.local.\nName:\texternalsvc.services-4144.svc.cluster.local\nAddress: 10.100.38.45\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4144, will wait for the garbage collector to delete the pods
Apr  6 22:40:40.713: INFO: Deleting ReplicationController externalsvc took: 3.417943ms
Apr  6 22:40:41.413: INFO: Terminating ReplicationController externalsvc pods took: 700.134805ms
Apr  6 22:40:54.849: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:40:54.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4144" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:25.895 seconds]
[sig-network] Services
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":275,"completed":6,"skipped":91,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:40:54.978: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6209
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Apr  6 22:40:55.161: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:40:59.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6209" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":275,"completed":7,"skipped":103,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:40:59.031: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6894
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr  6 22:41:00.309: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:41:00.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6894" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":275,"completed":8,"skipped":107,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:41:00.357: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-9588/configmap-test-c0374e3e-c983-4e93-81a2-e7b04c0f20dc
STEP: Creating a pod to test consume configMaps
Apr  6 22:41:00.598: INFO: Waiting up to 5m0s for pod "pod-configmaps-e49c03f3-d191-4e6b-892d-843f98722348" in namespace "configmap-9588" to be "Succeeded or Failed"
Apr  6 22:41:00.617: INFO: Pod "pod-configmaps-e49c03f3-d191-4e6b-892d-843f98722348": Phase="Pending", Reason="", readiness=false. Elapsed: 18.893706ms
Apr  6 22:41:02.619: INFO: Pod "pod-configmaps-e49c03f3-d191-4e6b-892d-843f98722348": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021182035s
STEP: Saw pod success
Apr  6 22:41:02.619: INFO: Pod "pod-configmaps-e49c03f3-d191-4e6b-892d-843f98722348" satisfied condition "Succeeded or Failed"
Apr  6 22:41:02.621: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-configmaps-e49c03f3-d191-4e6b-892d-843f98722348 container env-test: <nil>
STEP: delete the pod
Apr  6 22:41:02.645: INFO: Waiting for pod pod-configmaps-e49c03f3-d191-4e6b-892d-843f98722348 to disappear
Apr  6 22:41:02.649: INFO: Pod pod-configmaps-e49c03f3-d191-4e6b-892d-843f98722348 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:41:02.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9588" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":275,"completed":9,"skipped":121,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:41:02.653: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2439
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:41:15.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2439" for this suite.

â€¢ [SLOW TEST:13.291 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":275,"completed":10,"skipped":138,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:41:15.944: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8301
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-fdf3857c-a767-42f3-851e-0924895c8925
STEP: Creating secret with name s-test-opt-upd-cee343fa-a74f-4df8-ac83-7ee992b8ace5
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-fdf3857c-a767-42f3-851e-0924895c8925
STEP: Updating secret s-test-opt-upd-cee343fa-a74f-4df8-ac83-7ee992b8ace5
STEP: Creating secret with name s-test-opt-create-e2144c2d-ea81-4897-823a-efe128e009ec
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:41:20.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8301" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":11,"skipped":146,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:41:20.302: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-2355
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2355
STEP: Creating statefulset with conflicting port in namespace statefulset-2355
STEP: Waiting until pod test-pod will start running in namespace statefulset-2355
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2355
Apr  6 22:41:30.659: INFO: Observed stateful pod in namespace: statefulset-2355, name: ss-0, uid: fcd6bf5d-0719-4e4b-8a77-29ab9c107ebe, status phase: Failed. Waiting for statefulset controller to delete.
Apr  6 22:41:30.661: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2355
STEP: Removing pod with conflicting port in namespace statefulset-2355
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2355 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Apr  6 22:41:34.722: INFO: Deleting all statefulset in ns statefulset-2355
Apr  6 22:41:34.723: INFO: Scaling statefulset ss to 0
Apr  6 22:41:54.775: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 22:41:54.777: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:41:54.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2355" for this suite.

â€¢ [SLOW TEST:34.498 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":275,"completed":12,"skipped":154,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:41:54.801: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4302
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1418
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr  6 22:41:54.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-4302'
Apr  6 22:41:55.010: INFO: stderr: ""
Apr  6 22:41:55.010: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1423
Apr  6 22:41:55.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 delete pods e2e-test-httpd-pod --namespace=kubectl-4302'
Apr  6 22:41:59.695: INFO: stderr: ""
Apr  6 22:41:59.696: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:41:59.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4302" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":275,"completed":13,"skipped":193,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:41:59.701: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3922
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-7b5175c2-e1c3-4326-a9ed-2994a145a1a0
STEP: Creating a pod to test consume configMaps
Apr  6 22:41:59.899: INFO: Waiting up to 5m0s for pod "pod-configmaps-351842c8-5619-450c-9f5b-89d3013e9627" in namespace "configmap-3922" to be "Succeeded or Failed"
Apr  6 22:41:59.939: INFO: Pod "pod-configmaps-351842c8-5619-450c-9f5b-89d3013e9627": Phase="Pending", Reason="", readiness=false. Elapsed: 40.21688ms
Apr  6 22:42:01.941: INFO: Pod "pod-configmaps-351842c8-5619-450c-9f5b-89d3013e9627": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.041929617s
STEP: Saw pod success
Apr  6 22:42:01.941: INFO: Pod "pod-configmaps-351842c8-5619-450c-9f5b-89d3013e9627" satisfied condition "Succeeded or Failed"
Apr  6 22:42:01.942: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-configmaps-351842c8-5619-450c-9f5b-89d3013e9627 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 22:42:01.981: INFO: Waiting for pod pod-configmaps-351842c8-5619-450c-9f5b-89d3013e9627 to disappear
Apr  6 22:42:01.994: INFO: Pod pod-configmaps-351842c8-5619-450c-9f5b-89d3013e9627 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:42:01.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3922" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":14,"skipped":195,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:42:02.048: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5790
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Apr  6 22:42:08.331: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0406 22:42:08.331520      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:42:08.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5790" for this suite.

â€¢ [SLOW TEST:6.287 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":275,"completed":15,"skipped":207,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:42:08.335: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6762
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Apr  6 22:42:10.525: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6762 PodName:pod-sharedvolume-377464af-947c-4025-ae98-3929abe91820 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 22:42:10.525: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 22:42:10.606: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:42:10.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6762" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":275,"completed":16,"skipped":232,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:42:10.610: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3984
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 22:42:11.369: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 22:42:14.400: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:42:24.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3984" for this suite.
STEP: Destroying namespace "webhook-3984-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:14.391 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":275,"completed":17,"skipped":239,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:42:25.002: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-37
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7975
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2245
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:42:31.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-37" for this suite.
STEP: Destroying namespace "nsdeletetest-7975" for this suite.
Apr  6 22:42:31.717: INFO: Namespace nsdeletetest-7975 was already deleted
STEP: Destroying namespace "nsdeletetest-2245" for this suite.

â€¢ [SLOW TEST:6.717 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":275,"completed":18,"skipped":246,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:42:31.718: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8705
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-18d1e13b-43c1-4b5d-b4ef-4239ad51f659
STEP: Creating a pod to test consume configMaps
Apr  6 22:42:31.977: INFO: Waiting up to 5m0s for pod "pod-configmaps-bc2e9e56-e5d8-4a52-814f-7888928d9680" in namespace "configmap-8705" to be "Succeeded or Failed"
Apr  6 22:42:32.042: INFO: Pod "pod-configmaps-bc2e9e56-e5d8-4a52-814f-7888928d9680": Phase="Pending", Reason="", readiness=false. Elapsed: 64.372379ms
Apr  6 22:42:34.067: INFO: Pod "pod-configmaps-bc2e9e56-e5d8-4a52-814f-7888928d9680": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.089198634s
STEP: Saw pod success
Apr  6 22:42:34.067: INFO: Pod "pod-configmaps-bc2e9e56-e5d8-4a52-814f-7888928d9680" satisfied condition "Succeeded or Failed"
Apr  6 22:42:34.068: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-configmaps-bc2e9e56-e5d8-4a52-814f-7888928d9680 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 22:42:34.100: INFO: Waiting for pod pod-configmaps-bc2e9e56-e5d8-4a52-814f-7888928d9680 to disappear
Apr  6 22:42:34.112: INFO: Pod pod-configmaps-bc2e9e56-e5d8-4a52-814f-7888928d9680 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:42:34.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8705" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":19,"skipped":276,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:42:34.117: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2607
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 22:42:34.336: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ea8f4c3-6861-40ff-8692-9b842b7465f0" in namespace "downward-api-2607" to be "Succeeded or Failed"
Apr  6 22:42:34.347: INFO: Pod "downwardapi-volume-4ea8f4c3-6861-40ff-8692-9b842b7465f0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.360561ms
Apr  6 22:42:36.348: INFO: Pod "downwardapi-volume-4ea8f4c3-6861-40ff-8692-9b842b7465f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012037563s
STEP: Saw pod success
Apr  6 22:42:36.348: INFO: Pod "downwardapi-volume-4ea8f4c3-6861-40ff-8692-9b842b7465f0" satisfied condition "Succeeded or Failed"
Apr  6 22:42:36.350: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-4ea8f4c3-6861-40ff-8692-9b842b7465f0 container client-container: <nil>
STEP: delete the pod
Apr  6 22:42:36.374: INFO: Waiting for pod downwardapi-volume-4ea8f4c3-6861-40ff-8692-9b842b7465f0 to disappear
Apr  6 22:42:36.386: INFO: Pod downwardapi-volume-4ea8f4c3-6861-40ff-8692-9b842b7465f0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:42:36.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2607" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":275,"completed":20,"skipped":285,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:42:36.398: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr  6 22:42:36.581: INFO: Waiting up to 5m0s for pod "pod-2f1f5b23-3a86-4a1b-a3d6-3648aa72920d" in namespace "emptydir-5559" to be "Succeeded or Failed"
Apr  6 22:42:36.586: INFO: Pod "pod-2f1f5b23-3a86-4a1b-a3d6-3648aa72920d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.741783ms
Apr  6 22:42:38.605: INFO: Pod "pod-2f1f5b23-3a86-4a1b-a3d6-3648aa72920d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024074632s
Apr  6 22:42:40.607: INFO: Pod "pod-2f1f5b23-3a86-4a1b-a3d6-3648aa72920d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025992515s
STEP: Saw pod success
Apr  6 22:42:40.607: INFO: Pod "pod-2f1f5b23-3a86-4a1b-a3d6-3648aa72920d" satisfied condition "Succeeded or Failed"
Apr  6 22:42:40.608: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-2f1f5b23-3a86-4a1b-a3d6-3648aa72920d container test-container: <nil>
STEP: delete the pod
Apr  6 22:42:40.659: INFO: Waiting for pod pod-2f1f5b23-3a86-4a1b-a3d6-3648aa72920d to disappear
Apr  6 22:42:40.667: INFO: Pod pod-2f1f5b23-3a86-4a1b-a3d6-3648aa72920d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:42:40.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5559" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":21,"skipped":294,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:42:40.671: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-416
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-bbda5958-29e9-4a8a-bff8-221ae4e2b11c
STEP: Creating a pod to test consume secrets
Apr  6 22:42:40.878: INFO: Waiting up to 5m0s for pod "pod-secrets-bf23a5ac-2529-421a-99ea-8a924797711b" in namespace "secrets-416" to be "Succeeded or Failed"
Apr  6 22:42:40.930: INFO: Pod "pod-secrets-bf23a5ac-2529-421a-99ea-8a924797711b": Phase="Pending", Reason="", readiness=false. Elapsed: 51.287402ms
Apr  6 22:42:42.932: INFO: Pod "pod-secrets-bf23a5ac-2529-421a-99ea-8a924797711b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.053357951s
STEP: Saw pod success
Apr  6 22:42:42.932: INFO: Pod "pod-secrets-bf23a5ac-2529-421a-99ea-8a924797711b" satisfied condition "Succeeded or Failed"
Apr  6 22:42:42.933: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-secrets-bf23a5ac-2529-421a-99ea-8a924797711b container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 22:42:42.971: INFO: Waiting for pod pod-secrets-bf23a5ac-2529-421a-99ea-8a924797711b to disappear
Apr  6 22:42:42.983: INFO: Pod pod-secrets-bf23a5ac-2529-421a-99ea-8a924797711b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:42:42.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-416" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":22,"skipped":298,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:42:42.987: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Apr  6 22:42:43.202: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  6 22:42:43.222: INFO: Waiting for terminating namespaces to be deleted...
Apr  6 22:42:43.224: INFO: 
Logging pods the kubelet thinks is on node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net before test
Apr  6 22:42:43.235: INFO: hpecp-fsmount-g2knb from hpecp started at 2020-04-06 22:04:39 +0000 UTC (1 container statuses recorded)
Apr  6 22:42:43.235: INFO: 	Container hpecp-fsmount ready: true, restart count 0
Apr  6 22:42:43.235: INFO: csi-nodeplugin-kdf-jn84c from mapr-csi started at 2020-04-06 22:05:19 +0000 UTC (3 container statuses recorded)
Apr  6 22:42:43.235: INFO: 	Container driver-registrar ready: true, restart count 0
Apr  6 22:42:43.235: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 22:42:43.235: INFO: 	Container mapr-kdf ready: true, restart count 0
Apr  6 22:42:43.235: INFO: coredns-66bff467f8-b269l from kube-system started at 2020-04-06 22:01:29 +0000 UTC (1 container statuses recorded)
Apr  6 22:42:43.235: INFO: 	Container coredns ready: true, restart count 0
Apr  6 22:42:43.235: INFO: metricbeat-lvk5c from kube-system started at 2020-04-06 22:05:52 +0000 UTC (1 container statuses recorded)
Apr  6 22:42:43.235: INFO: 	Container metricbeat ready: true, restart count 0
Apr  6 22:42:43.235: INFO: sonobuoy-systemd-logs-daemon-set-c4700b4a2f8941f0-ttgxk from sonobuoy started at 2020-04-06 22:39:04 +0000 UTC (2 container statuses recorded)
Apr  6 22:42:43.235: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 22:42:43.235: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 22:42:43.235: INFO: hpecp-agent-76d5b65798-qhxrd from hpecp started at 2020-04-06 22:04:06 +0000 UTC (1 container statuses recorded)
Apr  6 22:42:43.235: INFO: 	Container hpecp-agent ready: true, restart count 0
Apr  6 22:42:43.235: INFO: dashboard-metrics-scraper-69449465bc-55bss from kubernetes-dashboard started at 2020-04-06 22:05:13 +0000 UTC (1 container statuses recorded)
Apr  6 22:42:43.235: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr  6 22:42:43.235: INFO: csi-provisioner-kdf-0 from mapr-csi started at 2020-04-06 22:05:19 +0000 UTC (5 container statuses recorded)
Apr  6 22:42:43.235: INFO: 	Container csi-attacher ready: true, restart count 0
Apr  6 22:42:43.235: INFO: 	Container csi-provisioner ready: true, restart count 0
Apr  6 22:42:43.235: INFO: 	Container csi-snapshot ready: true, restart count 0
Apr  6 22:42:43.235: INFO: 	Container kdf-provisioner ready: true, restart count 0
Apr  6 22:42:43.235: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 22:42:43.235: INFO: canal-zxrh2 from kube-system started at 2020-04-06 22:01:03 +0000 UTC (3 container statuses recorded)
Apr  6 22:42:43.235: INFO: 	Container calico-node ready: true, restart count 1
Apr  6 22:42:43.235: INFO: 	Container install-cni ready: true, restart count 0
Apr  6 22:42:43.235: INFO: 	Container kube-flannel ready: true, restart count 2
Apr  6 22:42:43.235: INFO: kube-proxy-7w486 from kube-system started at 2020-04-06 22:01:48 +0000 UTC (1 container statuses recorded)
Apr  6 22:42:43.235: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 22:42:43.235: INFO: 
Logging pods the kubelet thinks is on node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net before test
Apr  6 22:42:43.240: INFO: kubedirector-64b6488f6d-qpssh from hpecp started at 2020-04-06 22:04:39 +0000 UTC (1 container statuses recorded)
Apr  6 22:42:43.240: INFO: 	Container kubedirector ready: true, restart count 0
Apr  6 22:42:43.240: INFO: kube-state-metrics-5b5f5b558d-j5nkq from kube-system started at 2020-04-06 22:05:15 +0000 UTC (1 container statuses recorded)
Apr  6 22:42:43.240: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr  6 22:42:43.240: INFO: sonobuoy from sonobuoy started at 2020-04-06 22:38:57 +0000 UTC (1 container statuses recorded)
Apr  6 22:42:43.240: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  6 22:42:43.240: INFO: sonobuoy-systemd-logs-daemon-set-c4700b4a2f8941f0-p5vhq from sonobuoy started at 2020-04-06 22:39:04 +0000 UTC (2 container statuses recorded)
Apr  6 22:42:43.240: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 22:42:43.240: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 22:42:43.240: INFO: coredns-66bff467f8-l8lxj from kube-system started at 2020-04-06 22:01:27 +0000 UTC (1 container statuses recorded)
Apr  6 22:42:43.240: INFO: 	Container coredns ready: true, restart count 0
Apr  6 22:42:43.240: INFO: csi-nodeplugin-kdf-4zrwc from mapr-csi started at 2020-04-06 22:05:19 +0000 UTC (3 container statuses recorded)
Apr  6 22:42:43.240: INFO: 	Container driver-registrar ready: true, restart count 0
Apr  6 22:42:43.240: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 22:42:43.240: INFO: 	Container mapr-kdf ready: true, restart count 0
Apr  6 22:42:43.240: INFO: metricbeat-6cf8f85fb5-zpb92 from kube-system started at 2020-04-06 22:05:52 +0000 UTC (1 container statuses recorded)
Apr  6 22:42:43.240: INFO: 	Container metricbeat ready: true, restart count 0
Apr  6 22:42:43.240: INFO: kube-proxy-6fzdv from kube-system started at 2020-04-06 22:01:48 +0000 UTC (1 container statuses recorded)
Apr  6 22:42:43.240: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 22:42:43.240: INFO: hpecp-fsmount-n7jkz from hpecp started at 2020-04-06 22:04:39 +0000 UTC (1 container statuses recorded)
Apr  6 22:42:43.240: INFO: 	Container hpecp-fsmount ready: true, restart count 0
Apr  6 22:42:43.240: INFO: kubernetes-dashboard-6b49d498f6-9prxk from kubernetes-dashboard started at 2020-04-06 22:05:13 +0000 UTC (1 container statuses recorded)
Apr  6 22:42:43.240: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr  6 22:42:43.240: INFO: canal-rg6q8 from kube-system started at 2020-04-06 22:01:03 +0000 UTC (3 container statuses recorded)
Apr  6 22:42:43.240: INFO: 	Container calico-node ready: true, restart count 1
Apr  6 22:42:43.240: INFO: 	Container install-cni ready: true, restart count 0
Apr  6 22:42:43.240: INFO: 	Container kube-flannel ready: true, restart count 2
Apr  6 22:42:43.240: INFO: metricbeat-dsbrd from kube-system started at 2020-04-06 22:05:52 +0000 UTC (1 container statuses recorded)
Apr  6 22:42:43.240: INFO: 	Container metricbeat ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: verifying the node has the label node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net
STEP: verifying the node has the label node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod hpecp-agent-76d5b65798-qhxrd requesting resource cpu=0m on Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod hpecp-fsmount-g2knb requesting resource cpu=250m on Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod hpecp-fsmount-n7jkz requesting resource cpu=250m on Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod kubedirector-64b6488f6d-qpssh requesting resource cpu=0m on Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod canal-rg6q8 requesting resource cpu=250m on Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod canal-zxrh2 requesting resource cpu=250m on Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod coredns-66bff467f8-b269l requesting resource cpu=100m on Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod coredns-66bff467f8-l8lxj requesting resource cpu=100m on Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod kube-proxy-6fzdv requesting resource cpu=0m on Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod kube-proxy-7w486 requesting resource cpu=0m on Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod kube-state-metrics-5b5f5b558d-j5nkq requesting resource cpu=0m on Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod metricbeat-6cf8f85fb5-zpb92 requesting resource cpu=100m on Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod metricbeat-dsbrd requesting resource cpu=100m on Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod metricbeat-lvk5c requesting resource cpu=100m on Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod dashboard-metrics-scraper-69449465bc-55bss requesting resource cpu=0m on Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod kubernetes-dashboard-6b49d498f6-9prxk requesting resource cpu=0m on Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod csi-nodeplugin-kdf-4zrwc requesting resource cpu=0m on Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod csi-nodeplugin-kdf-jn84c requesting resource cpu=0m on Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod csi-provisioner-kdf-0 requesting resource cpu=0m on Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod sonobuoy requesting resource cpu=0m on Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod sonobuoy-systemd-logs-daemon-set-c4700b4a2f8941f0-p5vhq requesting resource cpu=0m on Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
Apr  6 22:42:43.301: INFO: Pod sonobuoy-systemd-logs-daemon-set-c4700b4a2f8941f0-ttgxk requesting resource cpu=0m on Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net
STEP: Starting Pods to consume most of the cluster CPU.
Apr  6 22:42:43.301: INFO: Creating a pod which consumes cpu=1680m on Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
Apr  6 22:42:43.306: INFO: Creating a pod which consumes cpu=1750m on Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f486cd1-3d0c-47b7-99c3-19445fd6af0e.16035c493c2eccb6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3489/filler-pod-7f486cd1-3d0c-47b7-99c3-19445fd6af0e to mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f486cd1-3d0c-47b7-99c3-19445fd6af0e.16035c49609455d6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f486cd1-3d0c-47b7-99c3-19445fd6af0e.16035c4961e0d446], Reason = [Created], Message = [Created container filler-pod-7f486cd1-3d0c-47b7-99c3-19445fd6af0e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7f486cd1-3d0c-47b7-99c3-19445fd6af0e.16035c496709a14a], Reason = [Started], Message = [Started container filler-pod-7f486cd1-3d0c-47b7-99c3-19445fd6af0e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d0e2ce4-d284-4564-985a-bed87efafb97.16035c493b545ffb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3489/filler-pod-9d0e2ce4-d284-4564-985a-bed87efafb97 to mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d0e2ce4-d284-4564-985a-bed87efafb97.16035c495ce1ecd9], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d0e2ce4-d284-4564-985a-bed87efafb97.16035c495e2f4ec3], Reason = [Created], Message = [Created container filler-pod-9d0e2ce4-d284-4564-985a-bed87efafb97]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d0e2ce4-d284-4564-985a-bed87efafb97.16035c496345232a], Reason = [Started], Message = [Started container filler-pod-9d0e2ce4-d284-4564-985a-bed87efafb97]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16035c49b44382a8], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16035c49b64e8cd9], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:42:46.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3489" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":275,"completed":23,"skipped":298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:42:46.476: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1675
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:43:46.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1675" for this suite.

â€¢ [SLOW TEST:60.194 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":275,"completed":24,"skipped":354,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:43:46.670: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 22:43:46.871: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19beaaa7-7ce2-4a80-9e9e-02a02aad7e8c" in namespace "downward-api-8157" to be "Succeeded or Failed"
Apr  6 22:43:46.878: INFO: Pod "downwardapi-volume-19beaaa7-7ce2-4a80-9e9e-02a02aad7e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.111389ms
Apr  6 22:43:48.880: INFO: Pod "downwardapi-volume-19beaaa7-7ce2-4a80-9e9e-02a02aad7e8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009119525s
STEP: Saw pod success
Apr  6 22:43:48.880: INFO: Pod "downwardapi-volume-19beaaa7-7ce2-4a80-9e9e-02a02aad7e8c" satisfied condition "Succeeded or Failed"
Apr  6 22:43:48.881: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-19beaaa7-7ce2-4a80-9e9e-02a02aad7e8c container client-container: <nil>
STEP: delete the pod
Apr  6 22:43:48.926: INFO: Waiting for pod downwardapi-volume-19beaaa7-7ce2-4a80-9e9e-02a02aad7e8c to disappear
Apr  6 22:43:48.934: INFO: Pod downwardapi-volume-19beaaa7-7ce2-4a80-9e9e-02a02aad7e8c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:43:48.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8157" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":275,"completed":25,"skipped":359,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:43:48.941: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2624
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:43:49.177: INFO: The status of Pod test-webserver-a3a14faf-45cd-4bb9-8fc0-3163006171f5 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 22:43:51.179: INFO: The status of Pod test-webserver-a3a14faf-45cd-4bb9-8fc0-3163006171f5 is Running (Ready = false)
Apr  6 22:43:53.179: INFO: The status of Pod test-webserver-a3a14faf-45cd-4bb9-8fc0-3163006171f5 is Running (Ready = false)
Apr  6 22:43:55.179: INFO: The status of Pod test-webserver-a3a14faf-45cd-4bb9-8fc0-3163006171f5 is Running (Ready = false)
Apr  6 22:43:57.178: INFO: The status of Pod test-webserver-a3a14faf-45cd-4bb9-8fc0-3163006171f5 is Running (Ready = false)
Apr  6 22:43:59.179: INFO: The status of Pod test-webserver-a3a14faf-45cd-4bb9-8fc0-3163006171f5 is Running (Ready = false)
Apr  6 22:44:01.179: INFO: The status of Pod test-webserver-a3a14faf-45cd-4bb9-8fc0-3163006171f5 is Running (Ready = false)
Apr  6 22:44:03.179: INFO: The status of Pod test-webserver-a3a14faf-45cd-4bb9-8fc0-3163006171f5 is Running (Ready = false)
Apr  6 22:44:05.179: INFO: The status of Pod test-webserver-a3a14faf-45cd-4bb9-8fc0-3163006171f5 is Running (Ready = false)
Apr  6 22:44:07.179: INFO: The status of Pod test-webserver-a3a14faf-45cd-4bb9-8fc0-3163006171f5 is Running (Ready = false)
Apr  6 22:44:09.179: INFO: The status of Pod test-webserver-a3a14faf-45cd-4bb9-8fc0-3163006171f5 is Running (Ready = true)
Apr  6 22:44:09.180: INFO: Container started at 2020-04-06 22:43:49 +0000 UTC, pod became ready at 2020-04-06 22:44:07 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:44:09.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2624" for this suite.

â€¢ [SLOW TEST:20.244 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":275,"completed":26,"skipped":360,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:44:09.185: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9345
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-e1e5fd2c-ad1b-431f-8949-f8b02639ed70
STEP: Creating a pod to test consume configMaps
Apr  6 22:44:09.419: INFO: Waiting up to 5m0s for pod "pod-configmaps-43efc242-325c-4d1b-8df1-56ce799539cf" in namespace "configmap-9345" to be "Succeeded or Failed"
Apr  6 22:44:09.432: INFO: Pod "pod-configmaps-43efc242-325c-4d1b-8df1-56ce799539cf": Phase="Pending", Reason="", readiness=false. Elapsed: 12.222063ms
Apr  6 22:44:11.434: INFO: Pod "pod-configmaps-43efc242-325c-4d1b-8df1-56ce799539cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014258052s
STEP: Saw pod success
Apr  6 22:44:11.434: INFO: Pod "pod-configmaps-43efc242-325c-4d1b-8df1-56ce799539cf" satisfied condition "Succeeded or Failed"
Apr  6 22:44:11.435: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-configmaps-43efc242-325c-4d1b-8df1-56ce799539cf container configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 22:44:11.458: INFO: Waiting for pod pod-configmaps-43efc242-325c-4d1b-8df1-56ce799539cf to disappear
Apr  6 22:44:11.463: INFO: Pod pod-configmaps-43efc242-325c-4d1b-8df1-56ce799539cf no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:44:11.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9345" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":275,"completed":27,"skipped":389,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:44:11.467: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-75760a7c-b6d6-488e-b71e-4f5148f22564
STEP: Creating a pod to test consume secrets
Apr  6 22:44:11.641: INFO: Waiting up to 5m0s for pod "pod-secrets-9b01db26-52eb-4d65-b2fa-e53dbd5603a3" in namespace "secrets-5255" to be "Succeeded or Failed"
Apr  6 22:44:11.647: INFO: Pod "pod-secrets-9b01db26-52eb-4d65-b2fa-e53dbd5603a3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.940946ms
Apr  6 22:44:13.648: INFO: Pod "pod-secrets-9b01db26-52eb-4d65-b2fa-e53dbd5603a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007735453s
STEP: Saw pod success
Apr  6 22:44:13.648: INFO: Pod "pod-secrets-9b01db26-52eb-4d65-b2fa-e53dbd5603a3" satisfied condition "Succeeded or Failed"
Apr  6 22:44:13.650: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-secrets-9b01db26-52eb-4d65-b2fa-e53dbd5603a3 container secret-env-test: <nil>
STEP: delete the pod
Apr  6 22:44:13.664: INFO: Waiting for pod pod-secrets-9b01db26-52eb-4d65-b2fa-e53dbd5603a3 to disappear
Apr  6 22:44:13.670: INFO: Pod pod-secrets-9b01db26-52eb-4d65-b2fa-e53dbd5603a3 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:44:13.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5255" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":275,"completed":28,"skipped":393,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:44:13.675: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-553
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Apr  6 22:44:13.875: INFO: namespace kubectl-553
Apr  6 22:44:13.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 create -f - --namespace=kubectl-553'
Apr  6 22:44:14.166: INFO: stderr: ""
Apr  6 22:44:14.166: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Apr  6 22:44:15.168: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 22:44:15.168: INFO: Found 0 / 1
Apr  6 22:44:16.169: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 22:44:16.169: INFO: Found 1 / 1
Apr  6 22:44:16.169: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr  6 22:44:16.171: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 22:44:16.171: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr  6 22:44:16.171: INFO: wait on agnhost-master startup in kubectl-553 
Apr  6 22:44:16.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 logs agnhost-master-9t2gl agnhost-master --namespace=kubectl-553'
Apr  6 22:44:16.239: INFO: stderr: ""
Apr  6 22:44:16.239: INFO: stdout: "Paused\n"
STEP: exposing RC
Apr  6 22:44:16.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-553'
Apr  6 22:44:16.409: INFO: stderr: ""
Apr  6 22:44:16.409: INFO: stdout: "service/rm2 exposed\n"
Apr  6 22:44:16.412: INFO: Service rm2 in namespace kubectl-553 found.
STEP: exposing service
Apr  6 22:44:18.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-553'
Apr  6 22:44:18.518: INFO: stderr: ""
Apr  6 22:44:18.518: INFO: stdout: "service/rm3 exposed\n"
Apr  6 22:44:18.535: INFO: Service rm3 in namespace kubectl-553 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:44:20.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-553" for this suite.

â€¢ [SLOW TEST:6.911 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1119
    should create services for rc  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":275,"completed":29,"skipped":454,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:44:20.587: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6192
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-67b735da-3296-4cd4-a072-52fbfdb7db53
STEP: Creating secret with name s-test-opt-upd-a9c76934-5a27-4969-989a-c4dc60f8628e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-67b735da-3296-4cd4-a072-52fbfdb7db53
STEP: Updating secret s-test-opt-upd-a9c76934-5a27-4969-989a-c4dc60f8628e
STEP: Creating secret with name s-test-opt-create-6af5795c-b4ae-41bf-b9b5-e2e1d61300df
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:44:26.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6192" for this suite.

â€¢ [SLOW TEST:6.319 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":30,"skipped":467,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:44:26.906: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4485
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:44:31.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4485" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":275,"completed":31,"skipped":478,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:44:31.150: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5117
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:44:33.491: INFO: Waiting up to 5m0s for pod "client-envvars-352a6fc7-f04f-4f3b-8fd5-b020bef7150a" in namespace "pods-5117" to be "Succeeded or Failed"
Apr  6 22:44:33.494: INFO: Pod "client-envvars-352a6fc7-f04f-4f3b-8fd5-b020bef7150a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.478399ms
Apr  6 22:44:35.496: INFO: Pod "client-envvars-352a6fc7-f04f-4f3b-8fd5-b020bef7150a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004893188s
STEP: Saw pod success
Apr  6 22:44:35.496: INFO: Pod "client-envvars-352a6fc7-f04f-4f3b-8fd5-b020bef7150a" satisfied condition "Succeeded or Failed"
Apr  6 22:44:35.497: INFO: Trying to get logs from node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net pod client-envvars-352a6fc7-f04f-4f3b-8fd5-b020bef7150a container env3cont: <nil>
STEP: delete the pod
Apr  6 22:44:35.521: INFO: Waiting for pod client-envvars-352a6fc7-f04f-4f3b-8fd5-b020bef7150a to disappear
Apr  6 22:44:35.527: INFO: Pod client-envvars-352a6fc7-f04f-4f3b-8fd5-b020bef7150a no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:44:35.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5117" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":275,"completed":32,"skipped":491,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:44:35.531: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7116
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 22:44:35.718: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6b108a35-95c2-47f8-99c4-4ca8dd0b665b" in namespace "downward-api-7116" to be "Succeeded or Failed"
Apr  6 22:44:35.738: INFO: Pod "downwardapi-volume-6b108a35-95c2-47f8-99c4-4ca8dd0b665b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.792702ms
Apr  6 22:44:37.752: INFO: Pod "downwardapi-volume-6b108a35-95c2-47f8-99c4-4ca8dd0b665b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034418643s
STEP: Saw pod success
Apr  6 22:44:37.752: INFO: Pod "downwardapi-volume-6b108a35-95c2-47f8-99c4-4ca8dd0b665b" satisfied condition "Succeeded or Failed"
Apr  6 22:44:37.754: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-6b108a35-95c2-47f8-99c4-4ca8dd0b665b container client-container: <nil>
STEP: delete the pod
Apr  6 22:44:37.778: INFO: Waiting for pod downwardapi-volume-6b108a35-95c2-47f8-99c4-4ca8dd0b665b to disappear
Apr  6 22:44:37.862: INFO: Pod downwardapi-volume-6b108a35-95c2-47f8-99c4-4ca8dd0b665b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:44:37.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7116" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":275,"completed":33,"skipped":511,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:44:37.867: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-3c416346-673d-496b-886b-e97f121a9baf
STEP: Creating a pod to test consume configMaps
Apr  6 22:44:38.037: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fe89e0e5-4452-4a8a-8aa8-c2115808df30" in namespace "projected-8559" to be "Succeeded or Failed"
Apr  6 22:44:38.061: INFO: Pod "pod-projected-configmaps-fe89e0e5-4452-4a8a-8aa8-c2115808df30": Phase="Pending", Reason="", readiness=false. Elapsed: 24.069049ms
Apr  6 22:44:40.063: INFO: Pod "pod-projected-configmaps-fe89e0e5-4452-4a8a-8aa8-c2115808df30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025981682s
STEP: Saw pod success
Apr  6 22:44:40.063: INFO: Pod "pod-projected-configmaps-fe89e0e5-4452-4a8a-8aa8-c2115808df30" satisfied condition "Succeeded or Failed"
Apr  6 22:44:40.066: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-projected-configmaps-fe89e0e5-4452-4a8a-8aa8-c2115808df30 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 22:44:40.111: INFO: Waiting for pod pod-projected-configmaps-fe89e0e5-4452-4a8a-8aa8-c2115808df30 to disappear
Apr  6 22:44:40.117: INFO: Pod pod-projected-configmaps-fe89e0e5-4452-4a8a-8aa8-c2115808df30 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:44:40.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8559" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":275,"completed":34,"skipped":516,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:44:40.121: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-318
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-092e758f-54c9-44d3-b316-4b38de11b590
STEP: Creating a pod to test consume secrets
Apr  6 22:44:40.345: INFO: Waiting up to 5m0s for pod "pod-secrets-362e81b2-bc9f-4482-a39c-531991f8d50d" in namespace "secrets-318" to be "Succeeded or Failed"
Apr  6 22:44:40.350: INFO: Pod "pod-secrets-362e81b2-bc9f-4482-a39c-531991f8d50d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.390281ms
Apr  6 22:44:42.352: INFO: Pod "pod-secrets-362e81b2-bc9f-4482-a39c-531991f8d50d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007249472s
STEP: Saw pod success
Apr  6 22:44:42.352: INFO: Pod "pod-secrets-362e81b2-bc9f-4482-a39c-531991f8d50d" satisfied condition "Succeeded or Failed"
Apr  6 22:44:42.353: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-secrets-362e81b2-bc9f-4482-a39c-531991f8d50d container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 22:44:42.409: INFO: Waiting for pod pod-secrets-362e81b2-bc9f-4482-a39c-531991f8d50d to disappear
Apr  6 22:44:42.467: INFO: Pod pod-secrets-362e81b2-bc9f-4482-a39c-531991f8d50d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:44:42.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-318" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":35,"skipped":537,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:44:42.502: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7301
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:44:58.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7301" for this suite.

â€¢ [SLOW TEST:16.472 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":275,"completed":36,"skipped":566,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:44:58.975: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1247
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-c6d594b3-1c1d-44e5-9ad4-50dafa9b5358
STEP: Creating a pod to test consume configMaps
Apr  6 22:44:59.167: INFO: Waiting up to 5m0s for pod "pod-configmaps-4f0a8de7-daa7-4311-8046-a73bc27a2e8c" in namespace "configmap-1247" to be "Succeeded or Failed"
Apr  6 22:44:59.173: INFO: Pod "pod-configmaps-4f0a8de7-daa7-4311-8046-a73bc27a2e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.336288ms
Apr  6 22:45:01.205: INFO: Pod "pod-configmaps-4f0a8de7-daa7-4311-8046-a73bc27a2e8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037457599s
STEP: Saw pod success
Apr  6 22:45:01.205: INFO: Pod "pod-configmaps-4f0a8de7-daa7-4311-8046-a73bc27a2e8c" satisfied condition "Succeeded or Failed"
Apr  6 22:45:01.206: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-configmaps-4f0a8de7-daa7-4311-8046-a73bc27a2e8c container configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 22:45:01.239: INFO: Waiting for pod pod-configmaps-4f0a8de7-daa7-4311-8046-a73bc27a2e8c to disappear
Apr  6 22:45:01.246: INFO: Pod pod-configmaps-4f0a8de7-daa7-4311-8046-a73bc27a2e8c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:45:01.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1247" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":275,"completed":37,"skipped":597,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:45:01.250: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8843
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:45:01.398: INFO: Creating deployment "test-recreate-deployment"
Apr  6 22:45:01.400: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr  6 22:45:01.430: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Apr  6 22:45:03.432: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr  6 22:45:03.433: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr  6 22:45:03.436: INFO: Updating deployment test-recreate-deployment
Apr  6 22:45:03.436: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Apr  6 22:45:03.754: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8843 /apis/apps/v1/namespaces/deployment-8843/deployments/test-recreate-deployment 106d3290-8942-43df-9a81-852e0b1bf173 12114 2 2020-04-06 22:45:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-04-06 22:45:03 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-04-06 22:45:03 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0037ece28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-04-06 22:45:03 +0000 UTC,LastTransitionTime:2020-04-06 22:45:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-d5667d9c7" is progressing.,LastUpdateTime:2020-04-06 22:45:03 +0000 UTC,LastTransitionTime:2020-04-06 22:45:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Apr  6 22:45:03.756: INFO: New ReplicaSet "test-recreate-deployment-d5667d9c7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-d5667d9c7  deployment-8843 /apis/apps/v1/namespaces/deployment-8843/replicasets/test-recreate-deployment-d5667d9c7 1f262b15-73b4-458e-812b-5fa57e3dcd75 12112 1 2020-04-06 22:45:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 106d3290-8942-43df-9a81-852e0b1bf173 0xc0037ed380 0xc0037ed381}] []  [{kube-controller-manager Update apps/v1 2020-04-06 22:45:03 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 49 48 54 100 51 50 57 48 45 56 57 52 50 45 52 51 100 102 45 57 97 56 49 45 56 53 50 101 48 98 49 98 102 49 55 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: d5667d9c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0037ed3f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 22:45:03.756: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr  6 22:45:03.756: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-74d98b5f7c  deployment-8843 /apis/apps/v1/namespaces/deployment-8843/replicasets/test-recreate-deployment-74d98b5f7c bd57b3aa-033d-485c-8bd6-6803e87b213d 12102 2 2020-04-06 22:45:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 106d3290-8942-43df-9a81-852e0b1bf173 0xc0037ed287 0xc0037ed288}] []  [{kube-controller-manager Update apps/v1 2020-04-06 22:45:03 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 49 48 54 100 51 50 57 48 45 56 57 52 50 45 52 51 100 102 45 57 97 56 49 45 56 53 50 101 48 98 49 98 102 49 55 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 74d98b5f7c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0037ed318 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 22:45:03.757: INFO: Pod "test-recreate-deployment-d5667d9c7-9scng" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-d5667d9c7-9scng test-recreate-deployment-d5667d9c7- deployment-8843 /api/v1/namespaces/deployment-8843/pods/test-recreate-deployment-d5667d9c7-9scng 595eb4e5-88ce-4332-9dd5-4bc46392115f 12113 0 2020-04-06 22:45:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet test-recreate-deployment-d5667d9c7 1f262b15-73b4-458e-812b-5fa57e3dcd75 0xc00382e990 0xc00382e991}] []  [{kube-controller-manager Update v1 2020-04-06 22:45:03 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 49 102 50 54 50 98 49 53 45 55 51 98 52 45 52 53 56 101 45 56 49 50 98 45 53 102 97 53 55 101 51 100 99 100 55 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-04-06 22:45:03 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fvcdr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fvcdr,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fvcdr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 22:45:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 22:45:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 22:45:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 22:45:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.74,PodIP:,StartTime:2020-04-06 22:45:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:45:03.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8843" for this suite.
â€¢{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":275,"completed":38,"skipped":601,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:45:03.762: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 22:45:04.237: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 22:45:07.335: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:45:07.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9782" for this suite.
STEP: Destroying namespace "webhook-9782-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":275,"completed":39,"skipped":610,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:45:07.855: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8966
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr  6 22:45:08.105: INFO: Waiting up to 5m0s for pod "pod-ddd207a1-adc6-43b1-8bfb-316c03cca5f3" in namespace "emptydir-8966" to be "Succeeded or Failed"
Apr  6 22:45:08.148: INFO: Pod "pod-ddd207a1-adc6-43b1-8bfb-316c03cca5f3": Phase="Pending", Reason="", readiness=false. Elapsed: 43.719362ms
Apr  6 22:45:10.150: INFO: Pod "pod-ddd207a1-adc6-43b1-8bfb-316c03cca5f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045326196s
STEP: Saw pod success
Apr  6 22:45:10.150: INFO: Pod "pod-ddd207a1-adc6-43b1-8bfb-316c03cca5f3" satisfied condition "Succeeded or Failed"
Apr  6 22:45:10.151: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-ddd207a1-adc6-43b1-8bfb-316c03cca5f3 container test-container: <nil>
STEP: delete the pod
Apr  6 22:45:10.208: INFO: Waiting for pod pod-ddd207a1-adc6-43b1-8bfb-316c03cca5f3 to disappear
Apr  6 22:45:10.228: INFO: Pod pod-ddd207a1-adc6-43b1-8bfb-316c03cca5f3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:45:10.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8966" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":40,"skipped":669,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:45:10.233: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8020
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-ee6d6dd1-74e3-48f2-aae9-3d9dbb3207ce in namespace container-probe-8020
Apr  6 22:45:12.563: INFO: Started pod liveness-ee6d6dd1-74e3-48f2-aae9-3d9dbb3207ce in namespace container-probe-8020
STEP: checking the pod's current state and verifying that restartCount is present
Apr  6 22:45:12.564: INFO: Initial restart count of pod liveness-ee6d6dd1-74e3-48f2-aae9-3d9dbb3207ce is 0
Apr  6 22:45:32.693: INFO: Restart count of pod container-probe-8020/liveness-ee6d6dd1-74e3-48f2-aae9-3d9dbb3207ce is now 1 (20.128970294s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:45:32.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8020" for this suite.

â€¢ [SLOW TEST:22.620 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":275,"completed":41,"skipped":670,"failed":0}
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:45:32.853: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9383
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Apr  6 22:45:33.094: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:45:36.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9383" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":275,"completed":42,"skipped":670,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:45:36.793: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Apr  6 22:45:36.961: INFO: Waiting up to 5m0s for pod "downward-api-18ff1445-04cf-4cd7-b27c-3743e4633061" in namespace "downward-api-7932" to be "Succeeded or Failed"
Apr  6 22:45:36.974: INFO: Pod "downward-api-18ff1445-04cf-4cd7-b27c-3743e4633061": Phase="Pending", Reason="", readiness=false. Elapsed: 13.212531ms
Apr  6 22:45:38.975: INFO: Pod "downward-api-18ff1445-04cf-4cd7-b27c-3743e4633061": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014645262s
STEP: Saw pod success
Apr  6 22:45:38.976: INFO: Pod "downward-api-18ff1445-04cf-4cd7-b27c-3743e4633061" satisfied condition "Succeeded or Failed"
Apr  6 22:45:38.977: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downward-api-18ff1445-04cf-4cd7-b27c-3743e4633061 container dapi-container: <nil>
STEP: delete the pod
Apr  6 22:45:39.002: INFO: Waiting for pod downward-api-18ff1445-04cf-4cd7-b27c-3743e4633061 to disappear
Apr  6 22:45:39.072: INFO: Pod downward-api-18ff1445-04cf-4cd7-b27c-3743e4633061 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:45:39.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7932" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":275,"completed":43,"skipped":687,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:45:39.077: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5619
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:45:39.370: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr  6 22:45:39.406: INFO: Number of nodes with available pods: 0
Apr  6 22:45:39.406: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr  6 22:45:39.431: INFO: Number of nodes with available pods: 0
Apr  6 22:45:39.431: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:45:40.433: INFO: Number of nodes with available pods: 0
Apr  6 22:45:40.433: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:45:41.433: INFO: Number of nodes with available pods: 1
Apr  6 22:45:41.433: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr  6 22:45:41.506: INFO: Number of nodes with available pods: 1
Apr  6 22:45:41.506: INFO: Number of running nodes: 0, number of available pods: 1
Apr  6 22:45:42.508: INFO: Number of nodes with available pods: 0
Apr  6 22:45:42.508: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr  6 22:45:42.570: INFO: Number of nodes with available pods: 0
Apr  6 22:45:42.570: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:45:43.574: INFO: Number of nodes with available pods: 0
Apr  6 22:45:43.574: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:45:44.572: INFO: Number of nodes with available pods: 0
Apr  6 22:45:44.572: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:45:45.572: INFO: Number of nodes with available pods: 0
Apr  6 22:45:45.572: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:45:46.575: INFO: Number of nodes with available pods: 1
Apr  6 22:45:46.575: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5619, will wait for the garbage collector to delete the pods
Apr  6 22:45:46.632: INFO: Deleting DaemonSet.extensions daemon-set took: 2.932017ms
Apr  6 22:45:47.232: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.151302ms
Apr  6 22:45:54.870: INFO: Number of nodes with available pods: 0
Apr  6 22:45:54.870: INFO: Number of running nodes: 0, number of available pods: 0
Apr  6 22:45:54.872: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5619/daemonsets","resourceVersion":"12612"},"items":null}

Apr  6 22:45:54.874: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5619/pods","resourceVersion":"12612"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:45:54.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5619" for this suite.

â€¢ [SLOW TEST:15.831 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":275,"completed":44,"skipped":782,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:45:54.908: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4595
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:45:55.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 version'
Apr  6 22:45:55.224: INFO: stderr: ""
Apr  6 22:45:55.224: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.0\", GitCommit:\"9e991415386e4cf155a24b1da15becaa390438d8\", GitTreeState:\"clean\", BuildDate:\"2020-03-25T14:58:59Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.0\", GitCommit:\"9e991415386e4cf155a24b1da15becaa390438d8\", GitTreeState:\"clean\", BuildDate:\"2020-03-25T14:50:46Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:45:55.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4595" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":275,"completed":45,"skipped":787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:45:55.228: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8501
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-fabdea9f-8d22-4e47-8af7-03255efa9c58
STEP: Creating a pod to test consume secrets
Apr  6 22:45:55.436: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4e4eb331-ff1a-4510-91a2-e37cfd37f1d9" in namespace "projected-8501" to be "Succeeded or Failed"
Apr  6 22:45:55.464: INFO: Pod "pod-projected-secrets-4e4eb331-ff1a-4510-91a2-e37cfd37f1d9": Phase="Pending", Reason="", readiness=false. Elapsed: 27.640745ms
Apr  6 22:45:57.466: INFO: Pod "pod-projected-secrets-4e4eb331-ff1a-4510-91a2-e37cfd37f1d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029764911s
STEP: Saw pod success
Apr  6 22:45:57.466: INFO: Pod "pod-projected-secrets-4e4eb331-ff1a-4510-91a2-e37cfd37f1d9" satisfied condition "Succeeded or Failed"
Apr  6 22:45:57.468: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-projected-secrets-4e4eb331-ff1a-4510-91a2-e37cfd37f1d9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  6 22:45:57.505: INFO: Waiting for pod pod-projected-secrets-4e4eb331-ff1a-4510-91a2-e37cfd37f1d9 to disappear
Apr  6 22:45:57.516: INFO: Pod pod-projected-secrets-4e4eb331-ff1a-4510-91a2-e37cfd37f1d9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:45:57.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8501" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":46,"skipped":825,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:45:57.560: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4090
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr  6 22:45:57.740: INFO: Waiting up to 5m0s for pod "pod-682b3934-6504-441e-a7de-cbd24e746fb6" in namespace "emptydir-4090" to be "Succeeded or Failed"
Apr  6 22:45:57.750: INFO: Pod "pod-682b3934-6504-441e-a7de-cbd24e746fb6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.656206ms
Apr  6 22:45:59.752: INFO: Pod "pod-682b3934-6504-441e-a7de-cbd24e746fb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011811914s
STEP: Saw pod success
Apr  6 22:45:59.752: INFO: Pod "pod-682b3934-6504-441e-a7de-cbd24e746fb6" satisfied condition "Succeeded or Failed"
Apr  6 22:45:59.753: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-682b3934-6504-441e-a7de-cbd24e746fb6 container test-container: <nil>
STEP: delete the pod
Apr  6 22:45:59.803: INFO: Waiting for pod pod-682b3934-6504-441e-a7de-cbd24e746fb6 to disappear
Apr  6 22:45:59.806: INFO: Pod pod-682b3934-6504-441e-a7de-cbd24e746fb6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:45:59.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4090" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":47,"skipped":828,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:45:59.853: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1780
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-f74016e8-ead9-491d-99a2-cce4554b5de0
STEP: Creating a pod to test consume secrets
Apr  6 22:46:00.001: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-098c07c3-ba38-4e6d-a27b-255ec730764b" in namespace "projected-1780" to be "Succeeded or Failed"
Apr  6 22:46:00.023: INFO: Pod "pod-projected-secrets-098c07c3-ba38-4e6d-a27b-255ec730764b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.994447ms
Apr  6 22:46:02.025: INFO: Pod "pod-projected-secrets-098c07c3-ba38-4e6d-a27b-255ec730764b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023736802s
STEP: Saw pod success
Apr  6 22:46:02.025: INFO: Pod "pod-projected-secrets-098c07c3-ba38-4e6d-a27b-255ec730764b" satisfied condition "Succeeded or Failed"
Apr  6 22:46:02.026: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-projected-secrets-098c07c3-ba38-4e6d-a27b-255ec730764b container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  6 22:46:02.062: INFO: Waiting for pod pod-projected-secrets-098c07c3-ba38-4e6d-a27b-255ec730764b to disappear
Apr  6 22:46:02.063: INFO: Pod pod-projected-secrets-098c07c3-ba38-4e6d-a27b-255ec730764b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:46:02.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1780" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":275,"completed":48,"skipped":828,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:46:02.069: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-201
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:46:08.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-201" for this suite.

â€¢ [SLOW TEST:6.235 seconds]
[sig-apps] Job
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":275,"completed":49,"skipped":849,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:46:08.304: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:46:08.463: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:46:10.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6350" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":275,"completed":50,"skipped":900,"failed":0}

------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:46:10.492: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2734
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr  6 22:46:10.709: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:10.728: INFO: Number of nodes with available pods: 0
Apr  6 22:46:10.728: INFO: Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:46:11.730: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:11.731: INFO: Number of nodes with available pods: 1
Apr  6 22:46:11.731: INFO: Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:46:12.731: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:12.732: INFO: Number of nodes with available pods: 2
Apr  6 22:46:12.732: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr  6 22:46:12.761: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:12.762: INFO: Number of nodes with available pods: 1
Apr  6 22:46:12.762: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:46:13.765: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:13.766: INFO: Number of nodes with available pods: 1
Apr  6 22:46:13.766: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:46:14.765: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:14.766: INFO: Number of nodes with available pods: 1
Apr  6 22:46:14.766: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:46:15.764: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:15.766: INFO: Number of nodes with available pods: 1
Apr  6 22:46:15.766: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:46:16.765: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:16.767: INFO: Number of nodes with available pods: 1
Apr  6 22:46:16.767: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:46:17.765: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:17.766: INFO: Number of nodes with available pods: 1
Apr  6 22:46:17.766: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:46:18.765: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:18.767: INFO: Number of nodes with available pods: 1
Apr  6 22:46:18.767: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:46:19.765: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:19.767: INFO: Number of nodes with available pods: 1
Apr  6 22:46:19.767: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:46:20.795: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:20.798: INFO: Number of nodes with available pods: 1
Apr  6 22:46:20.798: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:46:21.765: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:21.767: INFO: Number of nodes with available pods: 1
Apr  6 22:46:21.767: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:46:22.765: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:22.766: INFO: Number of nodes with available pods: 1
Apr  6 22:46:22.766: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:46:23.765: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:23.766: INFO: Number of nodes with available pods: 1
Apr  6 22:46:23.766: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:46:24.769: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:24.778: INFO: Number of nodes with available pods: 1
Apr  6 22:46:24.778: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 22:46:25.764: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 22:46:25.766: INFO: Number of nodes with available pods: 2
Apr  6 22:46:25.766: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2734, will wait for the garbage collector to delete the pods
Apr  6 22:46:25.821: INFO: Deleting DaemonSet.extensions daemon-set took: 2.591815ms
Apr  6 22:46:26.521: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.162315ms
Apr  6 22:46:37.422: INFO: Number of nodes with available pods: 0
Apr  6 22:46:37.422: INFO: Number of running nodes: 0, number of available pods: 0
Apr  6 22:46:37.423: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2734/daemonsets","resourceVersion":"13072"},"items":null}

Apr  6 22:46:37.424: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2734/pods","resourceVersion":"13072"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:46:37.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2734" for this suite.

â€¢ [SLOW TEST:26.940 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":275,"completed":51,"skipped":900,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:46:37.432: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-3a6ff481-c9f0-4cd1-bac9-0a28b86946f9
STEP: Creating a pod to test consume secrets
Apr  6 22:46:37.739: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3c86610d-6be9-4549-b087-2da2bb71e975" in namespace "projected-6932" to be "Succeeded or Failed"
Apr  6 22:46:37.753: INFO: Pod "pod-projected-secrets-3c86610d-6be9-4549-b087-2da2bb71e975": Phase="Pending", Reason="", readiness=false. Elapsed: 13.191332ms
Apr  6 22:46:39.755: INFO: Pod "pod-projected-secrets-3c86610d-6be9-4549-b087-2da2bb71e975": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015213221s
STEP: Saw pod success
Apr  6 22:46:39.755: INFO: Pod "pod-projected-secrets-3c86610d-6be9-4549-b087-2da2bb71e975" satisfied condition "Succeeded or Failed"
Apr  6 22:46:39.756: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-projected-secrets-3c86610d-6be9-4549-b087-2da2bb71e975 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  6 22:46:39.810: INFO: Waiting for pod pod-projected-secrets-3c86610d-6be9-4549-b087-2da2bb71e975 to disappear
Apr  6 22:46:39.816: INFO: Pod pod-projected-secrets-3c86610d-6be9-4549-b087-2da2bb71e975 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:46:39.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6932" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":52,"skipped":901,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:46:39.820: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-737
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-737
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  6 22:46:39.977: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr  6 22:46:40.012: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 22:46:42.014: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 22:46:44.014: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 22:46:46.014: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 22:46:48.014: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 22:46:50.026: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 22:46:52.015: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr  6 22:46:52.017: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Apr  6 22:46:54.052: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.192.1.70:8080/dial?request=hostname&protocol=udp&host=10.192.0.146&port=8081&tries=1'] Namespace:pod-network-test-737 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 22:46:54.052: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 22:46:54.137: INFO: Waiting for responses: map[]
Apr  6 22:46:54.139: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.192.1.70:8080/dial?request=hostname&protocol=udp&host=10.192.1.69&port=8081&tries=1'] Namespace:pod-network-test-737 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 22:46:54.139: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 22:46:54.219: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:46:54.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-737" for this suite.

â€¢ [SLOW TEST:14.403 seconds]
[sig-network] Networking
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":275,"completed":53,"skipped":915,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:46:54.224: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1764
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 22:46:54.999: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 22:46:58.037: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:47:10.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1764" for this suite.
STEP: Destroying namespace "webhook-1764-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:16.387 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":275,"completed":54,"skipped":946,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:47:10.610: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3326
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-c7908801-6868-4c1f-aeb1-b4a238326757
STEP: Creating a pod to test consume configMaps
Apr  6 22:47:10.804: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6add6d9f-4cf1-4c58-95fb-908d5b53523a" in namespace "projected-3326" to be "Succeeded or Failed"
Apr  6 22:47:10.826: INFO: Pod "pod-projected-configmaps-6add6d9f-4cf1-4c58-95fb-908d5b53523a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.734631ms
Apr  6 22:47:12.828: INFO: Pod "pod-projected-configmaps-6add6d9f-4cf1-4c58-95fb-908d5b53523a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024669236s
STEP: Saw pod success
Apr  6 22:47:12.828: INFO: Pod "pod-projected-configmaps-6add6d9f-4cf1-4c58-95fb-908d5b53523a" satisfied condition "Succeeded or Failed"
Apr  6 22:47:12.829: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-projected-configmaps-6add6d9f-4cf1-4c58-95fb-908d5b53523a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 22:47:12.903: INFO: Waiting for pod pod-projected-configmaps-6add6d9f-4cf1-4c58-95fb-908d5b53523a to disappear
Apr  6 22:47:12.916: INFO: Pod pod-projected-configmaps-6add6d9f-4cf1-4c58-95fb-908d5b53523a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:47:12.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3326" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":275,"completed":55,"skipped":957,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:47:12.921: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8958
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 22:47:13.099: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0175c5f3-34b8-487f-bf62-f9506e47ec20" in namespace "downward-api-8958" to be "Succeeded or Failed"
Apr  6 22:47:13.110: INFO: Pod "downwardapi-volume-0175c5f3-34b8-487f-bf62-f9506e47ec20": Phase="Pending", Reason="", readiness=false. Elapsed: 10.154126ms
Apr  6 22:47:15.111: INFO: Pod "downwardapi-volume-0175c5f3-34b8-487f-bf62-f9506e47ec20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012019805s
STEP: Saw pod success
Apr  6 22:47:15.111: INFO: Pod "downwardapi-volume-0175c5f3-34b8-487f-bf62-f9506e47ec20" satisfied condition "Succeeded or Failed"
Apr  6 22:47:15.113: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-0175c5f3-34b8-487f-bf62-f9506e47ec20 container client-container: <nil>
STEP: delete the pod
Apr  6 22:47:15.142: INFO: Waiting for pod downwardapi-volume-0175c5f3-34b8-487f-bf62-f9506e47ec20 to disappear
Apr  6 22:47:15.168: INFO: Pod downwardapi-volume-0175c5f3-34b8-487f-bf62-f9506e47ec20 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:47:15.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8958" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":275,"completed":56,"skipped":970,"failed":0}
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:47:15.171: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override command
Apr  6 22:47:15.339: INFO: Waiting up to 5m0s for pod "client-containers-bb3e3f6d-c149-4d2a-8ec9-124af83b3400" in namespace "containers-7493" to be "Succeeded or Failed"
Apr  6 22:47:15.357: INFO: Pod "client-containers-bb3e3f6d-c149-4d2a-8ec9-124af83b3400": Phase="Pending", Reason="", readiness=false. Elapsed: 18.648692ms
Apr  6 22:47:17.359: INFO: Pod "client-containers-bb3e3f6d-c149-4d2a-8ec9-124af83b3400": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02044606s
STEP: Saw pod success
Apr  6 22:47:17.359: INFO: Pod "client-containers-bb3e3f6d-c149-4d2a-8ec9-124af83b3400" satisfied condition "Succeeded or Failed"
Apr  6 22:47:17.360: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod client-containers-bb3e3f6d-c149-4d2a-8ec9-124af83b3400 container test-container: <nil>
STEP: delete the pod
Apr  6 22:47:17.408: INFO: Waiting for pod client-containers-bb3e3f6d-c149-4d2a-8ec9-124af83b3400 to disappear
Apr  6 22:47:17.415: INFO: Pod client-containers-bb3e3f6d-c149-4d2a-8ec9-124af83b3400 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:47:17.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7493" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":275,"completed":57,"skipped":976,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:47:17.426: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2277
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 22:47:17.847: INFO: Waiting up to 5m0s for pod "downwardapi-volume-46d7bdd0-bf44-48fa-b439-be5018d92175" in namespace "projected-2277" to be "Succeeded or Failed"
Apr  6 22:47:17.864: INFO: Pod "downwardapi-volume-46d7bdd0-bf44-48fa-b439-be5018d92175": Phase="Pending", Reason="", readiness=false. Elapsed: 16.899927ms
Apr  6 22:47:19.866: INFO: Pod "downwardapi-volume-46d7bdd0-bf44-48fa-b439-be5018d92175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018611208s
STEP: Saw pod success
Apr  6 22:47:19.866: INFO: Pod "downwardapi-volume-46d7bdd0-bf44-48fa-b439-be5018d92175" satisfied condition "Succeeded or Failed"
Apr  6 22:47:19.867: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-46d7bdd0-bf44-48fa-b439-be5018d92175 container client-container: <nil>
STEP: delete the pod
Apr  6 22:47:19.890: INFO: Waiting for pod downwardapi-volume-46d7bdd0-bf44-48fa-b439-be5018d92175 to disappear
Apr  6 22:47:19.897: INFO: Pod downwardapi-volume-46d7bdd0-bf44-48fa-b439-be5018d92175 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:47:19.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2277" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":275,"completed":58,"skipped":977,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:47:19.901: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5028
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:47:20.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5028" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":275,"completed":59,"skipped":1073,"failed":0}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:47:20.063: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-742
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Apr  6 22:47:20.237: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:47:22.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-742" for this suite.
â€¢{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":275,"completed":60,"skipped":1078,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:47:22.933: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8879
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl replace
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr  6 22:47:23.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-8879'
Apr  6 22:47:23.197: INFO: stderr: ""
Apr  6 22:47:23.197: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Apr  6 22:47:28.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pod e2e-test-httpd-pod --namespace=kubectl-8879 -o json'
Apr  6 22:47:28.304: INFO: stderr: ""
Apr  6 22:47:28.304: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.192.1.77/32\",\n            \"kubernetes.io/psp\": \"00-privileged\"\n        },\n        \"creationTimestamp\": \"2020-04-06T22:47:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:cni.projectcalico.org/podIP\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-04-06T22:47:23Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-04-06T22:47:23Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.192.1.77\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-04-06T22:47:23Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8879\",\n        \"resourceVersion\": \"13575\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8879/pods/e2e-test-httpd-pod\",\n        \"uid\": \"f74d4986-44c4-43df-a984-f9e70e3a0467\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-bnkqr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-bnkqr\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-bnkqr\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-04-06T22:47:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-04-06T22:47:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-04-06T22:47:23Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-04-06T22:47:23Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://9a55fa850611df3c429abc07345cf72b2e1fd6567212241c8a091cd18c92f7f1\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-04-06T22:47:23Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"16.0.8.74\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.192.1.77\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.192.1.77\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-04-06T22:47:23Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr  6 22:47:28.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 replace -f - --namespace=kubectl-8879'
Apr  6 22:47:28.528: INFO: stderr: ""
Apr  6 22:47:28.528: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Apr  6 22:47:28.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 delete pods e2e-test-httpd-pod --namespace=kubectl-8879'
Apr  6 22:47:34.767: INFO: stderr: ""
Apr  6 22:47:34.767: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:47:34.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8879" for this suite.

â€¢ [SLOW TEST:11.839 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1450
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":275,"completed":61,"skipped":1091,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:47:34.773: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9287
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Starting the proxy
Apr  6 22:47:34.921: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-285118746 proxy --unix-socket=/tmp/kubectl-proxy-unix690370309/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:47:34.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9287" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":275,"completed":62,"skipped":1096,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:47:34.968: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Apr  6 22:47:35.199: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  6 22:47:35.217: INFO: Waiting for terminating namespaces to be deleted...
Apr  6 22:47:35.218: INFO: 
Logging pods the kubelet thinks is on node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net before test
Apr  6 22:47:35.229: INFO: hpecp-fsmount-g2knb from hpecp started at 2020-04-06 22:04:39 +0000 UTC (1 container statuses recorded)
Apr  6 22:47:35.229: INFO: 	Container hpecp-fsmount ready: true, restart count 0
Apr  6 22:47:35.229: INFO: csi-nodeplugin-kdf-jn84c from mapr-csi started at 2020-04-06 22:05:19 +0000 UTC (3 container statuses recorded)
Apr  6 22:47:35.229: INFO: 	Container driver-registrar ready: true, restart count 0
Apr  6 22:47:35.229: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 22:47:35.229: INFO: 	Container mapr-kdf ready: true, restart count 0
Apr  6 22:47:35.229: INFO: coredns-66bff467f8-b269l from kube-system started at 2020-04-06 22:01:29 +0000 UTC (1 container statuses recorded)
Apr  6 22:47:35.229: INFO: 	Container coredns ready: true, restart count 0
Apr  6 22:47:35.229: INFO: hpecp-agent-76d5b65798-qhxrd from hpecp started at 2020-04-06 22:04:06 +0000 UTC (1 container statuses recorded)
Apr  6 22:47:35.229: INFO: 	Container hpecp-agent ready: true, restart count 0
Apr  6 22:47:35.229: INFO: dashboard-metrics-scraper-69449465bc-55bss from kubernetes-dashboard started at 2020-04-06 22:05:13 +0000 UTC (1 container statuses recorded)
Apr  6 22:47:35.229: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr  6 22:47:35.229: INFO: metricbeat-lvk5c from kube-system started at 2020-04-06 22:05:52 +0000 UTC (1 container statuses recorded)
Apr  6 22:47:35.229: INFO: 	Container metricbeat ready: true, restart count 0
Apr  6 22:47:35.229: INFO: sonobuoy-systemd-logs-daemon-set-c4700b4a2f8941f0-ttgxk from sonobuoy started at 2020-04-06 22:39:04 +0000 UTC (2 container statuses recorded)
Apr  6 22:47:35.229: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 22:47:35.229: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 22:47:35.229: INFO: canal-zxrh2 from kube-system started at 2020-04-06 22:01:03 +0000 UTC (3 container statuses recorded)
Apr  6 22:47:35.229: INFO: 	Container calico-node ready: true, restart count 1
Apr  6 22:47:35.229: INFO: 	Container install-cni ready: true, restart count 0
Apr  6 22:47:35.229: INFO: 	Container kube-flannel ready: true, restart count 2
Apr  6 22:47:35.229: INFO: kube-proxy-7w486 from kube-system started at 2020-04-06 22:01:48 +0000 UTC (1 container statuses recorded)
Apr  6 22:47:35.229: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 22:47:35.229: INFO: csi-provisioner-kdf-0 from mapr-csi started at 2020-04-06 22:05:19 +0000 UTC (5 container statuses recorded)
Apr  6 22:47:35.229: INFO: 	Container csi-attacher ready: true, restart count 0
Apr  6 22:47:35.229: INFO: 	Container csi-provisioner ready: true, restart count 0
Apr  6 22:47:35.229: INFO: 	Container csi-snapshot ready: true, restart count 0
Apr  6 22:47:35.229: INFO: 	Container kdf-provisioner ready: true, restart count 0
Apr  6 22:47:35.229: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 22:47:35.229: INFO: 
Logging pods the kubelet thinks is on node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net before test
Apr  6 22:47:35.235: INFO: metricbeat-dsbrd from kube-system started at 2020-04-06 22:05:52 +0000 UTC (1 container statuses recorded)
Apr  6 22:47:35.235: INFO: 	Container metricbeat ready: true, restart count 0
Apr  6 22:47:35.235: INFO: sonobuoy from sonobuoy started at 2020-04-06 22:38:57 +0000 UTC (1 container statuses recorded)
Apr  6 22:47:35.235: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  6 22:47:35.235: INFO: sonobuoy-systemd-logs-daemon-set-c4700b4a2f8941f0-p5vhq from sonobuoy started at 2020-04-06 22:39:04 +0000 UTC (2 container statuses recorded)
Apr  6 22:47:35.235: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 22:47:35.235: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 22:47:35.235: INFO: coredns-66bff467f8-l8lxj from kube-system started at 2020-04-06 22:01:27 +0000 UTC (1 container statuses recorded)
Apr  6 22:47:35.235: INFO: 	Container coredns ready: true, restart count 0
Apr  6 22:47:35.235: INFO: kubedirector-64b6488f6d-qpssh from hpecp started at 2020-04-06 22:04:39 +0000 UTC (1 container statuses recorded)
Apr  6 22:47:35.235: INFO: 	Container kubedirector ready: true, restart count 0
Apr  6 22:47:35.235: INFO: kube-state-metrics-5b5f5b558d-j5nkq from kube-system started at 2020-04-06 22:05:15 +0000 UTC (1 container statuses recorded)
Apr  6 22:47:35.235: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr  6 22:47:35.235: INFO: kube-proxy-6fzdv from kube-system started at 2020-04-06 22:01:48 +0000 UTC (1 container statuses recorded)
Apr  6 22:47:35.235: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 22:47:35.235: INFO: csi-nodeplugin-kdf-4zrwc from mapr-csi started at 2020-04-06 22:05:19 +0000 UTC (3 container statuses recorded)
Apr  6 22:47:35.235: INFO: 	Container driver-registrar ready: true, restart count 0
Apr  6 22:47:35.235: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 22:47:35.235: INFO: 	Container mapr-kdf ready: true, restart count 0
Apr  6 22:47:35.235: INFO: metricbeat-6cf8f85fb5-zpb92 from kube-system started at 2020-04-06 22:05:52 +0000 UTC (1 container statuses recorded)
Apr  6 22:47:35.235: INFO: 	Container metricbeat ready: true, restart count 0
Apr  6 22:47:35.235: INFO: canal-rg6q8 from kube-system started at 2020-04-06 22:01:03 +0000 UTC (3 container statuses recorded)
Apr  6 22:47:35.235: INFO: 	Container calico-node ready: true, restart count 1
Apr  6 22:47:35.235: INFO: 	Container install-cni ready: true, restart count 0
Apr  6 22:47:35.235: INFO: 	Container kube-flannel ready: true, restart count 2
Apr  6 22:47:35.235: INFO: hpecp-fsmount-n7jkz from hpecp started at 2020-04-06 22:04:39 +0000 UTC (1 container statuses recorded)
Apr  6 22:47:35.235: INFO: 	Container hpecp-fsmount ready: true, restart count 0
Apr  6 22:47:35.235: INFO: kubernetes-dashboard-6b49d498f6-9prxk from kubernetes-dashboard started at 2020-04-06 22:05:13 +0000 UTC (1 container statuses recorded)
Apr  6 22:47:35.235: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-454673d9-9db3-4f6b-bbe2-3a09cb910f17 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-454673d9-9db3-4f6b-bbe2-3a09cb910f17 off the node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
STEP: verifying the node doesn't have the label kubernetes.io/e2e-454673d9-9db3-4f6b-bbe2-3a09cb910f17
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:52:39.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-394" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

â€¢ [SLOW TEST:304.514 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":275,"completed":63,"skipped":1108,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:52:39.481: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4139
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test env composition
Apr  6 22:52:39.653: INFO: Waiting up to 5m0s for pod "var-expansion-755fff9a-8c8a-4d53-bf8e-713d0c7cfb9f" in namespace "var-expansion-4139" to be "Succeeded or Failed"
Apr  6 22:52:39.669: INFO: Pod "var-expansion-755fff9a-8c8a-4d53-bf8e-713d0c7cfb9f": Phase="Pending", Reason="", readiness=false. Elapsed: 15.819082ms
Apr  6 22:52:41.670: INFO: Pod "var-expansion-755fff9a-8c8a-4d53-bf8e-713d0c7cfb9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017388261s
STEP: Saw pod success
Apr  6 22:52:41.670: INFO: Pod "var-expansion-755fff9a-8c8a-4d53-bf8e-713d0c7cfb9f" satisfied condition "Succeeded or Failed"
Apr  6 22:52:41.671: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod var-expansion-755fff9a-8c8a-4d53-bf8e-713d0c7cfb9f container dapi-container: <nil>
STEP: delete the pod
Apr  6 22:52:41.719: INFO: Waiting for pod var-expansion-755fff9a-8c8a-4d53-bf8e-713d0c7cfb9f to disappear
Apr  6 22:52:41.725: INFO: Pod var-expansion-755fff9a-8c8a-4d53-bf8e-713d0c7cfb9f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:52:41.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4139" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":275,"completed":64,"skipped":1109,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:52:41.729: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-9301
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:52:41.885: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Creating first CR 
Apr  6 22:52:42.507: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-06T22:52:42Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-04-06T22:52:42Z]] name:name1 resourceVersion:14775 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a59bde93-7221-4719-a860-4070c84aa224] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Apr  6 22:52:52.511: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-06T22:52:52Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-04-06T22:52:52Z]] name:name2 resourceVersion:14841 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1f9910cf-c829-40ab-85c3-dd03beb55334] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Apr  6 22:53:02.515: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-06T22:52:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-04-06T22:53:02Z]] name:name1 resourceVersion:14880 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a59bde93-7221-4719-a860-4070c84aa224] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Apr  6 22:53:12.518: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-06T22:52:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-04-06T22:53:12Z]] name:name2 resourceVersion:14915 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1f9910cf-c829-40ab-85c3-dd03beb55334] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Apr  6 22:53:22.522: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-06T22:52:42Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-04-06T22:53:02Z]] name:name1 resourceVersion:14951 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:a59bde93-7221-4719-a860-4070c84aa224] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Apr  6 22:53:32.526: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-06T22:52:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-04-06T22:53:12Z]] name:name2 resourceVersion:14986 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1f9910cf-c829-40ab-85c3-dd03beb55334] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:53:43.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9301" for this suite.

â€¢ [SLOW TEST:61.313 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":275,"completed":65,"skipped":1112,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:53:43.043: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 22:53:43.876: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 22:53:46.903: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:53:46.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3753" for this suite.
STEP: Destroying namespace "webhook-3753-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":275,"completed":66,"skipped":1143,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:53:47.037: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8408
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 22:53:47.672: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 22:53:50.703: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:53:50.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8408" for this suite.
STEP: Destroying namespace "webhook-8408-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":275,"completed":67,"skipped":1152,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:53:50.978: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5558
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr  6 22:53:55.312: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  6 22:53:55.333: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  6 22:53:57.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  6 22:53:57.335: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  6 22:53:59.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  6 22:53:59.336: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  6 22:54:01.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  6 22:54:01.335: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  6 22:54:03.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  6 22:54:03.335: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  6 22:54:05.334: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  6 22:54:05.335: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:54:05.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5558" for this suite.

â€¢ [SLOW TEST:14.362 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":275,"completed":68,"skipped":1171,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:54:05.340: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9733
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr  6 22:54:05.559: INFO: Waiting up to 5m0s for pod "pod-ad9277fa-903c-435d-83d3-bdb39b4f3d00" in namespace "emptydir-9733" to be "Succeeded or Failed"
Apr  6 22:54:05.569: INFO: Pod "pod-ad9277fa-903c-435d-83d3-bdb39b4f3d00": Phase="Pending", Reason="", readiness=false. Elapsed: 9.828877ms
Apr  6 22:54:07.571: INFO: Pod "pod-ad9277fa-903c-435d-83d3-bdb39b4f3d00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011923071s
STEP: Saw pod success
Apr  6 22:54:07.571: INFO: Pod "pod-ad9277fa-903c-435d-83d3-bdb39b4f3d00" satisfied condition "Succeeded or Failed"
Apr  6 22:54:07.572: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-ad9277fa-903c-435d-83d3-bdb39b4f3d00 container test-container: <nil>
STEP: delete the pod
Apr  6 22:54:07.603: INFO: Waiting for pod pod-ad9277fa-903c-435d-83d3-bdb39b4f3d00 to disappear
Apr  6 22:54:07.608: INFO: Pod pod-ad9277fa-903c-435d-83d3-bdb39b4f3d00 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:54:07.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9733" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":69,"skipped":1175,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:54:07.612: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9712
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:54:07.765: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-10792dd8-e70e-4177-b6de-af661433225c" in namespace "security-context-test-9712" to be "Succeeded or Failed"
Apr  6 22:54:07.775: INFO: Pod "alpine-nnp-false-10792dd8-e70e-4177-b6de-af661433225c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.141228ms
Apr  6 22:54:09.777: INFO: Pod "alpine-nnp-false-10792dd8-e70e-4177-b6de-af661433225c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011855527s
Apr  6 22:54:11.778: INFO: Pod "alpine-nnp-false-10792dd8-e70e-4177-b6de-af661433225c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013626473s
Apr  6 22:54:11.779: INFO: Pod "alpine-nnp-false-10792dd8-e70e-4177-b6de-af661433225c" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:54:11.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9712" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":70,"skipped":1209,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:54:11.786: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7870
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7870, will wait for the garbage collector to delete the pods
Apr  6 22:54:14.059: INFO: Deleting Job.batch foo took: 3.145226ms
Apr  6 22:54:14.659: INFO: Terminating Job.batch foo pods took: 600.096532ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:54:54.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7870" for this suite.

â€¢ [SLOW TEST:42.978 seconds]
[sig-apps] Job
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":275,"completed":71,"skipped":1213,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:54:54.765: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9592
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr  6 22:54:55.961: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:54:55.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9592" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":275,"completed":72,"skipped":1271,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:54:56.004: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2406
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap that has name configmap-test-emptyKey-5dba5fd8-3e1f-4419-b5ad-5131d51aa188
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:54:56.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2406" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":275,"completed":73,"skipped":1274,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:54:56.174: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9447
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Apr  6 22:54:56.489: INFO: Waiting up to 5m0s for pod "downward-api-86e337f8-4030-4c99-9c7d-43f428ca297a" in namespace "downward-api-9447" to be "Succeeded or Failed"
Apr  6 22:54:56.507: INFO: Pod "downward-api-86e337f8-4030-4c99-9c7d-43f428ca297a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.847384ms
Apr  6 22:54:58.510: INFO: Pod "downward-api-86e337f8-4030-4c99-9c7d-43f428ca297a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020859964s
STEP: Saw pod success
Apr  6 22:54:58.510: INFO: Pod "downward-api-86e337f8-4030-4c99-9c7d-43f428ca297a" satisfied condition "Succeeded or Failed"
Apr  6 22:54:58.511: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downward-api-86e337f8-4030-4c99-9c7d-43f428ca297a container dapi-container: <nil>
STEP: delete the pod
Apr  6 22:54:58.551: INFO: Waiting for pod downward-api-86e337f8-4030-4c99-9c7d-43f428ca297a to disappear
Apr  6 22:54:58.581: INFO: Pod downward-api-86e337f8-4030-4c99-9c7d-43f428ca297a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:54:58.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9447" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":275,"completed":74,"skipped":1278,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:54:58.585: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr  6 22:55:02.943: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  6 22:55:02.970: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  6 22:55:04.970: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  6 22:55:04.988: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  6 22:55:06.970: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  6 22:55:06.972: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  6 22:55:08.970: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  6 22:55:08.972: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  6 22:55:10.970: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  6 22:55:10.972: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  6 22:55:12.970: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  6 22:55:12.972: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  6 22:55:14.970: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  6 22:55:14.972: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:55:14.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3960" for this suite.

â€¢ [SLOW TEST:16.394 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":275,"completed":75,"skipped":1305,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:55:14.979: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2981
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2981.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2981.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2981.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2981.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 22:55:25.252: INFO: DNS probes using dns-test-5db774a2-5bc7-4f81-b95b-6b13fb06a93c succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2981.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2981.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2981.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2981.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 22:55:27.374: INFO: File wheezy_udp@dns-test-service-3.dns-2981.svc.cluster.local from pod  dns-2981/dns-test-004dd608-d66b-40bc-bd19-f5bb56bf39bf contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 22:55:27.375: INFO: File jessie_udp@dns-test-service-3.dns-2981.svc.cluster.local from pod  dns-2981/dns-test-004dd608-d66b-40bc-bd19-f5bb56bf39bf contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 22:55:27.375: INFO: Lookups using dns-2981/dns-test-004dd608-d66b-40bc-bd19-f5bb56bf39bf failed for: [wheezy_udp@dns-test-service-3.dns-2981.svc.cluster.local jessie_udp@dns-test-service-3.dns-2981.svc.cluster.local]

Apr  6 22:55:32.378: INFO: File wheezy_udp@dns-test-service-3.dns-2981.svc.cluster.local from pod  dns-2981/dns-test-004dd608-d66b-40bc-bd19-f5bb56bf39bf contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 22:55:32.380: INFO: File jessie_udp@dns-test-service-3.dns-2981.svc.cluster.local from pod  dns-2981/dns-test-004dd608-d66b-40bc-bd19-f5bb56bf39bf contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 22:55:32.380: INFO: Lookups using dns-2981/dns-test-004dd608-d66b-40bc-bd19-f5bb56bf39bf failed for: [wheezy_udp@dns-test-service-3.dns-2981.svc.cluster.local jessie_udp@dns-test-service-3.dns-2981.svc.cluster.local]

Apr  6 22:55:37.378: INFO: File wheezy_udp@dns-test-service-3.dns-2981.svc.cluster.local from pod  dns-2981/dns-test-004dd608-d66b-40bc-bd19-f5bb56bf39bf contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 22:55:37.380: INFO: File jessie_udp@dns-test-service-3.dns-2981.svc.cluster.local from pod  dns-2981/dns-test-004dd608-d66b-40bc-bd19-f5bb56bf39bf contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 22:55:37.380: INFO: Lookups using dns-2981/dns-test-004dd608-d66b-40bc-bd19-f5bb56bf39bf failed for: [wheezy_udp@dns-test-service-3.dns-2981.svc.cluster.local jessie_udp@dns-test-service-3.dns-2981.svc.cluster.local]

Apr  6 22:55:42.378: INFO: File wheezy_udp@dns-test-service-3.dns-2981.svc.cluster.local from pod  dns-2981/dns-test-004dd608-d66b-40bc-bd19-f5bb56bf39bf contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 22:55:42.379: INFO: File jessie_udp@dns-test-service-3.dns-2981.svc.cluster.local from pod  dns-2981/dns-test-004dd608-d66b-40bc-bd19-f5bb56bf39bf contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 22:55:42.379: INFO: Lookups using dns-2981/dns-test-004dd608-d66b-40bc-bd19-f5bb56bf39bf failed for: [wheezy_udp@dns-test-service-3.dns-2981.svc.cluster.local jessie_udp@dns-test-service-3.dns-2981.svc.cluster.local]

Apr  6 22:55:47.380: INFO: File jessie_udp@dns-test-service-3.dns-2981.svc.cluster.local from pod  dns-2981/dns-test-004dd608-d66b-40bc-bd19-f5bb56bf39bf contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr  6 22:55:47.380: INFO: Lookups using dns-2981/dns-test-004dd608-d66b-40bc-bd19-f5bb56bf39bf failed for: [jessie_udp@dns-test-service-3.dns-2981.svc.cluster.local]

Apr  6 22:55:52.380: INFO: DNS probes using dns-test-004dd608-d66b-40bc-bd19-f5bb56bf39bf succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2981.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2981.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2981.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2981.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 22:55:54.533: INFO: DNS probes using dns-test-979a9065-e262-4596-846b-96598b51825a succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:55:54.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2981" for this suite.

â€¢ [SLOW TEST:39.649 seconds]
[sig-network] DNS
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":275,"completed":76,"skipped":1342,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:55:54.629: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-758
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-91697202-f857-40b0-961a-2511f6c7c70e
STEP: Creating a pod to test consume configMaps
Apr  6 22:55:54.856: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a7164779-ae55-42ed-a2d7-6cce9f408223" in namespace "projected-758" to be "Succeeded or Failed"
Apr  6 22:55:54.861: INFO: Pod "pod-projected-configmaps-a7164779-ae55-42ed-a2d7-6cce9f408223": Phase="Pending", Reason="", readiness=false. Elapsed: 5.239543ms
Apr  6 22:55:56.863: INFO: Pod "pod-projected-configmaps-a7164779-ae55-42ed-a2d7-6cce9f408223": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006753001s
STEP: Saw pod success
Apr  6 22:55:56.863: INFO: Pod "pod-projected-configmaps-a7164779-ae55-42ed-a2d7-6cce9f408223" satisfied condition "Succeeded or Failed"
Apr  6 22:55:56.864: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-projected-configmaps-a7164779-ae55-42ed-a2d7-6cce9f408223 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 22:55:56.889: INFO: Waiting for pod pod-projected-configmaps-a7164779-ae55-42ed-a2d7-6cce9f408223 to disappear
Apr  6 22:55:56.902: INFO: Pod pod-projected-configmaps-a7164779-ae55-42ed-a2d7-6cce9f408223 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:55:56.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-758" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":77,"skipped":1355,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:55:56.907: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3075
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:55:57.096: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr  6 22:55:59.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-3075 create -f -'
Apr  6 22:56:03.103: INFO: stderr: ""
Apr  6 22:56:03.103: INFO: stdout: "e2e-test-crd-publish-openapi-5725-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr  6 22:56:03.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-3075 delete e2e-test-crd-publish-openapi-5725-crds test-cr'
Apr  6 22:56:03.166: INFO: stderr: ""
Apr  6 22:56:03.166: INFO: stdout: "e2e-test-crd-publish-openapi-5725-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Apr  6 22:56:03.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-3075 apply -f -'
Apr  6 22:56:03.454: INFO: stderr: ""
Apr  6 22:56:03.454: INFO: stdout: "e2e-test-crd-publish-openapi-5725-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr  6 22:56:03.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-3075 delete e2e-test-crd-publish-openapi-5725-crds test-cr'
Apr  6 22:56:03.566: INFO: stderr: ""
Apr  6 22:56:03.566: INFO: stdout: "e2e-test-crd-publish-openapi-5725-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr  6 22:56:03.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 explain e2e-test-crd-publish-openapi-5725-crds'
Apr  6 22:56:03.908: INFO: stderr: ""
Apr  6 22:56:03.908: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5725-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:56:06.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3075" for this suite.

â€¢ [SLOW TEST:9.777 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":275,"completed":78,"skipped":1365,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:56:06.684: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7199
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:56:06.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 create -f - --namespace=kubectl-7199'
Apr  6 22:56:07.188: INFO: stderr: ""
Apr  6 22:56:07.188: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Apr  6 22:56:07.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 create -f - --namespace=kubectl-7199'
Apr  6 22:56:07.506: INFO: stderr: ""
Apr  6 22:56:07.506: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Apr  6 22:56:08.508: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 22:56:08.508: INFO: Found 0 / 1
Apr  6 22:56:09.508: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 22:56:09.508: INFO: Found 1 / 1
Apr  6 22:56:09.508: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr  6 22:56:09.510: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 22:56:09.510: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr  6 22:56:09.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 describe pod agnhost-master-l8m4m --namespace=kubectl-7199'
Apr  6 22:56:09.574: INFO: stderr: ""
Apr  6 22:56:09.574: INFO: stdout: "Name:         agnhost-master-l8m4m\nNamespace:    kubectl-7199\nPriority:     0\nNode:         mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/16.0.8.74\nStart Time:   Mon, 06 Apr 2020 22:56:07 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 10.192.1.96/32\n              kubernetes.io/psp: 00-privileged\nStatus:       Running\nIP:           10.192.1.96\nIPs:\n  IP:           10.192.1.96\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://658489532e20e66afa3862712e332bd964773dc0c684497ac9cda7d2634ab217\n    Image:          us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Image ID:       docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 06 Apr 2020 22:56:07 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rp5d4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-rp5d4:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-rp5d4\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                                  Message\n  ----    ------     ----       ----                                                  -------\n  Normal  Scheduled  <unknown>  default-scheduler                                     Successfully assigned kubectl-7199/agnhost-master-l8m4m to mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net\n  Normal  Pulled     2s         kubelet, mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net  Container image \"us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\" already present on machine\n  Normal  Created    2s         kubelet, mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net  Created container agnhost-master\n  Normal  Started    2s         kubelet, mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net  Started container agnhost-master\n"
Apr  6 22:56:09.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 describe rc agnhost-master --namespace=kubectl-7199'
Apr  6 22:56:09.641: INFO: stderr: ""
Apr  6 22:56:09.641: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-7199\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-master-l8m4m\n"
Apr  6 22:56:09.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 describe service agnhost-master --namespace=kubectl-7199'
Apr  6 22:56:09.730: INFO: stderr: ""
Apr  6 22:56:09.730: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-7199\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.103.24.6\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.192.1.96:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr  6 22:56:09.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 describe node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net'
Apr  6 22:56:09.824: INFO: stderr: ""
Apr  6 22:56:09.824: INFO: stdout: "Name:               mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"a2:93:47:96:3d:54\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 16.0.8.72\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 06 Apr 2020 21:59:35 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 06 Apr 2020 22:56:01 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 06 Apr 2020 22:54:57 +0000   Mon, 06 Apr 2020 21:59:34 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 06 Apr 2020 22:54:57 +0000   Mon, 06 Apr 2020 21:59:34 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 06 Apr 2020 22:54:57 +0000   Mon, 06 Apr 2020 21:59:34 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 06 Apr 2020 22:54:57 +0000   Mon, 06 Apr 2020 22:01:29 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  16.0.8.72\n  Hostname:    mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net\nCapacity:\n  cpu:                4\n  ephemeral-storage:  296876412Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             32780672Ki\n  pods:               110\nAllocatable:\n  cpu:                3200m\n  ephemeral-storage:  273601300847\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             26122624Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 5740e8ed6458418eb79e11d916b6da2c\n  System UUID:                F54F1142-7CC0-9E85-2E9E-DDB3C325DB5F\n  Boot ID:                    9047d894-932f-4267-9245-493760d68e52\n  Kernel Version:             3.10.0-1062.18.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.5\n  Kubelet Version:            v1.18.0\n  Kube-Proxy Version:         v1.18.0\nPodCIDR:                      10.192.0.0/25\nPodCIDRs:                     10.192.0.0/25\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                                   ------------  ----------  ---------------  -------------  ---\n  hpecp                       hpecp-fsmount-glj7p                                                    250m (7%)     250m (7%)   256Mi (1%)       256Mi (1%)     51m\n  kube-system                 canal-vf2gd                                                            250m (7%)     0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 etcd-mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kube-apiserver-mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net             250m (7%)     0 (0%)      0 (0%)           0 (0%)         54m\n  kube-system                 kube-controller-manager-mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net    200m (6%)     0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                 kube-proxy-4pzbq                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m\n  kube-system                 kube-scheduler-mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net             100m (3%)     0 (0%)      0 (0%)           0 (0%)         54m\n  kube-system                 metricbeat-sq4qv                                                       100m (3%)     0 (0%)      250Mi (0%)       500Mi (1%)     50m\n  kube-system                 metrics-server-5fb655647c-zl5gm                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         50m\n  sonobuoy                    sonobuoy-e2e-job-a649ebd8879e46af                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-c4700b4a2f8941f0-76rpk                0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1150m (35%)  250m (7%)\n  memory             506Mi (1%)   756Mi (2%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:\n  Type    Reason                   Age   From                                                     Message\n  ----    ------                   ----  ----                                                     -------\n  Normal  Starting                 56m   kubelet, mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net     Starting kubelet.\n  Normal  NodeHasSufficientMemory  56m   kubelet, mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net     Node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    56m   kubelet, mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net     Node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     56m   kubelet, mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net     Node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  56m   kubelet, mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net     Updated Node Allocatable limit across pods\n  Normal  NodeReady                54m   kubelet, mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net     Node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net status is now: NodeReady\n  Normal  Starting                 54m   kube-proxy, mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net  Starting kube-proxy.\n"
Apr  6 22:56:09.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 describe namespace kubectl-7199'
Apr  6 22:56:09.882: INFO: stderr: ""
Apr  6 22:56:09.882: INFO: stdout: "Name:         kubectl-7199\nLabels:       e2e-framework=kubectl\n              e2e-run=5a78746c-4c31-4ce8-bcd6-68eb0651b974\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:56:09.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7199" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":275,"completed":79,"skipped":1369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:56:09.886: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating all guestbook components
Apr  6 22:56:10.053: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Apr  6 22:56:10.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 create -f - --namespace=kubectl-4476'
Apr  6 22:56:10.359: INFO: stderr: ""
Apr  6 22:56:10.359: INFO: stdout: "service/agnhost-slave created\n"
Apr  6 22:56:10.359: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Apr  6 22:56:10.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 create -f - --namespace=kubectl-4476'
Apr  6 22:56:10.648: INFO: stderr: ""
Apr  6 22:56:10.648: INFO: stdout: "service/agnhost-master created\n"
Apr  6 22:56:10.648: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr  6 22:56:10.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 create -f - --namespace=kubectl-4476'
Apr  6 22:56:10.923: INFO: stderr: ""
Apr  6 22:56:10.923: INFO: stdout: "service/frontend created\n"
Apr  6 22:56:10.923: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Apr  6 22:56:10.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 create -f - --namespace=kubectl-4476'
Apr  6 22:56:11.185: INFO: stderr: ""
Apr  6 22:56:11.185: INFO: stdout: "deployment.apps/frontend created\n"
Apr  6 22:56:11.185: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr  6 22:56:11.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 create -f - --namespace=kubectl-4476'
Apr  6 22:56:11.460: INFO: stderr: ""
Apr  6 22:56:11.460: INFO: stdout: "deployment.apps/agnhost-master created\n"
Apr  6 22:56:11.460: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr  6 22:56:11.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 create -f - --namespace=kubectl-4476'
Apr  6 22:56:11.778: INFO: stderr: ""
Apr  6 22:56:11.778: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Apr  6 22:56:11.778: INFO: Waiting for all frontend pods to be Running.
Apr  6 22:56:16.831: INFO: Waiting for frontend to serve content.
Apr  6 22:56:16.848: INFO: Trying to add a new entry to the guestbook.
Apr  6 22:56:16.854: INFO: Verifying that added entry can be retrieved.
Apr  6 22:56:16.858: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Apr  6 22:56:21.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 delete --grace-period=0 --force -f - --namespace=kubectl-4476'
Apr  6 22:56:21.971: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 22:56:21.971: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Apr  6 22:56:21.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 delete --grace-period=0 --force -f - --namespace=kubectl-4476'
Apr  6 22:56:22.088: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 22:56:22.088: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Apr  6 22:56:22.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 delete --grace-period=0 --force -f - --namespace=kubectl-4476'
Apr  6 22:56:22.237: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 22:56:22.237: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr  6 22:56:22.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 delete --grace-period=0 --force -f - --namespace=kubectl-4476'
Apr  6 22:56:22.311: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 22:56:22.311: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr  6 22:56:22.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 delete --grace-period=0 --force -f - --namespace=kubectl-4476'
Apr  6 22:56:22.454: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 22:56:22.454: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Apr  6 22:56:22.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 delete --grace-period=0 --force -f - --namespace=kubectl-4476'
Apr  6 22:56:22.573: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 22:56:22.573: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:56:22.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4476" for this suite.

â€¢ [SLOW TEST:12.699 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:310
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":275,"completed":80,"skipped":1415,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:56:22.586: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5910
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Apr  6 22:56:22.763: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:56:36.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5910" for this suite.

â€¢ [SLOW TEST:13.430 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":275,"completed":81,"skipped":1437,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:56:36.017: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6414
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Apr  6 22:56:36.205: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 22:56:39.003: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:56:49.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6414" for this suite.

â€¢ [SLOW TEST:13.927 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":275,"completed":82,"skipped":1468,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:56:49.944: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-880
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:56:50.102: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Apr  6 22:56:52.122: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:56:53.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-880" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":275,"completed":83,"skipped":1481,"failed":0}

------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:56:53.244: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-projected-sn46
STEP: Creating a pod to test atomic-volume-subpath
Apr  6 22:56:53.485: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-sn46" in namespace "subpath-9481" to be "Succeeded or Failed"
Apr  6 22:56:53.527: INFO: Pod "pod-subpath-test-projected-sn46": Phase="Pending", Reason="", readiness=false. Elapsed: 41.926282ms
Apr  6 22:56:55.529: INFO: Pod "pod-subpath-test-projected-sn46": Phase="Running", Reason="", readiness=true. Elapsed: 2.043953334s
Apr  6 22:56:57.531: INFO: Pod "pod-subpath-test-projected-sn46": Phase="Running", Reason="", readiness=true. Elapsed: 4.045978776s
Apr  6 22:56:59.533: INFO: Pod "pod-subpath-test-projected-sn46": Phase="Running", Reason="", readiness=true. Elapsed: 6.047716141s
Apr  6 22:57:01.535: INFO: Pod "pod-subpath-test-projected-sn46": Phase="Running", Reason="", readiness=true. Elapsed: 8.049668504s
Apr  6 22:57:03.537: INFO: Pod "pod-subpath-test-projected-sn46": Phase="Running", Reason="", readiness=true. Elapsed: 10.051590107s
Apr  6 22:57:05.539: INFO: Pod "pod-subpath-test-projected-sn46": Phase="Running", Reason="", readiness=true. Elapsed: 12.053495672s
Apr  6 22:57:07.541: INFO: Pod "pod-subpath-test-projected-sn46": Phase="Running", Reason="", readiness=true. Elapsed: 14.055483157s
Apr  6 22:57:09.542: INFO: Pod "pod-subpath-test-projected-sn46": Phase="Running", Reason="", readiness=true. Elapsed: 16.057033293s
Apr  6 22:57:11.565: INFO: Pod "pod-subpath-test-projected-sn46": Phase="Running", Reason="", readiness=true. Elapsed: 18.079076371s
Apr  6 22:57:13.566: INFO: Pod "pod-subpath-test-projected-sn46": Phase="Running", Reason="", readiness=true. Elapsed: 20.080613195s
Apr  6 22:57:15.567: INFO: Pod "pod-subpath-test-projected-sn46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.082057133s
STEP: Saw pod success
Apr  6 22:57:15.568: INFO: Pod "pod-subpath-test-projected-sn46" satisfied condition "Succeeded or Failed"
Apr  6 22:57:15.569: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-subpath-test-projected-sn46 container test-container-subpath-projected-sn46: <nil>
STEP: delete the pod
Apr  6 22:57:15.598: INFO: Waiting for pod pod-subpath-test-projected-sn46 to disappear
Apr  6 22:57:15.603: INFO: Pod pod-subpath-test-projected-sn46 no longer exists
STEP: Deleting pod pod-subpath-test-projected-sn46
Apr  6 22:57:15.603: INFO: Deleting pod "pod-subpath-test-projected-sn46" in namespace "subpath-9481"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:57:15.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9481" for this suite.

â€¢ [SLOW TEST:22.364 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":275,"completed":84,"skipped":1481,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:57:15.608: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5447
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:57:15.788: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:57:16.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5447" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":275,"completed":85,"skipped":1500,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:57:16.804: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4403
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 22:57:17.645: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 22:57:20.703: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:57:21.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4403" for this suite.
STEP: Destroying namespace "webhook-4403-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":275,"completed":86,"skipped":1520,"failed":0}
SSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:57:21.420: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3062
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:157
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:57:21.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3062" for this suite.
â€¢{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":275,"completed":87,"skipped":1526,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:57:21.644: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1382
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:57:21.869: INFO: Waiting up to 5m0s for pod "busybox-user-65534-6d9a0e54-1cf7-4724-915e-cb0f7f143060" in namespace "security-context-test-1382" to be "Succeeded or Failed"
Apr  6 22:57:21.902: INFO: Pod "busybox-user-65534-6d9a0e54-1cf7-4724-915e-cb0f7f143060": Phase="Pending", Reason="", readiness=false. Elapsed: 32.678674ms
Apr  6 22:57:23.903: INFO: Pod "busybox-user-65534-6d9a0e54-1cf7-4724-915e-cb0f7f143060": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034280711s
Apr  6 22:57:23.903: INFO: Pod "busybox-user-65534-6d9a0e54-1cf7-4724-915e-cb0f7f143060" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:57:23.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1382" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":88,"skipped":1551,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:57:23.908: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-751
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr  6 22:57:28.169: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-751 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 22:57:28.169: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 22:57:28.255: INFO: Exec stderr: ""
Apr  6 22:57:28.255: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-751 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 22:57:28.255: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 22:57:28.340: INFO: Exec stderr: ""
Apr  6 22:57:28.340: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-751 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 22:57:28.340: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 22:57:28.450: INFO: Exec stderr: ""
Apr  6 22:57:28.450: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-751 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 22:57:28.450: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 22:57:28.533: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr  6 22:57:28.533: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-751 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 22:57:28.533: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 22:57:28.618: INFO: Exec stderr: ""
Apr  6 22:57:28.618: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-751 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 22:57:28.618: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 22:57:28.697: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr  6 22:57:28.697: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-751 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 22:57:28.697: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 22:57:28.779: INFO: Exec stderr: ""
Apr  6 22:57:28.779: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-751 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 22:57:28.779: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 22:57:28.860: INFO: Exec stderr: ""
Apr  6 22:57:28.861: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-751 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 22:57:28.861: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 22:57:28.948: INFO: Exec stderr: ""
Apr  6 22:57:28.948: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-751 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 22:57:28.948: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 22:57:29.028: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:57:29.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-751" for this suite.

â€¢ [SLOW TEST:5.183 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":89,"skipped":1595,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:57:29.091: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6784
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr  6 22:57:29.380: INFO: Waiting up to 5m0s for pod "pod-b598ef59-dfeb-4bb1-a652-5036479e5fa9" in namespace "emptydir-6784" to be "Succeeded or Failed"
Apr  6 22:57:29.432: INFO: Pod "pod-b598ef59-dfeb-4bb1-a652-5036479e5fa9": Phase="Pending", Reason="", readiness=false. Elapsed: 51.66524ms
Apr  6 22:57:31.433: INFO: Pod "pod-b598ef59-dfeb-4bb1-a652-5036479e5fa9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.05333839s
STEP: Saw pod success
Apr  6 22:57:31.433: INFO: Pod "pod-b598ef59-dfeb-4bb1-a652-5036479e5fa9" satisfied condition "Succeeded or Failed"
Apr  6 22:57:31.434: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-b598ef59-dfeb-4bb1-a652-5036479e5fa9 container test-container: <nil>
STEP: delete the pod
Apr  6 22:57:31.514: INFO: Waiting for pod pod-b598ef59-dfeb-4bb1-a652-5036479e5fa9 to disappear
Apr  6 22:57:31.520: INFO: Pod pod-b598ef59-dfeb-4bb1-a652-5036479e5fa9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:57:31.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6784" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":90,"skipped":1598,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:57:31.531: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6637
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-c964ca19-af10-4e4d-8e53-49d77b524deb
STEP: Creating a pod to test consume secrets
Apr  6 22:57:31.714: INFO: Waiting up to 5m0s for pod "pod-secrets-2af704cd-1755-45aa-a7bb-b23aa185e6ba" in namespace "secrets-6637" to be "Succeeded or Failed"
Apr  6 22:57:31.720: INFO: Pod "pod-secrets-2af704cd-1755-45aa-a7bb-b23aa185e6ba": Phase="Pending", Reason="", readiness=false. Elapsed: 5.568936ms
Apr  6 22:57:33.722: INFO: Pod "pod-secrets-2af704cd-1755-45aa-a7bb-b23aa185e6ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007580675s
STEP: Saw pod success
Apr  6 22:57:33.722: INFO: Pod "pod-secrets-2af704cd-1755-45aa-a7bb-b23aa185e6ba" satisfied condition "Succeeded or Failed"
Apr  6 22:57:33.723: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-secrets-2af704cd-1755-45aa-a7bb-b23aa185e6ba container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 22:57:33.811: INFO: Waiting for pod pod-secrets-2af704cd-1755-45aa-a7bb-b23aa185e6ba to disappear
Apr  6 22:57:33.844: INFO: Pod pod-secrets-2af704cd-1755-45aa-a7bb-b23aa185e6ba no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:57:33.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6637" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":275,"completed":91,"skipped":1604,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:57:33.879: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-555
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-555.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-555.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-555.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-555.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-555.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-555.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 22:57:36.170: INFO: DNS probes using dns-555/dns-test-3f8d76d4-56e2-46cd-9938-5b926a31bf6b succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:57:36.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-555" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":275,"completed":92,"skipped":1605,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:57:36.197: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-9505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test hostPath mode
Apr  6 22:57:36.360: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-9505" to be "Succeeded or Failed"
Apr  6 22:57:36.373: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 13.723939ms
Apr  6 22:57:38.375: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015442202s
STEP: Saw pod success
Apr  6 22:57:38.375: INFO: Pod "pod-host-path-test" satisfied condition "Succeeded or Failed"
Apr  6 22:57:38.376: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Apr  6 22:57:38.473: INFO: Waiting for pod pod-host-path-test to disappear
Apr  6 22:57:38.484: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:57:38.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-9505" for this suite.
â€¢{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":93,"skipped":1620,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:57:38.488: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2284
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:57:38.733: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-a0dae822-cd57-4f7c-9bef-48409f4b0688" in namespace "security-context-test-2284" to be "Succeeded or Failed"
Apr  6 22:57:38.742: INFO: Pod "busybox-readonly-false-a0dae822-cd57-4f7c-9bef-48409f4b0688": Phase="Pending", Reason="", readiness=false. Elapsed: 8.816889ms
Apr  6 22:57:40.744: INFO: Pod "busybox-readonly-false-a0dae822-cd57-4f7c-9bef-48409f4b0688": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010998655s
Apr  6 22:57:40.744: INFO: Pod "busybox-readonly-false-a0dae822-cd57-4f7c-9bef-48409f4b0688" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:57:40.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2284" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":275,"completed":94,"skipped":1637,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:57:40.749: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4360
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Apr  6 22:57:43.498: INFO: Successfully updated pod "annotationupdated350f350-eacc-4cf4-96b9-fe16c1f9693c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:57:47.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4360" for this suite.

â€¢ [SLOW TEST:6.772 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":275,"completed":95,"skipped":1679,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:57:47.521: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5382
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl logs
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1288
STEP: creating an pod
Apr  6 22:57:47.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 run logs-generator --image=us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 --namespace=kubectl-5382 -- logs-generator --log-lines-total 100 --run-duration 20s'
Apr  6 22:57:47.792: INFO: stderr: ""
Apr  6 22:57:47.792: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Waiting for log generator to start.
Apr  6 22:57:47.792: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Apr  6 22:57:47.792: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5382" to be "running and ready, or succeeded"
Apr  6 22:57:47.842: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 50.113356ms
Apr  6 22:57:49.844: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.051794215s
Apr  6 22:57:49.844: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Apr  6 22:57:49.844: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Apr  6 22:57:49.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 logs logs-generator logs-generator --namespace=kubectl-5382'
Apr  6 22:57:49.908: INFO: stderr: ""
Apr  6 22:57:49.908: INFO: stdout: "I0406 22:57:48.527517       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/8rs 535\nI0406 22:57:48.727635       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/wsg5 435\nI0406 22:57:48.927631       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/kvvn 465\nI0406 22:57:49.127708       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/pdr 495\nI0406 22:57:49.327648       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/9phv 521\nI0406 22:57:49.527646       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/mpfl 384\nI0406 22:57:49.727667       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/7fk 595\n"
STEP: limiting log lines
Apr  6 22:57:49.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 logs logs-generator logs-generator --namespace=kubectl-5382 --tail=1'
Apr  6 22:57:49.973: INFO: stderr: ""
Apr  6 22:57:49.973: INFO: stdout: "I0406 22:57:49.927660       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/f96 218\n"
Apr  6 22:57:49.973: INFO: got output "I0406 22:57:49.927660       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/f96 218\n"
STEP: limiting log bytes
Apr  6 22:57:49.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 logs logs-generator logs-generator --namespace=kubectl-5382 --limit-bytes=1'
Apr  6 22:57:50.034: INFO: stderr: ""
Apr  6 22:57:50.034: INFO: stdout: "I"
Apr  6 22:57:50.034: INFO: got output "I"
STEP: exposing timestamps
Apr  6 22:57:50.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 logs logs-generator logs-generator --namespace=kubectl-5382 --tail=1 --timestamps'
Apr  6 22:57:50.095: INFO: stderr: ""
Apr  6 22:57:50.095: INFO: stdout: "2020-04-06T22:57:49.92775373Z I0406 22:57:49.927660       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/f96 218\n"
Apr  6 22:57:50.095: INFO: got output "2020-04-06T22:57:49.92775373Z I0406 22:57:49.927660       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/f96 218\n"
STEP: restricting to a time range
Apr  6 22:57:52.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 logs logs-generator logs-generator --namespace=kubectl-5382 --since=1s'
Apr  6 22:57:52.680: INFO: stderr: ""
Apr  6 22:57:52.680: INFO: stdout: "I0406 22:57:51.727632       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/kckk 335\nI0406 22:57:51.927625       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/mxfb 439\nI0406 22:57:52.127634       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/fr4 206\nI0406 22:57:52.327638       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/x5tn 489\nI0406 22:57:52.527660       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/69xl 500\n"
Apr  6 22:57:52.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 logs logs-generator logs-generator --namespace=kubectl-5382 --since=24h'
Apr  6 22:57:52.769: INFO: stderr: ""
Apr  6 22:57:52.769: INFO: stdout: "I0406 22:57:48.527517       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/8rs 535\nI0406 22:57:48.727635       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/wsg5 435\nI0406 22:57:48.927631       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/kvvn 465\nI0406 22:57:49.127708       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/pdr 495\nI0406 22:57:49.327648       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/9phv 521\nI0406 22:57:49.527646       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/mpfl 384\nI0406 22:57:49.727667       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/7fk 595\nI0406 22:57:49.927660       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/f96 218\nI0406 22:57:50.127635       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/lnjg 442\nI0406 22:57:50.327654       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/927s 547\nI0406 22:57:50.527657       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/sc4n 431\nI0406 22:57:50.727638       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/vmkd 224\nI0406 22:57:50.927640       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/t54p 427\nI0406 22:57:51.127673       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/65v 422\nI0406 22:57:51.327641       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/4gbb 591\nI0406 22:57:51.527635       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/mxhg 371\nI0406 22:57:51.727632       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/kckk 335\nI0406 22:57:51.927625       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/mxfb 439\nI0406 22:57:52.127634       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/fr4 206\nI0406 22:57:52.327638       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/x5tn 489\nI0406 22:57:52.527660       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/69xl 500\nI0406 22:57:52.727672       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/6zk2 264\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1294
Apr  6 22:57:52.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 delete pod logs-generator --namespace=kubectl-5382'
Apr  6 22:57:54.853: INFO: stderr: ""
Apr  6 22:57:54.853: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:57:54.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5382" for this suite.

â€¢ [SLOW TEST:7.354 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1284
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":275,"completed":96,"skipped":1681,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:57:54.875: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5746
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr  6 22:57:55.646: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 22:57:58.696: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:57:58.697: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:57:59.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5746" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:5.210 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":275,"completed":97,"skipped":1703,"failed":0}
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:58:00.085: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr  6 22:58:02.801: INFO: Successfully updated pod "pod-update-a7248337-d9a9-40d6-98e6-b5d49132c960"
STEP: verifying the updated pod is in kubernetes
Apr  6 22:58:02.814: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:58:02.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5672" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":275,"completed":98,"skipped":1703,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:58:02.818: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1786
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 22:58:02.991: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c8f6753-07e6-43f0-96da-529712aa92c5" in namespace "projected-1786" to be "Succeeded or Failed"
Apr  6 22:58:02.997: INFO: Pod "downwardapi-volume-9c8f6753-07e6-43f0-96da-529712aa92c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.872933ms
Apr  6 22:58:05.072: INFO: Pod "downwardapi-volume-9c8f6753-07e6-43f0-96da-529712aa92c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081103277s
Apr  6 22:58:07.074: INFO: Pod "downwardapi-volume-9c8f6753-07e6-43f0-96da-529712aa92c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083187894s
STEP: Saw pod success
Apr  6 22:58:07.074: INFO: Pod "downwardapi-volume-9c8f6753-07e6-43f0-96da-529712aa92c5" satisfied condition "Succeeded or Failed"
Apr  6 22:58:07.075: INFO: Trying to get logs from node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net pod downwardapi-volume-9c8f6753-07e6-43f0-96da-529712aa92c5 container client-container: <nil>
STEP: delete the pod
Apr  6 22:58:07.141: INFO: Waiting for pod downwardapi-volume-9c8f6753-07e6-43f0-96da-529712aa92c5 to disappear
Apr  6 22:58:07.152: INFO: Pod downwardapi-volume-9c8f6753-07e6-43f0-96da-529712aa92c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:58:07.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1786" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":275,"completed":99,"skipped":1710,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:58:07.156: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 22:58:07.352: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b7521e3-f230-4683-80f0-be8be14de4b0" in namespace "downward-api-3643" to be "Succeeded or Failed"
Apr  6 22:58:07.361: INFO: Pod "downwardapi-volume-1b7521e3-f230-4683-80f0-be8be14de4b0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.019265ms
Apr  6 22:58:09.362: INFO: Pod "downwardapi-volume-1b7521e3-f230-4683-80f0-be8be14de4b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010917351s
STEP: Saw pod success
Apr  6 22:58:09.362: INFO: Pod "downwardapi-volume-1b7521e3-f230-4683-80f0-be8be14de4b0" satisfied condition "Succeeded or Failed"
Apr  6 22:58:09.364: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-1b7521e3-f230-4683-80f0-be8be14de4b0 container client-container: <nil>
STEP: delete the pod
Apr  6 22:58:09.438: INFO: Waiting for pod downwardapi-volume-1b7521e3-f230-4683-80f0-be8be14de4b0 to disappear
Apr  6 22:58:09.452: INFO: Pod downwardapi-volume-1b7521e3-f230-4683-80f0-be8be14de4b0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:58:09.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3643" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":275,"completed":100,"skipped":1720,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:58:09.462: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5004
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating secret secrets-5004/secret-test-179ff5e5-0e03-4b2b-9868-85222a8984c7
STEP: Creating a pod to test consume secrets
Apr  6 22:58:09.629: INFO: Waiting up to 5m0s for pod "pod-configmaps-08d3a7d0-ae56-42d1-b65e-ead8d0ac7e5b" in namespace "secrets-5004" to be "Succeeded or Failed"
Apr  6 22:58:09.656: INFO: Pod "pod-configmaps-08d3a7d0-ae56-42d1-b65e-ead8d0ac7e5b": Phase="Pending", Reason="", readiness=false. Elapsed: 27.203519ms
Apr  6 22:58:11.657: INFO: Pod "pod-configmaps-08d3a7d0-ae56-42d1-b65e-ead8d0ac7e5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028828585s
STEP: Saw pod success
Apr  6 22:58:11.657: INFO: Pod "pod-configmaps-08d3a7d0-ae56-42d1-b65e-ead8d0ac7e5b" satisfied condition "Succeeded or Failed"
Apr  6 22:58:11.659: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-configmaps-08d3a7d0-ae56-42d1-b65e-ead8d0ac7e5b container env-test: <nil>
STEP: delete the pod
Apr  6 22:58:11.706: INFO: Waiting for pod pod-configmaps-08d3a7d0-ae56-42d1-b65e-ead8d0ac7e5b to disappear
Apr  6 22:58:11.708: INFO: Pod pod-configmaps-08d3a7d0-ae56-42d1-b65e-ead8d0ac7e5b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:58:11.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5004" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":275,"completed":101,"skipped":1728,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:58:11.713: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5460
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 22:58:12.662: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721810692, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721810692, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-779fdc84d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721810692, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721810692, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 22:58:15.792: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:58:16.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5460" for this suite.
STEP: Destroying namespace "webhook-5460-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":275,"completed":102,"skipped":1728,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:58:16.382: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4578
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:58:16.609: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr  6 22:58:19.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-4578 create -f -'
Apr  6 22:58:22.485: INFO: stderr: ""
Apr  6 22:58:22.485: INFO: stdout: "e2e-test-crd-publish-openapi-2211-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr  6 22:58:22.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-4578 delete e2e-test-crd-publish-openapi-2211-crds test-cr'
Apr  6 22:58:22.555: INFO: stderr: ""
Apr  6 22:58:22.555: INFO: stdout: "e2e-test-crd-publish-openapi-2211-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Apr  6 22:58:22.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-4578 apply -f -'
Apr  6 22:58:22.804: INFO: stderr: ""
Apr  6 22:58:22.804: INFO: stdout: "e2e-test-crd-publish-openapi-2211-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr  6 22:58:22.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-4578 delete e2e-test-crd-publish-openapi-2211-crds test-cr'
Apr  6 22:58:22.863: INFO: stderr: ""
Apr  6 22:58:22.863: INFO: stdout: "e2e-test-crd-publish-openapi-2211-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Apr  6 22:58:22.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 explain e2e-test-crd-publish-openapi-2211-crds'
Apr  6 22:58:23.109: INFO: stderr: ""
Apr  6 22:58:23.109: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2211-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:58:24.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4578" for this suite.

â€¢ [SLOW TEST:8.512 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":275,"completed":103,"skipped":1728,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:58:24.894: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr  6 22:58:25.585: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 22:58:28.612: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:58:28.613: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:58:29.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5917" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":275,"completed":104,"skipped":1751,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:58:29.877: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6491
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:58:30.102: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr  6 22:58:32.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-6491 create -f -'
Apr  6 22:58:35.788: INFO: stderr: ""
Apr  6 22:58:35.788: INFO: stdout: "e2e-test-crd-publish-openapi-9451-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr  6 22:58:35.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-6491 delete e2e-test-crd-publish-openapi-9451-crds test-cr'
Apr  6 22:58:35.873: INFO: stderr: ""
Apr  6 22:58:35.873: INFO: stdout: "e2e-test-crd-publish-openapi-9451-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Apr  6 22:58:35.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-6491 apply -f -'
Apr  6 22:58:36.128: INFO: stderr: ""
Apr  6 22:58:36.128: INFO: stdout: "e2e-test-crd-publish-openapi-9451-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr  6 22:58:36.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-6491 delete e2e-test-crd-publish-openapi-9451-crds test-cr'
Apr  6 22:58:36.253: INFO: stderr: ""
Apr  6 22:58:36.253: INFO: stdout: "e2e-test-crd-publish-openapi-9451-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr  6 22:58:36.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 explain e2e-test-crd-publish-openapi-9451-crds'
Apr  6 22:58:36.475: INFO: stderr: ""
Apr  6 22:58:36.475: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9451-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:58:39.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6491" for this suite.

â€¢ [SLOW TEST:9.393 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":275,"completed":105,"skipped":1768,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:58:39.270: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-105
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 22:58:39.564: INFO: (0) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 6.274673ms)
Apr  6 22:58:39.566: INFO: (1) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 2.035587ms)
Apr  6 22:58:39.568: INFO: (2) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.834617ms)
Apr  6 22:58:39.570: INFO: (3) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.98802ms)
Apr  6 22:58:39.572: INFO: (4) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.546627ms)
Apr  6 22:58:39.574: INFO: (5) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 2.588393ms)
Apr  6 22:58:39.576: INFO: (6) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.660079ms)
Apr  6 22:58:39.578: INFO: (7) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.93372ms)
Apr  6 22:58:39.580: INFO: (8) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.693761ms)
Apr  6 22:58:39.582: INFO: (9) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 2.387104ms)
Apr  6 22:58:39.584: INFO: (10) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.76343ms)
Apr  6 22:58:39.586: INFO: (11) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.605093ms)
Apr  6 22:58:39.588: INFO: (12) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 2.71921ms)
Apr  6 22:58:39.591: INFO: (13) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 2.255545ms)
Apr  6 22:58:39.594: INFO: (14) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 2.952158ms)
Apr  6 22:58:39.596: INFO: (15) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.994003ms)
Apr  6 22:58:39.597: INFO: (16) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.700025ms)
Apr  6 22:58:39.599: INFO: (17) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.567489ms)
Apr  6 22:58:39.600: INFO: (18) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.592462ms)
Apr  6 22:58:39.602: INFO: (19) /api/v1/nodes/mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.564715ms)
[AfterEach] version v1
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:58:39.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-105" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":275,"completed":106,"skipped":1797,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:58:39.607: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8574
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-8574
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating stateful set ss in namespace statefulset-8574
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8574
Apr  6 22:58:39.837: INFO: Found 0 stateful pods, waiting for 1
Apr  6 22:58:49.851: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr  6 22:58:49.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-8574 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 22:58:49.990: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 22:58:49.990: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 22:58:49.990: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 22:58:49.992: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr  6 22:58:59.994: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 22:58:59.994: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 22:59:00.010: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Apr  6 22:59:00.010: INFO: ss-0  mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:58:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:58:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:58:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:58:39 +0000 UTC  }]
Apr  6 22:59:00.010: INFO: 
Apr  6 22:59:00.010: INFO: StatefulSet ss has not reached scale 3, at 1
Apr  6 22:59:01.049: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988789492s
Apr  6 22:59:02.051: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.949874999s
Apr  6 22:59:03.053: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.947868396s
Apr  6 22:59:04.055: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.945593122s
Apr  6 22:59:05.057: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.943453431s
Apr  6 22:59:06.091: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.94127643s
Apr  6 22:59:07.092: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.907811574s
Apr  6 22:59:08.094: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.906079089s
Apr  6 22:59:09.096: INFO: Verifying statefulset ss doesn't scale past 3 for another 904.19505ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8574
Apr  6 22:59:10.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-8574 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 22:59:10.239: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 22:59:10.239: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 22:59:10.239: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 22:59:10.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-8574 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 22:59:10.382: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr  6 22:59:10.382: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 22:59:10.382: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 22:59:10.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-8574 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 22:59:10.532: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr  6 22:59:10.532: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 22:59:10.532: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 22:59:10.536: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Apr  6 22:59:20.542: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 22:59:20.542: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 22:59:20.542: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr  6 22:59:20.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-8574 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 22:59:20.684: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 22:59:20.684: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 22:59:20.684: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 22:59:20.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-8574 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 22:59:20.857: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 22:59:20.857: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 22:59:20.857: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 22:59:20.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-8574 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 22:59:20.997: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 22:59:20.997: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 22:59:20.997: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 22:59:20.997: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 22:59:20.998: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr  6 22:59:31.002: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 22:59:31.002: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 22:59:31.002: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 22:59:31.029: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Apr  6 22:59:31.029: INFO: ss-0  mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:58:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:58:39 +0000 UTC  }]
Apr  6 22:59:31.029: INFO: ss-1  mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  }]
Apr  6 22:59:31.029: INFO: ss-2  mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  }]
Apr  6 22:59:31.029: INFO: 
Apr  6 22:59:31.029: INFO: StatefulSet ss has not reached scale 0, at 3
Apr  6 22:59:32.049: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Apr  6 22:59:32.049: INFO: ss-0  mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:58:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:58:39 +0000 UTC  }]
Apr  6 22:59:32.049: INFO: ss-1  mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  }]
Apr  6 22:59:32.049: INFO: ss-2  mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  }]
Apr  6 22:59:32.049: INFO: 
Apr  6 22:59:32.049: INFO: StatefulSet ss has not reached scale 0, at 3
Apr  6 22:59:33.097: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Apr  6 22:59:33.097: INFO: ss-1  mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  }]
Apr  6 22:59:33.097: INFO: ss-2  mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  }]
Apr  6 22:59:33.097: INFO: 
Apr  6 22:59:33.097: INFO: StatefulSet ss has not reached scale 0, at 2
Apr  6 22:59:34.099: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Apr  6 22:59:34.099: INFO: ss-1  mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  }]
Apr  6 22:59:34.099: INFO: 
Apr  6 22:59:34.099: INFO: StatefulSet ss has not reached scale 0, at 1
Apr  6 22:59:35.101: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Apr  6 22:59:35.101: INFO: ss-1  mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  }]
Apr  6 22:59:35.101: INFO: 
Apr  6 22:59:35.101: INFO: StatefulSet ss has not reached scale 0, at 1
Apr  6 22:59:36.103: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Apr  6 22:59:36.103: INFO: ss-1  mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  }]
Apr  6 22:59:36.103: INFO: 
Apr  6 22:59:36.103: INFO: StatefulSet ss has not reached scale 0, at 1
Apr  6 22:59:37.105: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Apr  6 22:59:37.105: INFO: ss-1  mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-06 22:59:00 +0000 UTC  }]
Apr  6 22:59:37.105: INFO: 
Apr  6 22:59:37.105: INFO: StatefulSet ss has not reached scale 0, at 1
Apr  6 22:59:38.107: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.899710963s
Apr  6 22:59:39.109: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.898004259s
Apr  6 22:59:40.111: INFO: Verifying statefulset ss doesn't scale past 0 for another 896.175471ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8574
Apr  6 22:59:41.113: INFO: Scaling statefulset ss to 0
Apr  6 22:59:41.117: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Apr  6 22:59:41.118: INFO: Deleting all statefulset in ns statefulset-8574
Apr  6 22:59:41.119: INFO: Scaling statefulset ss to 0
Apr  6 22:59:41.123: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 22:59:41.124: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 22:59:41.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8574" for this suite.

â€¢ [SLOW TEST:61.535 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":275,"completed":107,"skipped":1860,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 22:59:41.143: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-831
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-831
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  6 22:59:41.336: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr  6 22:59:41.407: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 22:59:43.409: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 22:59:45.409: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 22:59:47.409: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 22:59:49.409: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 22:59:51.409: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 22:59:53.409: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 22:59:55.409: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 22:59:57.409: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 22:59:59.408: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 23:00:01.409: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr  6 23:00:01.412: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Apr  6 23:00:03.474: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.192.0.153 8081 | grep -v '^\s*$'] Namespace:pod-network-test-831 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 23:00:03.474: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 23:00:04.553: INFO: Found all expected endpoints: [netserver-0]
Apr  6 23:00:04.555: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.192.1.123 8081 | grep -v '^\s*$'] Namespace:pod-network-test-831 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 23:00:04.555: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 23:00:05.638: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:00:05.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-831" for this suite.

â€¢ [SLOW TEST:24.501 seconds]
[sig-network] Networking
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":108,"skipped":1864,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:00:05.643: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3574
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Apr  6 23:00:08.395: INFO: Successfully updated pod "annotationupdate5f0365f8-8894-41b2-bcf3-44326743555a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:00:10.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3574" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":275,"completed":109,"skipped":1870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:00:10.419: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7900
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Apr  6 23:00:10.595: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  6 23:00:10.600: INFO: Waiting for terminating namespaces to be deleted...
Apr  6 23:00:10.601: INFO: 
Logging pods the kubelet thinks is on node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net before test
Apr  6 23:00:10.610: INFO: canal-zxrh2 from kube-system started at 2020-04-06 22:01:03 +0000 UTC (3 container statuses recorded)
Apr  6 23:00:10.610: INFO: 	Container calico-node ready: true, restart count 1
Apr  6 23:00:10.610: INFO: 	Container install-cni ready: true, restart count 0
Apr  6 23:00:10.610: INFO: 	Container kube-flannel ready: true, restart count 2
Apr  6 23:00:10.610: INFO: kube-proxy-7w486 from kube-system started at 2020-04-06 22:01:48 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.610: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 23:00:10.610: INFO: csi-provisioner-kdf-0 from mapr-csi started at 2020-04-06 22:05:19 +0000 UTC (5 container statuses recorded)
Apr  6 23:00:10.610: INFO: 	Container csi-attacher ready: true, restart count 0
Apr  6 23:00:10.610: INFO: 	Container csi-provisioner ready: true, restart count 0
Apr  6 23:00:10.610: INFO: 	Container csi-snapshot ready: true, restart count 0
Apr  6 23:00:10.610: INFO: 	Container kdf-provisioner ready: true, restart count 0
Apr  6 23:00:10.610: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 23:00:10.610: INFO: hpecp-fsmount-g2knb from hpecp started at 2020-04-06 22:04:39 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.610: INFO: 	Container hpecp-fsmount ready: true, restart count 0
Apr  6 23:00:10.610: INFO: csi-nodeplugin-kdf-jn84c from mapr-csi started at 2020-04-06 22:05:19 +0000 UTC (3 container statuses recorded)
Apr  6 23:00:10.610: INFO: 	Container driver-registrar ready: true, restart count 0
Apr  6 23:00:10.610: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 23:00:10.610: INFO: 	Container mapr-kdf ready: true, restart count 0
Apr  6 23:00:10.610: INFO: netserver-0 from pod-network-test-831 started at 2020-04-06 22:59:41 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.610: INFO: 	Container webserver ready: true, restart count 0
Apr  6 23:00:10.610: INFO: coredns-66bff467f8-b269l from kube-system started at 2020-04-06 22:01:29 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.610: INFO: 	Container coredns ready: true, restart count 0
Apr  6 23:00:10.610: INFO: sonobuoy-systemd-logs-daemon-set-c4700b4a2f8941f0-ttgxk from sonobuoy started at 2020-04-06 22:39:04 +0000 UTC (2 container statuses recorded)
Apr  6 23:00:10.610: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 23:00:10.610: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 23:00:10.610: INFO: hpecp-agent-76d5b65798-qhxrd from hpecp started at 2020-04-06 22:04:06 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.610: INFO: 	Container hpecp-agent ready: true, restart count 0
Apr  6 23:00:10.610: INFO: dashboard-metrics-scraper-69449465bc-55bss from kubernetes-dashboard started at 2020-04-06 22:05:13 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.610: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr  6 23:00:10.610: INFO: metricbeat-lvk5c from kube-system started at 2020-04-06 22:05:52 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.610: INFO: 	Container metricbeat ready: true, restart count 0
Apr  6 23:00:10.610: INFO: 
Logging pods the kubelet thinks is on node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net before test
Apr  6 23:00:10.616: INFO: canal-rg6q8 from kube-system started at 2020-04-06 22:01:03 +0000 UTC (3 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container calico-node ready: true, restart count 1
Apr  6 23:00:10.616: INFO: 	Container install-cni ready: true, restart count 0
Apr  6 23:00:10.616: INFO: 	Container kube-flannel ready: true, restart count 2
Apr  6 23:00:10.616: INFO: hpecp-fsmount-n7jkz from hpecp started at 2020-04-06 22:04:39 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container hpecp-fsmount ready: true, restart count 0
Apr  6 23:00:10.616: INFO: kubernetes-dashboard-6b49d498f6-9prxk from kubernetes-dashboard started at 2020-04-06 22:05:13 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr  6 23:00:10.616: INFO: metricbeat-dsbrd from kube-system started at 2020-04-06 22:05:52 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container metricbeat ready: true, restart count 0
Apr  6 23:00:10.616: INFO: host-test-container-pod from pod-network-test-831 started at 2020-04-06 23:00:01 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container agnhost ready: true, restart count 0
Apr  6 23:00:10.616: INFO: sonobuoy-systemd-logs-daemon-set-c4700b4a2f8941f0-p5vhq from sonobuoy started at 2020-04-06 22:39:04 +0000 UTC (2 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 23:00:10.616: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 23:00:10.616: INFO: netserver-1 from pod-network-test-831 started at 2020-04-06 22:59:41 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container webserver ready: true, restart count 0
Apr  6 23:00:10.616: INFO: test-container-pod from pod-network-test-831 started at 2020-04-06 23:00:01 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container webserver ready: true, restart count 0
Apr  6 23:00:10.616: INFO: coredns-66bff467f8-l8lxj from kube-system started at 2020-04-06 22:01:27 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container coredns ready: true, restart count 0
Apr  6 23:00:10.616: INFO: kubedirector-64b6488f6d-qpssh from hpecp started at 2020-04-06 22:04:39 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container kubedirector ready: true, restart count 0
Apr  6 23:00:10.616: INFO: kube-state-metrics-5b5f5b558d-j5nkq from kube-system started at 2020-04-06 22:05:15 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr  6 23:00:10.616: INFO: sonobuoy from sonobuoy started at 2020-04-06 22:38:57 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  6 23:00:10.616: INFO: kube-proxy-6fzdv from kube-system started at 2020-04-06 22:01:48 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 23:00:10.616: INFO: csi-nodeplugin-kdf-4zrwc from mapr-csi started at 2020-04-06 22:05:19 +0000 UTC (3 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container driver-registrar ready: true, restart count 0
Apr  6 23:00:10.616: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 23:00:10.616: INFO: 	Container mapr-kdf ready: true, restart count 0
Apr  6 23:00:10.616: INFO: metricbeat-6cf8f85fb5-zpb92 from kube-system started at 2020-04-06 22:05:52 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container metricbeat ready: true, restart count 0
Apr  6 23:00:10.616: INFO: annotationupdate5f0365f8-8894-41b2-bcf3-44326743555a from downward-api-3574 started at 2020-04-06 23:00:05 +0000 UTC (1 container statuses recorded)
Apr  6 23:00:10.616: INFO: 	Container client-container ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8d332e08-0b29-4302-9f09-bb5a82c6080e 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-8d332e08-0b29-4302-9f09-bb5a82c6080e off the node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8d332e08-0b29-4302-9f09-bb5a82c6080e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:00:14.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7900" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":275,"completed":110,"skipped":1892,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:00:14.859: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9939
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with configMap that has name projected-configmap-test-upd-4b90b2d4-cba5-4206-8952-838f314296ae
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-4b90b2d4-cba5-4206-8952-838f314296ae
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:00:19.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9939" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":111,"skipped":1892,"failed":0}

------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:00:19.116: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5218
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr  6 23:00:19.307: INFO: Pod name pod-release: Found 0 pods out of 1
Apr  6 23:00:24.309: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:00:25.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5218" for this suite.

â€¢ [SLOW TEST:6.228 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":275,"completed":112,"skipped":1892,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:00:25.344: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-19
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 23:00:25.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3da0440-bbf5-4a9c-89a1-eace191e4fe5" in namespace "downward-api-19" to be "Succeeded or Failed"
Apr  6 23:00:25.563: INFO: Pod "downwardapi-volume-e3da0440-bbf5-4a9c-89a1-eace191e4fe5": Phase="Pending", Reason="", readiness=false. Elapsed: 27.392244ms
Apr  6 23:00:27.565: INFO: Pod "downwardapi-volume-e3da0440-bbf5-4a9c-89a1-eace191e4fe5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029371239s
STEP: Saw pod success
Apr  6 23:00:27.565: INFO: Pod "downwardapi-volume-e3da0440-bbf5-4a9c-89a1-eace191e4fe5" satisfied condition "Succeeded or Failed"
Apr  6 23:00:27.567: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-e3da0440-bbf5-4a9c-89a1-eace191e4fe5 container client-container: <nil>
STEP: delete the pod
Apr  6 23:00:27.603: INFO: Waiting for pod downwardapi-volume-e3da0440-bbf5-4a9c-89a1-eace191e4fe5 to disappear
Apr  6 23:00:27.614: INFO: Pod downwardapi-volume-e3da0440-bbf5-4a9c-89a1-eace191e4fe5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:00:27.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-19" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":275,"completed":113,"skipped":1920,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:00:27.618: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4629
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-d797ba9c-5a65-4efe-a2a6-dfb35c6d1350
STEP: Creating a pod to test consume configMaps
Apr  6 23:00:27.834: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8f3367e9-7620-4f40-9dce-25146ffbf757" in namespace "projected-4629" to be "Succeeded or Failed"
Apr  6 23:00:27.868: INFO: Pod "pod-projected-configmaps-8f3367e9-7620-4f40-9dce-25146ffbf757": Phase="Pending", Reason="", readiness=false. Elapsed: 34.663351ms
Apr  6 23:00:29.894: INFO: Pod "pod-projected-configmaps-8f3367e9-7620-4f40-9dce-25146ffbf757": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.060529469s
STEP: Saw pod success
Apr  6 23:00:29.894: INFO: Pod "pod-projected-configmaps-8f3367e9-7620-4f40-9dce-25146ffbf757" satisfied condition "Succeeded or Failed"
Apr  6 23:00:29.896: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-projected-configmaps-8f3367e9-7620-4f40-9dce-25146ffbf757 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 23:00:29.927: INFO: Waiting for pod pod-projected-configmaps-8f3367e9-7620-4f40-9dce-25146ffbf757 to disappear
Apr  6 23:00:29.930: INFO: Pod pod-projected-configmaps-8f3367e9-7620-4f40-9dce-25146ffbf757 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:00:29.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4629" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":275,"completed":114,"skipped":1931,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:00:29.934: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7347
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Apr  6 23:00:32.631: INFO: Successfully updated pod "labelsupdate5e96115d-455b-499e-af36-c0e54b6aa695"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:00:36.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7347" for this suite.

â€¢ [SLOW TEST:6.719 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":275,"completed":115,"skipped":1984,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:00:36.653: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4810
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-4810
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating statefulset ss in namespace statefulset-4810
Apr  6 23:00:36.981: INFO: Found 0 stateful pods, waiting for 1
Apr  6 23:00:46.982: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Apr  6 23:00:47.013: INFO: Deleting all statefulset in ns statefulset-4810
Apr  6 23:00:47.092: INFO: Scaling statefulset ss to 0
Apr  6 23:01:07.157: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 23:01:07.158: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:01:07.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4810" for this suite.

â€¢ [SLOW TEST:30.524 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":275,"completed":116,"skipped":1999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:01:07.178: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's command
Apr  6 23:01:07.338: INFO: Waiting up to 5m0s for pod "var-expansion-9cdebec7-e5bb-421a-ab35-fb6711d8825c" in namespace "var-expansion-4333" to be "Succeeded or Failed"
Apr  6 23:01:07.347: INFO: Pod "var-expansion-9cdebec7-e5bb-421a-ab35-fb6711d8825c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.527221ms
Apr  6 23:01:09.349: INFO: Pod "var-expansion-9cdebec7-e5bb-421a-ab35-fb6711d8825c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011436238s
STEP: Saw pod success
Apr  6 23:01:09.349: INFO: Pod "var-expansion-9cdebec7-e5bb-421a-ab35-fb6711d8825c" satisfied condition "Succeeded or Failed"
Apr  6 23:01:09.351: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod var-expansion-9cdebec7-e5bb-421a-ab35-fb6711d8825c container dapi-container: <nil>
STEP: delete the pod
Apr  6 23:01:09.387: INFO: Waiting for pod var-expansion-9cdebec7-e5bb-421a-ab35-fb6711d8825c to disappear
Apr  6 23:01:09.388: INFO: Pod var-expansion-9cdebec7-e5bb-421a-ab35-fb6711d8825c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:01:09.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4333" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":275,"completed":117,"skipped":2026,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:01:09.392: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-9489
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Apr  6 23:01:09.594: INFO: Found 0 stateful pods, waiting for 3
Apr  6 23:01:19.597: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 23:01:19.597: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 23:01:19.597: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr  6 23:01:19.614: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr  6 23:01:29.667: INFO: Updating stateful set ss2
Apr  6 23:01:29.741: INFO: Waiting for Pod statefulset-9489/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Apr  6 23:01:39.873: INFO: Found 1 stateful pods, waiting for 3
Apr  6 23:01:49.904: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 23:01:49.904: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 23:01:49.904: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr  6 23:01:49.923: INFO: Updating stateful set ss2
Apr  6 23:01:49.927: INFO: Waiting for Pod statefulset-9489/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr  6 23:01:59.944: INFO: Updating stateful set ss2
Apr  6 23:01:59.972: INFO: Waiting for StatefulSet statefulset-9489/ss2 to complete update
Apr  6 23:01:59.972: INFO: Waiting for Pod statefulset-9489/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr  6 23:02:09.976: INFO: Waiting for StatefulSet statefulset-9489/ss2 to complete update
Apr  6 23:02:09.976: INFO: Waiting for Pod statefulset-9489/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr  6 23:02:19.975: INFO: Waiting for StatefulSet statefulset-9489/ss2 to complete update
Apr  6 23:02:19.975: INFO: Waiting for Pod statefulset-9489/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Apr  6 23:02:29.976: INFO: Deleting all statefulset in ns statefulset-9489
Apr  6 23:02:29.977: INFO: Scaling statefulset ss2 to 0
Apr  6 23:02:59.991: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 23:02:59.992: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:03:00.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9489" for this suite.

â€¢ [SLOW TEST:110.615 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":275,"completed":118,"skipped":2027,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:03:00.007: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3336
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:03:17.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3336" for this suite.

â€¢ [SLOW TEST:17.236 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":275,"completed":119,"skipped":2040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:03:17.244: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2478
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name projected-secret-test-8a472e83-b5a1-49fc-89c5-be4c49810d89
STEP: Creating a pod to test consume secrets
Apr  6 23:03:17.698: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6c62c755-14c1-41c6-ac85-0ad9670c476f" in namespace "projected-2478" to be "Succeeded or Failed"
Apr  6 23:03:17.731: INFO: Pod "pod-projected-secrets-6c62c755-14c1-41c6-ac85-0ad9670c476f": Phase="Pending", Reason="", readiness=false. Elapsed: 32.664981ms
Apr  6 23:03:19.733: INFO: Pod "pod-projected-secrets-6c62c755-14c1-41c6-ac85-0ad9670c476f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034715113s
STEP: Saw pod success
Apr  6 23:03:19.733: INFO: Pod "pod-projected-secrets-6c62c755-14c1-41c6-ac85-0ad9670c476f" satisfied condition "Succeeded or Failed"
Apr  6 23:03:19.734: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-projected-secrets-6c62c755-14c1-41c6-ac85-0ad9670c476f container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 23:03:19.762: INFO: Waiting for pod pod-projected-secrets-6c62c755-14c1-41c6-ac85-0ad9670c476f to disappear
Apr  6 23:03:19.771: INFO: Pod pod-projected-secrets-6c62c755-14c1-41c6-ac85-0ad9670c476f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:03:19.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2478" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":275,"completed":120,"skipped":2085,"failed":0}

------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:03:19.775: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6061
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service endpoint-test2 in namespace services-6061
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6061 to expose endpoints map[]
Apr  6 23:03:20.018: INFO: Get endpoints failed (6.337349ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Apr  6 23:03:21.033: INFO: successfully validated that service endpoint-test2 in namespace services-6061 exposes endpoints map[] (1.020943905s elapsed)
STEP: Creating pod pod1 in namespace services-6061
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6061 to expose endpoints map[pod1:[80]]
Apr  6 23:03:22.109: INFO: successfully validated that service endpoint-test2 in namespace services-6061 exposes endpoints map[pod1:[80]] (1.070103574s elapsed)
STEP: Creating pod pod2 in namespace services-6061
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6061 to expose endpoints map[pod1:[80] pod2:[80]]
Apr  6 23:03:24.168: INFO: successfully validated that service endpoint-test2 in namespace services-6061 exposes endpoints map[pod1:[80] pod2:[80]] (2.055285748s elapsed)
STEP: Deleting pod pod1 in namespace services-6061
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6061 to expose endpoints map[pod2:[80]]
Apr  6 23:03:25.256: INFO: successfully validated that service endpoint-test2 in namespace services-6061 exposes endpoints map[pod2:[80]] (1.085820079s elapsed)
STEP: Deleting pod pod2 in namespace services-6061
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6061 to expose endpoints map[]
Apr  6 23:03:25.276: INFO: successfully validated that service endpoint-test2 in namespace services-6061 exposes endpoints map[] (15.15563ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:03:25.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6061" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:5.607 seconds]
[sig-network] Services
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":275,"completed":121,"skipped":2085,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:03:25.382: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3347
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Apr  6 23:03:25.538: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Apr  6 23:03:36.242: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 23:03:39.025: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:03:48.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3347" for this suite.

â€¢ [SLOW TEST:23.524 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":275,"completed":122,"skipped":2132,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:03:48.907: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8327
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:03:49.099: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:03:50.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8327" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":275,"completed":123,"skipped":2151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:03:50.235: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7920
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-1467b30f-5062-4886-9407-585adf35e156
STEP: Creating a pod to test consume configMaps
Apr  6 23:03:50.508: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b215a5a-f239-4a29-9c41-959c5c46be33" in namespace "configmap-7920" to be "Succeeded or Failed"
Apr  6 23:03:50.545: INFO: Pod "pod-configmaps-9b215a5a-f239-4a29-9c41-959c5c46be33": Phase="Pending", Reason="", readiness=false. Elapsed: 37.289535ms
Apr  6 23:03:52.547: INFO: Pod "pod-configmaps-9b215a5a-f239-4a29-9c41-959c5c46be33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039003368s
STEP: Saw pod success
Apr  6 23:03:52.547: INFO: Pod "pod-configmaps-9b215a5a-f239-4a29-9c41-959c5c46be33" satisfied condition "Succeeded or Failed"
Apr  6 23:03:52.548: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-configmaps-9b215a5a-f239-4a29-9c41-959c5c46be33 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 23:03:52.589: INFO: Waiting for pod pod-configmaps-9b215a5a-f239-4a29-9c41-959c5c46be33 to disappear
Apr  6 23:03:52.595: INFO: Pod pod-configmaps-9b215a5a-f239-4a29-9c41-959c5c46be33 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:03:52.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7920" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":275,"completed":124,"skipped":2182,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:03:52.612: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2322
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-b6d5b3c6-976e-43df-8b00-3dfe001bff47
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:03:54.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2322" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":125,"skipped":2183,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:03:54.858: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8138
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Apr  6 23:04:05.232: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
W0406 23:04:05.232307      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr  6 23:04:05.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8138" for this suite.

â€¢ [SLOW TEST:10.378 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":275,"completed":126,"skipped":2233,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:04:05.236: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1503
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 23:04:05.946: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 23:04:08.977: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Apr  6 23:04:08.987: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:04:09.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1503" for this suite.
STEP: Destroying namespace "webhook-1503-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":275,"completed":127,"skipped":2241,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:04:09.435: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5762
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr  6 23:04:13.703: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  6 23:04:13.744: INFO: Pod pod-with-poststart-http-hook still exists
Apr  6 23:04:15.745: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  6 23:04:15.748: INFO: Pod pod-with-poststart-http-hook still exists
Apr  6 23:04:17.745: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  6 23:04:17.747: INFO: Pod pod-with-poststart-http-hook still exists
Apr  6 23:04:19.745: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  6 23:04:19.747: INFO: Pod pod-with-poststart-http-hook still exists
Apr  6 23:04:21.745: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  6 23:04:21.746: INFO: Pod pod-with-poststart-http-hook still exists
Apr  6 23:04:23.745: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  6 23:04:23.747: INFO: Pod pod-with-poststart-http-hook still exists
Apr  6 23:04:25.745: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  6 23:04:25.746: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:04:25.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5762" for this suite.

â€¢ [SLOW TEST:16.316 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":275,"completed":128,"skipped":2242,"failed":0}
SSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:04:25.751: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-2666
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:04:25.929: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2666
I0406 23:04:25.936075      23 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2666, replica count: 1
I0406 23:04:26.986424      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 23:04:27.103: INFO: Created: latency-svc-hpdm8
Apr  6 23:04:27.133: INFO: Got endpoints: latency-svc-hpdm8 [47.398722ms]
Apr  6 23:04:27.181: INFO: Created: latency-svc-f2ps5
Apr  6 23:04:27.205: INFO: Got endpoints: latency-svc-f2ps5 [71.109401ms]
Apr  6 23:04:27.230: INFO: Created: latency-svc-xxr85
Apr  6 23:04:27.267: INFO: Got endpoints: latency-svc-xxr85 [133.421641ms]
Apr  6 23:04:27.279: INFO: Created: latency-svc-f7fd7
Apr  6 23:04:27.304: INFO: Got endpoints: latency-svc-f7fd7 [170.691079ms]
Apr  6 23:04:27.331: INFO: Created: latency-svc-6dvbw
Apr  6 23:04:27.346: INFO: Got endpoints: latency-svc-6dvbw [211.967298ms]
Apr  6 23:04:27.363: INFO: Created: latency-svc-m5rnb
Apr  6 23:04:27.424: INFO: Got endpoints: latency-svc-m5rnb [290.490259ms]
Apr  6 23:04:27.428: INFO: Created: latency-svc-9jqcf
Apr  6 23:04:27.433: INFO: Got endpoints: latency-svc-9jqcf [298.981644ms]
Apr  6 23:04:27.457: INFO: Created: latency-svc-wd746
Apr  6 23:04:27.488: INFO: Got endpoints: latency-svc-wd746 [354.010037ms]
Apr  6 23:04:27.492: INFO: Created: latency-svc-tjp8p
Apr  6 23:04:27.505: INFO: Got endpoints: latency-svc-tjp8p [371.344253ms]
Apr  6 23:04:27.523: INFO: Created: latency-svc-k2q7c
Apr  6 23:04:27.574: INFO: Got endpoints: latency-svc-k2q7c [440.444922ms]
Apr  6 23:04:27.580: INFO: Created: latency-svc-zkpk4
Apr  6 23:04:27.585: INFO: Got endpoints: latency-svc-zkpk4 [451.049765ms]
Apr  6 23:04:27.604: INFO: Created: latency-svc-hmmgj
Apr  6 23:04:27.612: INFO: Got endpoints: latency-svc-hmmgj [477.745987ms]
Apr  6 23:04:27.638: INFO: Created: latency-svc-5c2fp
Apr  6 23:04:27.671: INFO: Got endpoints: latency-svc-5c2fp [537.127971ms]
Apr  6 23:04:27.733: INFO: Created: latency-svc-9md54
Apr  6 23:04:27.744: INFO: Got endpoints: latency-svc-9md54 [609.491045ms]
Apr  6 23:04:27.763: INFO: Created: latency-svc-2hv54
Apr  6 23:04:27.771: INFO: Got endpoints: latency-svc-2hv54 [637.176849ms]
Apr  6 23:04:27.796: INFO: Created: latency-svc-bdg77
Apr  6 23:04:27.801: INFO: Got endpoints: latency-svc-bdg77 [666.641768ms]
Apr  6 23:04:27.829: INFO: Created: latency-svc-95ldh
Apr  6 23:04:27.896: INFO: Got endpoints: latency-svc-95ldh [691.660607ms]
Apr  6 23:04:27.901: INFO: Created: latency-svc-5gjvb
Apr  6 23:04:27.913: INFO: Got endpoints: latency-svc-5gjvb [645.588525ms]
Apr  6 23:04:27.944: INFO: Created: latency-svc-d67lc
Apr  6 23:04:27.950: INFO: Got endpoints: latency-svc-d67lc [645.33741ms]
Apr  6 23:04:27.977: INFO: Created: latency-svc-qpk9w
Apr  6 23:04:27.977: INFO: Got endpoints: latency-svc-qpk9w [631.197872ms]
Apr  6 23:04:28.058: INFO: Created: latency-svc-pm5s4
Apr  6 23:04:28.079: INFO: Got endpoints: latency-svc-pm5s4 [654.879434ms]
Apr  6 23:04:28.081: INFO: Created: latency-svc-hpz8q
Apr  6 23:04:28.090: INFO: Got endpoints: latency-svc-hpz8q [657.302236ms]
Apr  6 23:04:28.115: INFO: Created: latency-svc-td94p
Apr  6 23:04:28.119: INFO: Got endpoints: latency-svc-td94p [630.666559ms]
Apr  6 23:04:28.138: INFO: Created: latency-svc-p9v4q
Apr  6 23:04:28.146: INFO: Got endpoints: latency-svc-p9v4q [640.23408ms]
Apr  6 23:04:28.273: INFO: Created: latency-svc-msnzq
Apr  6 23:04:28.281: INFO: Got endpoints: latency-svc-msnzq [707.015563ms]
Apr  6 23:04:28.320: INFO: Created: latency-svc-fkzcj
Apr  6 23:04:28.347: INFO: Got endpoints: latency-svc-fkzcj [760.681819ms]
Apr  6 23:04:28.373: INFO: Created: latency-svc-k8pnf
Apr  6 23:04:28.408: INFO: Got endpoints: latency-svc-k8pnf [796.440538ms]
Apr  6 23:04:28.421: INFO: Created: latency-svc-fr2tm
Apr  6 23:04:28.446: INFO: Created: latency-svc-bb5sh
Apr  6 23:04:28.447: INFO: Got endpoints: latency-svc-fr2tm [776.202738ms]
Apr  6 23:04:28.463: INFO: Got endpoints: latency-svc-bb5sh [719.513607ms]
Apr  6 23:04:28.480: INFO: Created: latency-svc-8w25t
Apr  6 23:04:28.497: INFO: Got endpoints: latency-svc-8w25t [725.645784ms]
Apr  6 23:04:28.543: INFO: Created: latency-svc-s8j8n
Apr  6 23:04:28.565: INFO: Got endpoints: latency-svc-s8j8n [764.701223ms]
Apr  6 23:04:28.565: INFO: Created: latency-svc-gzh5l
Apr  6 23:04:28.571: INFO: Got endpoints: latency-svc-gzh5l [674.583041ms]
Apr  6 23:04:28.590: INFO: Created: latency-svc-8v4rt
Apr  6 23:04:28.596: INFO: Got endpoints: latency-svc-8v4rt [683.255031ms]
Apr  6 23:04:28.622: INFO: Created: latency-svc-zhc74
Apr  6 23:04:28.637: INFO: Got endpoints: latency-svc-zhc74 [687.508226ms]
Apr  6 23:04:28.691: INFO: Created: latency-svc-wgh9t
Apr  6 23:04:28.705: INFO: Got endpoints: latency-svc-wgh9t [727.56148ms]
Apr  6 23:04:28.714: INFO: Created: latency-svc-x4rq4
Apr  6 23:04:28.721: INFO: Got endpoints: latency-svc-x4rq4 [641.053871ms]
Apr  6 23:04:28.747: INFO: Created: latency-svc-z9ngl
Apr  6 23:04:28.762: INFO: Got endpoints: latency-svc-z9ngl [671.375543ms]
Apr  6 23:04:28.789: INFO: Created: latency-svc-7v8bq
Apr  6 23:04:28.857: INFO: Got endpoints: latency-svc-7v8bq [738.123503ms]
Apr  6 23:04:28.859: INFO: Created: latency-svc-f6rfq
Apr  6 23:04:28.872: INFO: Got endpoints: latency-svc-f6rfq [725.903312ms]
Apr  6 23:04:28.904: INFO: Created: latency-svc-c6ncg
Apr  6 23:04:28.930: INFO: Got endpoints: latency-svc-c6ncg [649.050859ms]
Apr  6 23:04:28.931: INFO: Created: latency-svc-vczm6
Apr  6 23:04:28.948: INFO: Got endpoints: latency-svc-vczm6 [600.830428ms]
Apr  6 23:04:29.011: INFO: Created: latency-svc-ns2th
Apr  6 23:04:29.033: INFO: Created: latency-svc-j9fjb
Apr  6 23:04:29.033: INFO: Got endpoints: latency-svc-ns2th [624.719502ms]
Apr  6 23:04:29.038: INFO: Got endpoints: latency-svc-j9fjb [591.042403ms]
Apr  6 23:04:29.057: INFO: Created: latency-svc-j2qk6
Apr  6 23:04:29.072: INFO: Got endpoints: latency-svc-j2qk6 [608.311006ms]
Apr  6 23:04:29.104: INFO: Created: latency-svc-rgvrh
Apr  6 23:04:29.190: INFO: Got endpoints: latency-svc-rgvrh [693.462925ms]
Apr  6 23:04:29.193: INFO: Created: latency-svc-wnwzv
Apr  6 23:04:29.219: INFO: Got endpoints: latency-svc-wnwzv [653.994025ms]
Apr  6 23:04:29.254: INFO: Created: latency-svc-dfd7k
Apr  6 23:04:29.269: INFO: Got endpoints: latency-svc-dfd7k [698.336449ms]
Apr  6 23:04:29.359: INFO: Created: latency-svc-8knbf
Apr  6 23:04:29.406: INFO: Created: latency-svc-snpsk
Apr  6 23:04:29.407: INFO: Got endpoints: latency-svc-8knbf [811.053262ms]
Apr  6 23:04:29.440: INFO: Got endpoints: latency-svc-snpsk [803.014323ms]
Apr  6 23:04:29.562: INFO: Created: latency-svc-xzdlt
Apr  6 23:04:29.579: INFO: Got endpoints: latency-svc-xzdlt [874.085709ms]
Apr  6 23:04:29.655: INFO: Created: latency-svc-lxxhv
Apr  6 23:04:29.739: INFO: Got endpoints: latency-svc-lxxhv [1.018913393s]
Apr  6 23:04:29.744: INFO: Created: latency-svc-r9nq9
Apr  6 23:04:29.776: INFO: Got endpoints: latency-svc-r9nq9 [1.014696564s]
Apr  6 23:04:29.812: INFO: Created: latency-svc-rlxnl
Apr  6 23:04:29.819: INFO: Got endpoints: latency-svc-rlxnl [962.147347ms]
Apr  6 23:04:29.890: INFO: Created: latency-svc-cktwl
Apr  6 23:04:29.893: INFO: Got endpoints: latency-svc-cktwl [1.021819882s]
Apr  6 23:04:29.921: INFO: Created: latency-svc-swj8w
Apr  6 23:04:29.925: INFO: Got endpoints: latency-svc-swj8w [994.711018ms]
Apr  6 23:04:29.956: INFO: Created: latency-svc-js4hm
Apr  6 23:04:29.971: INFO: Got endpoints: latency-svc-js4hm [1.02366033s]
Apr  6 23:04:29.989: INFO: Created: latency-svc-rfsc2
Apr  6 23:04:30.064: INFO: Got endpoints: latency-svc-rfsc2 [1.031639089s]
Apr  6 23:04:30.067: INFO: Created: latency-svc-v96b5
Apr  6 23:04:30.076: INFO: Got endpoints: latency-svc-v96b5 [1.037990383s]
Apr  6 23:04:30.107: INFO: Created: latency-svc-25h85
Apr  6 23:04:30.112: INFO: Got endpoints: latency-svc-25h85 [1.039849359s]
Apr  6 23:04:30.140: INFO: Created: latency-svc-7rmlh
Apr  6 23:04:30.148: INFO: Got endpoints: latency-svc-7rmlh [957.16058ms]
Apr  6 23:04:30.231: INFO: Created: latency-svc-b2647
Apr  6 23:04:30.235: INFO: Got endpoints: latency-svc-b2647 [1.015281984s]
Apr  6 23:04:30.281: INFO: Created: latency-svc-2qcbs
Apr  6 23:04:30.288: INFO: Got endpoints: latency-svc-2qcbs [1.018142466s]
Apr  6 23:04:30.319: INFO: Created: latency-svc-s7vhc
Apr  6 23:04:30.324: INFO: Got endpoints: latency-svc-s7vhc [916.723514ms]
Apr  6 23:04:30.399: INFO: Created: latency-svc-gwlmv
Apr  6 23:04:30.424: INFO: Created: latency-svc-kfx4f
Apr  6 23:04:30.424: INFO: Got endpoints: latency-svc-gwlmv [983.841185ms]
Apr  6 23:04:30.437: INFO: Got endpoints: latency-svc-kfx4f [857.934834ms]
Apr  6 23:04:30.458: INFO: Created: latency-svc-hrlh8
Apr  6 23:04:30.465: INFO: Got endpoints: latency-svc-hrlh8 [725.979762ms]
Apr  6 23:04:30.490: INFO: Created: latency-svc-4vfq2
Apr  6 23:04:30.564: INFO: Got endpoints: latency-svc-4vfq2 [787.928701ms]
Apr  6 23:04:30.566: INFO: Created: latency-svc-mvpwx
Apr  6 23:04:30.580: INFO: Got endpoints: latency-svc-mvpwx [760.957764ms]
Apr  6 23:04:30.619: INFO: Created: latency-svc-256ds
Apr  6 23:04:30.623: INFO: Got endpoints: latency-svc-256ds [729.304978ms]
Apr  6 23:04:30.707: INFO: Created: latency-svc-zzq4l
Apr  6 23:04:30.710: INFO: Got endpoints: latency-svc-zzq4l [785.1411ms]
Apr  6 23:04:30.757: INFO: Created: latency-svc-sc5fw
Apr  6 23:04:30.771: INFO: Got endpoints: latency-svc-sc5fw [799.983913ms]
Apr  6 23:04:30.798: INFO: Created: latency-svc-b598c
Apr  6 23:04:30.864: INFO: Got endpoints: latency-svc-b598c [799.061239ms]
Apr  6 23:04:30.882: INFO: Created: latency-svc-jgvvr
Apr  6 23:04:30.885: INFO: Got endpoints: latency-svc-jgvvr [808.359339ms]
Apr  6 23:04:30.914: INFO: Created: latency-svc-bktb4
Apr  6 23:04:30.921: INFO: Got endpoints: latency-svc-bktb4 [809.944393ms]
Apr  6 23:04:31.015: INFO: Created: latency-svc-kj8rh
Apr  6 23:04:31.039: INFO: Got endpoints: latency-svc-kj8rh [891.809519ms]
Apr  6 23:04:31.040: INFO: Created: latency-svc-bm2j2
Apr  6 23:04:31.066: INFO: Got endpoints: latency-svc-bm2j2 [831.09089ms]
Apr  6 23:04:31.073: INFO: Created: latency-svc-ktznw
Apr  6 23:04:31.082: INFO: Got endpoints: latency-svc-ktznw [794.847775ms]
Apr  6 23:04:31.106: INFO: Created: latency-svc-6jhjh
Apr  6 23:04:31.205: INFO: Got endpoints: latency-svc-6jhjh [880.910003ms]
Apr  6 23:04:31.248: INFO: Created: latency-svc-p5gbg
Apr  6 23:04:31.297: INFO: Got endpoints: latency-svc-p5gbg [873.032178ms]
Apr  6 23:04:31.407: INFO: Created: latency-svc-7xwgz
Apr  6 23:04:31.409: INFO: Got endpoints: latency-svc-7xwgz [971.787631ms]
Apr  6 23:04:31.476: INFO: Created: latency-svc-777pr
Apr  6 23:04:31.580: INFO: Got endpoints: latency-svc-777pr [1.114826538s]
Apr  6 23:04:31.583: INFO: Created: latency-svc-srxfv
Apr  6 23:04:31.630: INFO: Got endpoints: latency-svc-srxfv [1.066099187s]
Apr  6 23:04:31.631: INFO: Created: latency-svc-4vh4q
Apr  6 23:04:31.655: INFO: Got endpoints: latency-svc-4vh4q [1.075360175s]
Apr  6 23:04:31.777: INFO: Created: latency-svc-l77lv
Apr  6 23:04:31.810: INFO: Got endpoints: latency-svc-l77lv [1.187002046s]
Apr  6 23:04:31.856: INFO: Created: latency-svc-889kw
Apr  6 23:04:31.870: INFO: Got endpoints: latency-svc-889kw [1.159209558s]
Apr  6 23:04:31.956: INFO: Created: latency-svc-h9qrk
Apr  6 23:04:31.964: INFO: Got endpoints: latency-svc-h9qrk [1.192173095s]
Apr  6 23:04:31.991: INFO: Created: latency-svc-4rlfb
Apr  6 23:04:31.993: INFO: Got endpoints: latency-svc-4rlfb [1.129637788s]
Apr  6 23:04:32.024: INFO: Created: latency-svc-hzn9v
Apr  6 23:04:32.027: INFO: Got endpoints: latency-svc-hzn9v [1.141781791s]
Apr  6 23:04:32.114: INFO: Created: latency-svc-qtxnw
Apr  6 23:04:32.139: INFO: Got endpoints: latency-svc-qtxnw [1.217400514s]
Apr  6 23:04:32.141: INFO: Created: latency-svc-qg2tv
Apr  6 23:04:32.148: INFO: Got endpoints: latency-svc-qg2tv [1.108750938s]
Apr  6 23:04:32.201: INFO: Created: latency-svc-xtg9q
Apr  6 23:04:32.210: INFO: Got endpoints: latency-svc-xtg9q [1.14422908s]
Apr  6 23:04:32.273: INFO: Created: latency-svc-6jkgz
Apr  6 23:04:32.306: INFO: Got endpoints: latency-svc-6jkgz [1.224070084s]
Apr  6 23:04:32.307: INFO: Created: latency-svc-4cv8x
Apr  6 23:04:32.331: INFO: Got endpoints: latency-svc-4cv8x [1.126061037s]
Apr  6 23:04:32.364: INFO: Created: latency-svc-v9mhk
Apr  6 23:04:32.439: INFO: Got endpoints: latency-svc-v9mhk [1.141211779s]
Apr  6 23:04:32.472: INFO: Created: latency-svc-brstz
Apr  6 23:04:32.490: INFO: Got endpoints: latency-svc-brstz [1.081388202s]
Apr  6 23:04:32.507: INFO: Created: latency-svc-4kbht
Apr  6 23:04:32.515: INFO: Got endpoints: latency-svc-4kbht [934.504271ms]
Apr  6 23:04:32.594: INFO: Created: latency-svc-qxj6v
Apr  6 23:04:32.605: INFO: Got endpoints: latency-svc-qxj6v [974.060578ms]
Apr  6 23:04:32.631: INFO: Created: latency-svc-vcj7h
Apr  6 23:04:32.638: INFO: Got endpoints: latency-svc-vcj7h [982.697085ms]
Apr  6 23:04:32.674: INFO: Created: latency-svc-cfqsk
Apr  6 23:04:32.747: INFO: Got endpoints: latency-svc-cfqsk [937.603791ms]
Apr  6 23:04:32.782: INFO: Created: latency-svc-w9xr9
Apr  6 23:04:32.790: INFO: Got endpoints: latency-svc-w9xr9 [920.744723ms]
Apr  6 23:04:32.806: INFO: Created: latency-svc-gwsjn
Apr  6 23:04:32.824: INFO: Got endpoints: latency-svc-gwsjn [859.923544ms]
Apr  6 23:04:32.888: INFO: Created: latency-svc-sj2l6
Apr  6 23:04:32.900: INFO: Got endpoints: latency-svc-sj2l6 [906.872994ms]
Apr  6 23:04:32.923: INFO: Created: latency-svc-9fdbv
Apr  6 23:04:32.927: INFO: Got endpoints: latency-svc-9fdbv [900.500241ms]
Apr  6 23:04:32.948: INFO: Created: latency-svc-8pngv
Apr  6 23:04:32.955: INFO: Got endpoints: latency-svc-8pngv [815.603482ms]
Apr  6 23:04:32.983: INFO: Created: latency-svc-qcxzn
Apr  6 23:04:33.037: INFO: Got endpoints: latency-svc-qcxzn [888.76366ms]
Apr  6 23:04:33.049: INFO: Created: latency-svc-nl78g
Apr  6 23:04:33.065: INFO: Got endpoints: latency-svc-nl78g [854.88674ms]
Apr  6 23:04:33.092: INFO: Created: latency-svc-8x687
Apr  6 23:04:33.097: INFO: Got endpoints: latency-svc-8x687 [60.34571ms]
Apr  6 23:04:33.124: INFO: Created: latency-svc-mhdlg
Apr  6 23:04:33.214: INFO: Got endpoints: latency-svc-mhdlg [907.522091ms]
Apr  6 23:04:33.215: INFO: Created: latency-svc-dv4f5
Apr  6 23:04:33.229: INFO: Got endpoints: latency-svc-dv4f5 [898.000402ms]
Apr  6 23:04:33.265: INFO: Created: latency-svc-p9rnp
Apr  6 23:04:33.282: INFO: Got endpoints: latency-svc-p9rnp [843.760214ms]
Apr  6 23:04:33.307: INFO: Created: latency-svc-vbcdh
Apr  6 23:04:33.412: INFO: Got endpoints: latency-svc-vbcdh [921.8577ms]
Apr  6 23:04:33.417: INFO: Created: latency-svc-g74kk
Apr  6 23:04:33.452: INFO: Got endpoints: latency-svc-g74kk [936.699926ms]
Apr  6 23:04:33.510: INFO: Created: latency-svc-fhgzf
Apr  6 23:04:33.579: INFO: Got endpoints: latency-svc-fhgzf [974.559019ms]
Apr  6 23:04:33.599: INFO: Created: latency-svc-2fl9l
Apr  6 23:04:33.607: INFO: Got endpoints: latency-svc-2fl9l [969.359247ms]
Apr  6 23:04:33.657: INFO: Created: latency-svc-flpnb
Apr  6 23:04:33.673: INFO: Got endpoints: latency-svc-flpnb [925.626052ms]
Apr  6 23:04:33.771: INFO: Created: latency-svc-rtt5h
Apr  6 23:04:33.810: INFO: Got endpoints: latency-svc-rtt5h [1.019384448s]
Apr  6 23:04:33.825: INFO: Created: latency-svc-s7jng
Apr  6 23:04:33.841: INFO: Got endpoints: latency-svc-s7jng [1.016916994s]
Apr  6 23:04:33.857: INFO: Created: latency-svc-p2fld
Apr  6 23:04:33.868: INFO: Got endpoints: latency-svc-p2fld [967.432187ms]
Apr  6 23:04:33.922: INFO: Created: latency-svc-jnfnx
Apr  6 23:04:33.930: INFO: Got endpoints: latency-svc-jnfnx [1.003277261s]
Apr  6 23:04:33.966: INFO: Created: latency-svc-4xc89
Apr  6 23:04:33.975: INFO: Got endpoints: latency-svc-4xc89 [1.020895918s]
Apr  6 23:04:33.999: INFO: Created: latency-svc-7zts8
Apr  6 23:04:34.004: INFO: Got endpoints: latency-svc-7zts8 [939.199265ms]
Apr  6 23:04:34.080: INFO: Created: latency-svc-jdmzr
Apr  6 23:04:34.092: INFO: Got endpoints: latency-svc-jdmzr [994.937819ms]
Apr  6 23:04:34.123: INFO: Created: latency-svc-thnvh
Apr  6 23:04:34.132: INFO: Got endpoints: latency-svc-thnvh [917.8199ms]
Apr  6 23:04:34.149: INFO: Created: latency-svc-kbf9g
Apr  6 23:04:34.157: INFO: Got endpoints: latency-svc-kbf9g [927.966665ms]
Apr  6 23:04:34.221: INFO: Created: latency-svc-v655q
Apr  6 23:04:34.232: INFO: Got endpoints: latency-svc-v655q [950.018687ms]
Apr  6 23:04:34.258: INFO: Created: latency-svc-xnffv
Apr  6 23:04:34.283: INFO: Got endpoints: latency-svc-xnffv [871.095039ms]
Apr  6 23:04:34.285: INFO: Created: latency-svc-2tv6d
Apr  6 23:04:34.290: INFO: Got endpoints: latency-svc-2tv6d [838.421571ms]
Apr  6 23:04:34.308: INFO: Created: latency-svc-whnsz
Apr  6 23:04:34.319: INFO: Got endpoints: latency-svc-whnsz [739.334306ms]
Apr  6 23:04:34.378: INFO: Created: latency-svc-2w7xk
Apr  6 23:04:34.384: INFO: Got endpoints: latency-svc-2w7xk [776.780947ms]
Apr  6 23:04:34.595: INFO: Created: latency-svc-8hmp9
Apr  6 23:04:34.601: INFO: Got endpoints: latency-svc-8hmp9 [927.647762ms]
Apr  6 23:04:34.648: INFO: Created: latency-svc-f7fqd
Apr  6 23:04:34.664: INFO: Got endpoints: latency-svc-f7fqd [853.792996ms]
Apr  6 23:04:34.763: INFO: Created: latency-svc-js2ln
Apr  6 23:04:34.794: INFO: Got endpoints: latency-svc-js2ln [953.739539ms]
Apr  6 23:04:34.906: INFO: Created: latency-svc-vlk4w
Apr  6 23:04:34.913: INFO: Got endpoints: latency-svc-vlk4w [1.044912189s]
Apr  6 23:04:34.961: INFO: Created: latency-svc-lkzv7
Apr  6 23:04:34.965: INFO: Got endpoints: latency-svc-lkzv7 [1.034840077s]
Apr  6 23:04:35.005: INFO: Created: latency-svc-5dhgm
Apr  6 23:04:35.078: INFO: Got endpoints: latency-svc-5dhgm [1.102288418s]
Apr  6 23:04:35.083: INFO: Created: latency-svc-fld6t
Apr  6 23:04:35.089: INFO: Got endpoints: latency-svc-fld6t [1.084863774s]
Apr  6 23:04:35.119: INFO: Created: latency-svc-d2vvs
Apr  6 23:04:35.124: INFO: Got endpoints: latency-svc-d2vvs [1.031996834s]
Apr  6 23:04:35.163: INFO: Created: latency-svc-9bc9h
Apr  6 23:04:35.169: INFO: Got endpoints: latency-svc-9bc9h [1.037067961s]
Apr  6 23:04:35.246: INFO: Created: latency-svc-mzxsc
Apr  6 23:04:35.278: INFO: Got endpoints: latency-svc-mzxsc [1.121198222s]
Apr  6 23:04:35.444: INFO: Created: latency-svc-7c4b6
Apr  6 23:04:35.486: INFO: Got endpoints: latency-svc-7c4b6 [1.253666294s]
Apr  6 23:04:35.488: INFO: Created: latency-svc-2c9w8
Apr  6 23:04:35.527: INFO: Got endpoints: latency-svc-2c9w8 [1.244270264s]
Apr  6 23:04:35.620: INFO: Created: latency-svc-r4zdr
Apr  6 23:04:35.637: INFO: Got endpoints: latency-svc-r4zdr [1.346916926s]
Apr  6 23:04:35.669: INFO: Created: latency-svc-x69nm
Apr  6 23:04:35.705: INFO: Got endpoints: latency-svc-x69nm [1.386407873s]
Apr  6 23:04:35.819: INFO: Created: latency-svc-xc7v6
Apr  6 23:04:35.860: INFO: Got endpoints: latency-svc-xc7v6 [1.475649214s]
Apr  6 23:04:35.895: INFO: Created: latency-svc-fr6tc
Apr  6 23:04:35.993: INFO: Got endpoints: latency-svc-fr6tc [1.39266094s]
Apr  6 23:04:35.996: INFO: Created: latency-svc-z6vwx
Apr  6 23:04:36.024: INFO: Got endpoints: latency-svc-z6vwx [1.360525782s]
Apr  6 23:04:36.170: INFO: Created: latency-svc-8csd7
Apr  6 23:04:36.221: INFO: Created: latency-svc-6fh79
Apr  6 23:04:36.221: INFO: Got endpoints: latency-svc-8csd7 [1.426327897s]
Apr  6 23:04:36.236: INFO: Got endpoints: latency-svc-6fh79 [1.322581622s]
Apr  6 23:04:36.244: INFO: Created: latency-svc-jnd47
Apr  6 23:04:36.259: INFO: Got endpoints: latency-svc-jnd47 [1.294171533s]
Apr  6 23:04:36.319: INFO: Created: latency-svc-znwnl
Apr  6 23:04:36.343: INFO: Got endpoints: latency-svc-znwnl [1.26500344s]
Apr  6 23:04:36.362: INFO: Created: latency-svc-qcffp
Apr  6 23:04:36.369: INFO: Got endpoints: latency-svc-qcffp [1.279743581s]
Apr  6 23:04:36.395: INFO: Created: latency-svc-m58nf
Apr  6 23:04:36.400: INFO: Got endpoints: latency-svc-m58nf [1.275431362s]
Apr  6 23:04:36.417: INFO: Created: latency-svc-87ngk
Apr  6 23:04:36.487: INFO: Got endpoints: latency-svc-87ngk [1.318384746s]
Apr  6 23:04:36.489: INFO: Created: latency-svc-xl6g7
Apr  6 23:04:36.504: INFO: Got endpoints: latency-svc-xl6g7 [1.225674845s]
Apr  6 23:04:36.552: INFO: Created: latency-svc-wghnp
Apr  6 23:04:36.564: INFO: Got endpoints: latency-svc-wghnp [1.076009458s]
Apr  6 23:04:36.637: INFO: Created: latency-svc-l7k6b
Apr  6 23:04:36.668: INFO: Got endpoints: latency-svc-l7k6b [1.141035593s]
Apr  6 23:04:36.670: INFO: Created: latency-svc-75fkl
Apr  6 23:04:36.693: INFO: Got endpoints: latency-svc-75fkl [1.056350831s]
Apr  6 23:04:36.720: INFO: Created: latency-svc-2lpwl
Apr  6 23:04:36.731: INFO: Got endpoints: latency-svc-2lpwl [1.025876085s]
Apr  6 23:04:36.785: INFO: Created: latency-svc-hcmqz
Apr  6 23:04:36.789: INFO: Got endpoints: latency-svc-hcmqz [929.046095ms]
Apr  6 23:04:36.818: INFO: Created: latency-svc-b8s8c
Apr  6 23:04:36.832: INFO: Got endpoints: latency-svc-b8s8c [838.149505ms]
Apr  6 23:04:36.860: INFO: Created: latency-svc-5ttqs
Apr  6 23:04:36.867: INFO: Got endpoints: latency-svc-5ttqs [842.410038ms]
Apr  6 23:04:36.953: INFO: Created: latency-svc-lwct9
Apr  6 23:04:36.976: INFO: Got endpoints: latency-svc-lwct9 [755.258904ms]
Apr  6 23:04:37.006: INFO: Created: latency-svc-wrfcd
Apr  6 23:04:37.014: INFO: Got endpoints: latency-svc-wrfcd [777.936942ms]
Apr  6 23:04:37.035: INFO: Created: latency-svc-d628d
Apr  6 23:04:37.041: INFO: Got endpoints: latency-svc-d628d [781.684193ms]
Apr  6 23:04:37.102: INFO: Created: latency-svc-t8tfg
Apr  6 23:04:37.112: INFO: Got endpoints: latency-svc-t8tfg [768.784851ms]
Apr  6 23:04:37.136: INFO: Created: latency-svc-h77vp
Apr  6 23:04:37.188: INFO: Got endpoints: latency-svc-h77vp [818.581773ms]
Apr  6 23:04:37.302: INFO: Created: latency-svc-r5fqp
Apr  6 23:04:37.343: INFO: Created: latency-svc-hr6hr
Apr  6 23:04:37.344: INFO: Got endpoints: latency-svc-r5fqp [943.874445ms]
Apr  6 23:04:37.352: INFO: Got endpoints: latency-svc-hr6hr [865.026192ms]
Apr  6 23:04:37.460: INFO: Created: latency-svc-nmns6
Apr  6 23:04:37.534: INFO: Created: latency-svc-wx7d8
Apr  6 23:04:37.535: INFO: Got endpoints: latency-svc-nmns6 [1.031411694s]
Apr  6 23:04:37.618: INFO: Created: latency-svc-xrbr7
Apr  6 23:04:37.619: INFO: Got endpoints: latency-svc-wx7d8 [1.055366099s]
Apr  6 23:04:37.622: INFO: Got endpoints: latency-svc-xrbr7 [953.981646ms]
Apr  6 23:04:37.662: INFO: Created: latency-svc-762n5
Apr  6 23:04:37.676: INFO: Got endpoints: latency-svc-762n5 [983.053122ms]
Apr  6 23:04:37.701: INFO: Created: latency-svc-zwpd4
Apr  6 23:04:37.708: INFO: Got endpoints: latency-svc-zwpd4 [977.504271ms]
Apr  6 23:04:37.776: INFO: Created: latency-svc-5dl9w
Apr  6 23:04:37.802: INFO: Got endpoints: latency-svc-5dl9w [1.012587399s]
Apr  6 23:04:37.827: INFO: Created: latency-svc-pxgj7
Apr  6 23:04:37.832: INFO: Got endpoints: latency-svc-pxgj7 [1.000729826s]
Apr  6 23:04:37.853: INFO: Created: latency-svc-wpq9h
Apr  6 23:04:37.862: INFO: Got endpoints: latency-svc-wpq9h [995.700468ms]
Apr  6 23:04:37.926: INFO: Created: latency-svc-xwbgf
Apr  6 23:04:37.946: INFO: Got endpoints: latency-svc-xwbgf [970.545057ms]
Apr  6 23:04:37.947: INFO: Created: latency-svc-hplq6
Apr  6 23:04:37.954: INFO: Got endpoints: latency-svc-hplq6 [940.555866ms]
Apr  6 23:04:37.985: INFO: Created: latency-svc-wmz99
Apr  6 23:04:37.994: INFO: Got endpoints: latency-svc-wmz99 [952.484112ms]
Apr  6 23:04:38.013: INFO: Created: latency-svc-qfsgk
Apr  6 23:04:38.020: INFO: Got endpoints: latency-svc-qfsgk [908.375642ms]
Apr  6 23:04:38.069: INFO: Created: latency-svc-7gzqb
Apr  6 23:04:38.087: INFO: Got endpoints: latency-svc-7gzqb [899.825717ms]
Apr  6 23:04:38.111: INFO: Created: latency-svc-5qnbm
Apr  6 23:04:38.133: INFO: Got endpoints: latency-svc-5qnbm [788.805745ms]
Apr  6 23:04:38.217: INFO: Created: latency-svc-p4ljq
Apr  6 23:04:38.223: INFO: Got endpoints: latency-svc-p4ljq [870.33645ms]
Apr  6 23:04:38.252: INFO: Created: latency-svc-xts9r
Apr  6 23:04:38.258: INFO: Got endpoints: latency-svc-xts9r [722.306721ms]
Apr  6 23:04:38.287: INFO: Created: latency-svc-xx2lw
Apr  6 23:04:38.317: INFO: Got endpoints: latency-svc-xx2lw [697.860716ms]
Apr  6 23:04:38.413: INFO: Created: latency-svc-68hp5
Apr  6 23:04:38.427: INFO: Got endpoints: latency-svc-68hp5 [804.5328ms]
Apr  6 23:04:38.477: INFO: Created: latency-svc-khjzq
Apr  6 23:04:38.500: INFO: Got endpoints: latency-svc-khjzq [823.349ms]
Apr  6 23:04:38.600: INFO: Created: latency-svc-6txck
Apr  6 23:04:38.604: INFO: Got endpoints: latency-svc-6txck [895.164386ms]
Apr  6 23:04:38.644: INFO: Created: latency-svc-r84m6
Apr  6 23:04:38.653: INFO: Got endpoints: latency-svc-r84m6 [850.915635ms]
Apr  6 23:04:38.742: INFO: Created: latency-svc-hzwnn
Apr  6 23:04:38.769: INFO: Got endpoints: latency-svc-hzwnn [936.243341ms]
Apr  6 23:04:38.770: INFO: Created: latency-svc-blj4r
Apr  6 23:04:38.785: INFO: Got endpoints: latency-svc-blj4r [922.464749ms]
Apr  6 23:04:38.802: INFO: Created: latency-svc-gtsdt
Apr  6 23:04:38.829: INFO: Got endpoints: latency-svc-gtsdt [882.117307ms]
Apr  6 23:04:38.901: INFO: Created: latency-svc-76km5
Apr  6 23:04:38.932: INFO: Got endpoints: latency-svc-76km5 [977.767371ms]
Apr  6 23:04:38.953: INFO: Created: latency-svc-dw8z2
Apr  6 23:04:38.964: INFO: Got endpoints: latency-svc-dw8z2 [970.402752ms]
Apr  6 23:04:39.042: INFO: Created: latency-svc-rvqzf
Apr  6 23:04:39.061: INFO: Got endpoints: latency-svc-rvqzf [1.040477086s]
Apr  6 23:04:39.061: INFO: Created: latency-svc-s4kfv
Apr  6 23:04:39.090: INFO: Got endpoints: latency-svc-s4kfv [1.002974958s]
Apr  6 23:04:39.111: INFO: Created: latency-svc-7v5kx
Apr  6 23:04:39.192: INFO: Got endpoints: latency-svc-7v5kx [1.059401969s]
Apr  6 23:04:39.194: INFO: Created: latency-svc-pp7pf
Apr  6 23:04:39.199: INFO: Got endpoints: latency-svc-pp7pf [976.097312ms]
Apr  6 23:04:39.233: INFO: Created: latency-svc-dvbqs
Apr  6 23:04:39.261: INFO: Created: latency-svc-6dcnb
Apr  6 23:04:39.262: INFO: Got endpoints: latency-svc-dvbqs [1.004066688s]
Apr  6 23:04:39.277: INFO: Got endpoints: latency-svc-6dcnb [960.451683ms]
Apr  6 23:04:39.343: INFO: Created: latency-svc-mdqmt
Apr  6 23:04:39.346: INFO: Got endpoints: latency-svc-mdqmt [918.551674ms]
Apr  6 23:04:39.346: INFO: Latencies: [60.34571ms 71.109401ms 133.421641ms 170.691079ms 211.967298ms 290.490259ms 298.981644ms 354.010037ms 371.344253ms 440.444922ms 451.049765ms 477.745987ms 537.127971ms 591.042403ms 600.830428ms 608.311006ms 609.491045ms 624.719502ms 630.666559ms 631.197872ms 637.176849ms 640.23408ms 641.053871ms 645.33741ms 645.588525ms 649.050859ms 653.994025ms 654.879434ms 657.302236ms 666.641768ms 671.375543ms 674.583041ms 683.255031ms 687.508226ms 691.660607ms 693.462925ms 697.860716ms 698.336449ms 707.015563ms 719.513607ms 722.306721ms 725.645784ms 725.903312ms 725.979762ms 727.56148ms 729.304978ms 738.123503ms 739.334306ms 755.258904ms 760.681819ms 760.957764ms 764.701223ms 768.784851ms 776.202738ms 776.780947ms 777.936942ms 781.684193ms 785.1411ms 787.928701ms 788.805745ms 794.847775ms 796.440538ms 799.061239ms 799.983913ms 803.014323ms 804.5328ms 808.359339ms 809.944393ms 811.053262ms 815.603482ms 818.581773ms 823.349ms 831.09089ms 838.149505ms 838.421571ms 842.410038ms 843.760214ms 850.915635ms 853.792996ms 854.88674ms 857.934834ms 859.923544ms 865.026192ms 870.33645ms 871.095039ms 873.032178ms 874.085709ms 880.910003ms 882.117307ms 888.76366ms 891.809519ms 895.164386ms 898.000402ms 899.825717ms 900.500241ms 906.872994ms 907.522091ms 908.375642ms 916.723514ms 917.8199ms 918.551674ms 920.744723ms 921.8577ms 922.464749ms 925.626052ms 927.647762ms 927.966665ms 929.046095ms 934.504271ms 936.243341ms 936.699926ms 937.603791ms 939.199265ms 940.555866ms 943.874445ms 950.018687ms 952.484112ms 953.739539ms 953.981646ms 957.16058ms 960.451683ms 962.147347ms 967.432187ms 969.359247ms 970.402752ms 970.545057ms 971.787631ms 974.060578ms 974.559019ms 976.097312ms 977.504271ms 977.767371ms 982.697085ms 983.053122ms 983.841185ms 994.711018ms 994.937819ms 995.700468ms 1.000729826s 1.002974958s 1.003277261s 1.004066688s 1.012587399s 1.014696564s 1.015281984s 1.016916994s 1.018142466s 1.018913393s 1.019384448s 1.020895918s 1.021819882s 1.02366033s 1.025876085s 1.031411694s 1.031639089s 1.031996834s 1.034840077s 1.037067961s 1.037990383s 1.039849359s 1.040477086s 1.044912189s 1.055366099s 1.056350831s 1.059401969s 1.066099187s 1.075360175s 1.076009458s 1.081388202s 1.084863774s 1.102288418s 1.108750938s 1.114826538s 1.121198222s 1.126061037s 1.129637788s 1.141035593s 1.141211779s 1.141781791s 1.14422908s 1.159209558s 1.187002046s 1.192173095s 1.217400514s 1.224070084s 1.225674845s 1.244270264s 1.253666294s 1.26500344s 1.275431362s 1.279743581s 1.294171533s 1.318384746s 1.322581622s 1.346916926s 1.360525782s 1.386407873s 1.39266094s 1.426327897s 1.475649214s]
Apr  6 23:04:39.346: INFO: 50 %ile: 918.551674ms
Apr  6 23:04:39.346: INFO: 90 %ile: 1.159209558s
Apr  6 23:04:39.346: INFO: 99 %ile: 1.426327897s
Apr  6 23:04:39.346: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:04:39.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2666" for this suite.

â€¢ [SLOW TEST:13.610 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":275,"completed":129,"skipped":2245,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:04:39.362: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5451
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr  6 23:04:42.036: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5451 pod-service-account-9c6e9588-57ce-4bb1-9b84-8efd4b34fecb -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr  6 23:04:42.175: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5451 pod-service-account-9c6e9588-57ce-4bb1-9b84-8efd4b34fecb -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr  6 23:04:42.312: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5451 pod-service-account-9c6e9588-57ce-4bb1-9b84-8efd4b34fecb -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:04:42.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5451" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":275,"completed":130,"skipped":2267,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:04:42.492: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override all
Apr  6 23:04:42.767: INFO: Waiting up to 5m0s for pod "client-containers-513d86cf-f029-4b6b-afda-d943fd48c37e" in namespace "containers-9425" to be "Succeeded or Failed"
Apr  6 23:04:42.778: INFO: Pod "client-containers-513d86cf-f029-4b6b-afda-d943fd48c37e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.08916ms
Apr  6 23:04:44.782: INFO: Pod "client-containers-513d86cf-f029-4b6b-afda-d943fd48c37e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015176542s
STEP: Saw pod success
Apr  6 23:04:44.782: INFO: Pod "client-containers-513d86cf-f029-4b6b-afda-d943fd48c37e" satisfied condition "Succeeded or Failed"
Apr  6 23:04:44.790: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod client-containers-513d86cf-f029-4b6b-afda-d943fd48c37e container test-container: <nil>
STEP: delete the pod
Apr  6 23:04:44.864: INFO: Waiting for pod client-containers-513d86cf-f029-4b6b-afda-d943fd48c37e to disappear
Apr  6 23:04:44.873: INFO: Pod client-containers-513d86cf-f029-4b6b-afda-d943fd48c37e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:04:44.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9425" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":275,"completed":131,"skipped":2276,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:04:44.909: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5597
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-4b5893d1-c5e6-42c6-b7b3-f196b47c68d4
STEP: Creating configMap with name cm-test-opt-upd-282366ac-c913-4bf8-973d-749184dd8ca5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-4b5893d1-c5e6-42c6-b7b3-f196b47c68d4
STEP: Updating configmap cm-test-opt-upd-282366ac-c913-4bf8-973d-749184dd8ca5
STEP: Creating configMap with name cm-test-opt-create-3b672904-cee9-4dbe-8524-0e377c6be760
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:04:51.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5597" for this suite.

â€¢ [SLOW TEST:6.684 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":132,"skipped":2299,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:04:51.593: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8532
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-8532
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  6 23:04:51.947: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr  6 23:04:52.175: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 23:04:54.186: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 23:04:56.184: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 23:04:58.179: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 23:05:00.194: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 23:05:02.210: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 23:05:04.198: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr  6 23:05:04.211: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr  6 23:05:06.230: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Apr  6 23:05:08.399: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.192.1.34:8080/dial?request=hostname&protocol=http&host=10.192.0.168&port=8080&tries=1'] Namespace:pod-network-test-8532 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 23:05:08.399: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 23:05:08.490: INFO: Waiting for responses: map[]
Apr  6 23:05:08.520: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.192.1.34:8080/dial?request=hostname&protocol=http&host=10.192.1.33&port=8080&tries=1'] Namespace:pod-network-test-8532 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 23:05:08.520: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 23:05:08.642: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:05:08.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8532" for this suite.

â€¢ [SLOW TEST:17.054 seconds]
[sig-network] Networking
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":275,"completed":133,"skipped":2320,"failed":0}
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:05:08.647: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1337
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:05:08.837: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr  6 23:05:08.849: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr  6 23:05:13.919: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr  6 23:05:13.919: INFO: Creating deployment "test-rolling-update-deployment"
Apr  6 23:05:13.930: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr  6 23:05:13.946: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr  6 23:05:15.954: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr  6 23:05:15.956: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721811113, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721811113, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721811115, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721811113, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-59d5cb45c7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 23:05:17.958: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Apr  6 23:05:17.962: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1337 /apis/apps/v1/namespaces/deployment-1337/deployments/test-rolling-update-deployment f7cbfa41-884b-4198-a6c4-28d9881fea6c 22675 1 2020-04-06 23:05:13 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-04-06 23:05:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-04-06 23:05:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001b0b1a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-04-06 23:05:13 +0000 UTC,LastTransitionTime:2020-04-06 23:05:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-59d5cb45c7" has successfully progressed.,LastUpdateTime:2020-04-06 23:05:15 +0000 UTC,LastTransitionTime:2020-04-06 23:05:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr  6 23:05:17.963: INFO: New ReplicaSet "test-rolling-update-deployment-59d5cb45c7" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7  deployment-1337 /apis/apps/v1/namespaces/deployment-1337/replicasets/test-rolling-update-deployment-59d5cb45c7 bdbb561c-5cea-4c6c-aad3-e0bd062379d0 22663 1 2020-04-06 23:05:13 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment f7cbfa41-884b-4198-a6c4-28d9881fea6c 0xc003acaa07 0xc003acaa08}] []  [{kube-controller-manager Update apps/v1 2020-04-06 23:05:15 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 55 99 98 102 97 52 49 45 56 56 52 98 45 52 49 57 56 45 97 54 99 52 45 50 56 100 57 56 56 49 102 101 97 54 99 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 59d5cb45c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003acab08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr  6 23:05:17.963: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr  6 23:05:17.963: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1337 /apis/apps/v1/namespaces/deployment-1337/replicasets/test-rolling-update-controller 6caca43f-5573-4b1c-9b3e-f175910900da 22674 2 2020-04-06 23:05:08 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment f7cbfa41-884b-4198-a6c4-28d9881fea6c 0xc003aca8a7 0xc003aca8a8}] []  [{e2e.test Update apps/v1 2020-04-06 23:05:08 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-04-06 23:05:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 55 99 98 102 97 52 49 45 56 56 52 98 45 52 49 57 56 45 97 54 99 52 45 50 56 100 57 56 56 49 102 101 97 54 99 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003aca978 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 23:05:17.965: INFO: Pod "test-rolling-update-deployment-59d5cb45c7-z4bjf" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7-z4bjf test-rolling-update-deployment-59d5cb45c7- deployment-1337 /api/v1/namespaces/deployment-1337/pods/test-rolling-update-deployment-59d5cb45c7-z4bjf b9e22b14-cb85-4421-9d24-4d3237d646eb 22661 0 2020-04-06 23:05:13 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[cni.projectcalico.org/podIP:10.192.1.36/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet test-rolling-update-deployment-59d5cb45c7 bdbb561c-5cea-4c6c-aad3-e0bd062379d0 0xc003acb297 0xc003acb298}] []  [{kube-controller-manager Update v1 2020-04-06 23:05:13 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 100 98 98 53 54 49 99 45 53 99 101 97 45 52 99 54 99 45 97 97 100 51 45 101 48 98 100 48 54 50 51 55 57 100 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-04-06 23:05:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-04-06 23:05:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 49 57 50 46 49 46 51 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tw7wj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tw7wj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tw7wj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:05:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:05:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:05:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:05:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.74,PodIP:10.192.1.36,StartTime:2020-04-06 23:05:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-06 23:05:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://61d6245e906e77e147382babb586b0962fbe421d4e8bf40358ee216cf5c137b0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.192.1.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:05:17.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1337" for this suite.

â€¢ [SLOW TEST:9.321 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":275,"completed":134,"skipped":2320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:05:17.969: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8699
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-projected-all-test-volume-7f102933-fe00-490f-abe6-12d63f686ab5
STEP: Creating secret with name secret-projected-all-test-volume-22fd0221-5f03-40a6-8a23-dc3d90dca04d
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr  6 23:05:18.216: INFO: Waiting up to 5m0s for pod "projected-volume-9bd9603f-dea0-421b-acfd-ca34d333bb13" in namespace "projected-8699" to be "Succeeded or Failed"
Apr  6 23:05:18.240: INFO: Pod "projected-volume-9bd9603f-dea0-421b-acfd-ca34d333bb13": Phase="Pending", Reason="", readiness=false. Elapsed: 24.091036ms
Apr  6 23:05:20.241: INFO: Pod "projected-volume-9bd9603f-dea0-421b-acfd-ca34d333bb13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025798644s
STEP: Saw pod success
Apr  6 23:05:20.241: INFO: Pod "projected-volume-9bd9603f-dea0-421b-acfd-ca34d333bb13" satisfied condition "Succeeded or Failed"
Apr  6 23:05:20.243: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod projected-volume-9bd9603f-dea0-421b-acfd-ca34d333bb13 container projected-all-volume-test: <nil>
STEP: delete the pod
Apr  6 23:05:20.274: INFO: Waiting for pod projected-volume-9bd9603f-dea0-421b-acfd-ca34d333bb13 to disappear
Apr  6 23:05:20.286: INFO: Pod projected-volume-9bd9603f-dea0-421b-acfd-ca34d333bb13 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:05:20.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8699" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":275,"completed":135,"skipped":2362,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:05:20.291: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3869
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr  6 23:05:20.494: INFO: Waiting up to 5m0s for pod "pod-8c17a27a-3d99-4e8e-8afc-6852dddb5a7d" in namespace "emptydir-3869" to be "Succeeded or Failed"
Apr  6 23:05:20.502: INFO: Pod "pod-8c17a27a-3d99-4e8e-8afc-6852dddb5a7d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.729259ms
Apr  6 23:05:22.504: INFO: Pod "pod-8c17a27a-3d99-4e8e-8afc-6852dddb5a7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010254244s
STEP: Saw pod success
Apr  6 23:05:22.504: INFO: Pod "pod-8c17a27a-3d99-4e8e-8afc-6852dddb5a7d" satisfied condition "Succeeded or Failed"
Apr  6 23:05:22.505: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-8c17a27a-3d99-4e8e-8afc-6852dddb5a7d container test-container: <nil>
STEP: delete the pod
Apr  6 23:05:22.548: INFO: Waiting for pod pod-8c17a27a-3d99-4e8e-8afc-6852dddb5a7d to disappear
Apr  6 23:05:22.559: INFO: Pod pod-8c17a27a-3d99-4e8e-8afc-6852dddb5a7d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:05:22.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3869" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":136,"skipped":2420,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:05:22.563: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting the proxy server
Apr  6 23:05:22.729: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-285118746 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:05:22.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5107" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":275,"completed":137,"skipped":2453,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:05:22.782: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating api versions
Apr  6 23:05:22.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 api-versions'
Apr  6 23:05:23.207: INFO: stderr: ""
Apr  6 23:05:23.207: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nhpecp.hpe.com/v1\nkubedirector.hpe.com/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nsnapshot.storage.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:05:23.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9498" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":275,"completed":138,"skipped":2479,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:05:23.219: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-934
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 23:05:23.468: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e30c4ce-e74a-4fab-a6f9-90a331dcd077" in namespace "projected-934" to be "Succeeded or Failed"
Apr  6 23:05:23.497: INFO: Pod "downwardapi-volume-5e30c4ce-e74a-4fab-a6f9-90a331dcd077": Phase="Pending", Reason="", readiness=false. Elapsed: 29.109093ms
Apr  6 23:05:25.499: INFO: Pod "downwardapi-volume-5e30c4ce-e74a-4fab-a6f9-90a331dcd077": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030825253s
STEP: Saw pod success
Apr  6 23:05:25.499: INFO: Pod "downwardapi-volume-5e30c4ce-e74a-4fab-a6f9-90a331dcd077" satisfied condition "Succeeded or Failed"
Apr  6 23:05:25.500: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-5e30c4ce-e74a-4fab-a6f9-90a331dcd077 container client-container: <nil>
STEP: delete the pod
Apr  6 23:05:25.523: INFO: Waiting for pod downwardapi-volume-5e30c4ce-e74a-4fab-a6f9-90a331dcd077 to disappear
Apr  6 23:05:25.525: INFO: Pod downwardapi-volume-5e30c4ce-e74a-4fab-a6f9-90a331dcd077 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:05:25.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-934" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":275,"completed":139,"skipped":2487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:05:25.530: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6045
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr  6 23:05:28.740: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:05:29.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6045" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":275,"completed":140,"skipped":2522,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:05:29.797: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8532
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 23:05:30.081: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0bcfb7f-4b25-460f-9e6c-e5e2d9d07df3" in namespace "projected-8532" to be "Succeeded or Failed"
Apr  6 23:05:30.127: INFO: Pod "downwardapi-volume-f0bcfb7f-4b25-460f-9e6c-e5e2d9d07df3": Phase="Pending", Reason="", readiness=false. Elapsed: 45.659122ms
Apr  6 23:05:32.129: INFO: Pod "downwardapi-volume-f0bcfb7f-4b25-460f-9e6c-e5e2d9d07df3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.04761313s
STEP: Saw pod success
Apr  6 23:05:32.129: INFO: Pod "downwardapi-volume-f0bcfb7f-4b25-460f-9e6c-e5e2d9d07df3" satisfied condition "Succeeded or Failed"
Apr  6 23:05:32.130: INFO: Trying to get logs from node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net pod downwardapi-volume-f0bcfb7f-4b25-460f-9e6c-e5e2d9d07df3 container client-container: <nil>
STEP: delete the pod
Apr  6 23:05:32.219: INFO: Waiting for pod downwardapi-volume-f0bcfb7f-4b25-460f-9e6c-e5e2d9d07df3 to disappear
Apr  6 23:05:32.248: INFO: Pod downwardapi-volume-f0bcfb7f-4b25-460f-9e6c-e5e2d9d07df3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:05:32.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8532" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":275,"completed":141,"skipped":2528,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:05:32.269: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8947
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Apr  6 23:05:42.434: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0406 23:05:42.434526      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:05:42.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8947" for this suite.

â€¢ [SLOW TEST:10.169 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":275,"completed":142,"skipped":2542,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:05:42.438: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1644
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-02d80118-87ef-4b42-826a-4779edd58047 in namespace container-probe-1644
Apr  6 23:05:44.657: INFO: Started pod busybox-02d80118-87ef-4b42-826a-4779edd58047 in namespace container-probe-1644
STEP: checking the pod's current state and verifying that restartCount is present
Apr  6 23:05:44.658: INFO: Initial restart count of pod busybox-02d80118-87ef-4b42-826a-4779edd58047 is 0
Apr  6 23:06:30.823: INFO: Restart count of pod container-probe-1644/busybox-02d80118-87ef-4b42-826a-4779edd58047 is now 1 (46.165060509s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:06:30.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1644" for this suite.

â€¢ [SLOW TEST:48.422 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":275,"completed":143,"skipped":2558,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:06:30.860: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6340
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-83a258f0-b100-48e3-a441-900cc319d691
STEP: Creating a pod to test consume configMaps
Apr  6 23:06:31.062: INFO: Waiting up to 5m0s for pod "pod-configmaps-a78dee92-f6cf-4e7b-8aba-edfed1c3899d" in namespace "configmap-6340" to be "Succeeded or Failed"
Apr  6 23:06:31.068: INFO: Pod "pod-configmaps-a78dee92-f6cf-4e7b-8aba-edfed1c3899d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.349703ms
Apr  6 23:06:33.070: INFO: Pod "pod-configmaps-a78dee92-f6cf-4e7b-8aba-edfed1c3899d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008215415s
STEP: Saw pod success
Apr  6 23:06:33.070: INFO: Pod "pod-configmaps-a78dee92-f6cf-4e7b-8aba-edfed1c3899d" satisfied condition "Succeeded or Failed"
Apr  6 23:06:33.071: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-configmaps-a78dee92-f6cf-4e7b-8aba-edfed1c3899d container configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 23:06:33.149: INFO: Waiting for pod pod-configmaps-a78dee92-f6cf-4e7b-8aba-edfed1c3899d to disappear
Apr  6 23:06:33.175: INFO: Pod pod-configmaps-a78dee92-f6cf-4e7b-8aba-edfed1c3899d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:06:33.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6340" for this suite.
â€¢{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":275,"completed":144,"skipped":2582,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:06:33.183: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Apr  6 23:06:33.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 create -f - --namespace=kubectl-9749'
Apr  6 23:06:33.739: INFO: stderr: ""
Apr  6 23:06:33.739: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  6 23:06:33.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9749'
Apr  6 23:06:33.813: INFO: stderr: ""
Apr  6 23:06:33.813: INFO: stdout: "update-demo-nautilus-nqn7f update-demo-nautilus-vs4sl "
Apr  6 23:06:33.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-nqn7f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9749'
Apr  6 23:06:33.903: INFO: stderr: ""
Apr  6 23:06:33.903: INFO: stdout: ""
Apr  6 23:06:33.903: INFO: update-demo-nautilus-nqn7f is created but not running
Apr  6 23:06:38.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9749'
Apr  6 23:06:38.959: INFO: stderr: ""
Apr  6 23:06:38.959: INFO: stdout: "update-demo-nautilus-nqn7f update-demo-nautilus-vs4sl "
Apr  6 23:06:38.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-nqn7f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9749'
Apr  6 23:06:39.013: INFO: stderr: ""
Apr  6 23:06:39.013: INFO: stdout: "true"
Apr  6 23:06:39.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-nqn7f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9749'
Apr  6 23:06:39.070: INFO: stderr: ""
Apr  6 23:06:39.070: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  6 23:06:39.070: INFO: validating pod update-demo-nautilus-nqn7f
Apr  6 23:06:39.072: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 23:06:39.072: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 23:06:39.072: INFO: update-demo-nautilus-nqn7f is verified up and running
Apr  6 23:06:39.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-vs4sl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9749'
Apr  6 23:06:39.126: INFO: stderr: ""
Apr  6 23:06:39.126: INFO: stdout: "true"
Apr  6 23:06:39.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-vs4sl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9749'
Apr  6 23:06:39.179: INFO: stderr: ""
Apr  6 23:06:39.179: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  6 23:06:39.179: INFO: validating pod update-demo-nautilus-vs4sl
Apr  6 23:06:39.182: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 23:06:39.182: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 23:06:39.182: INFO: update-demo-nautilus-vs4sl is verified up and running
STEP: scaling down the replication controller
Apr  6 23:06:39.183: INFO: scanned /root for discovery docs: <nil>
Apr  6 23:06:39.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9749'
Apr  6 23:06:40.276: INFO: stderr: ""
Apr  6 23:06:40.276: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  6 23:06:40.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9749'
Apr  6 23:06:40.335: INFO: stderr: ""
Apr  6 23:06:40.335: INFO: stdout: "update-demo-nautilus-nqn7f update-demo-nautilus-vs4sl "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr  6 23:06:45.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9749'
Apr  6 23:06:45.412: INFO: stderr: ""
Apr  6 23:06:45.412: INFO: stdout: "update-demo-nautilus-nqn7f "
Apr  6 23:06:45.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-nqn7f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9749'
Apr  6 23:06:45.484: INFO: stderr: ""
Apr  6 23:06:45.484: INFO: stdout: "true"
Apr  6 23:06:45.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-nqn7f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9749'
Apr  6 23:06:45.542: INFO: stderr: ""
Apr  6 23:06:45.542: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  6 23:06:45.542: INFO: validating pod update-demo-nautilus-nqn7f
Apr  6 23:06:45.544: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 23:06:45.544: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 23:06:45.544: INFO: update-demo-nautilus-nqn7f is verified up and running
STEP: scaling up the replication controller
Apr  6 23:06:45.545: INFO: scanned /root for discovery docs: <nil>
Apr  6 23:06:45.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9749'
Apr  6 23:06:46.623: INFO: stderr: ""
Apr  6 23:06:46.623: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  6 23:06:46.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9749'
Apr  6 23:06:46.680: INFO: stderr: ""
Apr  6 23:06:46.680: INFO: stdout: "update-demo-nautilus-cb2bl update-demo-nautilus-nqn7f "
Apr  6 23:06:46.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-cb2bl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9749'
Apr  6 23:06:46.735: INFO: stderr: ""
Apr  6 23:06:46.735: INFO: stdout: ""
Apr  6 23:06:46.735: INFO: update-demo-nautilus-cb2bl is created but not running
Apr  6 23:06:51.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9749'
Apr  6 23:06:51.819: INFO: stderr: ""
Apr  6 23:06:51.819: INFO: stdout: "update-demo-nautilus-cb2bl update-demo-nautilus-nqn7f "
Apr  6 23:06:51.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-cb2bl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9749'
Apr  6 23:06:51.873: INFO: stderr: ""
Apr  6 23:06:51.873: INFO: stdout: "true"
Apr  6 23:06:51.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-cb2bl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9749'
Apr  6 23:06:51.932: INFO: stderr: ""
Apr  6 23:06:51.932: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  6 23:06:51.932: INFO: validating pod update-demo-nautilus-cb2bl
Apr  6 23:06:51.935: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 23:06:51.935: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 23:06:51.935: INFO: update-demo-nautilus-cb2bl is verified up and running
Apr  6 23:06:51.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-nqn7f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9749'
Apr  6 23:06:51.999: INFO: stderr: ""
Apr  6 23:06:51.999: INFO: stdout: "true"
Apr  6 23:06:51.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-nqn7f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9749'
Apr  6 23:06:52.066: INFO: stderr: ""
Apr  6 23:06:52.066: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  6 23:06:52.066: INFO: validating pod update-demo-nautilus-nqn7f
Apr  6 23:06:52.068: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 23:06:52.068: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 23:06:52.068: INFO: update-demo-nautilus-nqn7f is verified up and running
STEP: using delete to clean up resources
Apr  6 23:06:52.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 delete --grace-period=0 --force -f - --namespace=kubectl-9749'
Apr  6 23:06:52.132: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 23:06:52.132: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr  6 23:06:52.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9749'
Apr  6 23:06:52.188: INFO: stderr: "No resources found in kubectl-9749 namespace.\n"
Apr  6 23:06:52.188: INFO: stdout: ""
Apr  6 23:06:52.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods -l name=update-demo --namespace=kubectl-9749 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  6 23:06:52.245: INFO: stderr: ""
Apr  6 23:06:52.245: INFO: stdout: "update-demo-nautilus-cb2bl\nupdate-demo-nautilus-nqn7f\n"
Apr  6 23:06:52.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9749'
Apr  6 23:06:52.806: INFO: stderr: "No resources found in kubectl-9749 namespace.\n"
Apr  6 23:06:52.806: INFO: stdout: ""
Apr  6 23:06:52.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods -l name=update-demo --namespace=kubectl-9749 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  6 23:06:52.867: INFO: stderr: ""
Apr  6 23:06:52.867: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:06:52.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9749" for this suite.

â€¢ [SLOW TEST:19.689 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":275,"completed":145,"skipped":2593,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:06:52.873: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:06:53.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6635" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":275,"completed":146,"skipped":2595,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:06:53.142: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-733
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:06:53.366: INFO: Create a RollingUpdate DaemonSet
Apr  6 23:06:53.368: INFO: Check that daemon pods launch on every node of the cluster
Apr  6 23:06:53.380: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:06:53.388: INFO: Number of nodes with available pods: 0
Apr  6 23:06:53.388: INFO: Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 23:06:54.390: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:06:54.392: INFO: Number of nodes with available pods: 1
Apr  6 23:06:54.392: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 23:06:55.391: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:06:55.392: INFO: Number of nodes with available pods: 2
Apr  6 23:06:55.392: INFO: Number of running nodes: 2, number of available pods: 2
Apr  6 23:06:55.392: INFO: Update the DaemonSet to trigger a rollout
Apr  6 23:06:55.396: INFO: Updating DaemonSet daemon-set
Apr  6 23:06:59.417: INFO: Roll back the DaemonSet before rollout is complete
Apr  6 23:06:59.421: INFO: Updating DaemonSet daemon-set
Apr  6 23:06:59.421: INFO: Make sure DaemonSet rollback is complete
Apr  6 23:06:59.467: INFO: Wrong image for pod: daemon-set-zscnf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr  6 23:06:59.468: INFO: Pod daemon-set-zscnf is not available
Apr  6 23:06:59.471: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:07:00.472: INFO: Wrong image for pod: daemon-set-zscnf. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr  6 23:07:00.472: INFO: Pod daemon-set-zscnf is not available
Apr  6 23:07:00.474: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:07:01.473: INFO: Pod daemon-set-nbcpl is not available
Apr  6 23:07:01.475: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-733, will wait for the garbage collector to delete the pods
Apr  6 23:07:01.609: INFO: Deleting DaemonSet.extensions daemon-set took: 80.890723ms
Apr  6 23:07:02.210: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.152919ms
Apr  6 23:07:14.811: INFO: Number of nodes with available pods: 0
Apr  6 23:07:14.811: INFO: Number of running nodes: 0, number of available pods: 0
Apr  6 23:07:14.812: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-733/daemonsets","resourceVersion":"23619"},"items":null}

Apr  6 23:07:14.813: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-733/pods","resourceVersion":"23619"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:07:14.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-733" for this suite.

â€¢ [SLOW TEST:21.679 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":275,"completed":147,"skipped":2626,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:07:14.822: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4240
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 23:07:15.385: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 23:07:18.475: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:07:19.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4240" for this suite.
STEP: Destroying namespace "webhook-4240-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":275,"completed":148,"skipped":2643,"failed":0}

------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:07:19.171: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3193
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:07:19.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3193" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
â€¢{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":275,"completed":149,"skipped":2643,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:07:19.424: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1843
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr  6 23:07:19.652: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1843 /api/v1/namespaces/watch-1843/configmaps/e2e-watch-test-label-changed eefbe280-04e3-48e6-a885-c6a5502524f9 23719 0 2020-04-06 23:07:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-04-06 23:07:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 23:07:19.652: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1843 /api/v1/namespaces/watch-1843/configmaps/e2e-watch-test-label-changed eefbe280-04e3-48e6-a885-c6a5502524f9 23720 0 2020-04-06 23:07:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-04-06 23:07:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 23:07:19.652: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1843 /api/v1/namespaces/watch-1843/configmaps/e2e-watch-test-label-changed eefbe280-04e3-48e6-a885-c6a5502524f9 23721 0 2020-04-06 23:07:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-04-06 23:07:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr  6 23:07:29.736: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1843 /api/v1/namespaces/watch-1843/configmaps/e2e-watch-test-label-changed eefbe280-04e3-48e6-a885-c6a5502524f9 23811 0 2020-04-06 23:07:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-04-06 23:07:29 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 23:07:29.736: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1843 /api/v1/namespaces/watch-1843/configmaps/e2e-watch-test-label-changed eefbe280-04e3-48e6-a885-c6a5502524f9 23812 0 2020-04-06 23:07:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-04-06 23:07:29 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 23:07:29.736: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1843 /api/v1/namespaces/watch-1843/configmaps/e2e-watch-test-label-changed eefbe280-04e3-48e6-a885-c6a5502524f9 23813 0 2020-04-06 23:07:19 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-04-06 23:07:29 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:07:29.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1843" for this suite.

â€¢ [SLOW TEST:10.321 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":275,"completed":150,"skipped":2671,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:07:29.746: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6350
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Apr  6 23:07:31.443: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0406 23:07:31.443691      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:07:31.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6350" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":275,"completed":151,"skipped":2727,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:07:31.447: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6009
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-42da9690-a46e-43a6-9805-892830e3aaa6
STEP: Creating configMap with name cm-test-opt-upd-ca37db4a-2444-4a52-96ff-46dba9a8af67
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-42da9690-a46e-43a6-9805-892830e3aaa6
STEP: Updating configmap cm-test-opt-upd-ca37db4a-2444-4a52-96ff-46dba9a8af67
STEP: Creating configMap with name cm-test-opt-create-9fd88a60-d2b4-420d-90ba-782ffb686138
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:07:37.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6009" for this suite.

â€¢ [SLOW TEST:6.480 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":152,"skipped":2733,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:07:37.928: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4430
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:07:38.116: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:07:38.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4430" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":275,"completed":153,"skipped":2737,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:07:38.737: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4876
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:07:55.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4876" for this suite.

â€¢ [SLOW TEST:16.345 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":275,"completed":154,"skipped":2756,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:07:55.082: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-380
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr  6 23:07:55.260: INFO: Waiting up to 5m0s for pod "pod-f3eee039-2c1a-4123-bbf1-c1d0507b32e5" in namespace "emptydir-380" to be "Succeeded or Failed"
Apr  6 23:07:55.283: INFO: Pod "pod-f3eee039-2c1a-4123-bbf1-c1d0507b32e5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.858303ms
Apr  6 23:07:57.285: INFO: Pod "pod-f3eee039-2c1a-4123-bbf1-c1d0507b32e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02493846s
STEP: Saw pod success
Apr  6 23:07:57.285: INFO: Pod "pod-f3eee039-2c1a-4123-bbf1-c1d0507b32e5" satisfied condition "Succeeded or Failed"
Apr  6 23:07:57.286: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-f3eee039-2c1a-4123-bbf1-c1d0507b32e5 container test-container: <nil>
STEP: delete the pod
Apr  6 23:07:57.312: INFO: Waiting for pod pod-f3eee039-2c1a-4123-bbf1-c1d0507b32e5 to disappear
Apr  6 23:07:57.339: INFO: Pod pod-f3eee039-2c1a-4123-bbf1-c1d0507b32e5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:07:57.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-380" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":155,"skipped":2785,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:07:57.343: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5193
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Apr  6 23:07:57.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 create -f - --namespace=kubectl-5193'
Apr  6 23:07:57.859: INFO: stderr: ""
Apr  6 23:07:57.859: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  6 23:07:57.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5193'
Apr  6 23:07:57.956: INFO: stderr: ""
Apr  6 23:07:57.956: INFO: stdout: "update-demo-nautilus-rrdrp update-demo-nautilus-rz67n "
Apr  6 23:07:57.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-rrdrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5193'
Apr  6 23:07:58.033: INFO: stderr: ""
Apr  6 23:07:58.033: INFO: stdout: ""
Apr  6 23:07:58.033: INFO: update-demo-nautilus-rrdrp is created but not running
Apr  6 23:08:03.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5193'
Apr  6 23:08:03.097: INFO: stderr: ""
Apr  6 23:08:03.097: INFO: stdout: "update-demo-nautilus-rrdrp update-demo-nautilus-rz67n "
Apr  6 23:08:03.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-rrdrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5193'
Apr  6 23:08:03.151: INFO: stderr: ""
Apr  6 23:08:03.151: INFO: stdout: "true"
Apr  6 23:08:03.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-rrdrp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5193'
Apr  6 23:08:03.206: INFO: stderr: ""
Apr  6 23:08:03.206: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  6 23:08:03.206: INFO: validating pod update-demo-nautilus-rrdrp
Apr  6 23:08:03.208: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 23:08:03.209: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 23:08:03.209: INFO: update-demo-nautilus-rrdrp is verified up and running
Apr  6 23:08:03.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-rz67n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5193'
Apr  6 23:08:03.263: INFO: stderr: ""
Apr  6 23:08:03.263: INFO: stdout: "true"
Apr  6 23:08:03.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods update-demo-nautilus-rz67n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5193'
Apr  6 23:08:03.319: INFO: stderr: ""
Apr  6 23:08:03.319: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  6 23:08:03.319: INFO: validating pod update-demo-nautilus-rz67n
Apr  6 23:08:03.321: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  6 23:08:03.322: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  6 23:08:03.322: INFO: update-demo-nautilus-rz67n is verified up and running
STEP: using delete to clean up resources
Apr  6 23:08:03.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 delete --grace-period=0 --force -f - --namespace=kubectl-5193'
Apr  6 23:08:03.383: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 23:08:03.383: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr  6 23:08:03.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5193'
Apr  6 23:08:03.442: INFO: stderr: "No resources found in kubectl-5193 namespace.\n"
Apr  6 23:08:03.442: INFO: stdout: ""
Apr  6 23:08:03.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods -l name=update-demo --namespace=kubectl-5193 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  6 23:08:03.501: INFO: stderr: ""
Apr  6 23:08:03.501: INFO: stdout: "update-demo-nautilus-rrdrp\nupdate-demo-nautilus-rz67n\n"
Apr  6 23:08:04.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5193'
Apr  6 23:08:04.078: INFO: stderr: "No resources found in kubectl-5193 namespace.\n"
Apr  6 23:08:04.078: INFO: stdout: ""
Apr  6 23:08:04.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods -l name=update-demo --namespace=kubectl-5193 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  6 23:08:04.137: INFO: stderr: ""
Apr  6 23:08:04.137: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:08:04.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5193" for this suite.

â€¢ [SLOW TEST:6.799 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:269
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":275,"completed":156,"skipped":2786,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:08:04.143: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3050
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr  6 23:08:04.432: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:08:04.483: INFO: Number of nodes with available pods: 0
Apr  6 23:08:04.483: INFO: Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 23:08:05.485: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:08:05.487: INFO: Number of nodes with available pods: 0
Apr  6 23:08:05.487: INFO: Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 23:08:06.486: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:08:06.487: INFO: Number of nodes with available pods: 2
Apr  6 23:08:06.487: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr  6 23:08:06.507: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:08:06.548: INFO: Number of nodes with available pods: 2
Apr  6 23:08:06.548: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3050, will wait for the garbage collector to delete the pods
Apr  6 23:08:07.625: INFO: Deleting DaemonSet.extensions daemon-set took: 2.55193ms
Apr  6 23:08:07.725: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.120768ms
Apr  6 23:09:13.427: INFO: Number of nodes with available pods: 0
Apr  6 23:09:13.427: INFO: Number of running nodes: 0, number of available pods: 0
Apr  6 23:09:13.428: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3050/daemonsets","resourceVersion":"24515"},"items":null}

Apr  6 23:09:13.429: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3050/pods","resourceVersion":"24515"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:09:13.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3050" for this suite.

â€¢ [SLOW TEST:69.295 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":275,"completed":157,"skipped":2828,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:09:13.438: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2492
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:09:36.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2492" for this suite.

â€¢ [SLOW TEST:23.478 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  blackbox test
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
    when starting a container that exits
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":275,"completed":158,"skipped":2835,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:09:36.915: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2632
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:09:37.108: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:09:43.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2632" for this suite.

â€¢ [SLOW TEST:6.536 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":275,"completed":159,"skipped":2844,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:09:43.451: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-43
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-07502d56-db3a-4c0c-b352-45fc8af70db9
STEP: Creating a pod to test consume configMaps
Apr  6 23:09:43.634: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e96ca16a-36cf-4611-a0bf-b90fd3853dcb" in namespace "projected-43" to be "Succeeded or Failed"
Apr  6 23:09:43.644: INFO: Pod "pod-projected-configmaps-e96ca16a-36cf-4611-a0bf-b90fd3853dcb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.280351ms
Apr  6 23:09:45.646: INFO: Pod "pod-projected-configmaps-e96ca16a-36cf-4611-a0bf-b90fd3853dcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012239603s
STEP: Saw pod success
Apr  6 23:09:45.646: INFO: Pod "pod-projected-configmaps-e96ca16a-36cf-4611-a0bf-b90fd3853dcb" satisfied condition "Succeeded or Failed"
Apr  6 23:09:45.648: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-projected-configmaps-e96ca16a-36cf-4611-a0bf-b90fd3853dcb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 23:09:45.713: INFO: Waiting for pod pod-projected-configmaps-e96ca16a-36cf-4611-a0bf-b90fd3853dcb to disappear
Apr  6 23:09:45.719: INFO: Pod pod-projected-configmaps-e96ca16a-36cf-4611-a0bf-b90fd3853dcb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:09:45.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-43" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":160,"skipped":2861,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:09:45.741: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 23:09:45.908: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a523462-83a4-4c0e-8ee3-117b691a5d27" in namespace "projected-1814" to be "Succeeded or Failed"
Apr  6 23:09:45.919: INFO: Pod "downwardapi-volume-0a523462-83a4-4c0e-8ee3-117b691a5d27": Phase="Pending", Reason="", readiness=false. Elapsed: 10.717629ms
Apr  6 23:09:47.921: INFO: Pod "downwardapi-volume-0a523462-83a4-4c0e-8ee3-117b691a5d27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012744608s
STEP: Saw pod success
Apr  6 23:09:47.921: INFO: Pod "downwardapi-volume-0a523462-83a4-4c0e-8ee3-117b691a5d27" satisfied condition "Succeeded or Failed"
Apr  6 23:09:47.922: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-0a523462-83a4-4c0e-8ee3-117b691a5d27 container client-container: <nil>
STEP: delete the pod
Apr  6 23:09:47.975: INFO: Waiting for pod downwardapi-volume-0a523462-83a4-4c0e-8ee3-117b691a5d27 to disappear
Apr  6 23:09:47.984: INFO: Pod downwardapi-volume-0a523462-83a4-4c0e-8ee3-117b691a5d27 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:09:47.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1814" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":161,"skipped":2869,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:09:47.988: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2785
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2785.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2785.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2785.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2785.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2785.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2785.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 23:09:50.211: INFO: DNS probes using dns-2785/dns-test-4c82c073-eda2-47e1-be64-972311145ee6 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:09:50.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2785" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":275,"completed":162,"skipped":2888,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:09:50.380: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4071
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 23:09:50.594: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fad73c45-1ff6-42f1-9b5f-90a08ceb8be4" in namespace "projected-4071" to be "Succeeded or Failed"
Apr  6 23:09:50.618: INFO: Pod "downwardapi-volume-fad73c45-1ff6-42f1-9b5f-90a08ceb8be4": Phase="Pending", Reason="", readiness=false. Elapsed: 24.143183ms
Apr  6 23:09:52.631: INFO: Pod "downwardapi-volume-fad73c45-1ff6-42f1-9b5f-90a08ceb8be4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03675306s
STEP: Saw pod success
Apr  6 23:09:52.631: INFO: Pod "downwardapi-volume-fad73c45-1ff6-42f1-9b5f-90a08ceb8be4" satisfied condition "Succeeded or Failed"
Apr  6 23:09:52.634: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-fad73c45-1ff6-42f1-9b5f-90a08ceb8be4 container client-container: <nil>
STEP: delete the pod
Apr  6 23:09:52.694: INFO: Waiting for pod downwardapi-volume-fad73c45-1ff6-42f1-9b5f-90a08ceb8be4 to disappear
Apr  6 23:09:52.727: INFO: Pod downwardapi-volume-fad73c45-1ff6-42f1-9b5f-90a08ceb8be4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:09:52.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4071" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":275,"completed":163,"skipped":2912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:09:52.764: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4827
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name secret-emptykey-test-2d1195a6-0c07-4b6a-8202-423a5fafb4d5
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:09:53.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4827" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":275,"completed":164,"skipped":2950,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:09:53.032: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5689
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 23:09:53.308: INFO: Waiting up to 5m0s for pod "downwardapi-volume-249f2e22-0b9d-468c-aa10-4d75737005f7" in namespace "projected-5689" to be "Succeeded or Failed"
Apr  6 23:09:53.342: INFO: Pod "downwardapi-volume-249f2e22-0b9d-468c-aa10-4d75737005f7": Phase="Pending", Reason="", readiness=false. Elapsed: 33.599012ms
Apr  6 23:09:55.344: INFO: Pod "downwardapi-volume-249f2e22-0b9d-468c-aa10-4d75737005f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035357041s
STEP: Saw pod success
Apr  6 23:09:55.344: INFO: Pod "downwardapi-volume-249f2e22-0b9d-468c-aa10-4d75737005f7" satisfied condition "Succeeded or Failed"
Apr  6 23:09:55.345: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-249f2e22-0b9d-468c-aa10-4d75737005f7 container client-container: <nil>
STEP: delete the pod
Apr  6 23:09:55.368: INFO: Waiting for pod downwardapi-volume-249f2e22-0b9d-468c-aa10-4d75737005f7 to disappear
Apr  6 23:09:55.374: INFO: Pod downwardapi-volume-249f2e22-0b9d-468c-aa10-4d75737005f7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:09:55.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5689" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":275,"completed":165,"skipped":2970,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:09:55.411: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5925
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 23:09:56.061: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 23:09:59.131: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:09:59.133: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:10:00.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5925" for this suite.
STEP: Destroying namespace "webhook-5925-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:5.237 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":275,"completed":166,"skipped":2973,"failed":0}
SSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:10:00.649: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1868
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr  6 23:10:02.909: INFO: &Pod{ObjectMeta:{send-events-76351ec2-da87-4936-98df-b576df93fc63  events-1868 /api/v1/namespaces/events-1868/pods/send-events-76351ec2-da87-4936-98df-b576df93fc63 cd266c44-9331-4668-81cc-ffcf2957581f 25101 0 2020-04-06 23:10:00 +0000 UTC <nil> <nil> map[name:foo time:856491486] map[cni.projectcalico.org/podIP:10.192.1.70/32 kubernetes.io/psp:00-privileged] [] []  [{e2e.test Update v1 2020-04-06 23:10:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 116 105 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 112 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 114 116 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 99 111 110 116 97 105 110 101 114 80 111 114 116 92 34 58 56 48 44 92 34 112 114 111 116 111 99 111 108 92 34 58 92 34 84 67 80 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 99 111 110 116 97 105 110 101 114 80 111 114 116 34 58 123 125 44 34 102 58 112 114 111 116 111 99 111 108 34 58 123 125 125 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-04-06 23:10:01 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:02 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 49 57 50 46 49 46 55 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-525t4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-525t4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-525t4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.74,PodIP:10.192.1.70,StartTime:2020-04-06 23:10:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-06 23:10:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://82ff25e7330b68ab21a929c90183966939c8be25a2a811144ee9f091070c08cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.192.1.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Apr  6 23:10:04.930: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr  6 23:10:06.931: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:10:06.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1868" for this suite.

â€¢ [SLOW TEST:6.329 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":275,"completed":167,"skipped":2979,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:10:06.978: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2238
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Apr  6 23:10:08.201: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:10:08.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0406 23:10:08.201959      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2238" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":275,"completed":168,"skipped":3015,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:10:08.206: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:10:08.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6438" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
â€¢{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":275,"completed":169,"skipped":3021,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:10:08.533: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1429
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr  6 23:10:08.861: INFO: Waiting up to 5m0s for pod "pod-039b72f0-7345-4d2d-93fb-9b3aa0f3e2fd" in namespace "emptydir-1429" to be "Succeeded or Failed"
Apr  6 23:10:08.875: INFO: Pod "pod-039b72f0-7345-4d2d-93fb-9b3aa0f3e2fd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.065762ms
Apr  6 23:10:10.877: INFO: Pod "pod-039b72f0-7345-4d2d-93fb-9b3aa0f3e2fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015664868s
STEP: Saw pod success
Apr  6 23:10:10.877: INFO: Pod "pod-039b72f0-7345-4d2d-93fb-9b3aa0f3e2fd" satisfied condition "Succeeded or Failed"
Apr  6 23:10:10.878: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-039b72f0-7345-4d2d-93fb-9b3aa0f3e2fd container test-container: <nil>
STEP: delete the pod
Apr  6 23:10:10.899: INFO: Waiting for pod pod-039b72f0-7345-4d2d-93fb-9b3aa0f3e2fd to disappear
Apr  6 23:10:10.908: INFO: Pod pod-039b72f0-7345-4d2d-93fb-9b3aa0f3e2fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:10:10.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1429" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":170,"skipped":3028,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:10:10.912: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9041
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 23:10:11.113: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd3e970a-3bf9-4601-a719-121e4ba544d3" in namespace "downward-api-9041" to be "Succeeded or Failed"
Apr  6 23:10:11.126: INFO: Pod "downwardapi-volume-fd3e970a-3bf9-4601-a719-121e4ba544d3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.839463ms
Apr  6 23:10:13.127: INFO: Pod "downwardapi-volume-fd3e970a-3bf9-4601-a719-121e4ba544d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014407134s
STEP: Saw pod success
Apr  6 23:10:13.127: INFO: Pod "downwardapi-volume-fd3e970a-3bf9-4601-a719-121e4ba544d3" satisfied condition "Succeeded or Failed"
Apr  6 23:10:13.128: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-fd3e970a-3bf9-4601-a719-121e4ba544d3 container client-container: <nil>
STEP: delete the pod
Apr  6 23:10:13.148: INFO: Waiting for pod downwardapi-volume-fd3e970a-3bf9-4601-a719-121e4ba544d3 to disappear
Apr  6 23:10:13.150: INFO: Pod downwardapi-volume-fd3e970a-3bf9-4601-a719-121e4ba544d3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:10:13.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9041" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":275,"completed":171,"skipped":3042,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:10:13.154: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8326
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-9vkd
STEP: Creating a pod to test atomic-volume-subpath
Apr  6 23:10:13.325: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9vkd" in namespace "subpath-8326" to be "Succeeded or Failed"
Apr  6 23:10:13.349: INFO: Pod "pod-subpath-test-configmap-9vkd": Phase="Pending", Reason="", readiness=false. Elapsed: 23.894028ms
Apr  6 23:10:15.351: INFO: Pod "pod-subpath-test-configmap-9vkd": Phase="Running", Reason="", readiness=true. Elapsed: 2.025809977s
Apr  6 23:10:17.353: INFO: Pod "pod-subpath-test-configmap-9vkd": Phase="Running", Reason="", readiness=true. Elapsed: 4.027654949s
Apr  6 23:10:19.355: INFO: Pod "pod-subpath-test-configmap-9vkd": Phase="Running", Reason="", readiness=true. Elapsed: 6.029658758s
Apr  6 23:10:21.357: INFO: Pod "pod-subpath-test-configmap-9vkd": Phase="Running", Reason="", readiness=true. Elapsed: 8.031566608s
Apr  6 23:10:23.359: INFO: Pod "pod-subpath-test-configmap-9vkd": Phase="Running", Reason="", readiness=true. Elapsed: 10.033379923s
Apr  6 23:10:25.361: INFO: Pod "pod-subpath-test-configmap-9vkd": Phase="Running", Reason="", readiness=true. Elapsed: 12.035127815s
Apr  6 23:10:27.362: INFO: Pod "pod-subpath-test-configmap-9vkd": Phase="Running", Reason="", readiness=true. Elapsed: 14.036719618s
Apr  6 23:10:29.364: INFO: Pod "pod-subpath-test-configmap-9vkd": Phase="Running", Reason="", readiness=true. Elapsed: 16.038308433s
Apr  6 23:10:31.388: INFO: Pod "pod-subpath-test-configmap-9vkd": Phase="Running", Reason="", readiness=true. Elapsed: 18.062152465s
Apr  6 23:10:33.389: INFO: Pod "pod-subpath-test-configmap-9vkd": Phase="Running", Reason="", readiness=true. Elapsed: 20.064013024s
Apr  6 23:10:35.391: INFO: Pod "pod-subpath-test-configmap-9vkd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.065993483s
STEP: Saw pod success
Apr  6 23:10:35.391: INFO: Pod "pod-subpath-test-configmap-9vkd" satisfied condition "Succeeded or Failed"
Apr  6 23:10:35.393: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-subpath-test-configmap-9vkd container test-container-subpath-configmap-9vkd: <nil>
STEP: delete the pod
Apr  6 23:10:35.463: INFO: Waiting for pod pod-subpath-test-configmap-9vkd to disappear
Apr  6 23:10:35.487: INFO: Pod pod-subpath-test-configmap-9vkd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9vkd
Apr  6 23:10:35.487: INFO: Deleting pod "pod-subpath-test-configmap-9vkd" in namespace "subpath-8326"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:10:35.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8326" for this suite.

â€¢ [SLOW TEST:22.350 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":275,"completed":172,"skipped":3047,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:10:35.504: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9585
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9585.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9585.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 23:10:37.810: INFO: DNS probes using dns-9585/dns-test-4c0a69b6-0250-44cb-a6f8-a9eb239d38e4 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:10:37.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9585" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":275,"completed":173,"skipped":3066,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:10:37.832: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6143
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:10:37.996: INFO: Creating deployment "webserver-deployment"
Apr  6 23:10:38.017: INFO: Waiting for observed generation 1
Apr  6 23:10:40.048: INFO: Waiting for all required pods to come up
Apr  6 23:10:40.051: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr  6 23:10:42.055: INFO: Waiting for deployment "webserver-deployment" to complete
Apr  6 23:10:42.058: INFO: Updating deployment "webserver-deployment" with a non-existent image
Apr  6 23:10:42.061: INFO: Updating deployment webserver-deployment
Apr  6 23:10:42.061: INFO: Waiting for observed generation 2
Apr  6 23:10:44.075: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr  6 23:10:44.077: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr  6 23:10:44.078: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr  6 23:10:44.081: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr  6 23:10:44.081: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr  6 23:10:44.082: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr  6 23:10:44.083: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Apr  6 23:10:44.083: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Apr  6 23:10:44.087: INFO: Updating deployment webserver-deployment
Apr  6 23:10:44.087: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Apr  6 23:10:44.137: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr  6 23:10:44.170: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Apr  6 23:10:44.275: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-6143 /apis/apps/v1/namespaces/deployment-6143/deployments/webserver-deployment d2dc794f-eae3-4fa4-b282-4c62afee632f 25671 3 2020-04-06 23:10:37 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003754ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-6676bcd6d4" is progressing.,LastUpdateTime:2020-04-06 23:10:42 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-04-06 23:10:44 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Apr  6 23:10:44.313: INFO: New ReplicaSet "webserver-deployment-6676bcd6d4" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-6676bcd6d4  deployment-6143 /apis/apps/v1/namespaces/deployment-6143/replicasets/webserver-deployment-6676bcd6d4 74412158-1f37-4057-8839-b7249e3aacf9 25715 3 2020-04-06 23:10:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment d2dc794f-eae3-4fa4-b282-4c62afee632f 0xc0038fab27 0xc0038fab28}] []  [{kube-controller-manager Update apps/v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 50 100 99 55 57 52 102 45 101 97 101 51 45 52 102 97 52 45 98 50 56 50 45 52 99 54 50 97 102 101 101 54 51 50 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 6676bcd6d4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0038faba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 23:10:44.313: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Apr  6 23:10:44.313: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-84855cf797  deployment-6143 /apis/apps/v1/namespaces/deployment-6143/replicasets/webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 25732 3 2020-04-06 23:10:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment d2dc794f-eae3-4fa4-b282-4c62afee632f 0xc0038fac07 0xc0038fac08}] []  [{kube-controller-manager Update apps/v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 100 50 100 99 55 57 52 102 45 101 97 101 51 45 52 102 97 52 45 98 50 56 50 45 52 99 54 50 97 102 101 101 54 51 50 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 84855cf797,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0038fac88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Apr  6 23:10:44.338: INFO: Pod "webserver-deployment-6676bcd6d4-4mm2g" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-4mm2g webserver-deployment-6676bcd6d4- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-6676bcd6d4-4mm2g 6dc6149e-d66e-4499-9ce3-cd12b38b3e4c 25701 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 74412158-1f37-4057-8839-b7249e3aacf9 0xc005108c27 0xc005108c28}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 52 52 49 50 49 53 56 45 49 102 51 55 45 52 48 53 55 45 56 56 51 57 45 98 55 50 52 57 101 51 97 97 99 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.338: INFO: Pod "webserver-deployment-6676bcd6d4-6465q" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-6465q webserver-deployment-6676bcd6d4- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-6676bcd6d4-6465q bcf3b6e1-5b0b-4b6c-86cb-d53f3f4291ee 25649 0 2020-04-06 23:10:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:10.192.1.84/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 74412158-1f37-4057-8839-b7249e3aacf9 0xc005108d70 0xc005108d71}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:42 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 52 52 49 50 49 53 56 45 49 102 51 55 45 52 48 53 55 45 56 56 51 57 45 98 55 50 52 57 101 51 97 97 99 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-04-06 23:10:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.74,PodIP:,StartTime:2020-04-06 23:10:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.339: INFO: Pod "webserver-deployment-6676bcd6d4-9lkxw" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-9lkxw webserver-deployment-6676bcd6d4- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-6676bcd6d4-9lkxw a059cffe-f7a6-47ea-8103-7a32a1fcb273 25708 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 74412158-1f37-4057-8839-b7249e3aacf9 0xc005108f30 0xc005108f31}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 52 52 49 50 49 53 56 45 49 102 51 55 45 52 48 53 55 45 56 56 51 57 45 98 55 50 52 57 101 51 97 97 99 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.339: INFO: Pod "webserver-deployment-6676bcd6d4-bb8wm" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-bb8wm webserver-deployment-6676bcd6d4- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-6676bcd6d4-bb8wm 780eb431-8297-4ab5-8996-7a79cd796b0c 25656 0 2020-04-06 23:10:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:10.192.1.83/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 74412158-1f37-4057-8839-b7249e3aacf9 0xc005109080 0xc005109081}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:42 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 52 52 49 50 49 53 56 45 49 102 51 55 45 52 48 53 55 45 56 56 51 57 45 98 55 50 52 57 101 51 97 97 99 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-04-06 23:10:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.74,PodIP:,StartTime:2020-04-06 23:10:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.339: INFO: Pod "webserver-deployment-6676bcd6d4-bjmqz" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-bjmqz webserver-deployment-6676bcd6d4- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-6676bcd6d4-bjmqz afc34987-9f19-4d2d-8489-22f17b7620c5 25706 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 74412158-1f37-4057-8839-b7249e3aacf9 0xc005109250 0xc005109251}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 52 52 49 50 49 53 56 45 49 102 51 55 45 52 48 53 55 45 56 56 51 57 45 98 55 50 52 57 101 51 97 97 99 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.339: INFO: Pod "webserver-deployment-6676bcd6d4-j76mt" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-j76mt webserver-deployment-6676bcd6d4- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-6676bcd6d4-j76mt d31ecc1b-deb6-43f7-ad3c-074de4cf3c31 25644 0 2020-04-06 23:10:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:10.192.1.82/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 74412158-1f37-4057-8839-b7249e3aacf9 0xc0051093a0 0xc0051093a1}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:42 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 52 52 49 50 49 53 56 45 49 102 51 55 45 52 48 53 55 45 56 56 51 57 45 98 55 50 52 57 101 51 97 97 99 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:42 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2020-04-06 23:10:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.74,PodIP:,StartTime:2020-04-06 23:10:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.339: INFO: Pod "webserver-deployment-6676bcd6d4-mwfdq" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-mwfdq webserver-deployment-6676bcd6d4- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-6676bcd6d4-mwfdq edac938c-4601-4d16-8e08-02e1b8f8bc18 25624 0 2020-04-06 23:10:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:10.192.0.178/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 74412158-1f37-4057-8839-b7249e3aacf9 0xc005109590 0xc005109591}] []  [{calico Update v1 2020-04-06 23:10:42 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}} {kube-controller-manager Update v1 2020-04-06 23:10:42 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 52 52 49 50 49 53 56 45 49 102 51 55 45 52 48 53 55 45 56 56 51 57 45 98 55 50 52 57 101 51 97 97 99 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:42 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.73,PodIP:,StartTime:2020-04-06 23:10:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.340: INFO: Pod "webserver-deployment-6676bcd6d4-nw2nx" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-nw2nx webserver-deployment-6676bcd6d4- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-6676bcd6d4-nw2nx df4d43d7-3b78-4b1b-a7c1-cf1a43c93a20 25707 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 74412158-1f37-4057-8839-b7249e3aacf9 0xc005109750 0xc005109751}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 52 52 49 50 49 53 56 45 49 102 51 55 45 52 48 53 55 45 56 56 51 57 45 98 55 50 52 57 101 51 97 97 99 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.340: INFO: Pod "webserver-deployment-6676bcd6d4-rs4rd" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-rs4rd webserver-deployment-6676bcd6d4- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-6676bcd6d4-rs4rd 017deb2e-d666-497b-8969-976e6934c0d6 25688 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 74412158-1f37-4057-8839-b7249e3aacf9 0xc005109890 0xc005109891}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 52 52 49 50 49 53 56 45 49 102 51 55 45 52 48 53 55 45 56 56 51 57 45 98 55 50 52 57 101 51 97 97 99 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.340: INFO: Pod "webserver-deployment-6676bcd6d4-swv94" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-swv94 webserver-deployment-6676bcd6d4- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-6676bcd6d4-swv94 35ed7904-0160-45ca-9f93-5e4e2c7dac1c 25726 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 74412158-1f37-4057-8839-b7249e3aacf9 0xc0051099d0 0xc0051099d1}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 52 52 49 50 49 53 56 45 49 102 51 55 45 52 48 53 55 45 56 56 51 57 45 98 55 50 52 57 101 51 97 97 99 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.340: INFO: Pod "webserver-deployment-6676bcd6d4-t9rct" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-t9rct webserver-deployment-6676bcd6d4- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-6676bcd6d4-t9rct edbc9312-f00d-449f-b8b0-b17bcee37060 25700 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 74412158-1f37-4057-8839-b7249e3aacf9 0xc005109b10 0xc005109b11}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 52 52 49 50 49 53 56 45 49 102 51 55 45 52 48 53 55 45 56 56 51 57 45 98 55 50 52 57 101 51 97 97 99 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.340: INFO: Pod "webserver-deployment-6676bcd6d4-tnrlg" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-tnrlg webserver-deployment-6676bcd6d4- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-6676bcd6d4-tnrlg 892ded53-4c59-4747-a468-0c87ca04e76f 25737 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 74412158-1f37-4057-8839-b7249e3aacf9 0xc005109c50 0xc005109c51}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 52 52 49 50 49 53 56 45 49 102 51 55 45 52 48 53 55 45 56 56 51 57 45 98 55 50 52 57 101 51 97 97 99 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.74,PodIP:,StartTime:2020-04-06 23:10:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.340: INFO: Pod "webserver-deployment-6676bcd6d4-z4pjk" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-z4pjk webserver-deployment-6676bcd6d4- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-6676bcd6d4-z4pjk cd14c416-1407-49da-b8e8-40c18b9388c1 25642 0 2020-04-06 23:10:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[cni.projectcalico.org/podIP:10.192.1.81/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 74412158-1f37-4057-8839-b7249e3aacf9 0xc005109e00 0xc005109e01}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:42 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 55 52 52 49 50 49 53 56 45 49 102 51 55 45 52 48 53 55 45 56 56 51 57 45 98 55 50 52 57 101 51 97 97 99 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:42 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}} {calico Update v1 2020-04-06 23:10:43 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.74,PodIP:,StartTime:2020-04-06 23:10:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.340: INFO: Pod "webserver-deployment-84855cf797-45258" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-45258 webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-45258 f15ad12c-ea8f-4c90-86cc-0b4983d6611d 25709 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc005109fc0 0xc005109fc1}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.340: INFO: Pod "webserver-deployment-84855cf797-5nksg" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-5nksg webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-5nksg 37d56db1-de86-4eec-9eb8-9bda38675c8d 25689 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391e0f0 0xc00391e0f1}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.341: INFO: Pod "webserver-deployment-84855cf797-685tq" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-685tq webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-685tq 5d3532e0-246b-4cf8-afe6-9006755e75b3 25680 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391e220 0xc00391e221}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.341: INFO: Pod "webserver-deployment-84855cf797-8k6sr" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-8k6sr webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-8k6sr 9c12b647-eafa-451a-b62f-557d2ba74aef 25509 0 2020-04-06 23:10:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.192.0.177/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391e360 0xc00391e361}] []  [{calico Update v1 2020-04-06 23:10:38 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}} {kube-controller-manager Update v1 2020-04-06 23:10:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 49 57 50 46 48 46 49 55 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.73,PodIP:10.192.0.177,StartTime:2020-04-06 23:10:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-06 23:10:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f06fa22c9754cf667be71e7bed5b5a1adaaf96a57745b92b1bf264907eec6b58,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.192.0.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.341: INFO: Pod "webserver-deployment-84855cf797-8nclf" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-8nclf webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-8nclf a4e9f43b-0180-4f9c-981d-798bc2a5d392 25710 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391e510 0xc00391e511}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.341: INFO: Pod "webserver-deployment-84855cf797-9b5fk" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-9b5fk webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-9b5fk f953dc91-e739-4c39-ad65-c5f23feca2b2 25560 0 2020-04-06 23:10:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.192.1.78/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391e650 0xc00391e651}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:38 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-04-06 23:10:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:41 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 49 57 50 46 49 46 55 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.74,PodIP:10.192.1.78,StartTime:2020-04-06 23:10:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-06 23:10:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://ce204f6073414ce08ad40439254d20ceab3c942af542c87b80aea138d328f390,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.192.1.78,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.341: INFO: Pod "webserver-deployment-84855cf797-9bcd6" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-9bcd6 webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-9bcd6 1943c801-bf3d-438b-b1f4-caac1b6edc1d 25544 0 2020-04-06 23:10:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.192.1.80/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391e820 0xc00391e821}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:38 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-04-06 23:10:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:40 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 49 57 50 46 49 46 56 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.74,PodIP:10.192.1.80,StartTime:2020-04-06 23:10:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-06 23:10:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://669f4e665c4e388b2c5f8e6dd5b87ddb65724524e11bb2b26245aac976c39330,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.192.1.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.342: INFO: Pod "webserver-deployment-84855cf797-ghnnl" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-ghnnl webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-ghnnl 5d0b1672-a30c-4599-86ea-e68f675d7cb5 25699 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391e9e0 0xc00391e9e1}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.73,PodIP:,StartTime:2020-04-06 23:10:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.342: INFO: Pod "webserver-deployment-84855cf797-h8mdx" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-h8mdx webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-h8mdx 12d5a161-e5cf-43bc-96c4-758aa8a7f0d2 25687 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391eb60 0xc00391eb61}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.342: INFO: Pod "webserver-deployment-84855cf797-hj55t" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-hj55t webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-hj55t 517d4d02-8df5-4cda-9399-a28032f5f17f 25716 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391eee0 0xc00391eee1}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.342: INFO: Pod "webserver-deployment-84855cf797-jf2bn" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-jf2bn webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-jf2bn d18a000d-16c7-457e-865e-62621440a804 25686 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391f1a0 0xc00391f1a1}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.342: INFO: Pod "webserver-deployment-84855cf797-jw9lk" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-jw9lk webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-jw9lk 63ddb5d2-5e89-41fa-bf7c-85adb04c6e1d 25518 0 2020-04-06 23:10:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.192.0.176/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391f2e0 0xc00391f2e1}] []  [{calico Update v1 2020-04-06 23:10:38 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}} {kube-controller-manager Update v1 2020-04-06 23:10:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 49 57 50 46 48 46 49 55 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.73,PodIP:10.192.0.176,StartTime:2020-04-06 23:10:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-06 23:10:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://95fcdc454fd37771bc9d2e62db2003af5172a1a2c1e06da9568bb3bb528ca8b6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.192.0.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.342: INFO: Pod "webserver-deployment-84855cf797-mn9mw" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-mn9mw webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-mn9mw 20a3e4e6-eb6c-4291-8631-d72edb886729 25733 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391f490 0xc00391f491}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.73,PodIP:,StartTime:2020-04-06 23:10:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.342: INFO: Pod "webserver-deployment-84855cf797-n6bnb" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-n6bnb webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-n6bnb d61dd052-444f-4005-8b03-32347715ad61 25698 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391f610 0xc00391f611}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.343: INFO: Pod "webserver-deployment-84855cf797-p5gr5" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-p5gr5 webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-p5gr5 fbbb40e0-9871-43a1-a43c-560050c0bb99 25533 0 2020-04-06 23:10:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.192.1.79/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391f750 0xc00391f751}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:38 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-04-06 23:10:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:40 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 49 57 50 46 49 46 55 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.74,PodIP:10.192.1.79,StartTime:2020-04-06 23:10:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-06 23:10:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://75295d61d86eebc760561e58d12629325f60e99044698c547d0ac552490a1a8e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.192.1.79,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.343: INFO: Pod "webserver-deployment-84855cf797-pcg4f" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-pcg4f webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-pcg4f b54e6335-aefa-432d-b7e0-cddcfc425e8b 25725 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391f910 0xc00391f911}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.343: INFO: Pod "webserver-deployment-84855cf797-qjrp5" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-qjrp5 webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-qjrp5 fecb0c67-5279-407b-b510-d506a26ab243 25512 0 2020-04-06 23:10:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.192.0.175/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391fa50 0xc00391fa51}] []  [{calico Update v1 2020-04-06 23:10:38 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}} {kube-controller-manager Update v1 2020-04-06 23:10:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 49 57 50 46 48 46 49 55 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.73,PodIP:10.192.0.175,StartTime:2020-04-06 23:10:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-06 23:10:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8a80738e83f981b7d28260222609910378279009b6282dcc4cb84111def075d6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.192.0.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.343: INFO: Pod "webserver-deployment-84855cf797-swzs5" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-swzs5 webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-swzs5 bd41df43-d4e5-42e0-89df-d6b6908f30cf 25505 0 2020-04-06 23:10:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.192.0.174/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391fc10 0xc00391fc11}] []  [{calico Update v1 2020-04-06 23:10:38 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}} {kube-controller-manager Update v1 2020-04-06 23:10:38 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 49 57 50 46 48 46 49 55 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.73,PodIP:10.192.0.174,StartTime:2020-04-06 23:10:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-06 23:10:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://7489f081ac252368aefc69cb226a8fffd1f92e03ac4c7024f18019e421c7c3ad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.192.0.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.343: INFO: Pod "webserver-deployment-84855cf797-tkfz4" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-tkfz4 webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-tkfz4 b26b9168-65b7-4171-b831-94344ea47fd1 25528 0 2020-04-06 23:10:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[cni.projectcalico.org/podIP:10.192.1.76/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391fdd0 0xc00391fdd1}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:38 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-04-06 23:10:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-04-06 23:10:39 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 49 57 50 46 49 46 55 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.74,PodIP:10.192.1.76,StartTime:2020-04-06 23:10:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-06 23:10:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f98e3bc34db4389708f3065a8b3ec146b6fa39a488c074842ece1940875b28ab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.192.1.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:10:44.344: INFO: Pod "webserver-deployment-84855cf797-x6n7k" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-x6n7k webserver-deployment-84855cf797- deployment-6143 /api/v1/namespaces/deployment-6143/pods/webserver-deployment-84855cf797-x6n7k 359cf1dd-23de-4492-8adc-2767d7737f62 25724 0 2020-04-06 23:10:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 ab5f84e9-b1a9-45ff-8cc7-56581f54bede 0xc00391ff90 0xc00391ff91}] []  [{kube-controller-manager Update v1 2020-04-06 23:10:44 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 98 53 102 56 52 101 57 45 98 49 97 57 45 52 53 102 102 45 56 99 99 55 45 53 54 53 56 49 102 53 52 98 101 100 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gkrkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gkrkc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gkrkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:10:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:10:44.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6143" for this suite.

â€¢ [SLOW TEST:6.630 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":275,"completed":174,"skipped":3067,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:10:44.463: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-7b630d71-1183-4df5-84ba-9eb530adc3d0
STEP: Creating a pod to test consume secrets
Apr  6 23:10:44.745: INFO: Waiting up to 5m0s for pod "pod-secrets-d3fca10b-982f-420b-91c4-f5036976cde4" in namespace "secrets-8128" to be "Succeeded or Failed"
Apr  6 23:10:44.778: INFO: Pod "pod-secrets-d3fca10b-982f-420b-91c4-f5036976cde4": Phase="Pending", Reason="", readiness=false. Elapsed: 32.438402ms
Apr  6 23:10:46.779: INFO: Pod "pod-secrets-d3fca10b-982f-420b-91c4-f5036976cde4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034019518s
Apr  6 23:10:48.818: INFO: Pod "pod-secrets-d3fca10b-982f-420b-91c4-f5036976cde4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072512344s
Apr  6 23:10:50.825: INFO: Pod "pod-secrets-d3fca10b-982f-420b-91c4-f5036976cde4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.079886501s
STEP: Saw pod success
Apr  6 23:10:50.825: INFO: Pod "pod-secrets-d3fca10b-982f-420b-91c4-f5036976cde4" satisfied condition "Succeeded or Failed"
Apr  6 23:10:50.841: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-secrets-d3fca10b-982f-420b-91c4-f5036976cde4 container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 23:10:50.884: INFO: Waiting for pod pod-secrets-d3fca10b-982f-420b-91c4-f5036976cde4 to disappear
Apr  6 23:10:50.890: INFO: Pod pod-secrets-d3fca10b-982f-420b-91c4-f5036976cde4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:10:50.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8128" for this suite.

â€¢ [SLOW TEST:6.447 seconds]
[sig-storage] Secrets
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":175,"skipped":3083,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:10:50.910: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2910
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-c4ba4c0d-b0f0-4d98-9065-e637b3d517de in namespace container-probe-2910
Apr  6 23:10:57.133: INFO: Started pod liveness-c4ba4c0d-b0f0-4d98-9065-e637b3d517de in namespace container-probe-2910
STEP: checking the pod's current state and verifying that restartCount is present
Apr  6 23:10:57.134: INFO: Initial restart count of pod liveness-c4ba4c0d-b0f0-4d98-9065-e637b3d517de is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:14:57.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2910" for this suite.

â€¢ [SLOW TEST:247.114 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":275,"completed":176,"skipped":3089,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:14:58.025: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-6407ed7e-2d8f-425d-aff1-70a57ae50dc1
STEP: Creating a pod to test consume configMaps
Apr  6 23:14:58.229: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8c4a2824-ca59-44cf-b764-959c208db551" in namespace "projected-9425" to be "Succeeded or Failed"
Apr  6 23:14:58.257: INFO: Pod "pod-projected-configmaps-8c4a2824-ca59-44cf-b764-959c208db551": Phase="Pending", Reason="", readiness=false. Elapsed: 27.935563ms
Apr  6 23:15:00.259: INFO: Pod "pod-projected-configmaps-8c4a2824-ca59-44cf-b764-959c208db551": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029811804s
STEP: Saw pod success
Apr  6 23:15:00.259: INFO: Pod "pod-projected-configmaps-8c4a2824-ca59-44cf-b764-959c208db551" satisfied condition "Succeeded or Failed"
Apr  6 23:15:00.260: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-projected-configmaps-8c4a2824-ca59-44cf-b764-959c208db551 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 23:15:00.315: INFO: Waiting for pod pod-projected-configmaps-8c4a2824-ca59-44cf-b764-959c208db551 to disappear
Apr  6 23:15:00.335: INFO: Pod pod-projected-configmaps-8c4a2824-ca59-44cf-b764-959c208db551 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:15:00.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9425" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":275,"completed":177,"skipped":3098,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:15:00.339: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating replication controller my-hostname-basic-691896cb-33aa-43a1-99a1-6e3e73f2103d
Apr  6 23:15:00.593: INFO: Pod name my-hostname-basic-691896cb-33aa-43a1-99a1-6e3e73f2103d: Found 0 pods out of 1
Apr  6 23:15:05.595: INFO: Pod name my-hostname-basic-691896cb-33aa-43a1-99a1-6e3e73f2103d: Found 1 pods out of 1
Apr  6 23:15:05.595: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-691896cb-33aa-43a1-99a1-6e3e73f2103d" are running
Apr  6 23:15:05.596: INFO: Pod "my-hostname-basic-691896cb-33aa-43a1-99a1-6e3e73f2103d-8g9jc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-06 23:15:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-06 23:15:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-06 23:15:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-06 23:15:00 +0000 UTC Reason: Message:}])
Apr  6 23:15:05.596: INFO: Trying to dial the pod
Apr  6 23:15:10.611: INFO: Controller my-hostname-basic-691896cb-33aa-43a1-99a1-6e3e73f2103d: Got expected result from replica 1 [my-hostname-basic-691896cb-33aa-43a1-99a1-6e3e73f2103d-8g9jc]: "my-hostname-basic-691896cb-33aa-43a1-99a1-6e3e73f2103d-8g9jc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:15:10.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3687" for this suite.

â€¢ [SLOW TEST:10.276 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":275,"completed":178,"skipped":3104,"failed":0}
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:15:10.615: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-6333/configmap-test-f84ab717-4a15-47d0-9e1d-259bc3975147
STEP: Creating a pod to test consume configMaps
Apr  6 23:15:10.777: INFO: Waiting up to 5m0s for pod "pod-configmaps-cfbb5340-dea0-4973-afba-45f610e176d4" in namespace "configmap-6333" to be "Succeeded or Failed"
Apr  6 23:15:10.815: INFO: Pod "pod-configmaps-cfbb5340-dea0-4973-afba-45f610e176d4": Phase="Pending", Reason="", readiness=false. Elapsed: 38.10556ms
Apr  6 23:15:12.817: INFO: Pod "pod-configmaps-cfbb5340-dea0-4973-afba-45f610e176d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03990934s
STEP: Saw pod success
Apr  6 23:15:12.817: INFO: Pod "pod-configmaps-cfbb5340-dea0-4973-afba-45f610e176d4" satisfied condition "Succeeded or Failed"
Apr  6 23:15:12.818: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-configmaps-cfbb5340-dea0-4973-afba-45f610e176d4 container env-test: <nil>
STEP: delete the pod
Apr  6 23:15:12.845: INFO: Waiting for pod pod-configmaps-cfbb5340-dea0-4973-afba-45f610e176d4 to disappear
Apr  6 23:15:12.848: INFO: Pod pod-configmaps-cfbb5340-dea0-4973-afba-45f610e176d4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:15:12.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6333" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":275,"completed":179,"skipped":3104,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:15:12.852: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4075
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-1883b714-c8f7-4476-949c-72b2d37c11b9 in namespace container-probe-4075
Apr  6 23:15:15.055: INFO: Started pod busybox-1883b714-c8f7-4476-949c-72b2d37c11b9 in namespace container-probe-4075
STEP: checking the pod's current state and verifying that restartCount is present
Apr  6 23:15:15.056: INFO: Initial restart count of pod busybox-1883b714-c8f7-4476-949c-72b2d37c11b9 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:19:15.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4075" for this suite.

â€¢ [SLOW TEST:242.743 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":275,"completed":180,"skipped":3122,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:19:15.595: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's args
Apr  6 23:19:15.764: INFO: Waiting up to 5m0s for pod "var-expansion-31153014-d6ad-4d97-ae61-7ef7d645f8e4" in namespace "var-expansion-390" to be "Succeeded or Failed"
Apr  6 23:19:15.775: INFO: Pod "var-expansion-31153014-d6ad-4d97-ae61-7ef7d645f8e4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.196917ms
Apr  6 23:19:17.777: INFO: Pod "var-expansion-31153014-d6ad-4d97-ae61-7ef7d645f8e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013109507s
STEP: Saw pod success
Apr  6 23:19:17.777: INFO: Pod "var-expansion-31153014-d6ad-4d97-ae61-7ef7d645f8e4" satisfied condition "Succeeded or Failed"
Apr  6 23:19:17.778: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod var-expansion-31153014-d6ad-4d97-ae61-7ef7d645f8e4 container dapi-container: <nil>
STEP: delete the pod
Apr  6 23:19:17.818: INFO: Waiting for pod var-expansion-31153014-d6ad-4d97-ae61-7ef7d645f8e4 to disappear
Apr  6 23:19:17.854: INFO: Pod var-expansion-31153014-d6ad-4d97-ae61-7ef7d645f8e4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:19:17.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-390" for this suite.
â€¢{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":275,"completed":181,"skipped":3123,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:19:17.858: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-38
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-38.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-38.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-38.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-38.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-38.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-38.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-38.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-38.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-38.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-38.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 23:19:20.113: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:20.114: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:20.116: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:20.117: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:20.121: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:20.122: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:20.124: INFO: Unable to read jessie_udp@dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:20.128: INFO: Lookups using dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local wheezy_udp@dns-test-service-2.dns-38.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-38.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local jessie_udp@dns-test-service-2.dns-38.svc.cluster.local]

Apr  6 23:19:25.130: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:25.131: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:25.133: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:25.138: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:25.140: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:25.146: INFO: Lookups using dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local wheezy_udp@dns-test-service-2.dns-38.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local]

Apr  6 23:19:30.130: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:30.132: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:30.139: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:30.141: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:30.146: INFO: Lookups using dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local]

Apr  6 23:19:35.140: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:35.142: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:35.148: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:35.149: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:35.155: INFO: Lookups using dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local]

Apr  6 23:19:40.130: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:40.132: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:40.139: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:40.141: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:40.149: INFO: Lookups using dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local]

Apr  6 23:19:45.146: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:45.147: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:45.155: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:45.156: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local from pod dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9: the server could not find the requested resource (get pods dns-test-55979059-d6db-484c-be7e-4f32625610f9)
Apr  6 23:19:45.161: INFO: Lookups using dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-38.svc.cluster.local]

Apr  6 23:19:50.146: INFO: DNS probes using dns-38/dns-test-55979059-d6db-484c-be7e-4f32625610f9 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:19:50.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-38" for this suite.

â€¢ [SLOW TEST:32.411 seconds]
[sig-network] DNS
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":275,"completed":182,"skipped":3123,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:19:50.270: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3384
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 23:19:51.037: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 23:19:54.111: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:19:54.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3384" for this suite.
STEP: Destroying namespace "webhook-3384-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":275,"completed":183,"skipped":3124,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:19:54.605: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4382
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-secret-vtm6
STEP: Creating a pod to test atomic-volume-subpath
Apr  6 23:19:54.876: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-vtm6" in namespace "subpath-4382" to be "Succeeded or Failed"
Apr  6 23:19:54.881: INFO: Pod "pod-subpath-test-secret-vtm6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.983728ms
Apr  6 23:19:56.883: INFO: Pod "pod-subpath-test-secret-vtm6": Phase="Running", Reason="", readiness=true. Elapsed: 2.00673151s
Apr  6 23:19:58.885: INFO: Pod "pod-subpath-test-secret-vtm6": Phase="Running", Reason="", readiness=true. Elapsed: 4.008527769s
Apr  6 23:20:00.886: INFO: Pod "pod-subpath-test-secret-vtm6": Phase="Running", Reason="", readiness=true. Elapsed: 6.010269877s
Apr  6 23:20:02.888: INFO: Pod "pod-subpath-test-secret-vtm6": Phase="Running", Reason="", readiness=true. Elapsed: 8.012114904s
Apr  6 23:20:04.890: INFO: Pod "pod-subpath-test-secret-vtm6": Phase="Running", Reason="", readiness=true. Elapsed: 10.013997098s
Apr  6 23:20:06.892: INFO: Pod "pod-subpath-test-secret-vtm6": Phase="Running", Reason="", readiness=true. Elapsed: 12.015526003s
Apr  6 23:20:08.893: INFO: Pod "pod-subpath-test-secret-vtm6": Phase="Running", Reason="", readiness=true. Elapsed: 14.017007271s
Apr  6 23:20:10.895: INFO: Pod "pod-subpath-test-secret-vtm6": Phase="Running", Reason="", readiness=true. Elapsed: 16.018917057s
Apr  6 23:20:12.897: INFO: Pod "pod-subpath-test-secret-vtm6": Phase="Running", Reason="", readiness=true. Elapsed: 18.020971396s
Apr  6 23:20:14.899: INFO: Pod "pod-subpath-test-secret-vtm6": Phase="Running", Reason="", readiness=true. Elapsed: 20.022947831s
Apr  6 23:20:16.901: INFO: Pod "pod-subpath-test-secret-vtm6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.024675784s
STEP: Saw pod success
Apr  6 23:20:16.901: INFO: Pod "pod-subpath-test-secret-vtm6" satisfied condition "Succeeded or Failed"
Apr  6 23:20:16.902: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-subpath-test-secret-vtm6 container test-container-subpath-secret-vtm6: <nil>
STEP: delete the pod
Apr  6 23:20:16.958: INFO: Waiting for pod pod-subpath-test-secret-vtm6 to disappear
Apr  6 23:20:16.967: INFO: Pod pod-subpath-test-secret-vtm6 no longer exists
STEP: Deleting pod pod-subpath-test-secret-vtm6
Apr  6 23:20:16.967: INFO: Deleting pod "pod-subpath-test-secret-vtm6" in namespace "subpath-4382"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:20:16.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4382" for this suite.

â€¢ [SLOW TEST:22.367 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":275,"completed":184,"skipped":3126,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:20:16.973: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3514
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3514 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3514;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3514 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3514;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3514.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3514.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3514.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3514.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3514.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3514.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3514.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 201.4.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.4.201_udp@PTR;check="$$(dig +tcp +noall +answer +search 201.4.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.4.201_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3514 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3514;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3514 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3514;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3514.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3514.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3514.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3514.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3514.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3514.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3514.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3514.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3514.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 201.4.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.4.201_udp@PTR;check="$$(dig +tcp +noall +answer +search 201.4.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.4.201_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 23:20:19.252: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.254: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.256: INFO: Unable to read wheezy_udp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.257: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.259: INFO: Unable to read wheezy_udp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.260: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.261: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.262: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.272: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.273: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.274: INFO: Unable to read jessie_udp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.276: INFO: Unable to read jessie_tcp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.277: INFO: Unable to read jessie_udp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.278: INFO: Unable to read jessie_tcp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.280: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.282: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:19.296: INFO: Lookups using dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3514 wheezy_tcp@dns-test-service.dns-3514 wheezy_udp@dns-test-service.dns-3514.svc wheezy_tcp@dns-test-service.dns-3514.svc wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3514 jessie_tcp@dns-test-service.dns-3514 jessie_udp@dns-test-service.dns-3514.svc jessie_tcp@dns-test-service.dns-3514.svc jessie_udp@_http._tcp.dns-test-service.dns-3514.svc jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc]

Apr  6 23:20:24.299: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.301: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.303: INFO: Unable to read wheezy_udp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.304: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.305: INFO: Unable to read wheezy_udp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.308: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.309: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.319: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.320: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.322: INFO: Unable to read jessie_udp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.323: INFO: Unable to read jessie_tcp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.324: INFO: Unable to read jessie_udp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.326: INFO: Unable to read jessie_tcp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.327: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.328: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:24.338: INFO: Lookups using dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3514 wheezy_tcp@dns-test-service.dns-3514 wheezy_udp@dns-test-service.dns-3514.svc wheezy_tcp@dns-test-service.dns-3514.svc wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3514 jessie_tcp@dns-test-service.dns-3514 jessie_udp@dns-test-service.dns-3514.svc jessie_tcp@dns-test-service.dns-3514.svc jessie_udp@_http._tcp.dns-test-service.dns-3514.svc jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc]

Apr  6 23:20:29.317: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.319: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.320: INFO: Unable to read wheezy_udp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.322: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.323: INFO: Unable to read wheezy_udp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.325: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.326: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.328: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.338: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.339: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.341: INFO: Unable to read jessie_udp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.342: INFO: Unable to read jessie_tcp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.344: INFO: Unable to read jessie_udp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.345: INFO: Unable to read jessie_tcp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.346: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.348: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:29.356: INFO: Lookups using dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3514 wheezy_tcp@dns-test-service.dns-3514 wheezy_udp@dns-test-service.dns-3514.svc wheezy_tcp@dns-test-service.dns-3514.svc wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3514 jessie_tcp@dns-test-service.dns-3514 jessie_udp@dns-test-service.dns-3514.svc jessie_tcp@dns-test-service.dns-3514.svc jessie_udp@_http._tcp.dns-test-service.dns-3514.svc jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc]

Apr  6 23:20:34.299: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.300: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.302: INFO: Unable to read wheezy_udp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.303: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.305: INFO: Unable to read wheezy_udp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.307: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.309: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.318: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.320: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.321: INFO: Unable to read jessie_udp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.322: INFO: Unable to read jessie_tcp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.324: INFO: Unable to read jessie_udp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.325: INFO: Unable to read jessie_tcp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.326: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.327: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:34.335: INFO: Lookups using dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3514 wheezy_tcp@dns-test-service.dns-3514 wheezy_udp@dns-test-service.dns-3514.svc wheezy_tcp@dns-test-service.dns-3514.svc wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3514 jessie_tcp@dns-test-service.dns-3514 jessie_udp@dns-test-service.dns-3514.svc jessie_tcp@dns-test-service.dns-3514.svc jessie_udp@_http._tcp.dns-test-service.dns-3514.svc jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc]

Apr  6 23:20:39.298: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.300: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.302: INFO: Unable to read wheezy_udp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.303: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.304: INFO: Unable to read wheezy_udp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.307: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.309: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.318: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.319: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.321: INFO: Unable to read jessie_udp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.322: INFO: Unable to read jessie_tcp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.324: INFO: Unable to read jessie_udp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.325: INFO: Unable to read jessie_tcp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.326: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.327: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:39.336: INFO: Lookups using dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3514 wheezy_tcp@dns-test-service.dns-3514 wheezy_udp@dns-test-service.dns-3514.svc wheezy_tcp@dns-test-service.dns-3514.svc wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3514 jessie_tcp@dns-test-service.dns-3514 jessie_udp@dns-test-service.dns-3514.svc jessie_tcp@dns-test-service.dns-3514.svc jessie_udp@_http._tcp.dns-test-service.dns-3514.svc jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc]

Apr  6 23:20:44.299: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.301: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.302: INFO: Unable to read wheezy_udp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.303: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.305: INFO: Unable to read wheezy_udp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.307: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.308: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.317: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.319: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.320: INFO: Unable to read jessie_udp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.321: INFO: Unable to read jessie_tcp@dns-test-service.dns-3514 from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.323: INFO: Unable to read jessie_udp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.324: INFO: Unable to read jessie_tcp@dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.325: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.326: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc from pod dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8: the server could not find the requested resource (get pods dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8)
Apr  6 23:20:44.334: INFO: Lookups using dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3514 wheezy_tcp@dns-test-service.dns-3514 wheezy_udp@dns-test-service.dns-3514.svc wheezy_tcp@dns-test-service.dns-3514.svc wheezy_udp@_http._tcp.dns-test-service.dns-3514.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3514.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3514 jessie_tcp@dns-test-service.dns-3514 jessie_udp@dns-test-service.dns-3514.svc jessie_tcp@dns-test-service.dns-3514.svc jessie_udp@_http._tcp.dns-test-service.dns-3514.svc jessie_tcp@_http._tcp.dns-test-service.dns-3514.svc]

Apr  6 23:20:49.352: INFO: DNS probes using dns-3514/dns-test-c625df67-b5c6-4306-ad53-8f0d8af058a8 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:20:49.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3514" for this suite.

â€¢ [SLOW TEST:32.593 seconds]
[sig-network] DNS
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":275,"completed":185,"skipped":3130,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:20:49.565: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9901
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Apr  6 23:20:49.791: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  6 23:20:49.815: INFO: Waiting for terminating namespaces to be deleted...
Apr  6 23:20:49.816: INFO: 
Logging pods the kubelet thinks is on node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net before test
Apr  6 23:20:49.827: INFO: coredns-66bff467f8-b269l from kube-system started at 2020-04-06 22:01:29 +0000 UTC (1 container statuses recorded)
Apr  6 23:20:49.827: INFO: 	Container coredns ready: true, restart count 0
Apr  6 23:20:49.827: INFO: hpecp-agent-76d5b65798-qhxrd from hpecp started at 2020-04-06 22:04:06 +0000 UTC (1 container statuses recorded)
Apr  6 23:20:49.827: INFO: 	Container hpecp-agent ready: true, restart count 0
Apr  6 23:20:49.827: INFO: dashboard-metrics-scraper-69449465bc-55bss from kubernetes-dashboard started at 2020-04-06 22:05:13 +0000 UTC (1 container statuses recorded)
Apr  6 23:20:49.827: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr  6 23:20:49.827: INFO: metricbeat-lvk5c from kube-system started at 2020-04-06 22:05:52 +0000 UTC (1 container statuses recorded)
Apr  6 23:20:49.827: INFO: 	Container metricbeat ready: true, restart count 0
Apr  6 23:20:49.827: INFO: sonobuoy-systemd-logs-daemon-set-c4700b4a2f8941f0-ttgxk from sonobuoy started at 2020-04-06 22:39:04 +0000 UTC (2 container statuses recorded)
Apr  6 23:20:49.827: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 23:20:49.827: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 23:20:49.827: INFO: canal-zxrh2 from kube-system started at 2020-04-06 22:01:03 +0000 UTC (3 container statuses recorded)
Apr  6 23:20:49.827: INFO: 	Container calico-node ready: true, restart count 1
Apr  6 23:20:49.827: INFO: 	Container install-cni ready: true, restart count 0
Apr  6 23:20:49.827: INFO: 	Container kube-flannel ready: true, restart count 2
Apr  6 23:20:49.827: INFO: kube-proxy-7w486 from kube-system started at 2020-04-06 22:01:48 +0000 UTC (1 container statuses recorded)
Apr  6 23:20:49.827: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 23:20:49.827: INFO: csi-provisioner-kdf-0 from mapr-csi started at 2020-04-06 22:05:19 +0000 UTC (5 container statuses recorded)
Apr  6 23:20:49.827: INFO: 	Container csi-attacher ready: true, restart count 0
Apr  6 23:20:49.827: INFO: 	Container csi-provisioner ready: true, restart count 0
Apr  6 23:20:49.827: INFO: 	Container csi-snapshot ready: true, restart count 0
Apr  6 23:20:49.827: INFO: 	Container kdf-provisioner ready: true, restart count 0
Apr  6 23:20:49.827: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 23:20:49.827: INFO: hpecp-fsmount-g2knb from hpecp started at 2020-04-06 22:04:39 +0000 UTC (1 container statuses recorded)
Apr  6 23:20:49.828: INFO: 	Container hpecp-fsmount ready: true, restart count 0
Apr  6 23:20:49.828: INFO: csi-nodeplugin-kdf-jn84c from mapr-csi started at 2020-04-06 22:05:19 +0000 UTC (3 container statuses recorded)
Apr  6 23:20:49.828: INFO: 	Container driver-registrar ready: true, restart count 0
Apr  6 23:20:49.828: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 23:20:49.828: INFO: 	Container mapr-kdf ready: true, restart count 0
Apr  6 23:20:49.828: INFO: 
Logging pods the kubelet thinks is on node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net before test
Apr  6 23:20:49.832: INFO: csi-nodeplugin-kdf-4zrwc from mapr-csi started at 2020-04-06 22:05:19 +0000 UTC (3 container statuses recorded)
Apr  6 23:20:49.832: INFO: 	Container driver-registrar ready: true, restart count 0
Apr  6 23:20:49.832: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 23:20:49.832: INFO: 	Container mapr-kdf ready: true, restart count 0
Apr  6 23:20:49.832: INFO: kubernetes-dashboard-6b49d498f6-9prxk from kubernetes-dashboard started at 2020-04-06 22:05:13 +0000 UTC (1 container statuses recorded)
Apr  6 23:20:49.832: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Apr  6 23:20:49.832: INFO: coredns-66bff467f8-l8lxj from kube-system started at 2020-04-06 22:01:27 +0000 UTC (1 container statuses recorded)
Apr  6 23:20:49.832: INFO: 	Container coredns ready: true, restart count 0
Apr  6 23:20:49.832: INFO: kube-state-metrics-5b5f5b558d-j5nkq from kube-system started at 2020-04-06 22:05:15 +0000 UTC (1 container statuses recorded)
Apr  6 23:20:49.832: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr  6 23:20:49.832: INFO: sonobuoy from sonobuoy started at 2020-04-06 22:38:57 +0000 UTC (1 container statuses recorded)
Apr  6 23:20:49.832: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  6 23:20:49.832: INFO: kube-proxy-6fzdv from kube-system started at 2020-04-06 22:01:48 +0000 UTC (1 container statuses recorded)
Apr  6 23:20:49.832: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 23:20:49.832: INFO: metricbeat-6cf8f85fb5-zpb92 from kube-system started at 2020-04-06 22:05:52 +0000 UTC (1 container statuses recorded)
Apr  6 23:20:49.832: INFO: 	Container metricbeat ready: true, restart count 0
Apr  6 23:20:49.832: INFO: canal-rg6q8 from kube-system started at 2020-04-06 22:01:03 +0000 UTC (3 container statuses recorded)
Apr  6 23:20:49.832: INFO: 	Container calico-node ready: true, restart count 1
Apr  6 23:20:49.832: INFO: 	Container install-cni ready: true, restart count 0
Apr  6 23:20:49.832: INFO: 	Container kube-flannel ready: true, restart count 2
Apr  6 23:20:49.832: INFO: hpecp-fsmount-n7jkz from hpecp started at 2020-04-06 22:04:39 +0000 UTC (1 container statuses recorded)
Apr  6 23:20:49.832: INFO: 	Container hpecp-fsmount ready: true, restart count 0
Apr  6 23:20:49.832: INFO: metricbeat-dsbrd from kube-system started at 2020-04-06 22:05:52 +0000 UTC (1 container statuses recorded)
Apr  6 23:20:49.832: INFO: 	Container metricbeat ready: true, restart count 0
Apr  6 23:20:49.832: INFO: kubedirector-64b6488f6d-qpssh from hpecp started at 2020-04-06 22:04:39 +0000 UTC (1 container statuses recorded)
Apr  6 23:20:49.832: INFO: 	Container kubedirector ready: true, restart count 0
Apr  6 23:20:49.832: INFO: sonobuoy-systemd-logs-daemon-set-c4700b4a2f8941f0-p5vhq from sonobuoy started at 2020-04-06 22:39:04 +0000 UTC (2 container statuses recorded)
Apr  6 23:20:49.832: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  6 23:20:49.832: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-fc1c2b31-9837-493a-8d3d-46f816714bce 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-fc1c2b31-9837-493a-8d3d-46f816714bce off the node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net
STEP: verifying the node doesn't have the label kubernetes.io/e2e-fc1c2b31-9837-493a-8d3d-46f816714bce
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:20:58.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9901" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

â€¢ [SLOW TEST:8.452 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":275,"completed":186,"skipped":3139,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:20:58.018: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8530
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-c04f2c8d-7787-42dc-9c23-59faba52cb8e
STEP: Creating a pod to test consume secrets
Apr  6 23:20:58.229: INFO: Waiting up to 5m0s for pod "pod-secrets-8055cfaf-2de7-4b35-a11d-c4a58f827a8b" in namespace "secrets-8530" to be "Succeeded or Failed"
Apr  6 23:20:58.234: INFO: Pod "pod-secrets-8055cfaf-2de7-4b35-a11d-c4a58f827a8b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.231198ms
Apr  6 23:21:00.236: INFO: Pod "pod-secrets-8055cfaf-2de7-4b35-a11d-c4a58f827a8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007175031s
STEP: Saw pod success
Apr  6 23:21:00.236: INFO: Pod "pod-secrets-8055cfaf-2de7-4b35-a11d-c4a58f827a8b" satisfied condition "Succeeded or Failed"
Apr  6 23:21:00.238: INFO: Trying to get logs from node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net pod pod-secrets-8055cfaf-2de7-4b35-a11d-c4a58f827a8b container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 23:21:00.263: INFO: Waiting for pod pod-secrets-8055cfaf-2de7-4b35-a11d-c4a58f827a8b to disappear
Apr  6 23:21:00.275: INFO: Pod pod-secrets-8055cfaf-2de7-4b35-a11d-c4a58f827a8b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:21:00.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8530" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":275,"completed":187,"skipped":3142,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:21:00.279: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3785
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:21:00.525: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr  6 23:21:05.551: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr  6 23:21:05.552: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Apr  6 23:21:05.587: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3785 /apis/apps/v1/namespaces/deployment-3785/deployments/test-cleanup-deployment 9fb0604c-2663-4239-ab6d-b14c2f6221bc 28675 1 2020-04-06 23:21:05 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-04-06 23:21:05 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002ffa7e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Apr  6 23:21:05.628: INFO: New ReplicaSet "test-cleanup-deployment-b4867b47f" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-b4867b47f  deployment-3785 /apis/apps/v1/namespaces/deployment-3785/replicasets/test-cleanup-deployment-b4867b47f 34f3a23c-184a-460a-b3df-67dba0fc3d41 28677 1 2020-04-06 23:21:05 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 9fb0604c-2663-4239-ab6d-b14c2f6221bc 0xc002fb6d90 0xc002fb6d91}] []  [{kube-controller-manager Update apps/v1 2020-04-06 23:21:05 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 102 98 48 54 48 52 99 45 50 54 54 51 45 52 50 51 57 45 97 98 54 100 45 98 49 52 99 50 102 54 50 50 49 98 99 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: b4867b47f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002fb6e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 23:21:05.628: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Apr  6 23:21:05.628: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3785 /apis/apps/v1/namespaces/deployment-3785/replicasets/test-cleanup-controller b7a77fe6-92f7-4a11-bc9f-1e4125f59fd5 28676 1 2020-04-06 23:21:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 9fb0604c-2663-4239-ab6d-b14c2f6221bc 0xc002fb6c87 0xc002fb6c88}] []  [{e2e.test Update apps/v1 2020-04-06 23:21:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-04-06 23:21:05 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 102 98 48 54 48 52 99 45 50 54 54 51 45 52 50 51 57 45 97 98 54 100 45 98 49 52 99 50 102 54 50 50 49 98 99 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002fb6d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr  6 23:21:05.644: INFO: Pod "test-cleanup-controller-htnr5" is available:
&Pod{ObjectMeta:{test-cleanup-controller-htnr5 test-cleanup-controller- deployment-3785 /api/v1/namespaces/deployment-3785/pods/test-cleanup-controller-htnr5 38c95f72-1ae7-4fc7-ba73-91f6f7a2b7bd 28626 0 2020-04-06 23:21:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.192.0.191/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet test-cleanup-controller b7a77fe6-92f7-4a11-bc9f-1e4125f59fd5 0xc002ffabe7 0xc002ffabe8}] []  [{kube-controller-manager Update v1 2020-04-06 23:21:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 55 97 55 55 102 101 54 45 57 50 102 55 45 52 97 49 49 45 98 99 57 102 45 49 101 52 49 50 53 102 53 57 102 100 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-04-06 23:21:01 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-04-06 23:21:02 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 49 57 50 46 48 46 49 57 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4jk9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4jk9l,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4jk9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:21:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:21:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:21:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:21:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.73,PodIP:10.192.0.191,StartTime:2020-04-06 23:21:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-06 23:21:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b53afaea3e77a500c8c130f21df39968eeab073b0efd7db02e0d2f9a04516508,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.192.0.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:21:05.645: INFO: Pod "test-cleanup-deployment-b4867b47f-nfkrq" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-b4867b47f-nfkrq test-cleanup-deployment-b4867b47f- deployment-3785 /api/v1/namespaces/deployment-3785/pods/test-cleanup-deployment-b4867b47f-nfkrq e274bb67-ec35-4e53-9f95-b5036149409c 28684 0 2020-04-06 23:21:05 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet test-cleanup-deployment-b4867b47f 34f3a23c-184a-460a-b3df-67dba0fc3d41 0xc002ffada0 0xc002ffada1}] []  [{kube-controller-manager Update v1 2020-04-06 23:21:05 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 51 52 102 51 97 50 51 99 45 49 56 52 97 45 52 54 48 97 45 98 51 100 102 45 54 55 100 98 97 48 102 99 51 100 52 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-4jk9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-4jk9l,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-4jk9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:21:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:21:05.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3785" for this suite.

â€¢ [SLOW TEST:5.405 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":275,"completed":188,"skipped":3154,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:21:05.685: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2195
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-2195
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  6 23:21:05.850: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr  6 23:21:05.892: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 23:21:07.908: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 23:21:09.894: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 23:21:11.894: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 23:21:13.894: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 23:21:15.894: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 23:21:17.910: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 23:21:19.894: INFO: The status of Pod netserver-0 is Running (Ready = false)
Apr  6 23:21:21.894: INFO: The status of Pod netserver-0 is Running (Ready = true)
Apr  6 23:21:21.897: INFO: The status of Pod netserver-1 is Running (Ready = false)
Apr  6 23:21:23.914: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Apr  6 23:21:25.959: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.192.0.192:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2195 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 23:21:25.959: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 23:21:26.042: INFO: Found all expected endpoints: [netserver-0]
Apr  6 23:21:26.043: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.192.1.110:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2195 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 23:21:26.043: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 23:21:26.122: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:21:26.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2195" for this suite.

â€¢ [SLOW TEST:20.441 seconds]
[sig-network] Networking
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":189,"skipped":3162,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:21:26.126: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6631
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr  6 23:21:26.343: INFO: Waiting up to 5m0s for pod "pod-42981115-a105-41d6-8823-8e25bd9960d9" in namespace "emptydir-6631" to be "Succeeded or Failed"
Apr  6 23:21:26.345: INFO: Pod "pod-42981115-a105-41d6-8823-8e25bd9960d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.413059ms
Apr  6 23:21:28.347: INFO: Pod "pod-42981115-a105-41d6-8823-8e25bd9960d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003426121s
STEP: Saw pod success
Apr  6 23:21:28.347: INFO: Pod "pod-42981115-a105-41d6-8823-8e25bd9960d9" satisfied condition "Succeeded or Failed"
Apr  6 23:21:28.348: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-42981115-a105-41d6-8823-8e25bd9960d9 container test-container: <nil>
STEP: delete the pod
Apr  6 23:21:28.404: INFO: Waiting for pod pod-42981115-a105-41d6-8823-8e25bd9960d9 to disappear
Apr  6 23:21:28.410: INFO: Pod pod-42981115-a105-41d6-8823-8e25bd9960d9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:21:28.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6631" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":190,"skipped":3164,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:21:28.414: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4266
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:21:44.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4266" for this suite.

â€¢ [SLOW TEST:16.250 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":275,"completed":191,"skipped":3197,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:21:44.665: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1666
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1666
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1666
STEP: creating replication controller externalsvc in namespace services-1666
I0406 23:21:44.994164      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1666, replica count: 2
I0406 23:21:48.044482      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Apr  6 23:21:48.107: INFO: Creating new exec pod
Apr  6 23:21:50.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=services-1666 execpod7bx9g -- /bin/sh -x -c nslookup nodeport-service'
Apr  6 23:21:52.868: INFO: stderr: "+ nslookup nodeport-service\n"
Apr  6 23:21:52.868: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-1666.svc.cluster.local\tcanonical name = externalsvc.services-1666.svc.cluster.local.\nName:\texternalsvc.services-1666.svc.cluster.local\nAddress: 10.104.244.155\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1666, will wait for the garbage collector to delete the pods
Apr  6 23:21:52.923: INFO: Deleting ReplicationController externalsvc took: 3.167008ms
Apr  6 23:21:53.523: INFO: Terminating ReplicationController externalsvc pods took: 600.14426ms
Apr  6 23:22:04.864: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:22:04.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1666" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:20.269 seconds]
[sig-network] Services
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":275,"completed":192,"skipped":3228,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:22:04.934: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9881
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Apr  6 23:22:05.096: INFO: Waiting up to 5m0s for pod "downward-api-d4b69c67-3503-4b40-8740-c20870cdee75" in namespace "downward-api-9881" to be "Succeeded or Failed"
Apr  6 23:22:05.113: INFO: Pod "downward-api-d4b69c67-3503-4b40-8740-c20870cdee75": Phase="Pending", Reason="", readiness=false. Elapsed: 16.924754ms
Apr  6 23:22:07.115: INFO: Pod "downward-api-d4b69c67-3503-4b40-8740-c20870cdee75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018718253s
STEP: Saw pod success
Apr  6 23:22:07.115: INFO: Pod "downward-api-d4b69c67-3503-4b40-8740-c20870cdee75" satisfied condition "Succeeded or Failed"
Apr  6 23:22:07.116: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downward-api-d4b69c67-3503-4b40-8740-c20870cdee75 container dapi-container: <nil>
STEP: delete the pod
Apr  6 23:22:07.163: INFO: Waiting for pod downward-api-d4b69c67-3503-4b40-8740-c20870cdee75 to disappear
Apr  6 23:22:07.172: INFO: Pod downward-api-d4b69c67-3503-4b40-8740-c20870cdee75 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:22:07.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9881" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":275,"completed":193,"skipped":3240,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:22:07.175: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3859
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Apr  6 23:22:09.876: INFO: Successfully updated pod "labelsupdatef0ac5631-93cd-4013-b8e4-17e36d11a40f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:22:13.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3859" for this suite.

â€¢ [SLOW TEST:6.741 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":275,"completed":194,"skipped":3248,"failed":0}
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:22:13.917: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3184
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Apr  6 23:22:14.137: INFO: PodSpec: initContainers in spec.initContainers
Apr  6 23:22:58.478: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-977e256d-fbd2-41c1-8216-ee570ae9eb9e", GenerateName:"", Namespace:"init-container-3184", SelfLink:"/api/v1/namespaces/init-container-3184/pods/pod-init-977e256d-fbd2-41c1-8216-ee570ae9eb9e", UID:"1ea66449-5750-42c8-ad1a-291446c403ed", ResourceVersion:"29433", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63721812134, loc:(*time.Location)(0x7b4e1c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"137208246"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.192.1.117/32", "kubernetes.io/psp":"00-privileged"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003361ac0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003361ae0)}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003361b00), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003361b20)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003361b40), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003361b60)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-qkfbl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc005de5080), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qkfbl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qkfbl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qkfbl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0027a6f78), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000743030), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027a6ff0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027a7010)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0027a7018), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0027a701c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812134, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812134, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812134, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812134, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"16.0.8.74", PodIP:"10.192.1.117", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.192.1.117"}}, StartTime:(*v1.Time)(0xc003361b80), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000743110)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000743180)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://84a124938b216d12e77ef81098eced36ce96f769def6b77611a59662f976d102", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003361bc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003361ba0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc0027a709f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:22:58.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3184" for this suite.

â€¢ [SLOW TEST:44.590 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":275,"completed":195,"skipped":3254,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:22:58.507: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8947
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override arguments
Apr  6 23:22:58.688: INFO: Waiting up to 5m0s for pod "client-containers-47289d08-33c9-4cc2-b0f5-36e89ba84b02" in namespace "containers-8947" to be "Succeeded or Failed"
Apr  6 23:22:58.705: INFO: Pod "client-containers-47289d08-33c9-4cc2-b0f5-36e89ba84b02": Phase="Pending", Reason="", readiness=false. Elapsed: 17.029745ms
Apr  6 23:23:00.707: INFO: Pod "client-containers-47289d08-33c9-4cc2-b0f5-36e89ba84b02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018986219s
STEP: Saw pod success
Apr  6 23:23:00.707: INFO: Pod "client-containers-47289d08-33c9-4cc2-b0f5-36e89ba84b02" satisfied condition "Succeeded or Failed"
Apr  6 23:23:00.708: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod client-containers-47289d08-33c9-4cc2-b0f5-36e89ba84b02 container test-container: <nil>
STEP: delete the pod
Apr  6 23:23:00.794: INFO: Waiting for pod client-containers-47289d08-33c9-4cc2-b0f5-36e89ba84b02 to disappear
Apr  6 23:23:00.805: INFO: Pod client-containers-47289d08-33c9-4cc2-b0f5-36e89ba84b02 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:23:00.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8947" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":275,"completed":196,"skipped":3262,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:23:00.810: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service multi-endpoint-test in namespace services-7735
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7735 to expose endpoints map[]
Apr  6 23:23:01.053: INFO: successfully validated that service multi-endpoint-test in namespace services-7735 exposes endpoints map[] (65.311134ms elapsed)
STEP: Creating pod pod1 in namespace services-7735
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7735 to expose endpoints map[pod1:[100]]
Apr  6 23:23:03.120: INFO: successfully validated that service multi-endpoint-test in namespace services-7735 exposes endpoints map[pod1:[100]] (2.044598649s elapsed)
STEP: Creating pod pod2 in namespace services-7735
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7735 to expose endpoints map[pod1:[100] pod2:[101]]
Apr  6 23:23:05.171: INFO: successfully validated that service multi-endpoint-test in namespace services-7735 exposes endpoints map[pod1:[100] pod2:[101]] (2.047521637s elapsed)
STEP: Deleting pod pod1 in namespace services-7735
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7735 to expose endpoints map[pod2:[101]]
Apr  6 23:23:06.222: INFO: successfully validated that service multi-endpoint-test in namespace services-7735 exposes endpoints map[pod2:[101]] (1.046615133s elapsed)
STEP: Deleting pod pod2 in namespace services-7735
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7735 to expose endpoints map[]
Apr  6 23:23:07.338: INFO: successfully validated that service multi-endpoint-test in namespace services-7735 exposes endpoints map[] (1.112751956s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:23:07.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7735" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:6.696 seconds]
[sig-network] Services
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":275,"completed":197,"skipped":3292,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:23:07.507: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9665
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:23:18.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9665" for this suite.

â€¢ [SLOW TEST:11.226 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":275,"completed":198,"skipped":3300,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:23:18.733: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:23:18.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1854" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":275,"completed":199,"skipped":3302,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:23:19.006: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3563
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-cf99732e-7ca8-40b5-b964-03604a9b5a33 in namespace container-probe-3563
Apr  6 23:23:21.222: INFO: Started pod liveness-cf99732e-7ca8-40b5-b964-03604a9b5a33 in namespace container-probe-3563
STEP: checking the pod's current state and verifying that restartCount is present
Apr  6 23:23:21.224: INFO: Initial restart count of pod liveness-cf99732e-7ca8-40b5-b964-03604a9b5a33 is 0
Apr  6 23:23:37.239: INFO: Restart count of pod container-probe-3563/liveness-cf99732e-7ca8-40b5-b964-03604a9b5a33 is now 1 (16.015037129s elapsed)
Apr  6 23:23:57.257: INFO: Restart count of pod container-probe-3563/liveness-cf99732e-7ca8-40b5-b964-03604a9b5a33 is now 2 (36.033259058s elapsed)
Apr  6 23:24:17.274: INFO: Restart count of pod container-probe-3563/liveness-cf99732e-7ca8-40b5-b964-03604a9b5a33 is now 3 (56.050792489s elapsed)
Apr  6 23:24:39.294: INFO: Restart count of pod container-probe-3563/liveness-cf99732e-7ca8-40b5-b964-03604a9b5a33 is now 4 (1m18.069814172s elapsed)
Apr  6 23:25:39.442: INFO: Restart count of pod container-probe-3563/liveness-cf99732e-7ca8-40b5-b964-03604a9b5a33 is now 5 (2m18.218717522s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:25:39.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3563" for this suite.

â€¢ [SLOW TEST:140.467 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":275,"completed":200,"skipped":3312,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:25:39.473: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7056
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:25:42.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7056" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":275,"completed":201,"skipped":3316,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:25:42.753: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5251
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:25:42.998: INFO: Creating ReplicaSet my-hostname-basic-0f000643-bbc1-4adb-ab68-c34af0b08253
Apr  6 23:25:43.019: INFO: Pod name my-hostname-basic-0f000643-bbc1-4adb-ab68-c34af0b08253: Found 0 pods out of 1
Apr  6 23:25:48.021: INFO: Pod name my-hostname-basic-0f000643-bbc1-4adb-ab68-c34af0b08253: Found 1 pods out of 1
Apr  6 23:25:48.021: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0f000643-bbc1-4adb-ab68-c34af0b08253" is running
Apr  6 23:25:48.022: INFO: Pod "my-hostname-basic-0f000643-bbc1-4adb-ab68-c34af0b08253-ds7mc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-06 23:25:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-06 23:25:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-06 23:25:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-06 23:25:43 +0000 UTC Reason: Message:}])
Apr  6 23:25:48.022: INFO: Trying to dial the pod
Apr  6 23:25:53.027: INFO: Controller my-hostname-basic-0f000643-bbc1-4adb-ab68-c34af0b08253: Got expected result from replica 1 [my-hostname-basic-0f000643-bbc1-4adb-ab68-c34af0b08253-ds7mc]: "my-hostname-basic-0f000643-bbc1-4adb-ab68-c34af0b08253-ds7mc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:25:53.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5251" for this suite.

â€¢ [SLOW TEST:10.279 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":275,"completed":202,"skipped":3330,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:25:53.032: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6766
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Apr  6 23:25:53.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 create -f - --namespace=kubectl-6766'
Apr  6 23:25:53.498: INFO: stderr: ""
Apr  6 23:25:53.498: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Apr  6 23:25:54.529: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 23:25:54.529: INFO: Found 1 / 1
Apr  6 23:25:54.529: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr  6 23:25:54.532: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 23:25:54.532: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr  6 23:25:54.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 patch pod agnhost-master-b4pzg --namespace=kubectl-6766 -p {"metadata":{"annotations":{"x":"y"}}}'
Apr  6 23:25:54.596: INFO: stderr: ""
Apr  6 23:25:54.596: INFO: stdout: "pod/agnhost-master-b4pzg patched\n"
STEP: checking annotations
Apr  6 23:25:54.612: INFO: Selector matched 1 pods for map[app:agnhost]
Apr  6 23:25:54.612: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:25:54.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6766" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":275,"completed":203,"skipped":3354,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:25:54.673: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr  6 23:25:59.154: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  6 23:25:59.156: INFO: Pod pod-with-prestop-http-hook still exists
Apr  6 23:26:01.156: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  6 23:26:01.159: INFO: Pod pod-with-prestop-http-hook still exists
Apr  6 23:26:03.156: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  6 23:26:03.158: INFO: Pod pod-with-prestop-http-hook still exists
Apr  6 23:26:05.156: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  6 23:26:05.157: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:26:05.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8300" for this suite.

â€¢ [SLOW TEST:10.502 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  when create a pod with lifecycle hook
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":275,"completed":204,"skipped":3381,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:26:05.175: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-569
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr  6 23:26:05.426: INFO: Waiting up to 5m0s for pod "pod-a5e9cdf2-e1f4-42b6-9d2a-1fa8e359bef3" in namespace "emptydir-569" to be "Succeeded or Failed"
Apr  6 23:26:05.437: INFO: Pod "pod-a5e9cdf2-e1f4-42b6-9d2a-1fa8e359bef3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.838682ms
Apr  6 23:26:07.439: INFO: Pod "pod-a5e9cdf2-e1f4-42b6-9d2a-1fa8e359bef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012604133s
STEP: Saw pod success
Apr  6 23:26:07.439: INFO: Pod "pod-a5e9cdf2-e1f4-42b6-9d2a-1fa8e359bef3" satisfied condition "Succeeded or Failed"
Apr  6 23:26:07.440: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-a5e9cdf2-e1f4-42b6-9d2a-1fa8e359bef3 container test-container: <nil>
STEP: delete the pod
Apr  6 23:26:07.490: INFO: Waiting for pod pod-a5e9cdf2-e1f4-42b6-9d2a-1fa8e359bef3 to disappear
Apr  6 23:26:07.494: INFO: Pod pod-a5e9cdf2-e1f4-42b6-9d2a-1fa8e359bef3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:26:07.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-569" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":205,"skipped":3390,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:26:07.498: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9184
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Apr  6 23:26:10.167: INFO: Successfully updated pod "adopt-release-2ttkz"
STEP: Checking that the Job readopts the Pod
Apr  6 23:26:10.167: INFO: Waiting up to 15m0s for pod "adopt-release-2ttkz" in namespace "job-9184" to be "adopted"
Apr  6 23:26:10.176: INFO: Pod "adopt-release-2ttkz": Phase="Running", Reason="", readiness=true. Elapsed: 9.917455ms
Apr  6 23:26:12.178: INFO: Pod "adopt-release-2ttkz": Phase="Running", Reason="", readiness=true. Elapsed: 2.011354799s
Apr  6 23:26:12.178: INFO: Pod "adopt-release-2ttkz" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Apr  6 23:26:12.683: INFO: Successfully updated pod "adopt-release-2ttkz"
STEP: Checking that the Job releases the Pod
Apr  6 23:26:12.683: INFO: Waiting up to 15m0s for pod "adopt-release-2ttkz" in namespace "job-9184" to be "released"
Apr  6 23:26:12.708: INFO: Pod "adopt-release-2ttkz": Phase="Running", Reason="", readiness=true. Elapsed: 25.00698ms
Apr  6 23:26:12.708: INFO: Pod "adopt-release-2ttkz" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:26:12.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9184" for this suite.

â€¢ [SLOW TEST:5.276 seconds]
[sig-apps] Job
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":275,"completed":206,"skipped":3402,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:26:12.775: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2206
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr  6 23:26:12.943: INFO: Waiting up to 5m0s for pod "pod-64c31155-d402-4ffa-b949-2e82083df749" in namespace "emptydir-2206" to be "Succeeded or Failed"
Apr  6 23:26:12.950: INFO: Pod "pod-64c31155-d402-4ffa-b949-2e82083df749": Phase="Pending", Reason="", readiness=false. Elapsed: 7.905407ms
Apr  6 23:26:14.952: INFO: Pod "pod-64c31155-d402-4ffa-b949-2e82083df749": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009818031s
STEP: Saw pod success
Apr  6 23:26:14.952: INFO: Pod "pod-64c31155-d402-4ffa-b949-2e82083df749" satisfied condition "Succeeded or Failed"
Apr  6 23:26:14.954: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-64c31155-d402-4ffa-b949-2e82083df749 container test-container: <nil>
STEP: delete the pod
Apr  6 23:26:15.019: INFO: Waiting for pod pod-64c31155-d402-4ffa-b949-2e82083df749 to disappear
Apr  6 23:26:15.032: INFO: Pod pod-64c31155-d402-4ffa-b949-2e82083df749 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:26:15.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2206" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":207,"skipped":3449,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:26:15.036: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7986
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1025
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:26:44.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8" for this suite.
STEP: Destroying namespace "nsdeletetest-7986" for this suite.
Apr  6 23:26:44.684: INFO: Namespace nsdeletetest-7986 was already deleted
STEP: Destroying namespace "nsdeletetest-1025" for this suite.

â€¢ [SLOW TEST:29.650 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":275,"completed":208,"skipped":3459,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:26:44.687: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3500
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3500
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3500
I0406 23:26:44.978270      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-3500, replica count: 2
Apr  6 23:26:48.028: INFO: Creating new exec pod
I0406 23:26:48.028577      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 23:26:51.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=services-3500 execpodfbzpp -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr  6 23:26:51.241: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr  6 23:26:51.241: INFO: stdout: ""
Apr  6 23:26:51.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=services-3500 execpodfbzpp -- /bin/sh -x -c nc -zv -t -w 2 10.106.121.138 80'
Apr  6 23:26:51.388: INFO: stderr: "+ nc -zv -t -w 2 10.106.121.138 80\nConnection to 10.106.121.138 80 port [tcp/http] succeeded!\n"
Apr  6 23:26:51.388: INFO: stdout: ""
Apr  6 23:26:51.388: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:26:51.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3500" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:6.766 seconds]
[sig-network] Services
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":275,"completed":209,"skipped":3482,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:26:51.454: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-194
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-dfc75347-b41e-470e-97be-686fd505833c
STEP: Creating a pod to test consume secrets
Apr  6 23:26:51.696: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6a66a6cd-ffe1-4740-80f1-5dbeef61584d" in namespace "projected-194" to be "Succeeded or Failed"
Apr  6 23:26:51.777: INFO: Pod "pod-projected-secrets-6a66a6cd-ffe1-4740-80f1-5dbeef61584d": Phase="Pending", Reason="", readiness=false. Elapsed: 81.581404ms
Apr  6 23:26:53.779: INFO: Pod "pod-projected-secrets-6a66a6cd-ffe1-4740-80f1-5dbeef61584d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.08364639s
STEP: Saw pod success
Apr  6 23:26:53.779: INFO: Pod "pod-projected-secrets-6a66a6cd-ffe1-4740-80f1-5dbeef61584d" satisfied condition "Succeeded or Failed"
Apr  6 23:26:53.781: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-projected-secrets-6a66a6cd-ffe1-4740-80f1-5dbeef61584d container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  6 23:26:53.836: INFO: Waiting for pod pod-projected-secrets-6a66a6cd-ffe1-4740-80f1-5dbeef61584d to disappear
Apr  6 23:26:53.845: INFO: Pod pod-projected-secrets-6a66a6cd-ffe1-4740-80f1-5dbeef61584d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:26:53.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-194" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":275,"completed":210,"skipped":3511,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:26:53.849: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-ndsb
STEP: Creating a pod to test atomic-volume-subpath
Apr  6 23:26:54.025: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ndsb" in namespace "subpath-2957" to be "Succeeded or Failed"
Apr  6 23:26:54.049: INFO: Pod "pod-subpath-test-configmap-ndsb": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009627ms
Apr  6 23:26:56.051: INFO: Pod "pod-subpath-test-configmap-ndsb": Phase="Running", Reason="", readiness=true. Elapsed: 2.025888328s
Apr  6 23:26:58.053: INFO: Pod "pod-subpath-test-configmap-ndsb": Phase="Running", Reason="", readiness=true. Elapsed: 4.027867055s
Apr  6 23:27:00.055: INFO: Pod "pod-subpath-test-configmap-ndsb": Phase="Running", Reason="", readiness=true. Elapsed: 6.029643316s
Apr  6 23:27:02.057: INFO: Pod "pod-subpath-test-configmap-ndsb": Phase="Running", Reason="", readiness=true. Elapsed: 8.031661511s
Apr  6 23:27:04.058: INFO: Pod "pod-subpath-test-configmap-ndsb": Phase="Running", Reason="", readiness=true. Elapsed: 10.033378585s
Apr  6 23:27:06.060: INFO: Pod "pod-subpath-test-configmap-ndsb": Phase="Running", Reason="", readiness=true. Elapsed: 12.035301452s
Apr  6 23:27:08.062: INFO: Pod "pod-subpath-test-configmap-ndsb": Phase="Running", Reason="", readiness=true. Elapsed: 14.037218044s
Apr  6 23:27:10.064: INFO: Pod "pod-subpath-test-configmap-ndsb": Phase="Running", Reason="", readiness=true. Elapsed: 16.038963942s
Apr  6 23:27:12.066: INFO: Pod "pod-subpath-test-configmap-ndsb": Phase="Running", Reason="", readiness=true. Elapsed: 18.040793157s
Apr  6 23:27:14.068: INFO: Pod "pod-subpath-test-configmap-ndsb": Phase="Running", Reason="", readiness=true. Elapsed: 20.042682995s
Apr  6 23:27:16.069: INFO: Pod "pod-subpath-test-configmap-ndsb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.044380332s
STEP: Saw pod success
Apr  6 23:27:16.069: INFO: Pod "pod-subpath-test-configmap-ndsb" satisfied condition "Succeeded or Failed"
Apr  6 23:27:16.071: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-subpath-test-configmap-ndsb container test-container-subpath-configmap-ndsb: <nil>
STEP: delete the pod
Apr  6 23:27:16.104: INFO: Waiting for pod pod-subpath-test-configmap-ndsb to disappear
Apr  6 23:27:16.109: INFO: Pod pod-subpath-test-configmap-ndsb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ndsb
Apr  6 23:27:16.109: INFO: Deleting pod "pod-subpath-test-configmap-ndsb" in namespace "subpath-2957"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:27:16.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2957" for this suite.

â€¢ [SLOW TEST:22.271 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":275,"completed":211,"skipped":3520,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:27:16.121: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4398
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating cluster-info
Apr  6 23:27:16.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 cluster-info'
Apr  6 23:27:16.347: INFO: stderr: ""
Apr  6 23:27:16.347: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:27:16.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4398" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":275,"completed":212,"skipped":3567,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:27:16.352: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7992
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr  6 23:27:16.582: INFO: Waiting up to 5m0s for pod "pod-98db8bfd-9dcc-4b6d-b4c4-a939d823e013" in namespace "emptydir-7992" to be "Succeeded or Failed"
Apr  6 23:27:16.600: INFO: Pod "pod-98db8bfd-9dcc-4b6d-b4c4-a939d823e013": Phase="Pending", Reason="", readiness=false. Elapsed: 18.455234ms
Apr  6 23:27:18.602: INFO: Pod "pod-98db8bfd-9dcc-4b6d-b4c4-a939d823e013": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020153544s
STEP: Saw pod success
Apr  6 23:27:18.602: INFO: Pod "pod-98db8bfd-9dcc-4b6d-b4c4-a939d823e013" satisfied condition "Succeeded or Failed"
Apr  6 23:27:18.603: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-98db8bfd-9dcc-4b6d-b4c4-a939d823e013 container test-container: <nil>
STEP: delete the pod
Apr  6 23:27:18.626: INFO: Waiting for pod pod-98db8bfd-9dcc-4b6d-b4c4-a939d823e013 to disappear
Apr  6 23:27:18.632: INFO: Pod pod-98db8bfd-9dcc-4b6d-b4c4-a939d823e013 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:27:18.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7992" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":213,"skipped":3574,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:27:18.661: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6745
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:27:18.826: INFO: (0) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 3.875551ms)
Apr  6 23:27:18.827: INFO: (1) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.714221ms)
Apr  6 23:27:18.829: INFO: (2) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.573953ms)
Apr  6 23:27:18.831: INFO: (3) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.610234ms)
Apr  6 23:27:18.832: INFO: (4) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.484853ms)
Apr  6 23:27:18.834: INFO: (5) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.612791ms)
Apr  6 23:27:18.835: INFO: (6) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.516674ms)
Apr  6 23:27:18.837: INFO: (7) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.559345ms)
Apr  6 23:27:18.838: INFO: (8) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.53668ms)
Apr  6 23:27:18.841: INFO: (9) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 2.392957ms)
Apr  6 23:27:18.842: INFO: (10) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.460039ms)
Apr  6 23:27:18.844: INFO: (11) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.569824ms)
Apr  6 23:27:18.846: INFO: (12) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.647983ms)
Apr  6 23:27:18.847: INFO: (13) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.432529ms)
Apr  6 23:27:18.848: INFO: (14) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.412653ms)
Apr  6 23:27:18.850: INFO: (15) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.483022ms)
Apr  6 23:27:18.851: INFO: (16) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.512822ms)
Apr  6 23:27:18.853: INFO: (17) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.457461ms)
Apr  6 23:27:18.854: INFO: (18) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.50086ms)
Apr  6 23:27:18.856: INFO: (19) /api/v1/nodes/mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="bluedata/">bluedata/... (200; 1.563703ms)
[AfterEach] version v1
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:27:18.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6745" for this suite.
â€¢{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":275,"completed":214,"skipped":3593,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:27:18.867: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9298
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Apr  6 23:27:19.064: INFO: Created pod &Pod{ObjectMeta:{dns-9298  dns-9298 /api/v1/namespaces/dns-9298/pods/dns-9298 d4f69508-e462-4d42-958f-85331435ee2f 31032 0 2020-04-06 23:27:19 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:00-privileged] [] []  [{e2e.test Update v1 2020-04-06 23:27:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 67 111 110 102 105 103 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 115 101 114 118 101 114 115 34 58 123 125 44 34 102 58 115 101 97 114 99 104 101 115 34 58 123 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vcz6b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vcz6b,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vcz6b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr  6 23:27:19.074: INFO: The status of Pod dns-9298 is Pending, waiting for it to be Running (with Ready = true)
Apr  6 23:27:21.076: INFO: The status of Pod dns-9298 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Apr  6 23:27:21.076: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9298 PodName:dns-9298 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 23:27:21.076: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Verifying customized DNS server is configured on pod...
Apr  6 23:27:21.171: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9298 PodName:dns-9298 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  6 23:27:21.171: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 23:27:21.280: INFO: Deleting pod dns-9298...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:27:21.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9298" for this suite.
â€¢{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":275,"completed":215,"skipped":3599,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:27:21.357: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2155
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr  6 23:27:21.589: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2155 /api/v1/namespaces/watch-2155/configmaps/e2e-watch-test-configmap-a 2c646fd8-2707-432e-8825-f04f23c03d30 31075 0 2020-04-06 23:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-04-06 23:27:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 23:27:21.589: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2155 /api/v1/namespaces/watch-2155/configmaps/e2e-watch-test-configmap-a 2c646fd8-2707-432e-8825-f04f23c03d30 31075 0 2020-04-06 23:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-04-06 23:27:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr  6 23:27:31.593: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2155 /api/v1/namespaces/watch-2155/configmaps/e2e-watch-test-configmap-a 2c646fd8-2707-432e-8825-f04f23c03d30 31141 0 2020-04-06 23:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-04-06 23:27:31 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 23:27:31.593: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2155 /api/v1/namespaces/watch-2155/configmaps/e2e-watch-test-configmap-a 2c646fd8-2707-432e-8825-f04f23c03d30 31141 0 2020-04-06 23:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-04-06 23:27:31 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr  6 23:27:41.596: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2155 /api/v1/namespaces/watch-2155/configmaps/e2e-watch-test-configmap-a 2c646fd8-2707-432e-8825-f04f23c03d30 31174 0 2020-04-06 23:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-04-06 23:27:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 23:27:41.597: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2155 /api/v1/namespaces/watch-2155/configmaps/e2e-watch-test-configmap-a 2c646fd8-2707-432e-8825-f04f23c03d30 31174 0 2020-04-06 23:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-04-06 23:27:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr  6 23:27:51.600: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2155 /api/v1/namespaces/watch-2155/configmaps/e2e-watch-test-configmap-a 2c646fd8-2707-432e-8825-f04f23c03d30 31208 0 2020-04-06 23:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-04-06 23:27:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 23:27:51.600: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2155 /api/v1/namespaces/watch-2155/configmaps/e2e-watch-test-configmap-a 2c646fd8-2707-432e-8825-f04f23c03d30 31208 0 2020-04-06 23:27:21 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-04-06 23:27:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr  6 23:28:01.604: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2155 /api/v1/namespaces/watch-2155/configmaps/e2e-watch-test-configmap-b 710743e4-42c6-423b-8428-6e3981b3e271 31243 0 2020-04-06 23:28:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-04-06 23:28:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 23:28:01.604: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2155 /api/v1/namespaces/watch-2155/configmaps/e2e-watch-test-configmap-b 710743e4-42c6-423b-8428-6e3981b3e271 31243 0 2020-04-06 23:28:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-04-06 23:28:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr  6 23:28:11.608: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2155 /api/v1/namespaces/watch-2155/configmaps/e2e-watch-test-configmap-b 710743e4-42c6-423b-8428-6e3981b3e271 31278 0 2020-04-06 23:28:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-04-06 23:28:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 23:28:11.608: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2155 /api/v1/namespaces/watch-2155/configmaps/e2e-watch-test-configmap-b 710743e4-42c6-423b-8428-6e3981b3e271 31278 0 2020-04-06 23:28:01 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-04-06 23:28:01 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:28:21.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2155" for this suite.

â€¢ [SLOW TEST:60.256 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":275,"completed":216,"skipped":3604,"failed":0}
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:28:21.613: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9758
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr  6 23:28:21.808: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9758 /api/v1/namespaces/watch-9758/configmaps/e2e-watch-test-watch-closed 1f8f5d9e-008b-4014-a16b-3d94e13e102b 31319 0 2020-04-06 23:28:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-04-06 23:28:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 23:28:21.809: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9758 /api/v1/namespaces/watch-9758/configmaps/e2e-watch-test-watch-closed 1f8f5d9e-008b-4014-a16b-3d94e13e102b 31320 0 2020-04-06 23:28:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-04-06 23:28:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr  6 23:28:21.839: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9758 /api/v1/namespaces/watch-9758/configmaps/e2e-watch-test-watch-closed 1f8f5d9e-008b-4014-a16b-3d94e13e102b 31321 0 2020-04-06 23:28:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-04-06 23:28:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 23:28:21.839: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9758 /api/v1/namespaces/watch-9758/configmaps/e2e-watch-test-watch-closed 1f8f5d9e-008b-4014-a16b-3d94e13e102b 31322 0 2020-04-06 23:28:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-04-06 23:28:21 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:28:21.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9758" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":275,"completed":217,"skipped":3604,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:28:21.851: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7206
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-d191c880-250b-4fa0-9dbe-d796c61d740f
STEP: Creating a pod to test consume secrets
Apr  6 23:28:22.061: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f001589d-d8d0-463e-bd77-c0fc7d27b16f" in namespace "projected-7206" to be "Succeeded or Failed"
Apr  6 23:28:22.073: INFO: Pod "pod-projected-secrets-f001589d-d8d0-463e-bd77-c0fc7d27b16f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.98491ms
Apr  6 23:28:24.074: INFO: Pod "pod-projected-secrets-f001589d-d8d0-463e-bd77-c0fc7d27b16f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013658574s
STEP: Saw pod success
Apr  6 23:28:24.075: INFO: Pod "pod-projected-secrets-f001589d-d8d0-463e-bd77-c0fc7d27b16f" satisfied condition "Succeeded or Failed"
Apr  6 23:28:24.076: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-projected-secrets-f001589d-d8d0-463e-bd77-c0fc7d27b16f container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  6 23:28:24.106: INFO: Waiting for pod pod-projected-secrets-f001589d-d8d0-463e-bd77-c0fc7d27b16f to disappear
Apr  6 23:28:24.114: INFO: Pod pod-projected-secrets-f001589d-d8d0-463e-bd77-c0fc7d27b16f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:28:24.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7206" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":218,"skipped":3626,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:28:24.118: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-831
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-downwardapi-t4ht
STEP: Creating a pod to test atomic-volume-subpath
Apr  6 23:28:24.317: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-t4ht" in namespace "subpath-831" to be "Succeeded or Failed"
Apr  6 23:28:24.338: INFO: Pod "pod-subpath-test-downwardapi-t4ht": Phase="Pending", Reason="", readiness=false. Elapsed: 20.237696ms
Apr  6 23:28:26.339: INFO: Pod "pod-subpath-test-downwardapi-t4ht": Phase="Running", Reason="", readiness=true. Elapsed: 2.021756094s
Apr  6 23:28:28.341: INFO: Pod "pod-subpath-test-downwardapi-t4ht": Phase="Running", Reason="", readiness=true. Elapsed: 4.023198061s
Apr  6 23:28:30.346: INFO: Pod "pod-subpath-test-downwardapi-t4ht": Phase="Running", Reason="", readiness=true. Elapsed: 6.028962928s
Apr  6 23:28:32.348: INFO: Pod "pod-subpath-test-downwardapi-t4ht": Phase="Running", Reason="", readiness=true. Elapsed: 8.030662521s
Apr  6 23:28:34.350: INFO: Pod "pod-subpath-test-downwardapi-t4ht": Phase="Running", Reason="", readiness=true. Elapsed: 10.032613277s
Apr  6 23:28:36.352: INFO: Pod "pod-subpath-test-downwardapi-t4ht": Phase="Running", Reason="", readiness=true. Elapsed: 12.034441678s
Apr  6 23:28:38.353: INFO: Pod "pod-subpath-test-downwardapi-t4ht": Phase="Running", Reason="", readiness=true. Elapsed: 14.03611629s
Apr  6 23:28:40.355: INFO: Pod "pod-subpath-test-downwardapi-t4ht": Phase="Running", Reason="", readiness=true. Elapsed: 16.038032711s
Apr  6 23:28:42.357: INFO: Pod "pod-subpath-test-downwardapi-t4ht": Phase="Running", Reason="", readiness=true. Elapsed: 18.039576859s
Apr  6 23:28:44.359: INFO: Pod "pod-subpath-test-downwardapi-t4ht": Phase="Running", Reason="", readiness=true. Elapsed: 20.041602271s
Apr  6 23:28:46.361: INFO: Pod "pod-subpath-test-downwardapi-t4ht": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.043535837s
STEP: Saw pod success
Apr  6 23:28:46.361: INFO: Pod "pod-subpath-test-downwardapi-t4ht" satisfied condition "Succeeded or Failed"
Apr  6 23:28:46.363: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-subpath-test-downwardapi-t4ht container test-container-subpath-downwardapi-t4ht: <nil>
STEP: delete the pod
Apr  6 23:28:46.408: INFO: Waiting for pod pod-subpath-test-downwardapi-t4ht to disappear
Apr  6 23:28:46.419: INFO: Pod pod-subpath-test-downwardapi-t4ht no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-t4ht
Apr  6 23:28:46.419: INFO: Deleting pod "pod-subpath-test-downwardapi-t4ht" in namespace "subpath-831"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:28:46.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-831" for this suite.

â€¢ [SLOW TEST:22.330 seconds]
[sig-storage] Subpath
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":275,"completed":219,"skipped":3634,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:28:46.449: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr  6 23:28:47.284: INFO: Pod name wrapped-volume-race-36f0351a-16ef-4196-b45d-625cbacfc7dd: Found 0 pods out of 5
Apr  6 23:28:52.287: INFO: Pod name wrapped-volume-race-36f0351a-16ef-4196-b45d-625cbacfc7dd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-36f0351a-16ef-4196-b45d-625cbacfc7dd in namespace emptydir-wrapper-3108, will wait for the garbage collector to delete the pods
Apr  6 23:29:02.351: INFO: Deleting ReplicationController wrapped-volume-race-36f0351a-16ef-4196-b45d-625cbacfc7dd took: 3.009729ms
Apr  6 23:29:03.051: INFO: Terminating ReplicationController wrapped-volume-race-36f0351a-16ef-4196-b45d-625cbacfc7dd pods took: 700.161582ms
STEP: Creating RC which spawns configmap-volume pods
Apr  6 23:29:07.249: INFO: Pod name wrapped-volume-race-2e3a61d6-0e98-45d7-a720-697e0b09a668: Found 1 pods out of 5
Apr  6 23:29:12.252: INFO: Pod name wrapped-volume-race-2e3a61d6-0e98-45d7-a720-697e0b09a668: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2e3a61d6-0e98-45d7-a720-697e0b09a668 in namespace emptydir-wrapper-3108, will wait for the garbage collector to delete the pods
Apr  6 23:29:20.422: INFO: Deleting ReplicationController wrapped-volume-race-2e3a61d6-0e98-45d7-a720-697e0b09a668 took: 38.044445ms
Apr  6 23:29:20.522: INFO: Terminating ReplicationController wrapped-volume-race-2e3a61d6-0e98-45d7-a720-697e0b09a668 pods took: 100.157038ms
STEP: Creating RC which spawns configmap-volume pods
Apr  6 23:29:37.567: INFO: Pod name wrapped-volume-race-cdaf9c36-a5cd-46cc-aaf9-fac707513f28: Found 0 pods out of 5
Apr  6 23:29:42.571: INFO: Pod name wrapped-volume-race-cdaf9c36-a5cd-46cc-aaf9-fac707513f28: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-cdaf9c36-a5cd-46cc-aaf9-fac707513f28 in namespace emptydir-wrapper-3108, will wait for the garbage collector to delete the pods
Apr  6 23:29:52.637: INFO: Deleting ReplicationController wrapped-volume-race-cdaf9c36-a5cd-46cc-aaf9-fac707513f28 took: 3.632227ms
Apr  6 23:29:53.238: INFO: Terminating ReplicationController wrapped-volume-race-cdaf9c36-a5cd-46cc-aaf9-fac707513f28 pods took: 600.474249ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:30:08.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3108" for this suite.

â€¢ [SLOW TEST:81.731 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":275,"completed":220,"skipped":3654,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:30:08.179: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-7471
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:30:10.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7471" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":275,"completed":221,"skipped":3655,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:30:10.487: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7031
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:30:10.735: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c0cfc558-9480-4d9b-ac5f-70cdb14eb5f8", Controller:(*bool)(0xc0030ea3a6), BlockOwnerDeletion:(*bool)(0xc0030ea3a7)}}
Apr  6 23:30:10.752: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"d674c8cb-8131-4797-8eb0-3dc6fc833d24", Controller:(*bool)(0xc00320ef6e), BlockOwnerDeletion:(*bool)(0xc00320ef6f)}}
Apr  6 23:30:10.767: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2182e865-f6f5-4d60-a680-dc1b82217272", Controller:(*bool)(0xc0031cb486), BlockOwnerDeletion:(*bool)(0xc0031cb487)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:30:15.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7031" for this suite.

â€¢ [SLOW TEST:5.374 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":275,"completed":222,"skipped":3667,"failed":0}
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:30:15.861: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3532
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3532
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-3532
I0406 23:30:16.164831      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-3532, replica count: 2
Apr  6 23:30:19.219: INFO: Creating new exec pod
I0406 23:30:19.219079      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 23:30:22.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=services-3532 execpodfqzbb -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr  6 23:30:22.425: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr  6 23:30:22.425: INFO: stdout: ""
Apr  6 23:30:22.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=services-3532 execpodfqzbb -- /bin/sh -x -c nc -zv -t -w 2 10.98.79.92 80'
Apr  6 23:30:22.568: INFO: stderr: "+ nc -zv -t -w 2 10.98.79.92 80\nConnection to 10.98.79.92 80 port [tcp/http] succeeded!\n"
Apr  6 23:30:22.568: INFO: stdout: ""
Apr  6 23:30:22.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=services-3532 execpodfqzbb -- /bin/sh -x -c nc -zv -t -w 2 16.0.8.73 31402'
Apr  6 23:30:22.733: INFO: stderr: "+ nc -zv -t -w 2 16.0.8.73 31402\nConnection to 16.0.8.73 31402 port [tcp/31402] succeeded!\n"
Apr  6 23:30:22.733: INFO: stdout: ""
Apr  6 23:30:22.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=services-3532 execpodfqzbb -- /bin/sh -x -c nc -zv -t -w 2 16.0.8.74 31402'
Apr  6 23:30:22.868: INFO: stderr: "+ nc -zv -t -w 2 16.0.8.74 31402\nConnection to 16.0.8.74 31402 port [tcp/31402] succeeded!\n"
Apr  6 23:30:22.868: INFO: stdout: ""
Apr  6 23:30:22.868: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:30:22.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3532" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:7.172 seconds]
[sig-network] Services
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":275,"completed":223,"skipped":3667,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:30:23.032: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3775
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating pod
Apr  6 23:30:25.266: INFO: Pod pod-hostip-a7e9425c-e0f0-43fa-86de-9d02f218a3e7 has hostIP: 16.0.8.74
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:30:25.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3775" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":275,"completed":224,"skipped":3668,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:30:25.270: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8117
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr  6 23:30:27.990: INFO: Successfully updated pod "pod-update-activedeadlineseconds-89d53ed5-8cc0-4854-8fd6-d234dc1e9437"
Apr  6 23:30:27.990: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-89d53ed5-8cc0-4854-8fd6-d234dc1e9437" in namespace "pods-8117" to be "terminated due to deadline exceeded"
Apr  6 23:30:27.998: INFO: Pod "pod-update-activedeadlineseconds-89d53ed5-8cc0-4854-8fd6-d234dc1e9437": Phase="Running", Reason="", readiness=true. Elapsed: 8.344006ms
Apr  6 23:30:30.022: INFO: Pod "pod-update-activedeadlineseconds-89d53ed5-8cc0-4854-8fd6-d234dc1e9437": Phase="Running", Reason="", readiness=true. Elapsed: 2.032338155s
Apr  6 23:30:32.024: INFO: Pod "pod-update-activedeadlineseconds-89d53ed5-8cc0-4854-8fd6-d234dc1e9437": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.034338344s
Apr  6 23:30:32.024: INFO: Pod "pod-update-activedeadlineseconds-89d53ed5-8cc0-4854-8fd6-d234dc1e9437" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:30:32.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8117" for this suite.

â€¢ [SLOW TEST:6.758 seconds]
[k8s.io] Pods
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":275,"completed":225,"skipped":3683,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:30:32.028: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-9853
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Apr  6 23:30:32.210: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the sample API server.
Apr  6 23:30:32.663: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Apr  6 23:30:34.756: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 23:30:36.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 23:30:38.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 23:30:40.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 23:30:42.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 23:30:44.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 23:30:46.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 23:30:48.758: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812632, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 23:30:53.785: INFO: Waited 3.022548875s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:30:54.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9853" for this suite.

â€¢ [SLOW TEST:22.627 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":275,"completed":226,"skipped":3704,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:30:54.656: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9044
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:30:54.847: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Apr  6 23:30:57.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-9044 create -f -'
Apr  6 23:31:00.765: INFO: stderr: ""
Apr  6 23:31:00.765: INFO: stdout: "e2e-test-crd-publish-openapi-3928-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr  6 23:31:00.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-9044 delete e2e-test-crd-publish-openapi-3928-crds test-foo'
Apr  6 23:31:00.825: INFO: stderr: ""
Apr  6 23:31:00.825: INFO: stdout: "e2e-test-crd-publish-openapi-3928-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Apr  6 23:31:00.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-9044 apply -f -'
Apr  6 23:31:01.064: INFO: stderr: ""
Apr  6 23:31:01.064: INFO: stdout: "e2e-test-crd-publish-openapi-3928-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr  6 23:31:01.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-9044 delete e2e-test-crd-publish-openapi-3928-crds test-foo'
Apr  6 23:31:01.125: INFO: stderr: ""
Apr  6 23:31:01.125: INFO: stdout: "e2e-test-crd-publish-openapi-3928-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Apr  6 23:31:01.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-9044 create -f -'
Apr  6 23:31:01.381: INFO: rc: 1
Apr  6 23:31:01.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-9044 apply -f -'
Apr  6 23:31:01.614: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Apr  6 23:31:01.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-9044 create -f -'
Apr  6 23:31:01.865: INFO: rc: 1
Apr  6 23:31:01.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 --namespace=crd-publish-openapi-9044 apply -f -'
Apr  6 23:31:02.157: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Apr  6 23:31:02.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 explain e2e-test-crd-publish-openapi-3928-crds'
Apr  6 23:31:02.393: INFO: stderr: ""
Apr  6 23:31:02.393: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3928-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Apr  6 23:31:02.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 explain e2e-test-crd-publish-openapi-3928-crds.metadata'
Apr  6 23:31:02.645: INFO: stderr: ""
Apr  6 23:31:02.645: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3928-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Apr  6 23:31:02.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 explain e2e-test-crd-publish-openapi-3928-crds.spec'
Apr  6 23:31:02.899: INFO: stderr: ""
Apr  6 23:31:02.899: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3928-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Apr  6 23:31:02.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 explain e2e-test-crd-publish-openapi-3928-crds.spec.bars'
Apr  6 23:31:03.136: INFO: stderr: ""
Apr  6 23:31:03.136: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3928-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Apr  6 23:31:03.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 explain e2e-test-crd-publish-openapi-3928-crds.spec.bars2'
Apr  6 23:31:03.376: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:31:06.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9044" for this suite.

â€¢ [SLOW TEST:11.475 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":275,"completed":227,"skipped":3710,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:31:06.131: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-5376
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Apr  6 23:31:06.300: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Apr  6 23:31:06.312: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr  6 23:31:06.312: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Apr  6 23:31:06.322: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr  6 23:31:06.322: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Apr  6 23:31:06.390: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Apr  6 23:31:06.390: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Apr  6 23:31:13.515: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:31:13.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-5376" for this suite.

â€¢ [SLOW TEST:7.406 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":275,"completed":228,"skipped":3726,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:31:13.537: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6793
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:31:13.702: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:31:15.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6793" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":275,"completed":229,"skipped":3728,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:31:15.900: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:31:18.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4112" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":230,"skipped":3837,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:31:18.105: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4580
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-e44ed39d-3171-4b21-8a59-6ab0576e7f63
STEP: Creating a pod to test consume secrets
Apr  6 23:31:18.318: INFO: Waiting up to 5m0s for pod "pod-secrets-7db1a307-4e6d-48cd-abd9-287d66ebe3f3" in namespace "secrets-4580" to be "Succeeded or Failed"
Apr  6 23:31:18.322: INFO: Pod "pod-secrets-7db1a307-4e6d-48cd-abd9-287d66ebe3f3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.778157ms
Apr  6 23:31:20.323: INFO: Pod "pod-secrets-7db1a307-4e6d-48cd-abd9-287d66ebe3f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005623588s
Apr  6 23:31:22.326: INFO: Pod "pod-secrets-7db1a307-4e6d-48cd-abd9-287d66ebe3f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008104046s
STEP: Saw pod success
Apr  6 23:31:22.326: INFO: Pod "pod-secrets-7db1a307-4e6d-48cd-abd9-287d66ebe3f3" satisfied condition "Succeeded or Failed"
Apr  6 23:31:22.344: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-secrets-7db1a307-4e6d-48cd-abd9-287d66ebe3f3 container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 23:31:22.386: INFO: Waiting for pod pod-secrets-7db1a307-4e6d-48cd-abd9-287d66ebe3f3 to disappear
Apr  6 23:31:22.394: INFO: Pod pod-secrets-7db1a307-4e6d-48cd-abd9-287d66ebe3f3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:31:22.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4580" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":275,"completed":231,"skipped":3850,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:31:22.399: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 23:31:22.920: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr  6 23:31:24.925: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812682, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812682, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812683, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812682, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 23:31:27.952: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:31:28.015: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-969-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:31:29.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4271" for this suite.
STEP: Destroying namespace "webhook-4271-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:7.078 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":275,"completed":232,"skipped":3862,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:31:29.477: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8351
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Apr  6 23:31:29.793: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:31:44.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8351" for this suite.

â€¢ [SLOW TEST:14.953 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":275,"completed":233,"skipped":3872,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:31:44.431: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1723
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
Apr  6 23:31:45.099: INFO: created pod pod-service-account-defaultsa
Apr  6 23:31:45.099: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr  6 23:31:45.125: INFO: created pod pod-service-account-mountsa
Apr  6 23:31:45.125: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr  6 23:31:45.135: INFO: created pod pod-service-account-nomountsa
Apr  6 23:31:45.135: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr  6 23:31:45.159: INFO: created pod pod-service-account-defaultsa-mountspec
Apr  6 23:31:45.159: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr  6 23:31:45.214: INFO: created pod pod-service-account-mountsa-mountspec
Apr  6 23:31:45.214: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr  6 23:31:45.226: INFO: created pod pod-service-account-nomountsa-mountspec
Apr  6 23:31:45.226: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr  6 23:31:45.250: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr  6 23:31:45.250: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr  6 23:31:45.267: INFO: created pod pod-service-account-mountsa-nomountspec
Apr  6 23:31:45.267: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr  6 23:31:45.285: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr  6 23:31:45.285: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:31:45.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1723" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":275,"completed":234,"skipped":3886,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:31:45.340: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7370
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-ae4766a7-69b2-4500-9fa5-4149099c299c-134
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:31:45.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7370" for this suite.
STEP: Destroying namespace "nspatchtest-ae4766a7-69b2-4500-9fa5-4149099c299c-134" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":275,"completed":235,"skipped":3891,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:31:45.711: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-1391
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:31:46.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1391" for this suite.
â€¢{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":275,"completed":236,"skipped":3906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:31:46.033: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3812
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-3812
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Apr  6 23:31:46.275: INFO: Found 0 stateful pods, waiting for 3
Apr  6 23:31:56.278: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 23:31:56.278: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 23:31:56.278: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 23:31:56.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-3812 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 23:31:56.417: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 23:31:56.417: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 23:31:56.417: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr  6 23:32:06.436: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr  6 23:32:16.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-3812 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 23:32:16.636: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 23:32:16.636: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 23:32:16.636: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 23:32:26.646: INFO: Waiting for StatefulSet statefulset-3812/ss2 to complete update
Apr  6 23:32:26.646: INFO: Waiting for Pod statefulset-3812/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr  6 23:32:26.646: INFO: Waiting for Pod statefulset-3812/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr  6 23:32:26.646: INFO: Waiting for Pod statefulset-3812/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr  6 23:32:36.672: INFO: Waiting for StatefulSet statefulset-3812/ss2 to complete update
STEP: Rolling back to a previous revision
Apr  6 23:32:46.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-3812 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 23:32:46.793: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 23:32:46.793: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 23:32:46.793: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 23:32:56.813: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr  6 23:33:06.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-3812 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 23:33:07.016: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 23:33:07.016: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 23:33:07.016: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Apr  6 23:33:27.026: INFO: Deleting all statefulset in ns statefulset-3812
Apr  6 23:33:27.027: INFO: Scaling statefulset ss2 to 0
Apr  6 23:33:47.063: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 23:33:47.064: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:33:47.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3812" for this suite.

â€¢ [SLOW TEST:121.049 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":275,"completed":237,"skipped":3929,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:33:47.084: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1406
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:33:49.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1406" for this suite.
â€¢{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":275,"completed":238,"skipped":3972,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:33:49.343: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5883
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:33:49.535: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr  6 23:33:54.539: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr  6 23:33:54.539: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr  6 23:33:56.568: INFO: Creating deployment "test-rollover-deployment"
Apr  6 23:33:56.594: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr  6 23:33:58.608: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr  6 23:33:58.611: INFO: Ensure that both replica sets have 1 created replica
Apr  6 23:33:58.614: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr  6 23:33:58.618: INFO: Updating deployment test-rollover-deployment
Apr  6 23:33:58.618: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr  6 23:34:00.661: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr  6 23:34:00.664: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr  6 23:34:00.667: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 23:34:00.667: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812840, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 23:34:02.691: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 23:34:02.691: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812840, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 23:34:04.670: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 23:34:04.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812840, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 23:34:06.703: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 23:34:06.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812840, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 23:34:08.670: INFO: all replica sets need to contain the pod-template-hash label
Apr  6 23:34:08.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812840, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 23:34:10.784: INFO: 
Apr  6 23:34:10.784: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812850, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812836, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  6 23:34:12.688: INFO: 
Apr  6 23:34:12.688: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Apr  6 23:34:12.692: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5883 /apis/apps/v1/namespaces/deployment-5883/deployments/test-rollover-deployment 5c63977e-50f5-4c10-9fba-9e4786afde81 34744 2 2020-04-06 23:33:56 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-04-06 23:33:58 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-04-06 23:34:10 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0037d1078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-04-06 23:33:56 +0000 UTC,LastTransitionTime:2020-04-06 23:33:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-84f7f6f64b" has successfully progressed.,LastUpdateTime:2020-04-06 23:34:10 +0000 UTC,LastTransitionTime:2020-04-06 23:33:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr  6 23:34:12.693: INFO: New ReplicaSet "test-rollover-deployment-84f7f6f64b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-84f7f6f64b  deployment-5883 /apis/apps/v1/namespaces/deployment-5883/replicasets/test-rollover-deployment-84f7f6f64b 25d74f3c-83d8-4037-a53e-c2fb0f4b542b 34732 2 2020-04-06 23:33:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 5c63977e-50f5-4c10-9fba-9e4786afde81 0xc004e50ae7 0xc004e50ae8}] []  [{kube-controller-manager Update apps/v1 2020-04-06 23:34:10 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 53 99 54 51 57 55 55 101 45 53 48 102 53 45 52 99 49 48 45 57 102 98 97 45 57 101 52 55 56 54 97 102 100 101 56 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 84f7f6f64b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004e50b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr  6 23:34:12.693: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr  6 23:34:12.693: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5883 /apis/apps/v1/namespaces/deployment-5883/replicasets/test-rollover-controller 84d35886-2d63-4d6c-8ca4-837bd4932497 34743 2 2020-04-06 23:33:49 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 5c63977e-50f5-4c10-9fba-9e4786afde81 0xc004e508d7 0xc004e508d8}] []  [{e2e.test Update apps/v1 2020-04-06 23:33:49 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-04-06 23:34:10 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 53 99 54 51 57 55 55 101 45 53 48 102 53 45 52 99 49 48 45 57 102 98 97 45 57 101 52 55 56 54 97 102 100 101 56 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004e50978 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 23:34:12.694: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-5686c4cfd5  deployment-5883 /apis/apps/v1/namespaces/deployment-5883/replicasets/test-rollover-deployment-5686c4cfd5 568e9707-19d2-4f48-b1ec-fbf185f05f4b 34682 2 2020-04-06 23:33:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 5c63977e-50f5-4c10-9fba-9e4786afde81 0xc004e509e7 0xc004e509e8}] []  [{kube-controller-manager Update apps/v1 2020-04-06 23:33:58 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 53 99 54 51 57 55 55 101 45 53 48 102 53 45 52 99 49 48 45 57 102 98 97 45 57 101 52 55 56 54 97 102 100 101 56 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 114 101 100 105 115 45 115 108 97 118 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5686c4cfd5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004e50a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr  6 23:34:12.695: INFO: Pod "test-rollover-deployment-84f7f6f64b-ntt4f" is available:
&Pod{ObjectMeta:{test-rollover-deployment-84f7f6f64b-ntt4f test-rollover-deployment-84f7f6f64b- deployment-5883 /api/v1/namespaces/deployment-5883/pods/test-rollover-deployment-84f7f6f64b-ntt4f 16a7f51d-1dca-4b66-b550-c7334210745f 34694 0 2020-04-06 23:33:58 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[cni.projectcalico.org/podIP:10.192.1.50/32 kubernetes.io/psp:00-privileged] [{apps/v1 ReplicaSet test-rollover-deployment-84f7f6f64b 25d74f3c-83d8-4037-a53e-c2fb0f4b542b 0xc004e51417 0xc004e51418}] []  [{kube-controller-manager Update v1 2020-04-06 23:33:58 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 50 53 100 55 52 102 51 99 45 56 51 100 56 45 52 48 51 55 45 97 53 51 101 45 99 50 102 98 48 102 52 98 53 52 50 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {calico Update v1 2020-04-06 23:33:59 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 99 110 105 46 112 114 111 106 101 99 116 99 97 108 105 99 111 46 111 114 103 47 112 111 100 73 80 34 58 123 125 125 125 125],}} {kubelet Update v1 2020-04-06 23:34:00 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 49 57 50 46 49 46 53 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-rcwth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-rcwth,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-rcwth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:33:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:34:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:34:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-06 23:33:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:16.0.8.74,PodIP:10.192.1.50,StartTime:2020-04-06 23:33:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-06 23:33:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://081ee369f424d96d5daa58dc64e3b130e72cce3bd30476f124095c04addf9713,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.192.1.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:34:12.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5883" for this suite.

â€¢ [SLOW TEST:23.355 seconds]
[sig-apps] Deployment
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":275,"completed":239,"skipped":3993,"failed":0}
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:34:12.699: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:34:15.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1519" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":240,"skipped":3993,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:34:15.032: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 23:34:15.516: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 23:34:18.573: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:34:18.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8477" for this suite.
STEP: Destroying namespace "webhook-8477-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":275,"completed":241,"skipped":4073,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:34:18.743: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9728
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:34:18.996: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr  6 23:34:19.031: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:34:19.042: INFO: Number of nodes with available pods: 0
Apr  6 23:34:19.042: INFO: Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 23:34:20.045: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:34:20.048: INFO: Number of nodes with available pods: 0
Apr  6 23:34:20.048: INFO: Node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 23:34:21.044: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:34:21.051: INFO: Number of nodes with available pods: 2
Apr  6 23:34:21.051: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr  6 23:34:21.076: INFO: Wrong image for pod: daemon-set-cfgzx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:21.076: INFO: Wrong image for pod: daemon-set-fmzwb. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:21.105: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:34:22.107: INFO: Wrong image for pod: daemon-set-cfgzx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:22.107: INFO: Wrong image for pod: daemon-set-fmzwb. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:22.109: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:34:23.107: INFO: Wrong image for pod: daemon-set-cfgzx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:23.107: INFO: Wrong image for pod: daemon-set-fmzwb. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:23.108: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:34:24.107: INFO: Wrong image for pod: daemon-set-cfgzx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:24.107: INFO: Wrong image for pod: daemon-set-fmzwb. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:24.107: INFO: Pod daemon-set-fmzwb is not available
Apr  6 23:34:24.109: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:34:25.125: INFO: Wrong image for pod: daemon-set-cfgzx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:25.125: INFO: Wrong image for pod: daemon-set-fmzwb. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:25.125: INFO: Pod daemon-set-fmzwb is not available
Apr  6 23:34:25.126: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:34:26.109: INFO: Wrong image for pod: daemon-set-cfgzx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:26.109: INFO: Wrong image for pod: daemon-set-fmzwb. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:26.109: INFO: Pod daemon-set-fmzwb is not available
Apr  6 23:34:26.111: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:34:27.107: INFO: Wrong image for pod: daemon-set-cfgzx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:27.107: INFO: Wrong image for pod: daemon-set-fmzwb. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:27.107: INFO: Pod daemon-set-fmzwb is not available
Apr  6 23:34:27.109: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:34:28.139: INFO: Wrong image for pod: daemon-set-cfgzx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:28.139: INFO: Pod daemon-set-thdhn is not available
Apr  6 23:34:28.141: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:34:29.107: INFO: Wrong image for pod: daemon-set-cfgzx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:29.109: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:34:30.107: INFO: Wrong image for pod: daemon-set-cfgzx. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Apr  6 23:34:30.107: INFO: Pod daemon-set-cfgzx is not available
Apr  6 23:34:30.109: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:34:31.112: INFO: Pod daemon-set-5jns9 is not available
Apr  6 23:34:31.114: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Apr  6 23:34:31.116: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:34:31.117: INFO: Number of nodes with available pods: 1
Apr  6 23:34:31.117: INFO: Node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net is running more than one daemon pod
Apr  6 23:34:32.119: INFO: DaemonSet pods can't tolerate node mip-bd-ap06-n1-vm01.mip.storage.hpecorp.net with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr  6 23:34:32.121: INFO: Number of nodes with available pods: 2
Apr  6 23:34:32.121: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9728, will wait for the garbage collector to delete the pods
Apr  6 23:34:32.181: INFO: Deleting DaemonSet.extensions daemon-set took: 3.104342ms
Apr  6 23:34:32.781: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.135099ms
Apr  6 23:34:44.782: INFO: Number of nodes with available pods: 0
Apr  6 23:34:44.782: INFO: Number of running nodes: 0, number of available pods: 0
Apr  6 23:34:44.783: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9728/daemonsets","resourceVersion":"35077"},"items":null}

Apr  6 23:34:44.785: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9728/pods","resourceVersion":"35077"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:34:44.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9728" for this suite.

â€¢ [SLOW TEST:26.051 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":275,"completed":242,"skipped":4080,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:34:44.794: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 23:34:45.381: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr  6 23:34:47.385: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812885, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812885, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812885, loc:(*time.Location)(0x7b4e1c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63721812885, loc:(*time.Location)(0x7b4e1c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 23:34:50.454: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:34:50.458: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3040-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:34:51.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-781" for this suite.
STEP: Destroying namespace "webhook-781-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

â€¢ [SLOW TEST:7.168 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":275,"completed":243,"skipped":4086,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:34:51.962: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-668
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-282
STEP: Creating secret with name secret-test-282afe93-71d4-4fc5-b71f-a0234b142173
STEP: Creating a pod to test consume secrets
Apr  6 23:34:52.379: INFO: Waiting up to 5m0s for pod "pod-secrets-c7930dab-8cce-4005-b253-cc30f997f8da" in namespace "secrets-668" to be "Succeeded or Failed"
Apr  6 23:34:52.466: INFO: Pod "pod-secrets-c7930dab-8cce-4005-b253-cc30f997f8da": Phase="Pending", Reason="", readiness=false. Elapsed: 86.647367ms
Apr  6 23:34:54.468: INFO: Pod "pod-secrets-c7930dab-8cce-4005-b253-cc30f997f8da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.088461977s
STEP: Saw pod success
Apr  6 23:34:54.468: INFO: Pod "pod-secrets-c7930dab-8cce-4005-b253-cc30f997f8da" satisfied condition "Succeeded or Failed"
Apr  6 23:34:54.480: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-secrets-c7930dab-8cce-4005-b253-cc30f997f8da container secret-volume-test: <nil>
STEP: delete the pod
Apr  6 23:34:54.548: INFO: Waiting for pod pod-secrets-c7930dab-8cce-4005-b253-cc30f997f8da to disappear
Apr  6 23:34:54.557: INFO: Pod pod-secrets-c7930dab-8cce-4005-b253-cc30f997f8da no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:34:54.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-668" for this suite.
STEP: Destroying namespace "secret-namespace-282" for this suite.
â€¢{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":275,"completed":244,"skipped":4095,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:34:54.583: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5975
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 23:34:54.759: INFO: Waiting up to 5m0s for pod "downwardapi-volume-205c1fd1-d460-4441-834c-5228244fe808" in namespace "downward-api-5975" to be "Succeeded or Failed"
Apr  6 23:34:54.766: INFO: Pod "downwardapi-volume-205c1fd1-d460-4441-834c-5228244fe808": Phase="Pending", Reason="", readiness=false. Elapsed: 7.139041ms
Apr  6 23:34:56.768: INFO: Pod "downwardapi-volume-205c1fd1-d460-4441-834c-5228244fe808": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009103394s
STEP: Saw pod success
Apr  6 23:34:56.768: INFO: Pod "downwardapi-volume-205c1fd1-d460-4441-834c-5228244fe808" satisfied condition "Succeeded or Failed"
Apr  6 23:34:56.769: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-205c1fd1-d460-4441-834c-5228244fe808 container client-container: <nil>
STEP: delete the pod
Apr  6 23:34:56.813: INFO: Waiting for pod downwardapi-volume-205c1fd1-d460-4441-834c-5228244fe808 to disappear
Apr  6 23:34:56.824: INFO: Pod downwardapi-volume-205c1fd1-d460-4441-834c-5228244fe808 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:34:56.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5975" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":245,"skipped":4118,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:34:56.828: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8142
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Apr  6 23:34:57.003: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
Apr  6 23:34:59.839: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:35:09.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8142" for this suite.

â€¢ [SLOW TEST:12.892 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":275,"completed":246,"skipped":4121,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:35:09.721: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-953
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:35:21.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-953" for this suite.

â€¢ [SLOW TEST:11.414 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":275,"completed":247,"skipped":4129,"failed":0}
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:35:21.135: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating server pod server in namespace prestop-327
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-327
STEP: Deleting pre-stop pod
Apr  6 23:35:30.342: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:35:30.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-327" for this suite.

â€¢ [SLOW TEST:9.281 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":275,"completed":248,"skipped":4130,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:35:30.417: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-657
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on node default medium
Apr  6 23:35:30.588: INFO: Waiting up to 5m0s for pod "pod-edc9b3cb-d9ec-4fc2-b86f-e3be8d21eca2" in namespace "emptydir-657" to be "Succeeded or Failed"
Apr  6 23:35:30.598: INFO: Pod "pod-edc9b3cb-d9ec-4fc2-b86f-e3be8d21eca2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.307723ms
Apr  6 23:35:32.600: INFO: Pod "pod-edc9b3cb-d9ec-4fc2-b86f-e3be8d21eca2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012037435s
STEP: Saw pod success
Apr  6 23:35:32.600: INFO: Pod "pod-edc9b3cb-d9ec-4fc2-b86f-e3be8d21eca2" satisfied condition "Succeeded or Failed"
Apr  6 23:35:32.601: INFO: Trying to get logs from node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net pod pod-edc9b3cb-d9ec-4fc2-b86f-e3be8d21eca2 container test-container: <nil>
STEP: delete the pod
Apr  6 23:35:32.621: INFO: Waiting for pod pod-edc9b3cb-d9ec-4fc2-b86f-e3be8d21eca2 to disappear
Apr  6 23:35:32.643: INFO: Pod pod-edc9b3cb-d9ec-4fc2-b86f-e3be8d21eca2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:35:32.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-657" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":249,"skipped":4197,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:35:32.660: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7786
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr  6 23:35:33.185: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr  6 23:35:36.234: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:35:36.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7786" for this suite.
STEP: Destroying namespace "webhook-7786-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
â€¢{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":275,"completed":250,"skipped":4215,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:35:36.561: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5220
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 23:35:36.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f09700a-8daa-4481-b5c8-9f3583f4d983" in namespace "projected-5220" to be "Succeeded or Failed"
Apr  6 23:35:36.883: INFO: Pod "downwardapi-volume-3f09700a-8daa-4481-b5c8-9f3583f4d983": Phase="Pending", Reason="", readiness=false. Elapsed: 75.499093ms
Apr  6 23:35:38.885: INFO: Pod "downwardapi-volume-3f09700a-8daa-4481-b5c8-9f3583f4d983": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.078056414s
STEP: Saw pod success
Apr  6 23:35:38.885: INFO: Pod "downwardapi-volume-3f09700a-8daa-4481-b5c8-9f3583f4d983" satisfied condition "Succeeded or Failed"
Apr  6 23:35:38.887: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-3f09700a-8daa-4481-b5c8-9f3583f4d983 container client-container: <nil>
STEP: delete the pod
Apr  6 23:35:38.925: INFO: Waiting for pod downwardapi-volume-3f09700a-8daa-4481-b5c8-9f3583f4d983 to disappear
Apr  6 23:35:38.927: INFO: Pod downwardapi-volume-3f09700a-8daa-4481-b5c8-9f3583f4d983 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:35:38.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5220" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":251,"skipped":4215,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:35:38.931: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2738
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod test-webserver-de29f2da-c439-4e73-b172-79e3d49cf7ea in namespace container-probe-2738
Apr  6 23:35:41.150: INFO: Started pod test-webserver-de29f2da-c439-4e73-b172-79e3d49cf7ea in namespace container-probe-2738
STEP: checking the pod's current state and verifying that restartCount is present
Apr  6 23:35:41.151: INFO: Initial restart count of pod test-webserver-de29f2da-c439-4e73-b172-79e3d49cf7ea is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:39:41.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2738" for this suite.

â€¢ [SLOW TEST:242.909 seconds]
[k8s.io] Probing container
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":275,"completed":252,"skipped":4228,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:39:41.841: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr  6 23:39:42.115: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7895 /api/v1/namespaces/watch-7895/configmaps/e2e-watch-test-resource-version 2529d926-5a63-442d-9c71-9e4813b723ba 36555 0 2020-04-06 23:39:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-04-06 23:39:42 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr  6 23:39:42.115: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7895 /api/v1/namespaces/watch-7895/configmaps/e2e-watch-test-resource-version 2529d926-5a63-442d-9c71-9e4813b723ba 36556 0 2020-04-06 23:39:42 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-04-06 23:39:42 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:39:42.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7895" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":275,"completed":253,"skipped":4254,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:39:42.124: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2249
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 23:39:42.355: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fed38f4c-85a2-4734-b18a-93a1a576b2fc" in namespace "downward-api-2249" to be "Succeeded or Failed"
Apr  6 23:39:42.363: INFO: Pod "downwardapi-volume-fed38f4c-85a2-4734-b18a-93a1a576b2fc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.071328ms
Apr  6 23:39:44.365: INFO: Pod "downwardapi-volume-fed38f4c-85a2-4734-b18a-93a1a576b2fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009761431s
STEP: Saw pod success
Apr  6 23:39:44.365: INFO: Pod "downwardapi-volume-fed38f4c-85a2-4734-b18a-93a1a576b2fc" satisfied condition "Succeeded or Failed"
Apr  6 23:39:44.366: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-fed38f4c-85a2-4734-b18a-93a1a576b2fc container client-container: <nil>
STEP: delete the pod
Apr  6 23:39:44.394: INFO: Waiting for pod downwardapi-volume-fed38f4c-85a2-4734-b18a-93a1a576b2fc to disappear
Apr  6 23:39:44.396: INFO: Pod downwardapi-volume-fed38f4c-85a2-4734-b18a-93a1a576b2fc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:39:44.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2249" for this suite.
â€¢{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":254,"skipped":4271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:39:44.400: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7885
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:39:51.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7885" for this suite.

â€¢ [SLOW TEST:7.180 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":275,"completed":255,"skipped":4298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:39:51.580: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-375
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Apr  6 23:39:51.816: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b6b7ff3c-dd94-4079-a331-4588357f2382" in namespace "security-context-test-375" to be "Succeeded or Failed"
Apr  6 23:39:51.840: INFO: Pod "busybox-privileged-false-b6b7ff3c-dd94-4079-a331-4588357f2382": Phase="Pending", Reason="", readiness=false. Elapsed: 22.713392ms
Apr  6 23:39:53.841: INFO: Pod "busybox-privileged-false-b6b7ff3c-dd94-4079-a331-4588357f2382": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024153745s
Apr  6 23:39:53.841: INFO: Pod "busybox-privileged-false-b6b7ff3c-dd94-4079-a331-4588357f2382" satisfied condition "Succeeded or Failed"
Apr  6 23:39:53.845: INFO: Got logs for pod "busybox-privileged-false-b6b7ff3c-dd94-4079-a331-4588357f2382": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:39:53.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-375" for this suite.
â€¢{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":256,"skipped":4333,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:39:53.849: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6788
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:40:05.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6788" for this suite.

â€¢ [SLOW TEST:11.262 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":275,"completed":257,"skipped":4336,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:40:05.111: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9604
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:40:09.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9604" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":275,"completed":258,"skipped":4345,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:40:09.337: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3175
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr  6 23:40:09.546: INFO: Waiting up to 5m0s for pod "pod-ee645de4-3e0b-4034-a2f0-164d0fb77f89" in namespace "emptydir-3175" to be "Succeeded or Failed"
Apr  6 23:40:09.553: INFO: Pod "pod-ee645de4-3e0b-4034-a2f0-164d0fb77f89": Phase="Pending", Reason="", readiness=false. Elapsed: 6.870084ms
Apr  6 23:40:11.555: INFO: Pod "pod-ee645de4-3e0b-4034-a2f0-164d0fb77f89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00875766s
STEP: Saw pod success
Apr  6 23:40:11.555: INFO: Pod "pod-ee645de4-3e0b-4034-a2f0-164d0fb77f89" satisfied condition "Succeeded or Failed"
Apr  6 23:40:11.557: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-ee645de4-3e0b-4034-a2f0-164d0fb77f89 container test-container: <nil>
STEP: delete the pod
Apr  6 23:40:11.587: INFO: Waiting for pod pod-ee645de4-3e0b-4034-a2f0-164d0fb77f89 to disappear
Apr  6 23:40:11.593: INFO: Pod pod-ee645de4-3e0b-4034-a2f0-164d0fb77f89 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:40:11.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3175" for this suite.
â€¢{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":275,"completed":259,"skipped":4349,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:40:11.597: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Apr  6 23:40:11.788: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  6 23:40:11.820: INFO: Waiting for terminating namespaces to be deleted...
Apr  6 23:40:11.822: INFO: 
Logging pods the kubelet thinks is on node mip-bd-ap06-n1-vm02.mip.storage.hpecorp.net before test
Apr  6 23:40:11.832: INFO: canal-zxrh2 from kube-system started at 2020-04-06 22:01:03 +0000 UTC (3 container statuses recorded)
Apr  6 23:40:11.832: INFO: 	Container calico-node ready: true, restart count 1
Apr  6 23:40:11.832: INFO: 	Container install-cni ready: true, restart count 0
Apr  6 23:40:11.832: INFO: 	Container kube-flannel ready: true, restart count 2
Apr  6 23:40:11.832: INFO: kube-proxy-7w486 from kube-system started at 2020-04-06 22:01:48 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.832: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 23:40:11.832: INFO: csi-provisioner-kdf-0 from mapr-csi started at 2020-04-06 22:05:19 +0000 UTC (5 container statuses recorded)
Apr  6 23:40:11.832: INFO: 	Container csi-attacher ready: true, restart count 0
Apr  6 23:40:11.832: INFO: 	Container csi-provisioner ready: true, restart count 0
Apr  6 23:40:11.832: INFO: 	Container csi-snapshot ready: true, restart count 0
Apr  6 23:40:11.832: INFO: 	Container kdf-provisioner ready: true, restart count 0
Apr  6 23:40:11.832: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 23:40:11.832: INFO: hpecp-fsmount-g2knb from hpecp started at 2020-04-06 22:04:39 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.832: INFO: 	Container hpecp-fsmount ready: true, restart count 0
Apr  6 23:40:11.832: INFO: csi-nodeplugin-kdf-jn84c from mapr-csi started at 2020-04-06 22:05:19 +0000 UTC (3 container statuses recorded)
Apr  6 23:40:11.832: INFO: 	Container driver-registrar ready: true, restart count 0
Apr  6 23:40:11.832: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 23:40:11.832: INFO: 	Container mapr-kdf ready: true, restart count 0
Apr  6 23:40:11.832: INFO: coredns-66bff467f8-b269l from kube-system started at 2020-04-06 22:01:29 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.832: INFO: 	Container coredns ready: true, restart count 0
Apr  6 23:40:11.832: INFO: hpecp-agent-76d5b65798-qhxrd from hpecp started at 2020-04-06 22:04:06 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.832: INFO: 	Container hpecp-agent ready: true, restart count 0
Apr  6 23:40:11.832: INFO: dashboard-metrics-scraper-69449465bc-55bss from kubernetes-dashboard started at 2020-04-06 22:05:13 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.832: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Apr  6 23:40:11.832: INFO: metricbeat-lvk5c from kube-system started at 2020-04-06 22:05:52 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.832: INFO: 	Container metricbeat ready: true, restart count 0
Apr  6 23:40:11.832: INFO: sonobuoy-systemd-logs-daemon-set-c4700b4a2f8941f0-ttgxk from sonobuoy started at 2020-04-06 22:39:04 +0000 UTC (2 container statuses recorded)
Apr  6 23:40:11.832: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr  6 23:40:11.832: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 23:40:11.832: INFO: 
Logging pods the kubelet thinks is on node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net before test
Apr  6 23:40:11.837: INFO: coredns-66bff467f8-l8lxj from kube-system started at 2020-04-06 22:01:27 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.837: INFO: 	Container coredns ready: true, restart count 0
Apr  6 23:40:11.837: INFO: kube-state-metrics-5b5f5b558d-j5nkq from kube-system started at 2020-04-06 22:05:15 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.837: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr  6 23:40:11.837: INFO: sonobuoy from sonobuoy started at 2020-04-06 22:38:57 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.837: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  6 23:40:11.837: INFO: kube-proxy-6fzdv from kube-system started at 2020-04-06 22:01:48 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.837: INFO: 	Container kube-proxy ready: true, restart count 0
Apr  6 23:40:11.837: INFO: metricbeat-6cf8f85fb5-zpb92 from kube-system started at 2020-04-06 22:05:52 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.837: INFO: 	Container metricbeat ready: true, restart count 0
Apr  6 23:40:11.837: INFO: canal-rg6q8 from kube-system started at 2020-04-06 22:01:03 +0000 UTC (3 container statuses recorded)
Apr  6 23:40:11.837: INFO: 	Container calico-node ready: true, restart count 1
Apr  6 23:40:11.837: INFO: 	Container install-cni ready: true, restart count 0
Apr  6 23:40:11.837: INFO: 	Container kube-flannel ready: true, restart count 2
Apr  6 23:40:11.837: INFO: hpecp-fsmount-n7jkz from hpecp started at 2020-04-06 22:04:39 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.837: INFO: 	Container hpecp-fsmount ready: true, restart count 0
Apr  6 23:40:11.837: INFO: metricbeat-dsbrd from kube-system started at 2020-04-06 22:05:52 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.837: INFO: 	Container metricbeat ready: true, restart count 0
Apr  6 23:40:11.837: INFO: kubedirector-64b6488f6d-qpssh from hpecp started at 2020-04-06 22:04:39 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.837: INFO: 	Container kubedirector ready: true, restart count 0
Apr  6 23:40:11.837: INFO: sonobuoy-systemd-logs-daemon-set-c4700b4a2f8941f0-p5vhq from sonobuoy started at 2020-04-06 22:39:04 +0000 UTC (2 container statuses recorded)
Apr  6 23:40:11.837: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr  6 23:40:11.837: INFO: 	Container systemd-logs ready: true, restart count 0
Apr  6 23:40:11.837: INFO: csi-nodeplugin-kdf-4zrwc from mapr-csi started at 2020-04-06 22:05:19 +0000 UTC (3 container statuses recorded)
Apr  6 23:40:11.837: INFO: 	Container driver-registrar ready: true, restart count 0
Apr  6 23:40:11.837: INFO: 	Container liveness-probe ready: true, restart count 0
Apr  6 23:40:11.837: INFO: 	Container mapr-kdf ready: true, restart count 0
Apr  6 23:40:11.837: INFO: bin-false64cec0e5-f34b-4039-9439-f5bd25a90fba from kubelet-test-9604 started at 2020-04-06 23:40:05 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.837: INFO: 	Container bin-false64cec0e5-f34b-4039-9439-f5bd25a90fba ready: false, restart count 0
Apr  6 23:40:11.837: INFO: kubernetes-dashboard-6b49d498f6-9prxk from kubernetes-dashboard started at 2020-04-06 22:05:13 +0000 UTC (1 container statuses recorded)
Apr  6 23:40:11.837: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16035f6c25dc874b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16035f6c27def1fd], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:40:12.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5527" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":275,"completed":260,"skipped":4356,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:40:12.860: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1634
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-3280fc50-0073-4cbf-8615-7dfef88538ae
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-3280fc50-0073-4cbf-8615-7dfef88538ae
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:41:25.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1634" for this suite.

â€¢ [SLOW TEST:72.451 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":275,"completed":261,"skipped":4365,"failed":0}
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:41:25.310: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-5489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:41:25.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-5489" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":275,"completed":262,"skipped":4365,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:41:25.489: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3319
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-eaefeb0f-37c6-46de-af70-b4379afb0b92
STEP: Creating a pod to test consume configMaps
Apr  6 23:41:25.696: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c6487b28-9b83-471f-b06e-59422e955b83" in namespace "projected-3319" to be "Succeeded or Failed"
Apr  6 23:41:25.711: INFO: Pod "pod-projected-configmaps-c6487b28-9b83-471f-b06e-59422e955b83": Phase="Pending", Reason="", readiness=false. Elapsed: 14.837817ms
Apr  6 23:41:27.713: INFO: Pod "pod-projected-configmaps-c6487b28-9b83-471f-b06e-59422e955b83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01675795s
STEP: Saw pod success
Apr  6 23:41:27.713: INFO: Pod "pod-projected-configmaps-c6487b28-9b83-471f-b06e-59422e955b83" satisfied condition "Succeeded or Failed"
Apr  6 23:41:27.714: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod pod-projected-configmaps-c6487b28-9b83-471f-b06e-59422e955b83 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  6 23:41:27.777: INFO: Waiting for pod pod-projected-configmaps-c6487b28-9b83-471f-b06e-59422e955b83 to disappear
Apr  6 23:41:27.781: INFO: Pod pod-projected-configmaps-c6487b28-9b83-471f-b06e-59422e955b83 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:41:27.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3319" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":275,"completed":263,"skipped":4370,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:41:27.798: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4112
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4112.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4112.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4112.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4112.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4112.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4112.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4112.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 237.64.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.64.237_udp@PTR;check="$$(dig +tcp +noall +answer +search 237.64.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.64.237_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4112.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4112.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4112.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4112.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4112.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4112.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4112.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4112.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 237.64.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.64.237_udp@PTR;check="$$(dig +tcp +noall +answer +search 237.64.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.64.237_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  6 23:41:30.098: INFO: Unable to read wheezy_udp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:30.099: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:30.100: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:30.102: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:30.112: INFO: Unable to read jessie_udp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:30.113: INFO: Unable to read jessie_tcp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:30.114: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:30.115: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:30.123: INFO: Lookups using dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f failed for: [wheezy_udp@dns-test-service.dns-4112.svc.cluster.local wheezy_tcp@dns-test-service.dns-4112.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local jessie_udp@dns-test-service.dns-4112.svc.cluster.local jessie_tcp@dns-test-service.dns-4112.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local]

Apr  6 23:41:35.125: INFO: Unable to read wheezy_udp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:35.126: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:35.128: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:35.129: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:35.138: INFO: Unable to read jessie_udp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:35.140: INFO: Unable to read jessie_tcp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:35.141: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:35.142: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:35.150: INFO: Lookups using dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f failed for: [wheezy_udp@dns-test-service.dns-4112.svc.cluster.local wheezy_tcp@dns-test-service.dns-4112.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local jessie_udp@dns-test-service.dns-4112.svc.cluster.local jessie_tcp@dns-test-service.dns-4112.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local]

Apr  6 23:41:40.124: INFO: Unable to read wheezy_udp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:40.126: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:40.127: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:40.128: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:40.153: INFO: Unable to read jessie_udp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:40.154: INFO: Unable to read jessie_tcp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:40.156: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:40.157: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:40.165: INFO: Lookups using dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f failed for: [wheezy_udp@dns-test-service.dns-4112.svc.cluster.local wheezy_tcp@dns-test-service.dns-4112.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local jessie_udp@dns-test-service.dns-4112.svc.cluster.local jessie_tcp@dns-test-service.dns-4112.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local]

Apr  6 23:41:45.125: INFO: Unable to read wheezy_udp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:45.126: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:45.128: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:45.129: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:45.139: INFO: Unable to read jessie_udp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:45.140: INFO: Unable to read jessie_tcp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:45.141: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:45.142: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:45.150: INFO: Lookups using dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f failed for: [wheezy_udp@dns-test-service.dns-4112.svc.cluster.local wheezy_tcp@dns-test-service.dns-4112.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local jessie_udp@dns-test-service.dns-4112.svc.cluster.local jessie_tcp@dns-test-service.dns-4112.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local]

Apr  6 23:41:50.125: INFO: Unable to read wheezy_udp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:50.127: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:50.129: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:50.130: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:50.147: INFO: Unable to read jessie_udp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:50.149: INFO: Unable to read jessie_tcp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:50.150: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:50.152: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:50.174: INFO: Lookups using dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f failed for: [wheezy_udp@dns-test-service.dns-4112.svc.cluster.local wheezy_tcp@dns-test-service.dns-4112.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local jessie_udp@dns-test-service.dns-4112.svc.cluster.local jessie_tcp@dns-test-service.dns-4112.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local]

Apr  6 23:41:55.125: INFO: Unable to read wheezy_udp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:55.127: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:55.128: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:55.129: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:55.139: INFO: Unable to read jessie_udp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:55.140: INFO: Unable to read jessie_tcp@dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:55.142: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:55.143: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local from pod dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f: the server could not find the requested resource (get pods dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f)
Apr  6 23:41:55.150: INFO: Lookups using dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f failed for: [wheezy_udp@dns-test-service.dns-4112.svc.cluster.local wheezy_tcp@dns-test-service.dns-4112.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local jessie_udp@dns-test-service.dns-4112.svc.cluster.local jessie_tcp@dns-test-service.dns-4112.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4112.svc.cluster.local]

Apr  6 23:42:00.149: INFO: DNS probes using dns-4112/dns-test-5e6be90e-1ce5-4a00-8246-afc11aaed53f succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:42:00.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4112" for this suite.

â€¢ [SLOW TEST:32.582 seconds]
[sig-network] DNS
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":275,"completed":264,"skipped":4372,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:42:00.380: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4766
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Apr  6 23:42:00.554: INFO: Waiting up to 5m0s for pod "downward-api-c62d75e8-a531-496a-9125-f031103e9bc5" in namespace "downward-api-4766" to be "Succeeded or Failed"
Apr  6 23:42:00.572: INFO: Pod "downward-api-c62d75e8-a531-496a-9125-f031103e9bc5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.936076ms
Apr  6 23:42:02.574: INFO: Pod "downward-api-c62d75e8-a531-496a-9125-f031103e9bc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019879092s
STEP: Saw pod success
Apr  6 23:42:02.574: INFO: Pod "downward-api-c62d75e8-a531-496a-9125-f031103e9bc5" satisfied condition "Succeeded or Failed"
Apr  6 23:42:02.575: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downward-api-c62d75e8-a531-496a-9125-f031103e9bc5 container dapi-container: <nil>
STEP: delete the pod
Apr  6 23:42:02.621: INFO: Waiting for pod downward-api-c62d75e8-a531-496a-9125-f031103e9bc5 to disappear
Apr  6 23:42:02.629: INFO: Pod downward-api-c62d75e8-a531-496a-9125-f031103e9bc5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:42:02.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4766" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":275,"completed":265,"skipped":4412,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:42:02.633: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-472
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Apr  6 23:42:02.802: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d2a0b6e-ed80-426a-a144-faaa3152e631" in namespace "projected-472" to be "Succeeded or Failed"
Apr  6 23:42:02.812: INFO: Pod "downwardapi-volume-9d2a0b6e-ed80-426a-a144-faaa3152e631": Phase="Pending", Reason="", readiness=false. Elapsed: 10.048002ms
Apr  6 23:42:04.814: INFO: Pod "downwardapi-volume-9d2a0b6e-ed80-426a-a144-faaa3152e631": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011717657s
STEP: Saw pod success
Apr  6 23:42:04.814: INFO: Pod "downwardapi-volume-9d2a0b6e-ed80-426a-a144-faaa3152e631" satisfied condition "Succeeded or Failed"
Apr  6 23:42:04.815: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downwardapi-volume-9d2a0b6e-ed80-426a-a144-faaa3152e631 container client-container: <nil>
STEP: delete the pod
Apr  6 23:42:04.829: INFO: Waiting for pod downwardapi-volume-9d2a0b6e-ed80-426a-a144-faaa3152e631 to disappear
Apr  6 23:42:04.853: INFO: Pod downwardapi-volume-9d2a0b6e-ed80-426a-a144-faaa3152e631 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:42:04.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-472" for this suite.
â€¢{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":275,"completed":266,"skipped":4420,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:42:04.858: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4843
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:42:09.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4843" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":275,"completed":267,"skipped":4440,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:42:09.739: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5792
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr  6 23:42:11.237: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:42:11.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5792" for this suite.
â€¢{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":275,"completed":268,"skipped":4471,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:42:11.386: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Apr  6 23:42:51.590: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
W0406 23:42:51.590415      23 metrics_grabber.go:84] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr  6 23:42:51.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7477" for this suite.

â€¢ [SLOW TEST:40.208 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":275,"completed":269,"skipped":4481,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:42:51.594: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-2blnc in namespace proxy-5285
I0406 23:42:51.802371      23 runners.go:190] Created replication controller with name: proxy-service-2blnc, namespace: proxy-5285, replica count: 1
I0406 23:42:52.852665      23 runners.go:190] proxy-service-2blnc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0406 23:42:53.852799      23 runners.go:190] proxy-service-2blnc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0406 23:42:54.852941      23 runners.go:190] proxy-service-2blnc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0406 23:42:55.853076      23 runners.go:190] proxy-service-2blnc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 23:42:55.877: INFO: setup took 4.100101562s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr  6 23:42:55.881: INFO: (0) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 4.265774ms)
Apr  6 23:42:55.881: INFO: (0) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 4.550518ms)
Apr  6 23:42:55.881: INFO: (0) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 4.590987ms)
Apr  6 23:42:55.881: INFO: (0) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 4.753772ms)
Apr  6 23:42:55.883: INFO: (0) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 6.048393ms)
Apr  6 23:42:55.883: INFO: (0) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 6.039956ms)
Apr  6 23:42:55.883: INFO: (0) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 6.387126ms)
Apr  6 23:42:55.885: INFO: (0) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 7.883095ms)
Apr  6 23:42:55.885: INFO: (0) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 7.868972ms)
Apr  6 23:42:55.885: INFO: (0) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 7.842325ms)
Apr  6 23:42:55.885: INFO: (0) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 7.888144ms)
Apr  6 23:42:55.885: INFO: (0) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 8.560171ms)
Apr  6 23:42:55.885: INFO: (0) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 7.189754ms)
Apr  6 23:42:55.885: INFO: (0) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 8.659522ms)
Apr  6 23:42:55.885: INFO: (0) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 8.742056ms)
Apr  6 23:42:55.885: INFO: (0) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 8.673086ms)
Apr  6 23:42:55.890: INFO: (1) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 3.945116ms)
Apr  6 23:42:55.890: INFO: (1) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 4.228286ms)
Apr  6 23:42:55.890: INFO: (1) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 4.508832ms)
Apr  6 23:42:55.890: INFO: (1) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 4.818499ms)
Apr  6 23:42:55.890: INFO: (1) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 4.594673ms)
Apr  6 23:42:55.890: INFO: (1) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 4.669488ms)
Apr  6 23:42:55.891: INFO: (1) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 5.861791ms)
Apr  6 23:42:55.891: INFO: (1) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 5.592422ms)
Apr  6 23:42:55.891: INFO: (1) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 5.840661ms)
Apr  6 23:42:55.892: INFO: (1) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 6.911896ms)
Apr  6 23:42:55.892: INFO: (1) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 6.493732ms)
Apr  6 23:42:55.892: INFO: (1) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 6.772994ms)
Apr  6 23:42:55.892: INFO: (1) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 6.817806ms)
Apr  6 23:42:55.892: INFO: (1) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 6.574727ms)
Apr  6 23:42:55.892: INFO: (1) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 6.70325ms)
Apr  6 23:42:55.892: INFO: (1) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 6.504223ms)
Apr  6 23:42:55.897: INFO: (2) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 4.271179ms)
Apr  6 23:42:55.897: INFO: (2) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 4.341578ms)
Apr  6 23:42:55.897: INFO: (2) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 4.270216ms)
Apr  6 23:42:55.899: INFO: (2) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 6.285594ms)
Apr  6 23:42:55.899: INFO: (2) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 6.163608ms)
Apr  6 23:42:55.899: INFO: (2) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 6.193122ms)
Apr  6 23:42:55.899: INFO: (2) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 6.241327ms)
Apr  6 23:42:55.899: INFO: (2) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 6.207801ms)
Apr  6 23:42:55.899: INFO: (2) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 6.199516ms)
Apr  6 23:42:55.899: INFO: (2) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 6.329576ms)
Apr  6 23:42:55.899: INFO: (2) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 6.324043ms)
Apr  6 23:42:55.899: INFO: (2) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 6.281518ms)
Apr  6 23:42:55.899: INFO: (2) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 6.414464ms)
Apr  6 23:42:55.899: INFO: (2) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 6.39184ms)
Apr  6 23:42:55.899: INFO: (2) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 6.372815ms)
Apr  6 23:42:55.899: INFO: (2) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 6.489091ms)
Apr  6 23:42:55.904: INFO: (3) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 4.987859ms)
Apr  6 23:42:55.904: INFO: (3) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 5.265303ms)
Apr  6 23:42:55.904: INFO: (3) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 4.966726ms)
Apr  6 23:42:55.904: INFO: (3) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 5.061538ms)
Apr  6 23:42:55.905: INFO: (3) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 4.879413ms)
Apr  6 23:42:55.905: INFO: (3) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 4.78698ms)
Apr  6 23:42:55.905: INFO: (3) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 4.762734ms)
Apr  6 23:42:55.905: INFO: (3) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 4.994591ms)
Apr  6 23:42:55.905: INFO: (3) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 4.895499ms)
Apr  6 23:42:55.905: INFO: (3) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 5.277909ms)
Apr  6 23:42:55.905: INFO: (3) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 4.976745ms)
Apr  6 23:42:55.905: INFO: (3) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 5.189443ms)
Apr  6 23:42:55.905: INFO: (3) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 4.901406ms)
Apr  6 23:42:55.905: INFO: (3) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 5.112129ms)
Apr  6 23:42:55.905: INFO: (3) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 5.317929ms)
Apr  6 23:42:55.905: INFO: (3) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 5.296291ms)
Apr  6 23:42:55.909: INFO: (4) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 4.349428ms)
Apr  6 23:42:55.909: INFO: (4) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 4.553809ms)
Apr  6 23:42:55.909: INFO: (4) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 3.947543ms)
Apr  6 23:42:55.909: INFO: (4) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 4.226558ms)
Apr  6 23:42:55.909: INFO: (4) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 4.199462ms)
Apr  6 23:42:55.909: INFO: (4) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 4.445613ms)
Apr  6 23:42:55.909: INFO: (4) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 4.134391ms)
Apr  6 23:42:55.909: INFO: (4) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 4.328991ms)
Apr  6 23:42:55.909: INFO: (4) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 4.364646ms)
Apr  6 23:42:55.909: INFO: (4) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 4.056657ms)
Apr  6 23:42:55.911: INFO: (4) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 5.647841ms)
Apr  6 23:42:55.911: INFO: (4) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 5.756612ms)
Apr  6 23:42:55.911: INFO: (4) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 5.98096ms)
Apr  6 23:42:55.911: INFO: (4) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 5.933156ms)
Apr  6 23:42:55.911: INFO: (4) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 6.047743ms)
Apr  6 23:42:55.911: INFO: (4) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 5.834882ms)
Apr  6 23:42:55.914: INFO: (5) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 2.747237ms)
Apr  6 23:42:55.915: INFO: (5) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 3.631928ms)
Apr  6 23:42:55.915: INFO: (5) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 3.749513ms)
Apr  6 23:42:55.915: INFO: (5) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 3.864935ms)
Apr  6 23:42:55.915: INFO: (5) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 3.639894ms)
Apr  6 23:42:55.915: INFO: (5) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 3.74343ms)
Apr  6 23:42:55.915: INFO: (5) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 3.818585ms)
Apr  6 23:42:55.915: INFO: (5) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 3.740365ms)
Apr  6 23:42:55.916: INFO: (5) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 2.544793ms)
Apr  6 23:42:55.916: INFO: (5) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 3.824446ms)
Apr  6 23:42:55.916: INFO: (5) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 3.802771ms)
Apr  6 23:42:55.916: INFO: (5) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 3.912074ms)
Apr  6 23:42:55.916: INFO: (5) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 2.714302ms)
Apr  6 23:42:55.916: INFO: (5) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 5.040361ms)
Apr  6 23:42:55.917: INFO: (5) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 3.529067ms)
Apr  6 23:42:55.917: INFO: (5) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 3.612885ms)
Apr  6 23:42:55.918: INFO: (6) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 1.218585ms)
Apr  6 23:42:55.921: INFO: (6) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 4.141244ms)
Apr  6 23:42:55.921: INFO: (6) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 4.369692ms)
Apr  6 23:42:55.927: INFO: (6) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 9.984812ms)
Apr  6 23:42:55.927: INFO: (6) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 9.951359ms)
Apr  6 23:42:55.930: INFO: (6) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 12.544544ms)
Apr  6 23:42:55.930: INFO: (6) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 12.822831ms)
Apr  6 23:42:55.930: INFO: (6) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 12.758388ms)
Apr  6 23:42:55.930: INFO: (6) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 12.902023ms)
Apr  6 23:42:55.930: INFO: (6) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 13.031044ms)
Apr  6 23:42:55.930: INFO: (6) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 13.249983ms)
Apr  6 23:42:55.930: INFO: (6) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 13.386236ms)
Apr  6 23:42:55.931: INFO: (6) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 14.350677ms)
Apr  6 23:42:55.932: INFO: (6) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 14.746496ms)
Apr  6 23:42:55.932: INFO: (6) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 15.325943ms)
Apr  6 23:42:55.933: INFO: (6) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 15.432354ms)
Apr  6 23:42:55.934: INFO: (7) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 1.764772ms)
Apr  6 23:42:55.938: INFO: (7) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 4.781288ms)
Apr  6 23:42:55.939: INFO: (7) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 5.776608ms)
Apr  6 23:42:55.939: INFO: (7) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 6.111551ms)
Apr  6 23:42:55.939: INFO: (7) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 5.916583ms)
Apr  6 23:42:55.939: INFO: (7) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 5.959538ms)
Apr  6 23:42:55.939: INFO: (7) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 5.684785ms)
Apr  6 23:42:55.939: INFO: (7) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 5.892097ms)
Apr  6 23:42:55.939: INFO: (7) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 5.784315ms)
Apr  6 23:42:55.939: INFO: (7) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 5.827626ms)
Apr  6 23:42:55.939: INFO: (7) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 5.897966ms)
Apr  6 23:42:55.939: INFO: (7) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 6.126007ms)
Apr  6 23:42:55.939: INFO: (7) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 6.165158ms)
Apr  6 23:42:55.939: INFO: (7) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 6.24489ms)
Apr  6 23:42:55.939: INFO: (7) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 5.886249ms)
Apr  6 23:42:55.939: INFO: (7) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 6.223911ms)
Apr  6 23:42:55.945: INFO: (8) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 5.981782ms)
Apr  6 23:42:55.945: INFO: (8) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 4.591641ms)
Apr  6 23:42:55.945: INFO: (8) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 4.539986ms)
Apr  6 23:42:55.945: INFO: (8) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 4.482836ms)
Apr  6 23:42:55.945: INFO: (8) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 4.466487ms)
Apr  6 23:42:55.945: INFO: (8) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 5.963881ms)
Apr  6 23:42:55.945: INFO: (8) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 4.746096ms)
Apr  6 23:42:55.945: INFO: (8) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 6.139501ms)
Apr  6 23:42:55.945: INFO: (8) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 6.029177ms)
Apr  6 23:42:55.945: INFO: (8) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 4.853425ms)
Apr  6 23:42:55.945: INFO: (8) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 4.726712ms)
Apr  6 23:42:55.945: INFO: (8) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 5.055368ms)
Apr  6 23:42:55.945: INFO: (8) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 5.2367ms)
Apr  6 23:42:55.946: INFO: (8) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 5.319136ms)
Apr  6 23:42:55.946: INFO: (8) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 5.571032ms)
Apr  6 23:42:55.946: INFO: (8) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 5.424212ms)
Apr  6 23:42:55.950: INFO: (9) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 4.127064ms)
Apr  6 23:42:55.952: INFO: (9) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 5.491571ms)
Apr  6 23:42:55.952: INFO: (9) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 5.616469ms)
Apr  6 23:42:55.952: INFO: (9) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 5.565682ms)
Apr  6 23:42:55.952: INFO: (9) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 4.799621ms)
Apr  6 23:42:55.952: INFO: (9) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 4.842156ms)
Apr  6 23:42:55.952: INFO: (9) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 4.575475ms)
Apr  6 23:42:55.952: INFO: (9) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 5.528607ms)
Apr  6 23:42:55.952: INFO: (9) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 4.672921ms)
Apr  6 23:42:55.952: INFO: (9) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 5.474701ms)
Apr  6 23:42:55.952: INFO: (9) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 4.724571ms)
Apr  6 23:42:55.952: INFO: (9) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 4.986308ms)
Apr  6 23:42:55.953: INFO: (9) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 6.319064ms)
Apr  6 23:42:55.953: INFO: (9) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 6.053986ms)
Apr  6 23:42:55.953: INFO: (9) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 6.179105ms)
Apr  6 23:42:55.953: INFO: (9) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 6.247895ms)
Apr  6 23:42:55.956: INFO: (10) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 3.14739ms)
Apr  6 23:42:55.956: INFO: (10) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 2.726221ms)
Apr  6 23:42:55.959: INFO: (10) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 5.233231ms)
Apr  6 23:42:55.959: INFO: (10) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 5.119871ms)
Apr  6 23:42:55.959: INFO: (10) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 5.42645ms)
Apr  6 23:42:55.959: INFO: (10) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 4.981055ms)
Apr  6 23:42:55.959: INFO: (10) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 5.115493ms)
Apr  6 23:42:55.959: INFO: (10) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 5.319798ms)
Apr  6 23:42:55.959: INFO: (10) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 5.233147ms)
Apr  6 23:42:55.959: INFO: (10) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 5.420654ms)
Apr  6 23:42:55.959: INFO: (10) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 5.174344ms)
Apr  6 23:42:55.959: INFO: (10) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 5.291405ms)
Apr  6 23:42:55.959: INFO: (10) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 5.061538ms)
Apr  6 23:42:55.959: INFO: (10) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 5.360334ms)
Apr  6 23:42:55.959: INFO: (10) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 5.468115ms)
Apr  6 23:42:55.959: INFO: (10) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 5.368515ms)
Apr  6 23:42:55.962: INFO: (11) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 3.294971ms)
Apr  6 23:42:55.963: INFO: (11) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 3.984265ms)
Apr  6 23:42:55.964: INFO: (11) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 4.275024ms)
Apr  6 23:42:55.964: INFO: (11) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 4.134247ms)
Apr  6 23:42:55.964: INFO: (11) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 4.513937ms)
Apr  6 23:42:55.964: INFO: (11) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 4.537584ms)
Apr  6 23:42:55.964: INFO: (11) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 4.888293ms)
Apr  6 23:42:55.964: INFO: (11) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 4.640002ms)
Apr  6 23:42:55.966: INFO: (11) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 6.407595ms)
Apr  6 23:42:55.966: INFO: (11) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 6.838591ms)
Apr  6 23:42:55.966: INFO: (11) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 6.499343ms)
Apr  6 23:42:55.966: INFO: (11) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 6.821752ms)
Apr  6 23:42:55.966: INFO: (11) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 6.747729ms)
Apr  6 23:42:55.966: INFO: (11) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 6.72241ms)
Apr  6 23:42:55.966: INFO: (11) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 6.744849ms)
Apr  6 23:42:55.966: INFO: (11) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 6.944639ms)
Apr  6 23:42:55.970: INFO: (12) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 3.633516ms)
Apr  6 23:42:55.972: INFO: (12) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 5.537992ms)
Apr  6 23:42:55.972: INFO: (12) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 5.620239ms)
Apr  6 23:42:55.972: INFO: (12) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 5.609857ms)
Apr  6 23:42:55.972: INFO: (12) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 5.509983ms)
Apr  6 23:42:55.972: INFO: (12) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 5.847886ms)
Apr  6 23:42:55.972: INFO: (12) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 5.726341ms)
Apr  6 23:42:55.972: INFO: (12) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 5.580886ms)
Apr  6 23:42:55.972: INFO: (12) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 5.719118ms)
Apr  6 23:42:55.972: INFO: (12) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 5.963624ms)
Apr  6 23:42:55.972: INFO: (12) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 6.011922ms)
Apr  6 23:42:55.972: INFO: (12) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 5.846749ms)
Apr  6 23:42:55.972: INFO: (12) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 5.954155ms)
Apr  6 23:42:55.972: INFO: (12) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 5.903673ms)
Apr  6 23:42:55.972: INFO: (12) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 5.584887ms)
Apr  6 23:42:55.972: INFO: (12) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 5.856059ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 6.575709ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 6.658609ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 6.473318ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 6.841105ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 6.315203ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 6.360498ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 6.396896ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 6.769015ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 6.433827ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 6.661612ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 6.648541ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 6.520772ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 6.840904ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 6.598384ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 6.506809ms)
Apr  6 23:42:55.979: INFO: (13) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 6.634744ms)
Apr  6 23:42:55.985: INFO: (14) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 5.834334ms)
Apr  6 23:42:55.985: INFO: (14) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 5.706843ms)
Apr  6 23:42:55.985: INFO: (14) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 5.857628ms)
Apr  6 23:42:55.985: INFO: (14) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 5.860032ms)
Apr  6 23:42:55.985: INFO: (14) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 5.757729ms)
Apr  6 23:42:55.985: INFO: (14) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 5.73843ms)
Apr  6 23:42:55.985: INFO: (14) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 5.869766ms)
Apr  6 23:42:55.986: INFO: (14) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 6.957087ms)
Apr  6 23:42:55.986: INFO: (14) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 7.17777ms)
Apr  6 23:42:55.987: INFO: (14) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 6.876333ms)
Apr  6 23:42:55.987: INFO: (14) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 6.855377ms)
Apr  6 23:42:55.987: INFO: (14) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 7.075532ms)
Apr  6 23:42:55.987: INFO: (14) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 6.891873ms)
Apr  6 23:42:55.987: INFO: (14) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 6.994496ms)
Apr  6 23:42:55.987: INFO: (14) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 7.023191ms)
Apr  6 23:42:55.987: INFO: (14) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 7.193831ms)
Apr  6 23:42:55.990: INFO: (15) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 3.756687ms)
Apr  6 23:42:55.990: INFO: (15) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 3.663024ms)
Apr  6 23:42:55.990: INFO: (15) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 3.767658ms)
Apr  6 23:42:55.990: INFO: (15) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 3.735918ms)
Apr  6 23:42:55.990: INFO: (15) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 3.721858ms)
Apr  6 23:42:55.990: INFO: (15) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 3.800927ms)
Apr  6 23:42:55.990: INFO: (15) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 3.724166ms)
Apr  6 23:42:55.990: INFO: (15) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 3.893854ms)
Apr  6 23:42:55.990: INFO: (15) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 3.707457ms)
Apr  6 23:42:55.992: INFO: (15) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 5.487419ms)
Apr  6 23:42:55.992: INFO: (15) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 5.685972ms)
Apr  6 23:42:55.992: INFO: (15) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 5.689299ms)
Apr  6 23:42:55.992: INFO: (15) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 5.80198ms)
Apr  6 23:42:55.992: INFO: (15) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 5.785352ms)
Apr  6 23:42:55.993: INFO: (15) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 5.75127ms)
Apr  6 23:42:55.993: INFO: (15) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 5.88126ms)
Apr  6 23:42:55.998: INFO: (16) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 4.812142ms)
Apr  6 23:42:55.998: INFO: (16) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 4.865736ms)
Apr  6 23:42:55.998: INFO: (16) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 5.281426ms)
Apr  6 23:42:55.998: INFO: (16) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 5.243036ms)
Apr  6 23:42:55.998: INFO: (16) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 4.983765ms)
Apr  6 23:42:55.998: INFO: (16) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 4.860328ms)
Apr  6 23:42:55.998: INFO: (16) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 5.118382ms)
Apr  6 23:42:55.998: INFO: (16) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 5.033332ms)
Apr  6 23:42:55.998: INFO: (16) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 5.236653ms)
Apr  6 23:42:55.998: INFO: (16) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 5.184753ms)
Apr  6 23:42:55.999: INFO: (16) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 5.327523ms)
Apr  6 23:42:56.000: INFO: (16) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 6.610836ms)
Apr  6 23:42:56.000: INFO: (16) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 6.541557ms)
Apr  6 23:42:56.000: INFO: (16) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 6.680737ms)
Apr  6 23:42:56.000: INFO: (16) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 6.789364ms)
Apr  6 23:42:56.000: INFO: (16) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 5.763874ms)
Apr  6 23:42:56.004: INFO: (17) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 4.054578ms)
Apr  6 23:42:56.004: INFO: (17) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 3.551778ms)
Apr  6 23:42:56.006: INFO: (17) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 5.651246ms)
Apr  6 23:42:56.006: INFO: (17) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 5.972966ms)
Apr  6 23:42:56.006: INFO: (17) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 5.803778ms)
Apr  6 23:42:56.006: INFO: (17) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 5.743353ms)
Apr  6 23:42:56.006: INFO: (17) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 5.943214ms)
Apr  6 23:42:56.006: INFO: (17) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 5.92987ms)
Apr  6 23:42:56.006: INFO: (17) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 5.750097ms)
Apr  6 23:42:56.006: INFO: (17) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 6.06017ms)
Apr  6 23:42:56.006: INFO: (17) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 6.155364ms)
Apr  6 23:42:56.006: INFO: (17) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 5.821209ms)
Apr  6 23:42:56.006: INFO: (17) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 5.929413ms)
Apr  6 23:42:56.006: INFO: (17) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 6.125303ms)
Apr  6 23:42:56.006: INFO: (17) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 5.917735ms)
Apr  6 23:42:56.006: INFO: (17) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 6.128742ms)
Apr  6 23:42:56.008: INFO: (18) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 2.225433ms)
Apr  6 23:42:56.008: INFO: (18) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 2.171138ms)
Apr  6 23:42:56.016: INFO: (18) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 8.868303ms)
Apr  6 23:42:56.016: INFO: (18) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 8.636307ms)
Apr  6 23:42:56.016: INFO: (18) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 10.04269ms)
Apr  6 23:42:56.016: INFO: (18) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 8.742015ms)
Apr  6 23:42:56.016: INFO: (18) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 8.723542ms)
Apr  6 23:42:56.016: INFO: (18) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 8.716391ms)
Apr  6 23:42:56.016: INFO: (18) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 8.65073ms)
Apr  6 23:42:56.016: INFO: (18) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 8.612713ms)
Apr  6 23:42:56.016: INFO: (18) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 8.710535ms)
Apr  6 23:42:56.016: INFO: (18) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 8.713851ms)
Apr  6 23:42:56.016: INFO: (18) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 8.660864ms)
Apr  6 23:42:56.016: INFO: (18) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 8.776831ms)
Apr  6 23:42:56.016: INFO: (18) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 10.091463ms)
Apr  6 23:42:56.017: INFO: (18) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 9.72155ms)
Apr  6 23:42:56.022: INFO: (19) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:1080/proxy/rewriteme">... (200; 5.044313ms)
Apr  6 23:42:56.022: INFO: (19) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:162/proxy/: bar (200; 4.738433ms)
Apr  6 23:42:56.022: INFO: (19) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:162/proxy/: bar (200; 4.923851ms)
Apr  6 23:42:56.022: INFO: (19) /api/v1/namespaces/proxy-5285/pods/http:proxy-service-2blnc-frbms:160/proxy/: foo (200; 5.075154ms)
Apr  6 23:42:56.023: INFO: (19) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms/proxy/rewriteme">test</a> (200; 4.864211ms)
Apr  6 23:42:56.023: INFO: (19) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:160/proxy/: foo (200; 5.242221ms)
Apr  6 23:42:56.023: INFO: (19) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:443/proxy/tlsrewritem... (200; 4.956149ms)
Apr  6 23:42:56.023: INFO: (19) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:460/proxy/: tls baz (200; 5.108239ms)
Apr  6 23:42:56.023: INFO: (19) /api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/: <a href="/api/v1/namespaces/proxy-5285/pods/proxy-service-2blnc-frbms:1080/proxy/rewriteme">test<... (200; 4.8955ms)
Apr  6 23:42:56.024: INFO: (19) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname1/proxy/: foo (200; 6.247004ms)
Apr  6 23:42:56.024: INFO: (19) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname2/proxy/: tls qux (200; 6.750285ms)
Apr  6 23:42:56.024: INFO: (19) /api/v1/namespaces/proxy-5285/services/https:proxy-service-2blnc:tlsportname1/proxy/: tls baz (200; 6.48686ms)
Apr  6 23:42:56.024: INFO: (19) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname2/proxy/: bar (200; 6.259765ms)
Apr  6 23:42:56.024: INFO: (19) /api/v1/namespaces/proxy-5285/services/proxy-service-2blnc:portname1/proxy/: foo (200; 6.63927ms)
Apr  6 23:42:56.024: INFO: (19) /api/v1/namespaces/proxy-5285/services/http:proxy-service-2blnc:portname2/proxy/: bar (200; 6.616923ms)
Apr  6 23:42:56.024: INFO: (19) /api/v1/namespaces/proxy-5285/pods/https:proxy-service-2blnc-frbms:462/proxy/: tls qux (200; 6.390295ms)
STEP: deleting ReplicationController proxy-service-2blnc in namespace proxy-5285, will wait for the garbage collector to delete the pods
Apr  6 23:42:56.078: INFO: Deleting ReplicationController proxy-service-2blnc took: 2.791824ms
Apr  6 23:42:56.679: INFO: Terminating ReplicationController proxy-service-2blnc pods took: 600.862363ms
[AfterEach] version v1
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:43:07.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5285" for this suite.

â€¢ [SLOW TEST:15.795 seconds]
[sig-network] Proxy
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":275,"completed":270,"skipped":4508,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:43:07.389: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-2481
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2481
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2481
Apr  6 23:43:07.598: INFO: Found 0 stateful pods, waiting for 1
Apr  6 23:43:17.601: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr  6 23:43:17.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-2481 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 23:43:20.286: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 23:43:20.286: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 23:43:20.286: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 23:43:20.288: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr  6 23:43:30.294: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 23:43:30.294: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 23:43:30.330: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999801s
Apr  6 23:43:31.332: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.973551356s
Apr  6 23:43:32.334: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.97159796s
Apr  6 23:43:33.336: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.969968779s
Apr  6 23:43:34.338: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.967829841s
Apr  6 23:43:35.340: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.966304525s
Apr  6 23:43:36.342: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.964391953s
Apr  6 23:43:37.344: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.961877648s
Apr  6 23:43:38.346: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.960062813s
Apr  6 23:43:39.348: INFO: Verifying statefulset ss doesn't scale past 1 for another 958.274106ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2481
Apr  6 23:43:40.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-2481 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 23:43:40.500: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 23:43:40.500: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 23:43:40.500: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 23:43:40.502: INFO: Found 1 stateful pods, waiting for 3
Apr  6 23:43:50.504: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 23:43:50.504: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  6 23:43:50.504: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr  6 23:43:50.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-2481 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 23:43:50.649: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 23:43:50.649: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 23:43:50.649: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 23:43:50.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-2481 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 23:43:50.879: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 23:43:50.879: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 23:43:50.879: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 23:43:50.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-2481 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr  6 23:43:51.026: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr  6 23:43:51.026: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr  6 23:43:51.026: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr  6 23:43:51.026: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 23:43:51.028: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr  6 23:44:01.031: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 23:44:01.031: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 23:44:01.031: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr  6 23:44:01.042: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999796s
Apr  6 23:44:02.044: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992274063s
Apr  6 23:44:03.046: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990188728s
Apr  6 23:44:04.066: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988136417s
Apr  6 23:44:05.068: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.968069044s
Apr  6 23:44:06.074: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.962132658s
Apr  6 23:44:07.076: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.959871084s
Apr  6 23:44:08.078: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957945232s
Apr  6 23:44:09.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955895484s
Apr  6 23:44:10.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 946.38283ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2481
Apr  6 23:44:11.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-2481 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 23:44:11.231: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 23:44:11.231: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 23:44:11.231: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 23:44:11.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-2481 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 23:44:11.378: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 23:44:11.378: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 23:44:11.378: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 23:44:11.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=statefulset-2481 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr  6 23:44:11.514: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr  6 23:44:11.514: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr  6 23:44:11.514: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr  6 23:44:11.514: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Apr  6 23:44:41.534: INFO: Deleting all statefulset in ns statefulset-2481
Apr  6 23:44:41.535: INFO: Scaling statefulset ss to 0
Apr  6 23:44:41.552: INFO: Waiting for statefulset status.replicas updated to 0
Apr  6 23:44:41.553: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:44:41.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2481" for this suite.

â€¢ [SLOW TEST:94.176 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":275,"completed":271,"skipped":4519,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:44:41.565: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6903
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl label
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1206
STEP: creating the pod
Apr  6 23:44:41.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 create -f - --namespace=kubectl-6903'
Apr  6 23:44:42.003: INFO: stderr: ""
Apr  6 23:44:42.003: INFO: stdout: "pod/pause created\n"
Apr  6 23:44:42.003: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr  6 23:44:42.003: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6903" to be "running and ready"
Apr  6 23:44:42.024: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 21.086113ms
Apr  6 23:44:44.026: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.022968537s
Apr  6 23:44:44.026: INFO: Pod "pause" satisfied condition "running and ready"
Apr  6 23:44:44.026: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: adding the label testing-label with value testing-label-value to a pod
Apr  6 23:44:44.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 label pods pause testing-label=testing-label-value --namespace=kubectl-6903'
Apr  6 23:44:44.086: INFO: stderr: ""
Apr  6 23:44:44.086: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr  6 23:44:44.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pod pause -L testing-label --namespace=kubectl-6903'
Apr  6 23:44:44.150: INFO: stderr: ""
Apr  6 23:44:44.150: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr  6 23:44:44.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 label pods pause testing-label- --namespace=kubectl-6903'
Apr  6 23:44:44.210: INFO: stderr: ""
Apr  6 23:44:44.210: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr  6 23:44:44.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pod pause -L testing-label --namespace=kubectl-6903'
Apr  6 23:44:44.262: INFO: stderr: ""
Apr  6 23:44:44.262: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1213
STEP: using delete to clean up resources
Apr  6 23:44:44.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 delete --grace-period=0 --force -f - --namespace=kubectl-6903'
Apr  6 23:44:44.349: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  6 23:44:44.349: INFO: stdout: "pod \"pause\" force deleted\n"
Apr  6 23:44:44.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get rc,svc -l name=pause --no-headers --namespace=kubectl-6903'
Apr  6 23:44:44.408: INFO: stderr: "No resources found in kubectl-6903 namespace.\n"
Apr  6 23:44:44.408: INFO: stdout: ""
Apr  6 23:44:44.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 get pods -l name=pause --namespace=kubectl-6903 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  6 23:44:44.461: INFO: stderr: ""
Apr  6 23:44:44.461: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:44:44.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6903" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":275,"completed":272,"skipped":4522,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:44:44.470: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4202
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service nodeport-test with type=NodePort in namespace services-4202
STEP: creating replication controller nodeport-test in namespace services-4202
I0406 23:44:44.719960      23 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-4202, replica count: 2
Apr  6 23:44:47.770: INFO: Creating new exec pod
I0406 23:44:47.770262      23 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  6 23:44:50.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=services-4202 execpod9xlzc -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Apr  6 23:44:50.942: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr  6 23:44:50.942: INFO: stdout: ""
Apr  6 23:44:50.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=services-4202 execpod9xlzc -- /bin/sh -x -c nc -zv -t -w 2 10.103.178.6 80'
Apr  6 23:44:51.081: INFO: stderr: "+ nc -zv -t -w 2 10.103.178.6 80\nConnection to 10.103.178.6 80 port [tcp/http] succeeded!\n"
Apr  6 23:44:51.081: INFO: stdout: ""
Apr  6 23:44:51.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=services-4202 execpod9xlzc -- /bin/sh -x -c nc -zv -t -w 2 16.0.8.73 31980'
Apr  6 23:44:51.218: INFO: stderr: "+ nc -zv -t -w 2 16.0.8.73 31980\nConnection to 16.0.8.73 31980 port [tcp/31980] succeeded!\n"
Apr  6 23:44:51.218: INFO: stdout: ""
Apr  6 23:44:51.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-285118746 exec --namespace=services-4202 execpod9xlzc -- /bin/sh -x -c nc -zv -t -w 2 16.0.8.74 31980'
Apr  6 23:44:51.371: INFO: stderr: "+ nc -zv -t -w 2 16.0.8.74 31980\nConnection to 16.0.8.74 31980 port [tcp/31980] succeeded!\n"
Apr  6 23:44:51.371: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:44:51.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4202" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702

â€¢ [SLOW TEST:6.906 seconds]
[sig-network] Services
/workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":275,"completed":273,"skipped":4524,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:44:51.377: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2394
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Apr  6 23:44:51.569: INFO: Waiting up to 5m0s for pod "downward-api-ae596ba7-b9f9-4e34-b194-47f27b2fc5f8" in namespace "downward-api-2394" to be "Succeeded or Failed"
Apr  6 23:44:51.579: INFO: Pod "downward-api-ae596ba7-b9f9-4e34-b194-47f27b2fc5f8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.765914ms
Apr  6 23:44:53.600: INFO: Pod "downward-api-ae596ba7-b9f9-4e34-b194-47f27b2fc5f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031445153s
STEP: Saw pod success
Apr  6 23:44:53.600: INFO: Pod "downward-api-ae596ba7-b9f9-4e34-b194-47f27b2fc5f8" satisfied condition "Succeeded or Failed"
Apr  6 23:44:53.602: INFO: Trying to get logs from node mip-bd-ap06-n1-vm03.mip.storage.hpecorp.net pod downward-api-ae596ba7-b9f9-4e34-b194-47f27b2fc5f8 container dapi-container: <nil>
STEP: delete the pod
Apr  6 23:44:53.644: INFO: Waiting for pod downward-api-ae596ba7-b9f9-4e34-b194-47f27b2fc5f8 to disappear
Apr  6 23:44:53.652: INFO: Pod downward-api-ae596ba7-b9f9-4e34-b194-47f27b2fc5f8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:44:53.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2394" for this suite.
â€¢{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":275,"completed":274,"skipped":4652,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Apr  6 23:44:53.656: INFO: >>> kubeConfig: /tmp/kubeconfig-285118746
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6265
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.18.0-rc.1.21+8be33caaf953ac/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Apr  6 23:44:53.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6265" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":275,"completed":275,"skipped":4693,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSApr  6 23:44:53.911: INFO: Running AfterSuite actions on all nodes
Apr  6 23:44:53.911: INFO: Running AfterSuite actions on node 1
Apr  6 23:44:53.911: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":275,"completed":275,"skipped":4717,"failed":0}

Ran 275 of 4992 Specs in 3919.838 seconds
SUCCESS! -- 275 Passed | 0 Failed | 0 Pending | 4717 Skipped
PASS

Ginkgo ran 1 suite in 1h5m21.03601114s
Test Suite Passed
