**************************************************************************
bootstrap.py is deprecated!
test-infra oncall does not support any job still using bootstrap.py.
Please migrate your job to podutils!
https://github.com/kubernetes/test-infra/blob/master/prow/pod-utilities.md
**************************************************************************
Args: --job=ci-kubernetes-gce-conformance-latest-1-18 --service-account=/etc/service-account/service-account.json --upload=gs://kubernetes-jenkins/logs --timeout=220 --bare --scenario=kubernetes_e2e -- --extract=ci/latest-1.18 --gcp-master-image=gci --gcp-node-image=gci --gcp-zone=us-west1-b --provider=gce '--test_args=--ginkgo.focus=\[Conformance\]' --timeout=200m
Bootstrap ci-kubernetes-gce-conformance-latest-1-18...
Builder: fe64b46b-6f87-11ea-9710-126912a3969e
Image: gcr.io/k8s-testimages/kubekins-e2e:v20200324-0897dba-1.18
Gubernator results at https://gubernator.k8s.io/build/kubernetes-jenkins/logs/ci-kubernetes-gce-conformance-latest-1-18/1243229791620960256
Call:  gcloud auth activate-service-account --key-file=/etc/service-account/service-account.json
Activated service account credentials for: [pr-kubekins@kubernetes-jenkins-pull.iam.gserviceaccount.com]
process 35 exited with code 0 after 0.0m
Call:  gcloud config get-value account
process 49 exited with code 0 after 0.0m
Will upload results to gs://kubernetes-jenkins/logs using pr-kubekins@kubernetes-jenkins-pull.iam.gserviceaccount.com
Root: /workspace
cd to /workspace
Configure environment...
Call:  git show -s --format=format:%ct HEAD
fatal: not a git repository (or any of the parent directories): .git
process 63 exited with code 128 after 0.0m
Unable to print commit date for HEAD
Call:  gcloud auth activate-service-account --key-file=/etc/service-account/service-account.json
Activated service account credentials for: [pr-kubekins@kubernetes-jenkins-pull.iam.gserviceaccount.com]
process 64 exited with code 0 after 0.0m
Call:  gcloud config get-value account
process 78 exited with code 0 after 0.0m
Will upload results to gs://kubernetes-jenkins/logs using pr-kubekins@kubernetes-jenkins-pull.iam.gserviceaccount.com
Start 1243229791620960256 at ...
Call:  gsutil -q -h Content-Type:application/json cp /tmp/gsutil_IsuVVs gs://kubernetes-jenkins/logs/ci-kubernetes-gce-conformance-latest-1-18/1243229791620960256/started.json
process 92 exited with code 0 after 0.0m
Call:  /workspace/./test-infra/jenkins/../scenarios/kubernetes_e2e.py --extract=ci/latest-1.18 --gcp-master-image=gci --gcp-node-image=gci --gcp-zone=us-west1-b --provider=gce '--test_args=--ginkgo.focus=\[Conformance\]' --timeout=200m
starts with local mode
Environment:
ARTIFACTS=/workspace/_artifacts
AWS_SSH_PRIVATE_KEY_FILE=/root/.ssh/kube_aws_rsa
AWS_SSH_PUBLIC_KEY_FILE=/root/.ssh/kube_aws_rsa.pub
BAZEL_REMOTE_CACHE_ENABLED=false
BAZEL_VERSION=2.2.0
BOOTSTRAP_MIGRATION=yes
BOSKOS_METRICS_PORT=tcp://10.63.252.110:9090
BOSKOS_METRICS_PORT_9090_TCP=tcp://10.63.252.110:9090
BOSKOS_METRICS_PORT_9090_TCP_ADDR=10.63.252.110
BOSKOS_METRICS_PORT_9090_TCP_PORT=9090
BOSKOS_METRICS_PORT_9090_TCP_PROTO=tcp
BOSKOS_METRICS_SERVICE_HOST=10.63.252.110
BOSKOS_METRICS_SERVICE_PORT=9090
BOSKOS_METRICS_SERVICE_PORT_METRICS=9090
BOSKOS_PORT=tcp://10.63.250.132:80
BOSKOS_PORT_80_TCP=tcp://10.63.250.132:80
BOSKOS_PORT_80_TCP_ADDR=10.63.250.132
BOSKOS_PORT_80_TCP_PORT=80
BOSKOS_PORT_80_TCP_PROTO=tcp
BOSKOS_SERVICE_HOST=10.63.250.132
BOSKOS_SERVICE_PORT=80
BOSKOS_SERVICE_PORT_DEFAULT=80
BUILD_ID=1243229791620960256
BUILD_NUMBER=1243229791620960256
CI=true
CLOUDSDK_COMPONENT_MANAGER_DISABLE_UPDATE_CHECK=true
CLOUDSDK_CONFIG=/workspace/.config/gcloud
CLOUDSDK_CORE_DISABLE_PROMPTS=1
CLOUDSDK_EXPERIMENTAL_FAST_COMPONENT_UPDATE=false
DOCKER_IN_DOCKER_ENABLED=false
DOCKER_IN_DOCKER_IPV6_ENABLED=false
E2E_GOOGLE_APPLICATION_CREDENTIALS=/etc/service-account/service-account.json
GCS_ARTIFACTS_DIR=gs://kubernetes-jenkins/logs/ci-kubernetes-gce-conformance-latest-1-18/1243229791620960256/artifacts
GOOGLE_APPLICATION_CREDENTIALS=/etc/service-account/service-account.json
GOOGLE_APPLICATION_CREDENTIALS_DEPRECATED=Migrate to workload identity, contact sig-testing
GOPATH=/go
GOPROXY=https://proxy.golang.org
GO_TARBALL=go1.13.8.linux-amd64.tar.gz
HOME=/workspace
HOSTNAME=fe64b46b-6f87-11ea-9710-126912a3969e
IMAGE=gcr.io/k8s-testimages/kubekins-e2e:v20200324-0897dba-1.18
INSTANCE_PREFIX=bootstrap-e2e
JENKINS_GCE_SSH_PRIVATE_KEY_FILE=/workspace/.ssh/google_compute_engine
JENKINS_GCE_SSH_PUBLIC_KEY_FILE=/workspace/.ssh/google_compute_engine.pub
JOB_NAME=ci-kubernetes-gce-conformance-latest-1-18
JOB_SPEC={"type":"periodic","job":"ci-kubernetes-gce-conformance-latest-1-18","buildid":"1243229791620960256","prowjobid":"fe64b46b-6f87-11ea-9710-126912a3969e"}
JOB_TYPE=periodic
KUBERNETES_PORT=tcp://10.63.240.1:443
KUBERNETES_PORT_443_TCP=tcp://10.63.240.1:443
KUBERNETES_PORT_443_TCP_ADDR=10.63.240.1
KUBERNETES_PORT_443_TCP_PORT=443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_SERVICE_HOST=10.63.240.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBETEST_IN_DOCKER=true
KUBETEST_MANUAL_DUMP=y
KUBE_AWS_INSTANCE_PREFIX=bootstrap-e2e
KUBE_GCE_INSTANCE_PREFIX=bootstrap-e2e
NODE_NAME=fe64b46b-6f87-11ea-9710-126912a3969e
PATH=/go/bin:/go/bin:/usr/local/go/bin:/google-cloud-sdk/bin:/workspace:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
PROW_JOB_ID=fe64b46b-6f87-11ea-9710-126912a3969e
PWD=/workspace
SHLVL=2
SOURCE_DATE_EPOCH=
TERM=xterm
USER=prow
WORKSPACE=/workspace
_=./test-infra/jenkins/bootstrap.py
Run: ('kubetest', '--dump=/workspace/_artifacts', '--gcp-service-account=/etc/service-account/service-account.json', '--up', '--down', '--test', '--provider=gce', '--cluster=bootstrap-e2e', '--gcp-network=bootstrap-e2e', '--extract=ci/latest-1.18', '--gcp-master-image=gci', '--gcp-node-image=gci', '--gcp-zone=us-west1-b', '--test_args=--ginkgo.focus=\\[Conformance\\]', '--timeout=200m')
2020/03/26 17:34:33 Warning: Couldn't find directory src/sigs.k8s.io/cloud-provider-azure under any of GOPATH /go, defaulting to /go/src/k8s.io/cloud-provider-azure
2020/03/26 17:34:33 main.go:329: Limiting testing to 3h20m0s
2020/03/26 17:34:33 process.go:153: Running: gcloud auth activate-service-account --key-file=/etc/service-account/service-account.json
Activated service account credentials for: [pr-kubekins@kubernetes-jenkins-pull.iam.gserviceaccount.com]
2020/03/26 17:34:33 process.go:155: Step 'gcloud auth activate-service-account --key-file=/etc/service-account/service-account.json' finished in 655.433992ms
2020/03/26 17:34:33 main.go:731: --gcp-project is missing, trying to fetch a project from boskos.
(for local runs please set --gcp-project to your dev project)
2020/03/26 17:34:33 main.go:743: provider gce, will acquire project type gce-project from boskos
2020/03/26 17:34:33 process.go:153: Running: gcloud config set project k8s-boskos-gce-project-10
Updated property [core/project].
2020/03/26 17:34:34 process.go:155: Step 'gcloud config set project k8s-boskos-gce-project-10' finished in 653.836136ms
2020/03/26 17:34:34 main.go:782: Checking existing of GCP ssh keys...
2020/03/26 17:34:34 main.go:792: Checking presence of public key in k8s-boskos-gce-project-10
2020/03/26 17:34:34 process.go:153: Running: gcloud compute --project=k8s-boskos-gce-project-10 project-info describe
2020/03/26 17:34:35 process.go:155: Step 'gcloud compute --project=k8s-boskos-gce-project-10 project-info describe' finished in 1.059163593s
2020/03/26 17:34:35 extract_k8s.go:136: rm kubernetes
2020/03/26 17:34:35 extract_k8s.go:288: U=https://storage.googleapis.com/kubernetes-release-dev/ci R=v1.18.1-beta.0.5+d5dfb5cb416fcc get-kube.sh
2020/03/26 17:34:35 process.go:153: Running: ./get-kube.sh
Downloading kubernetes release v1.18.1-beta.0.5+d5dfb5cb416fcc
  from https://storage.googleapis.com/kubernetes-release-dev/ci/v1.18.1-beta.0.5+d5dfb5cb416fcc/kubernetes.tar.gz
  to /workspace/kubernetes.tar.gz
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  453k  100  453k    0     0  24.5M      0 --:--:-- --:--:-- --:--:-- 24.5M
Unpacking kubernetes release v1.18.1-beta.0.5+d5dfb5cb416fcc
Kubernetes release: v1.18.1-beta.0.5+d5dfb5cb416fcc
Server: linux/amd64  (to override, set KUBERNETES_SERVER_ARCH)
Client: linux/amd64  (autodetected)  (to override, set KUBERNETES_CLIENT_OS and/or KUBERNETES_CLIENT_ARCH)

Will download kubernetes-server-linux-amd64.tar.gz from https://storage.googleapis.com/kubernetes-release-dev/ci/v1.18.1-beta.0.5+d5dfb5cb416fcc
Will download and extract kubernetes-client-linux-amd64.tar.gz from https://storage.googleapis.com/kubernetes-release-dev/ci/v1.18.1-beta.0.5+d5dfb5cb416fcc
Will download and extract kubernetes-test tarball(s) from https://storage.googleapis.com/kubernetes-release-dev/ci/v1.18.1-beta.0.5+d5dfb5cb416fcc
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0 27  518M   27  141M    0     0   179M      0  0:00:02 --:--:--  0:00:02  179M 65  518M   65  337M    0     0   188M      0  0:00:02  0:00:01  0:00:01  188M100  518M  100  518M    0     0   187M      0  0:00:02  0:00:02 --:--:--  187M

md5sum(kubernetes-server-linux-amd64.tar.gz)=71ce0ca3482f5f8b9604d25dec5694c7
sha1sum(kubernetes-server-linux-amd64.tar.gz)=9090b63797d96f2401f1834fdf983bc46c66688b

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100 22.2M  100 22.2M    0     0  57.3M      0 --:--:-- --:--:-- --:--:-- 57.4M

md5sum(kubernetes-client-linux-amd64.tar.gz)=e1670ec17d8ea69214b3b450381c4aa8
sha1sum(kubernetes-client-linux-amd64.tar.gz)=0bd607fb5ceff841730744163a4dd267db56916a

Extracting /workspace/kubernetes/client/kubernetes-client-linux-amd64.tar.gz into /workspace/kubernetes/platforms/linux/amd64
Add '/workspace/kubernetes/client/bin' to your PATH to use newly-installed binaries.
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100  211k  100  211k    0     0  15.8M      0 --:--:-- --:--:-- --:--:-- 17.2M

md5sum(kubernetes-test-portable.tar.gz)=b1775bdfba8c5dae0e61e395896e7710
sha1sum(kubernetes-test-portable.tar.gz)=67870c08706cdf70088ecbfd26697a9faff4cbd4

Extracting kubernetes-test-portable.tar.gz into /workspace/kubernetes
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0 45  348M   45  159M    0     0   186M      0  0:00:01 --:--:--  0:00:01  186M100  348M  100  348M    0     0   197M      0  0:00:01  0:00:01 --:--:--  197M

md5sum(kubernetes-test-linux-amd64.tar.gz)=c945de613f46ccb61673ae1983c44220
sha1sum(kubernetes-test-linux-amd64.tar.gz)=3e52e7582a207de704918d5a34e2ce93aaf7a544

Extracting /workspace/kubernetes/test/kubernetes-test-linux-amd64.tar.gz into /workspace/kubernetes/platforms/linux/amd64
2020/03/26 17:34:58 process.go:155: Step './get-kube.sh' finished in 23.192526096s
2020/03/26 17:34:58 process.go:153: Running: ./hack/e2e-internal/e2e-down.sh
Project: k8s-boskos-gce-project-10
Network Project: k8s-boskos-gce-project-10
Zone: us-west1-b
Shutting down test cluster in background.
Bringing down cluster using provider: gce
... calling verify-prereqs
... calling verify-kube-binaries
... calling kube-down
Project: k8s-boskos-gce-project-10
Network Project: k8s-boskos-gce-project-10
Zone: us-west1-b
INSTANCE_GROUPS=
NODE_NAMES=
Bringing down cluster
ERROR: (gcloud.compute.instances.list) Some requests did not succeed:
 - Invalid value for field 'zone': 'us-west4-a'. Unknown zone.
 - Invalid value for field 'zone': 'us-west4-b'. Unknown zone.
 - Invalid value for field 'zone': 'us-west4-c'. Unknown zone.

ERROR: (gcloud.compute.instances.list) Some requests did not succeed:
 - Invalid value for field 'zone': 'us-west4-a'. Unknown zone.
 - Invalid value for field 'zone': 'us-west4-b'. Unknown zone.
 - Invalid value for field 'zone': 'us-west4-c'. Unknown zone.

Deleting firewall rules remaining in network bootstrap-e2e: 
W0326 17:35:29.671235    1072 loader.go:223] Config not found: /workspace/.kube/config
W0326 17:35:29.833965    1121 loader.go:223] Config not found: /workspace/.kube/config
W0326 17:35:29.838168    1121 loader.go:223] Config not found: /workspace/.kube/config
Property "clusters.k8s-boskos-gce-project-10_bootstrap-e2e" unset.
Property "users.k8s-boskos-gce-project-10_bootstrap-e2e" unset.
W0326 17:35:30.007997    1170 loader.go:223] Config not found: /workspace/.kube/config
W0326 17:35:30.008205    1170 loader.go:223] Config not found: /workspace/.kube/config
W0326 17:35:30.164705    1219 loader.go:223] Config not found: /workspace/.kube/config
W0326 17:35:30.164912    1219 loader.go:223] Config not found: /workspace/.kube/config
Property "users.k8s-boskos-gce-project-10_bootstrap-e2e-basic-auth" unset.
Property "contexts.k8s-boskos-gce-project-10_bootstrap-e2e" unset.
Cleared config for k8s-boskos-gce-project-10_bootstrap-e2e from /workspace/.kube/config
Done
W0326 17:35:30.326573    1268 loader.go:223] Config not found: /workspace/.kube/config
W0326 17:35:30.326764    1268 loader.go:223] Config not found: /workspace/.kube/config
2020/03/26 17:35:30 process.go:155: Step './hack/e2e-internal/e2e-down.sh' finished in 31.456547183s
2020/03/26 17:35:30 process.go:153: Running: ./hack/e2e-internal/e2e-up.sh
Project: k8s-boskos-gce-project-10
Network Project: k8s-boskos-gce-project-10
Zone: us-west1-b
... Starting cluster in us-west1-b using provider gce
... calling verify-prereqs
... calling verify-kube-binaries
... calling verify-release-tars
... calling kube-up
Project: k8s-boskos-gce-project-10
Network Project: k8s-boskos-gce-project-10
Zone: us-west1-b
+++ Staging tars to Google Storage: gs://kubernetes-staging-b8ab84bf52/bootstrap-e2e-devel
+++ kubernetes-server-linux-amd64.tar.gz uploaded (sha1 = 9090b63797d96f2401f1834fdf983bc46c66688b)
+++ kubernetes-manifests.tar.gz uploaded (sha1 = ae347622047b19c078f45d5189315ddf7916705e)
Creating new auto network: bootstrap-e2e
Created [https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/global/networks/bootstrap-e2e].
NAME           SUBNET_MODE  BGP_ROUTING_MODE  IPV4_RANGE  GATEWAY_IPV4
bootstrap-e2e  AUTO         REGIONAL

Instances on this network will not be reachable until firewall rules
are created. As an example, you can allow all internal traffic between
instances as well as SSH, RDP, and ICMP by running:

$ gcloud compute firewall-rules create <FIREWALL_NAME> --network bootstrap-e2e --allow tcp,udp,icmp --source-ranges <IP_RANGE>
$ gcloud compute firewall-rules create <FIREWALL_NAME> --network bootstrap-e2e --allow tcp:22,tcp:3389,icmp

Creating firewall...
Creating firewall...
..Creating firewall...
IP aliases are disabled.
Found subnet for region us-west1 in network bootstrap-e2e: bootstrap-e2e
Starting master and configuring firewalls
Configuring firewall for apiserver konnectivity server
...Creating firewall...
......Created [https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/global/firewalls/bootstrap-e2e-default-internal-master].
done.
NAME                                   NETWORK        DIRECTION  PRIORITY  ALLOW                                       DENY  DISABLED
bootstrap-e2e-default-internal-master  bootstrap-e2e  INGRESS    1000      tcp:1-2379,tcp:2382-65535,udp:1-65535,icmp        False
Created [https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/zones/us-west1-b/disks/bootstrap-e2e-master-pd].
NAME                     ZONE        SIZE_GB  TYPE    STATUS
bootstrap-e2e-master-pd  us-west1-b  20       pd-ssd  READY

New disks are unformatted. You must format and mount a disk before it
can be used. You can find instructions on how to do this at:

https://cloud.google.com/compute/docs/disks/add-persistent-disk#formatting

..Created [https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/global/firewalls/bootstrap-e2e-default-ssh].
done.
NAME                       NETWORK        DIRECTION  PRIORITY  ALLOW   DENY  DISABLED
bootstrap-e2e-default-ssh  bootstrap-e2e  INGRESS    1000      tcp:22        False
.Creating firewall...
...Created [https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/global/firewalls/bootstrap-e2e-default-internal-node].
done.
NAME                                 NETWORK        DIRECTION  PRIORITY  ALLOW                         DENY  DISABLED
bootstrap-e2e-default-internal-node  bootstrap-e2e  INGRESS    1000      tcp:1-65535,udp:1-65535,icmp        False
.Created [https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/regions/us-west1/addresses/bootstrap-e2e-master-ip].
..Created [https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/global/firewalls/bootstrap-e2e-master-https].
done.
NAME                        NETWORK        DIRECTION  PRIORITY  ALLOW    DENY  DISABLED
bootstrap-e2e-master-https  bootstrap-e2e  INGRESS    1000      tcp:443        False
...Created [https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/global/firewalls/bootstrap-e2e-master-etcd].
Generating certs for alternate-names: IP:34.83.25.82,IP:10.0.0.1,DNS:kubernetes,DNS:kubernetes.default,DNS:kubernetes.default.svc,DNS:kubernetes.default.svc.cluster.local,DNS:bootstrap-e2e-master
done.
NAME                       NETWORK        DIRECTION  PRIORITY  ALLOW              DENY  DISABLED
bootstrap-e2e-master-etcd  bootstrap-e2e  INGRESS    1000      tcp:2380,tcp:2381        False
2020/03/26 17:36:48 [INFO] generating a new CA key and certificate from CSR
2020/03/26 17:36:48 [INFO] generate received request
2020/03/26 17:36:48 [INFO] received CSR
2020/03/26 17:36:48 [INFO] generating key: ecdsa-256
2020/03/26 17:36:48 [INFO] encoded CSR
2020/03/26 17:36:48 [INFO] signed certificate with serial number 641652551942175850409648479962907933037599950825
2020/03/26 17:36:48 [INFO] generate received request
2020/03/26 17:36:48 [INFO] received CSR
2020/03/26 17:36:48 [INFO] generating key: ecdsa-256
2020/03/26 17:36:48 [INFO] encoded CSR
2020/03/26 17:36:48 [INFO] signed certificate with serial number 500180309263807457366581812954864741918548261995
2020/03/26 17:36:48 [INFO] generate received request
2020/03/26 17:36:48 [INFO] received CSR
2020/03/26 17:36:48 [INFO] generating key: ecdsa-256
2020/03/26 17:36:48 [INFO] encoded CSR
2020/03/26 17:36:48 [INFO] signed certificate with serial number 267915893431698868063745558694459320505471372398
2020/03/26 17:36:48 [INFO] generate received request
2020/03/26 17:36:48 [INFO] received CSR
2020/03/26 17:36:48 [INFO] generating key: ecdsa-256
2020/03/26 17:36:48 [INFO] encoded CSR
2020/03/26 17:36:48 [INFO] signed certificate with serial number 206892434558338021399714377802212545273206663165
2020/03/26 17:36:48 [WARNING] This certificate lacks a "hosts" field. This makes it unsuitable for
websites. For more information see the Baseline Requirements for the Issuance and Management
of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);
specifically, section 10.2.3 ("Information Requirements").
Generate peer certificates...
Generate server certificates...
Generate client certificates...
+++ Logging using Fluentd to gcp
Creating firewall...
.....Created [https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/global/firewalls/bootstrap-e2e-minion-all].
done.
NAME                      NETWORK        DIRECTION  PRIORITY  ALLOW                     DENY  DISABLED
bootstrap-e2e-minion-all  bootstrap-e2e  INGRESS    1000      tcp,udp,icmp,esp,ah,sctp        False
WARNING: You have selected a disk size of under [200GB]. This may result in poor I/O performance. For more information, see: https://developers.google.com/compute/docs/disks#performance.
Created [https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/zones/us-west1-b/instances/bootstrap-e2e-master].
WARNING: Some requests generated warnings:
 - Disk size: '20 GB' is larger than image size: '10 GB'. You might need to resize the root repartition manually if the operating system does not support automatic resizing. See https://cloud.google.com/compute/docs/disks/add-persistent-disk#resize_pd for details.
 - The resource 'projects/cos-cloud/global/images/cos-77-12371-175-0' is deprecated. A suggested replacement is 'projects/cos-cloud/global/images/cos-77-12371-183-0'.

NAME                  ZONE        MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP  STATUS
bootstrap-e2e-master  us-west1-b  n1-standard-1               10.138.0.2   34.83.25.82  RUNNING
Creating nodes.
Using subnet bootstrap-e2e
Attempt 1 to create bootstrap-e2e-minion-template
WARNING: You have selected a disk size of under [200GB]. This may result in poor I/O performance. For more information, see: https://developers.google.com/compute/docs/disks#performance.
Created [https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/global/instanceTemplates/bootstrap-e2e-minion-template].
NAME                           MACHINE_TYPE   PREEMPTIBLE  CREATION_TIMESTAMP
bootstrap-e2e-minion-template  n1-standard-2               2020-03-26T10:37:05.055-07:00
Using subnet bootstrap-e2e
Attempt 1 to create bootstrap-e2e-windows-node-template
WARNING: You have selected a disk size of under [200GB]. This may result in poor I/O performance. For more information, see: https://developers.google.com/compute/docs/disks#performance.
Created [https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/global/instanceTemplates/bootstrap-e2e-windows-node-template].
NAME                                 MACHINE_TYPE   PREEMPTIBLE  CREATION_TIMESTAMP
bootstrap-e2e-windows-node-template  n1-standard-2               2020-03-26T10:37:09.835-07:00
Created [https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/zones/us-west1-b/instanceGroupManagers/bootstrap-e2e-minion-group].
NAME                        LOCATION    SCOPE  BASE_INSTANCE_NAME          SIZE  TARGET_SIZE  INSTANCE_TEMPLATE              AUTOSCALED
bootstrap-e2e-minion-group  us-west1-b  zone   bootstrap-e2e-minion-group  0     3            bootstrap-e2e-minion-template  no
WARNING: `gcloud compute instance-groups managed wait-until-stable` is deprecated. Please use `gcloud compute instance-groups managed wait-until --stable` instead.
Waiting for group to become stable, current operations: creating: 3
Waiting for group to become stable, current operations: creating: 3
Waiting for group to become stable, current operations: creating: 3
Waiting for group to become stable, current operations: creating: 1
Group is stable
INSTANCE_GROUPS=bootstrap-e2e-minion-group
NODE_NAMES=bootstrap-e2e-minion-group-1r9w bootstrap-e2e-minion-group-d5w7 bootstrap-e2e-minion-group-gs7c
Trying to find master named 'bootstrap-e2e-master'
Looking for address 'bootstrap-e2e-master-ip'
Using master: bootstrap-e2e-master (external IP: 34.83.25.82; internal IP: (not set))
Waiting up to 300 seconds for cluster initialization.

  This will continually check to see if the API for kubernetes is reachable.
  This may time out if there was some uncaught error during start up.

..........Kubernetes cluster created.
Cluster "k8s-boskos-gce-project-10_bootstrap-e2e" set.
User "k8s-boskos-gce-project-10_bootstrap-e2e" set.
Context "k8s-boskos-gce-project-10_bootstrap-e2e" created.
Switched to context "k8s-boskos-gce-project-10_bootstrap-e2e".
User "k8s-boskos-gce-project-10_bootstrap-e2e-basic-auth" set.
Wrote config for k8s-boskos-gce-project-10_bootstrap-e2e to /workspace/.kube/config
Updated [https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/zones/us-west1-b/instances/bootstrap-e2e-master].

Kubernetes cluster is running.  The master is running at:

  https://34.83.25.82

The user name and password to use is located in /workspace/.kube/config.

Validating gce cluster, MULTIZONE=
... calling validate-cluster
Project: k8s-boskos-gce-project-10
Network Project: k8s-boskos-gce-project-10
Zone: us-west1-b
WARNING: Some requests did not succeed.
 - Invalid value for field 'zone': 'us-west4-a'. Unknown zone.
 - Invalid value for field 'zone': 'us-west4-b'. Unknown zone.
 - Invalid value for field 'zone': 'us-west4-c'. Unknown zone.

No resources found in default namespace.
Waiting for 4 ready nodes. 0 ready nodes, 0 registered. Retrying.
Waiting for 4 ready nodes. 1 ready nodes, 3 registered. Retrying.
Found 4 node(s).
NAME                              STATUS                     ROLES    AGE   VERSION
bootstrap-e2e-master              Ready,SchedulingDisabled   <none>   6s    v1.18.1-beta.0.5+d5dfb5cb416fcc
bootstrap-e2e-minion-group-1r9w   Ready                      <none>   19s   v1.18.1-beta.0.5+d5dfb5cb416fcc
bootstrap-e2e-minion-group-d5w7   Ready                      <none>   19s   v1.18.1-beta.0.5+d5dfb5cb416fcc
bootstrap-e2e-minion-group-gs7c   Ready                      <none>   18s   v1.18.1-beta.0.5+d5dfb5cb416fcc
Validate output:
NAME                 STATUS    MESSAGE             ERROR
etcd-1               Healthy   {"health":"true"}   
controller-manager   Healthy   ok                  
scheduler            Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   
Cluster validation succeeded
Done, listing cluster services:

Kubernetes master is running at https://34.83.25.82
GLBCDefaultBackend is running at https://34.83.25.82/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy
CoreDNS is running at https://34.83.25.82/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
kubernetes-dashboard is running at https://34.83.25.82/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy
Metrics-server is running at https://34.83.25.82/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

NAME                           NETWORK        DIRECTION  PRIORITY  ALLOW            DENY  DISABLED
bootstrap-e2e-minion-http-alt  bootstrap-e2e  INGRESS    1000      tcp:80,tcp:8080        False
allowed:
- IPProtocol: tcp
  ports:
  - '80'
- IPProtocol: tcp
  ports:
  - '8080'
creationTimestamp: '2020-03-26T10:39:44.283-07:00'
description: ''
direction: INGRESS
disabled: false
id: '1668806538544895503'
kind: compute#firewall
logConfig:
  enable: false
name: bootstrap-e2e-minion-http-alt
network: https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/global/networks/bootstrap-e2e
priority: 1000
selfLink: https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/global/firewalls/bootstrap-e2e-minion-http-alt
sourceRanges:
- 0.0.0.0/0
targetTags:
- bootstrap-e2e-minion
NAME                            NETWORK        DIRECTION  PRIORITY  ALLOW                            DENY  DISABLED
bootstrap-e2e-minion-nodeports  bootstrap-e2e  INGRESS    1000      tcp:30000-32767,udp:30000-32767        False
allowed:
- IPProtocol: tcp
  ports:
  - 30000-32767
- IPProtocol: udp
  ports:
  - 30000-32767
creationTimestamp: '2020-03-26T10:39:50.088-07:00'
description: ''
direction: INGRESS
disabled: false
id: '4234901214159001097'
kind: compute#firewall
logConfig:
  enable: false
name: bootstrap-e2e-minion-nodeports
network: https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/global/networks/bootstrap-e2e
priority: 1000
selfLink: https://www.googleapis.com/compute/v1/projects/k8s-boskos-gce-project-10/global/firewalls/bootstrap-e2e-minion-nodeports
sourceRanges:
- 0.0.0.0/0
targetTags:
- bootstrap-e2e-minion
2020/03/26 17:39:54 process.go:155: Step './hack/e2e-internal/e2e-up.sh' finished in 4m24.634053901s
2020/03/26 17:39:54 process.go:153: Running: ./cluster/kubectl.sh --match-server-version=false version
2020/03/26 17:39:55 process.go:155: Step './cluster/kubectl.sh --match-server-version=false version' finished in 289.759749ms
2020/03/26 17:39:55 process.go:153: Running: ./cluster/kubectl.sh --match-server-version=false get nodes -oyaml
2020/03/26 17:39:55 process.go:155: Step './cluster/kubectl.sh --match-server-version=false get nodes -oyaml' finished in 422.757486ms
2020/03/26 17:39:55 process.go:153: Running: ./hack/e2e-internal/e2e-status.sh
Project: k8s-boskos-gce-project-10
Network Project: k8s-boskos-gce-project-10
Zone: us-west1-b
Client Version: version.Info{Major:"1", Minor:"18+", GitVersion:"v1.18.1-beta.0.5+d5dfb5cb416fcc", GitCommit:"d5dfb5cb416fcc32ad556b0d253307c9267b1d30", GitTreeState:"clean", BuildDate:"2020-03-25T22:16:25Z", GoVersion:"go1.13.9", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"18+", GitVersion:"v1.18.1-beta.0.5+d5dfb5cb416fcc", GitCommit:"d5dfb5cb416fcc32ad556b0d253307c9267b1d30", GitTreeState:"clean", BuildDate:"2020-03-25T22:16:25Z", GoVersion:"go1.13.9", Compiler:"gc", Platform:"linux/amd64"}
2020/03/26 17:39:56 process.go:155: Step './hack/e2e-internal/e2e-status.sh' finished in 450.605856ms
2020/03/26 17:39:56 process.go:153: Running: ./cluster/kubectl.sh --match-server-version=false version
2020/03/26 17:39:56 process.go:155: Step './cluster/kubectl.sh --match-server-version=false version' finished in 300.527521ms
2020/03/26 17:39:56 process.go:153: Running: ./hack/ginkgo-e2e.sh --ginkgo.focus=\[Conformance\] --report-dir=/workspace/_artifacts --disable-log-dump=true
Setting up for KUBERNETES_PROVIDER="gce".
Project: k8s-boskos-gce-project-10
Network Project: k8s-boskos-gce-project-10
Zone: us-west1-b
Trying to find master named 'bootstrap-e2e-master'
Looking for address 'bootstrap-e2e-master-ip'
Using master: bootstrap-e2e-master (external IP: 34.83.25.82; internal IP: (not set))
Mar 26 17:39:59.012: INFO: Fetching cloud provider for "gce"
I0326 17:39:59.012930    9212 test_context.go:423] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0326 17:39:59.013689    9212 gce.go:861] Using DefaultTokenSource &oauth2.reuseTokenSource{new:jwt.jwtSource{ctx:(*context.emptyCtx)(0xc0000ec010), conf:(*jwt.Config)(0xc00274a460)}, mu:sync.Mutex{state:0, sema:0x0}, t:(*oauth2.Token)(nil)}
W0326 17:39:59.097359    9212 gce.go:461] No network name or URL specified.
I0326 17:39:59.097574    9212 e2e.go:124] Starting e2e run "a0745263-a32d-41d3-98dd-d629e3e59f62" on Ginkgo node 1
{"msg":"Test Suite starting","total":277,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1585244397 - Will randomize all specs
Will run 277 of 4992 specs

Mar 26 17:40:04.356: INFO: cluster-master-image: cos-77-12371-175-0
Mar 26 17:40:04.357: INFO: cluster-node-image: cos-77-12371-175-0
Mar 26 17:40:04.357: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 17:40:04.362: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 26 17:40:04.510: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 26 17:40:04.669: INFO: 28 / 28 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 26 17:40:04.669: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Mar 26 17:40:04.669: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 26 17:40:04.717: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'fluentd-gcp-v3.2.0' (0 seconds elapsed)
Mar 26 17:40:04.717: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'metadata-proxy-v0.1' (0 seconds elapsed)
Mar 26 17:40:04.717: INFO: e2e test version: v1.18.1-beta.0.5+d5dfb5cb416fcc
Mar 26 17:40:04.750: INFO: kube-apiserver version: v1.18.1-beta.0.5+d5dfb5cb416fcc
Mar 26 17:40:04.750: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 17:40:04.789: INFO: Cluster IP family: ipv4
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:40:04.790: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
Mar 26 17:40:04.936: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:40:18.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1319" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":277,"completed":1,"skipped":8,"failed":0}
S
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:40:18.483: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should find a service from listing all namespaces [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching services
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:40:18.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7620" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":277,"completed":2,"skipped":9,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:40:18.809: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-4c3354ee-7c51-4261-af9f-ff530ec36a4e
STEP: Creating a pod to test consume configMaps
Mar 26 17:40:19.044: INFO: Waiting up to 5m0s for pod "pod-configmaps-e3e9f379-34dc-4bbc-8d5b-f79147e5d6b4" in namespace "configmap-162" to be "Succeeded or Failed"
Mar 26 17:40:19.085: INFO: Pod "pod-configmaps-e3e9f379-34dc-4bbc-8d5b-f79147e5d6b4": Phase="Pending", Reason="", readiness=false. Elapsed: 41.451142ms
Mar 26 17:40:21.121: INFO: Pod "pod-configmaps-e3e9f379-34dc-4bbc-8d5b-f79147e5d6b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077175756s
STEP: Saw pod success
Mar 26 17:40:21.121: INFO: Pod "pod-configmaps-e3e9f379-34dc-4bbc-8d5b-f79147e5d6b4" satisfied condition "Succeeded or Failed"
Mar 26 17:40:21.156: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-configmaps-e3e9f379-34dc-4bbc-8d5b-f79147e5d6b4 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 17:40:21.259: INFO: Waiting for pod pod-configmaps-e3e9f379-34dc-4bbc-8d5b-f79147e5d6b4 to disappear
Mar 26 17:40:21.294: INFO: Pod pod-configmaps-e3e9f379-34dc-4bbc-8d5b-f79147e5d6b4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:40:21.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-162" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":3,"skipped":25,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:40:21.371: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar 26 17:40:21.512: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 17:40:25.179: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:40:39.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-700" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":277,"completed":4,"skipped":25,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:40:39.889: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:40:48.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4539" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":277,"completed":5,"skipped":47,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:40:48.384: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:40:48.585: INFO: Waiting up to 5m0s for pod "downwardapi-volume-130cf27c-0180-47a1-b6b5-45ccc89818bf" in namespace "downward-api-2093" to be "Succeeded or Failed"
Mar 26 17:40:48.627: INFO: Pod "downwardapi-volume-130cf27c-0180-47a1-b6b5-45ccc89818bf": Phase="Pending", Reason="", readiness=false. Elapsed: 41.911981ms
Mar 26 17:40:50.670: INFO: Pod "downwardapi-volume-130cf27c-0180-47a1-b6b5-45ccc89818bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.084594309s
Mar 26 17:40:52.751: INFO: Pod "downwardapi-volume-130cf27c-0180-47a1-b6b5-45ccc89818bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.165573569s
STEP: Saw pod success
Mar 26 17:40:52.751: INFO: Pod "downwardapi-volume-130cf27c-0180-47a1-b6b5-45ccc89818bf" satisfied condition "Succeeded or Failed"
Mar 26 17:40:52.809: INFO: Trying to get logs from node bootstrap-e2e-minion-group-d5w7 pod downwardapi-volume-130cf27c-0180-47a1-b6b5-45ccc89818bf container client-container: <nil>
STEP: delete the pod
Mar 26 17:40:52.940: INFO: Waiting for pod downwardapi-volume-130cf27c-0180-47a1-b6b5-45ccc89818bf to disappear
Mar 26 17:40:52.982: INFO: Pod downwardapi-volume-130cf27c-0180-47a1-b6b5-45ccc89818bf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:40:52.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2093" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":6,"skipped":50,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:40:53.070: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-e1c20eca-1c70-482a-b6c3-088524cce987
STEP: Creating a pod to test consume secrets
Mar 26 17:40:53.358: INFO: Waiting up to 5m0s for pod "pod-secrets-1630d2a1-7709-4aaa-85a6-b62eb929aae9" in namespace "secrets-2480" to be "Succeeded or Failed"
Mar 26 17:40:53.422: INFO: Pod "pod-secrets-1630d2a1-7709-4aaa-85a6-b62eb929aae9": Phase="Pending", Reason="", readiness=false. Elapsed: 64.15225ms
Mar 26 17:40:55.462: INFO: Pod "pod-secrets-1630d2a1-7709-4aaa-85a6-b62eb929aae9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.104341347s
STEP: Saw pod success
Mar 26 17:40:55.462: INFO: Pod "pod-secrets-1630d2a1-7709-4aaa-85a6-b62eb929aae9" satisfied condition "Succeeded or Failed"
Mar 26 17:40:55.502: INFO: Trying to get logs from node bootstrap-e2e-minion-group-d5w7 pod pod-secrets-1630d2a1-7709-4aaa-85a6-b62eb929aae9 container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 17:40:55.595: INFO: Waiting for pod pod-secrets-1630d2a1-7709-4aaa-85a6-b62eb929aae9 to disappear
Mar 26 17:40:55.633: INFO: Pod pod-secrets-1630d2a1-7709-4aaa-85a6-b62eb929aae9 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:40:55.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2480" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":7,"skipped":102,"failed":0}
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:40:55.719: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 26 17:41:08.243: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:41:08.283: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 17:41:10.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:41:10.323: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 17:41:12.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:41:12.323: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 17:41:14.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:41:14.324: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 17:41:16.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:41:16.323: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 17:41:18.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:41:18.323: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 17:41:20.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:41:20.323: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 17:41:22.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:41:22.323: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 17:41:24.283: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 17:41:24.322: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:41:24.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9605" for this suite.
•{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":277,"completed":8,"skipped":104,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:41:24.452: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:41:24.607: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8211
I0326 17:41:24.663444    9212 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8211, replica count: 1
I0326 17:41:25.714163    9212 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0326 17:41:26.714434    9212 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 26 17:41:26.869: INFO: Created: latency-svc-zqhhf
Mar 26 17:41:26.879: INFO: Got endpoints: latency-svc-zqhhf [64.409085ms]
Mar 26 17:41:26.927: INFO: Created: latency-svc-qnttw
Mar 26 17:41:26.938: INFO: Got endpoints: latency-svc-qnttw [59.491774ms]
Mar 26 17:41:26.951: INFO: Created: latency-svc-cbp4l
Mar 26 17:41:26.954: INFO: Got endpoints: latency-svc-cbp4l [75.128383ms]
Mar 26 17:41:26.985: INFO: Created: latency-svc-fz9vf
Mar 26 17:41:27.008: INFO: Got endpoints: latency-svc-fz9vf [129.497243ms]
Mar 26 17:41:27.025: INFO: Created: latency-svc-sjx7r
Mar 26 17:41:27.052: INFO: Created: latency-svc-plv9v
Mar 26 17:41:27.068: INFO: Got endpoints: latency-svc-sjx7r [188.696641ms]
Mar 26 17:41:27.073: INFO: Got endpoints: latency-svc-plv9v [194.318578ms]
Mar 26 17:41:27.086: INFO: Created: latency-svc-2snh4
Mar 26 17:41:27.093: INFO: Created: latency-svc-q2v9f
Mar 26 17:41:27.108: INFO: Got endpoints: latency-svc-2snh4 [229.444485ms]
Mar 26 17:41:27.110: INFO: Got endpoints: latency-svc-q2v9f [231.359143ms]
Mar 26 17:41:27.121: INFO: Created: latency-svc-jntg9
Mar 26 17:41:27.132: INFO: Created: latency-svc-52dh4
Mar 26 17:41:27.144: INFO: Got endpoints: latency-svc-52dh4 [264.630722ms]
Mar 26 17:41:27.144: INFO: Got endpoints: latency-svc-jntg9 [264.665214ms]
Mar 26 17:41:27.191: INFO: Created: latency-svc-2mqtc
Mar 26 17:41:27.191: INFO: Created: latency-svc-h4cgv
Mar 26 17:41:27.249: INFO: Got endpoints: latency-svc-2mqtc [369.644537ms]
Mar 26 17:41:27.249: INFO: Got endpoints: latency-svc-h4cgv [369.680727ms]
Mar 26 17:41:27.274: INFO: Created: latency-svc-brt29
Mar 26 17:41:27.280: INFO: Got endpoints: latency-svc-brt29 [400.959703ms]
Mar 26 17:41:27.285: INFO: Created: latency-svc-7vh5b
Mar 26 17:41:27.305: INFO: Created: latency-svc-lkhd4
Mar 26 17:41:27.317: INFO: Got endpoints: latency-svc-7vh5b [437.52719ms]
Mar 26 17:41:27.322: INFO: Got endpoints: latency-svc-lkhd4 [443.139092ms]
Mar 26 17:41:27.328: INFO: Created: latency-svc-vvgxm
Mar 26 17:41:27.339: INFO: Got endpoints: latency-svc-vvgxm [459.851403ms]
Mar 26 17:41:27.355: INFO: Created: latency-svc-9ggsg
Mar 26 17:41:27.362: INFO: Got endpoints: latency-svc-9ggsg [81.987379ms]
Mar 26 17:41:27.368: INFO: Created: latency-svc-gbzc2
Mar 26 17:41:27.384: INFO: Created: latency-svc-6pfln
Mar 26 17:41:27.391: INFO: Got endpoints: latency-svc-gbzc2 [452.467157ms]
Mar 26 17:41:27.399: INFO: Got endpoints: latency-svc-6pfln [444.520482ms]
Mar 26 17:41:27.404: INFO: Created: latency-svc-t6qrr
Mar 26 17:41:27.417: INFO: Got endpoints: latency-svc-t6qrr [408.750986ms]
Mar 26 17:41:27.429: INFO: Created: latency-svc-wlzdj
Mar 26 17:41:27.446: INFO: Got endpoints: latency-svc-wlzdj [378.464009ms]
Mar 26 17:41:27.451: INFO: Created: latency-svc-gmc5q
Mar 26 17:41:27.466: INFO: Got endpoints: latency-svc-gmc5q [393.198368ms]
Mar 26 17:41:27.469: INFO: Created: latency-svc-mjmbb
Mar 26 17:41:27.482: INFO: Created: latency-svc-vp8w9
Mar 26 17:41:27.499: INFO: Got endpoints: latency-svc-mjmbb [390.476298ms]
Mar 26 17:41:27.504: INFO: Created: latency-svc-lkfpx
Mar 26 17:41:27.508: INFO: Got endpoints: latency-svc-vp8w9 [397.917261ms]
Mar 26 17:41:27.524: INFO: Created: latency-svc-2ds2h
Mar 26 17:41:27.540: INFO: Got endpoints: latency-svc-lkfpx [395.938163ms]
Mar 26 17:41:27.558: INFO: Got endpoints: latency-svc-2ds2h [414.81483ms]
Mar 26 17:41:27.585: INFO: Created: latency-svc-qhq6r
Mar 26 17:41:27.604: INFO: Got endpoints: latency-svc-qhq6r [355.451318ms]
Mar 26 17:41:27.611: INFO: Created: latency-svc-b5f6s
Mar 26 17:41:27.621: INFO: Created: latency-svc-8fdkw
Mar 26 17:41:27.628: INFO: Got endpoints: latency-svc-b5f6s [379.472796ms]
Mar 26 17:41:27.634: INFO: Got endpoints: latency-svc-8fdkw [317.733317ms]
Mar 26 17:41:27.655: INFO: Created: latency-svc-bgq5h
Mar 26 17:41:27.675: INFO: Created: latency-svc-pr48q
Mar 26 17:41:27.684: INFO: Got endpoints: latency-svc-bgq5h [361.711561ms]
Mar 26 17:41:27.709: INFO: Got endpoints: latency-svc-pr48q [369.962586ms]
Mar 26 17:41:27.716: INFO: Created: latency-svc-chmqr
Mar 26 17:41:27.726: INFO: Got endpoints: latency-svc-chmqr [363.841479ms]
Mar 26 17:41:27.736: INFO: Created: latency-svc-mxm24
Mar 26 17:41:27.753: INFO: Got endpoints: latency-svc-mxm24 [362.227776ms]
Mar 26 17:41:27.756: INFO: Created: latency-svc-rlt8b
Mar 26 17:41:27.772: INFO: Got endpoints: latency-svc-rlt8b [373.470071ms]
Mar 26 17:41:27.775: INFO: Created: latency-svc-grvwx
Mar 26 17:41:27.789: INFO: Got endpoints: latency-svc-grvwx [371.731421ms]
Mar 26 17:41:27.807: INFO: Created: latency-svc-rmjts
Mar 26 17:41:27.814: INFO: Got endpoints: latency-svc-rmjts [367.803756ms]
Mar 26 17:41:27.818: INFO: Created: latency-svc-9td44
Mar 26 17:41:27.833: INFO: Got endpoints: latency-svc-9td44 [367.061307ms]
Mar 26 17:41:27.844: INFO: Created: latency-svc-nrmmh
Mar 26 17:41:27.851: INFO: Got endpoints: latency-svc-nrmmh [352.347193ms]
Mar 26 17:41:27.860: INFO: Created: latency-svc-r9rvj
Mar 26 17:41:27.877: INFO: Got endpoints: latency-svc-r9rvj [368.295871ms]
Mar 26 17:41:27.881: INFO: Created: latency-svc-9mfmc
Mar 26 17:41:27.901: INFO: Created: latency-svc-rpbd9
Mar 26 17:41:27.908: INFO: Got endpoints: latency-svc-9mfmc [368.617849ms]
Mar 26 17:41:27.917: INFO: Got endpoints: latency-svc-rpbd9 [358.892275ms]
Mar 26 17:41:27.924: INFO: Created: latency-svc-6qql9
Mar 26 17:41:27.947: INFO: Got endpoints: latency-svc-6qql9 [343.071811ms]
Mar 26 17:41:27.948: INFO: Created: latency-svc-v7r4n
Mar 26 17:41:27.960: INFO: Got endpoints: latency-svc-v7r4n [332.038051ms]
Mar 26 17:41:27.967: INFO: Created: latency-svc-n8mqs
Mar 26 17:41:27.986: INFO: Created: latency-svc-lwgx8
Mar 26 17:41:27.991: INFO: Got endpoints: latency-svc-n8mqs [356.814504ms]
Mar 26 17:41:28.003: INFO: Created: latency-svc-nlt7h
Mar 26 17:41:28.029: INFO: Got endpoints: latency-svc-lwgx8 [344.555708ms]
Mar 26 17:41:28.048: INFO: Got endpoints: latency-svc-nlt7h [338.88024ms]
Mar 26 17:41:28.074: INFO: Created: latency-svc-gdkz7
Mar 26 17:41:28.093: INFO: Got endpoints: latency-svc-gdkz7 [367.493447ms]
Mar 26 17:41:28.099: INFO: Created: latency-svc-qqksc
Mar 26 17:41:28.109: INFO: Created: latency-svc-wftrk
Mar 26 17:41:28.117: INFO: Got endpoints: latency-svc-qqksc [364.284525ms]
Mar 26 17:41:28.124: INFO: Got endpoints: latency-svc-wftrk [351.897507ms]
Mar 26 17:41:28.127: INFO: Created: latency-svc-5bw8t
Mar 26 17:41:28.143: INFO: Got endpoints: latency-svc-5bw8t [353.465087ms]
Mar 26 17:41:28.152: INFO: Created: latency-svc-pnw9m
Mar 26 17:41:28.154: INFO: Got endpoints: latency-svc-pnw9m [340.46956ms]
Mar 26 17:41:28.166: INFO: Created: latency-svc-qq4cf
Mar 26 17:41:28.178: INFO: Got endpoints: latency-svc-qq4cf [344.063365ms]
Mar 26 17:41:28.190: INFO: Created: latency-svc-lfwt2
Mar 26 17:41:28.197: INFO: Got endpoints: latency-svc-lfwt2 [345.818986ms]
Mar 26 17:41:28.209: INFO: Created: latency-svc-75gm6
Mar 26 17:41:28.213: INFO: Got endpoints: latency-svc-75gm6 [336.091716ms]
Mar 26 17:41:28.229: INFO: Created: latency-svc-qwvdc
Mar 26 17:41:28.250: INFO: Created: latency-svc-rxxhj
Mar 26 17:41:28.255: INFO: Got endpoints: latency-svc-qwvdc [347.014184ms]
Mar 26 17:41:28.271: INFO: Got endpoints: latency-svc-rxxhj [353.201543ms]
Mar 26 17:41:28.276: INFO: Created: latency-svc-27p6m
Mar 26 17:41:28.292: INFO: Got endpoints: latency-svc-27p6m [344.635313ms]
Mar 26 17:41:28.300: INFO: Created: latency-svc-cf5cc
Mar 26 17:41:28.313: INFO: Got endpoints: latency-svc-cf5cc [352.776617ms]
Mar 26 17:41:28.321: INFO: Created: latency-svc-2mcvn
Mar 26 17:41:28.330: INFO: Created: latency-svc-42ltz
Mar 26 17:41:28.345: INFO: Got endpoints: latency-svc-2mcvn [353.74431ms]
Mar 26 17:41:28.349: INFO: Created: latency-svc-nk2hz
Mar 26 17:41:28.360: INFO: Created: latency-svc-f2lnp
Mar 26 17:41:28.367: INFO: Created: latency-svc-7l5ff
Mar 26 17:41:28.383: INFO: Created: latency-svc-sbvsr
Mar 26 17:41:28.390: INFO: Got endpoints: latency-svc-42ltz [361.044304ms]
Mar 26 17:41:28.400: INFO: Created: latency-svc-pv2xd
Mar 26 17:41:28.413: INFO: Created: latency-svc-vlk5m
Mar 26 17:41:28.419: INFO: Created: latency-svc-r6ffc
Mar 26 17:41:28.427: INFO: Created: latency-svc-xcx86
Mar 26 17:41:28.440: INFO: Got endpoints: latency-svc-nk2hz [392.197772ms]
Mar 26 17:41:28.452: INFO: Created: latency-svc-6zl8q
Mar 26 17:41:28.461: INFO: Created: latency-svc-5rdb4
Mar 26 17:41:28.472: INFO: Created: latency-svc-czzxh
Mar 26 17:41:28.514: INFO: Got endpoints: latency-svc-f2lnp [420.582076ms]
Mar 26 17:41:28.516: INFO: Created: latency-svc-m22vx
Mar 26 17:41:28.536: INFO: Created: latency-svc-gct4c
Mar 26 17:41:28.537: INFO: Got endpoints: latency-svc-7l5ff [419.633206ms]
Mar 26 17:41:28.548: INFO: Created: latency-svc-zrtnt
Mar 26 17:41:28.558: INFO: Created: latency-svc-7vrs2
Mar 26 17:41:28.565: INFO: Created: latency-svc-6s658
Mar 26 17:41:28.572: INFO: Created: latency-svc-2x8jd
Mar 26 17:41:28.588: INFO: Got endpoints: latency-svc-sbvsr [463.694475ms]
Mar 26 17:41:28.590: INFO: Created: latency-svc-pwplw
Mar 26 17:41:28.637: INFO: Got endpoints: latency-svc-pv2xd [494.461885ms]
Mar 26 17:41:28.642: INFO: Created: latency-svc-mcz7c
Mar 26 17:41:28.686: INFO: Got endpoints: latency-svc-vlk5m [531.013922ms]
Mar 26 17:41:28.693: INFO: Created: latency-svc-srwsf
Mar 26 17:41:28.736: INFO: Created: latency-svc-2gmnr
Mar 26 17:41:28.740: INFO: Got endpoints: latency-svc-r6ffc [562.727427ms]
Mar 26 17:41:28.789: INFO: Got endpoints: latency-svc-xcx86 [591.499431ms]
Mar 26 17:41:28.797: INFO: Created: latency-svc-rxvn7
Mar 26 17:41:28.849: INFO: Got endpoints: latency-svc-6zl8q [636.459884ms]
Mar 26 17:41:28.853: INFO: Created: latency-svc-qhqbw
Mar 26 17:41:28.884: INFO: Got endpoints: latency-svc-5rdb4 [628.694007ms]
Mar 26 17:41:28.900: INFO: Created: latency-svc-dx28p
Mar 26 17:41:28.934: INFO: Created: latency-svc-2f8hs
Mar 26 17:41:28.936: INFO: Got endpoints: latency-svc-czzxh [665.040685ms]
Mar 26 17:41:28.996: INFO: Got endpoints: latency-svc-m22vx [703.639371ms]
Mar 26 17:41:29.006: INFO: Created: latency-svc-fgtd2
Mar 26 17:41:29.033: INFO: Got endpoints: latency-svc-gct4c [719.925913ms]
Mar 26 17:41:29.052: INFO: Created: latency-svc-qwkwp
Mar 26 17:41:29.088: INFO: Created: latency-svc-2pbkm
Mar 26 17:41:29.092: INFO: Got endpoints: latency-svc-zrtnt [747.222149ms]
Mar 26 17:41:29.139: INFO: Got endpoints: latency-svc-7vrs2 [749.765499ms]
Mar 26 17:41:29.147: INFO: Created: latency-svc-zzsmz
Mar 26 17:41:29.186: INFO: Got endpoints: latency-svc-6s658 [745.975053ms]
Mar 26 17:41:29.199: INFO: Created: latency-svc-6m842
Mar 26 17:41:29.237: INFO: Got endpoints: latency-svc-2x8jd [722.99387ms]
Mar 26 17:41:29.254: INFO: Created: latency-svc-8jrd2
Mar 26 17:41:29.289: INFO: Got endpoints: latency-svc-pwplw [751.936034ms]
Mar 26 17:41:29.292: INFO: Created: latency-svc-4zdmp
Mar 26 17:41:29.338: INFO: Got endpoints: latency-svc-mcz7c [750.145033ms]
Mar 26 17:41:29.343: INFO: Created: latency-svc-bsk5q
Mar 26 17:41:29.387: INFO: Got endpoints: latency-svc-srwsf [749.790859ms]
Mar 26 17:41:29.394: INFO: Created: latency-svc-d98jd
Mar 26 17:41:29.453: INFO: Got endpoints: latency-svc-2gmnr [767.76369ms]
Mar 26 17:41:29.462: INFO: Created: latency-svc-jkzk8
Mar 26 17:41:29.516: INFO: Got endpoints: latency-svc-rxvn7 [775.484646ms]
Mar 26 17:41:29.535: INFO: Created: latency-svc-q5vf8
Mar 26 17:41:29.537: INFO: Got endpoints: latency-svc-qhqbw [748.415835ms]
Mar 26 17:41:29.566: INFO: Created: latency-svc-xsr8p
Mar 26 17:41:29.591: INFO: Got endpoints: latency-svc-dx28p [741.139724ms]
Mar 26 17:41:29.596: INFO: Created: latency-svc-bfr6d
Mar 26 17:41:29.641: INFO: Got endpoints: latency-svc-2f8hs [757.076929ms]
Mar 26 17:41:29.650: INFO: Created: latency-svc-qvpfq
Mar 26 17:41:29.693: INFO: Got endpoints: latency-svc-fgtd2 [757.401207ms]
Mar 26 17:41:29.709: INFO: Created: latency-svc-n9vsj
Mar 26 17:41:29.742: INFO: Got endpoints: latency-svc-qwkwp [746.090845ms]
Mar 26 17:41:29.758: INFO: Created: latency-svc-bx78z
Mar 26 17:41:29.787: INFO: Got endpoints: latency-svc-2pbkm [753.722469ms]
Mar 26 17:41:29.813: INFO: Created: latency-svc-zjlmd
Mar 26 17:41:29.839: INFO: Got endpoints: latency-svc-zzsmz [746.681135ms]
Mar 26 17:41:29.852: INFO: Created: latency-svc-mqvlb
Mar 26 17:41:29.899: INFO: Got endpoints: latency-svc-6m842 [759.634956ms]
Mar 26 17:41:29.907: INFO: Created: latency-svc-5qqwc
Mar 26 17:41:29.960: INFO: Created: latency-svc-w5b95
Mar 26 17:41:29.961: INFO: Got endpoints: latency-svc-8jrd2 [774.64901ms]
Mar 26 17:41:30.013: INFO: Got endpoints: latency-svc-4zdmp [775.716539ms]
Mar 26 17:41:30.020: INFO: Created: latency-svc-7zhsh
Mar 26 17:41:30.040: INFO: Got endpoints: latency-svc-bsk5q [750.619791ms]
Mar 26 17:41:30.067: INFO: Created: latency-svc-25xk6
Mar 26 17:41:30.094: INFO: Got endpoints: latency-svc-d98jd [755.731642ms]
Mar 26 17:41:30.106: INFO: Created: latency-svc-jk22s
Mar 26 17:41:30.154: INFO: Got endpoints: latency-svc-jkzk8 [767.16585ms]
Mar 26 17:41:30.188: INFO: Created: latency-svc-gsjhn
Mar 26 17:41:30.197: INFO: Got endpoints: latency-svc-q5vf8 [743.754113ms]
Mar 26 17:41:30.214: INFO: Created: latency-svc-kqlz4
Mar 26 17:41:30.237: INFO: Got endpoints: latency-svc-xsr8p [721.126632ms]
Mar 26 17:41:30.270: INFO: Created: latency-svc-8z74x
Mar 26 17:41:30.291: INFO: Got endpoints: latency-svc-bfr6d [754.044546ms]
Mar 26 17:41:30.301: INFO: Created: latency-svc-wpgpd
Mar 26 17:41:30.336: INFO: Got endpoints: latency-svc-qvpfq [745.499294ms]
Mar 26 17:41:30.345: INFO: Created: latency-svc-w7586
Mar 26 17:41:30.390: INFO: Got endpoints: latency-svc-n9vsj [748.415075ms]
Mar 26 17:41:30.393: INFO: Created: latency-svc-fhnkq
Mar 26 17:41:30.437: INFO: Got endpoints: latency-svc-bx78z [744.077177ms]
Mar 26 17:41:30.448: INFO: Created: latency-svc-xgzpj
Mar 26 17:41:30.491: INFO: Got endpoints: latency-svc-zjlmd [748.632793ms]
Mar 26 17:41:30.491: INFO: Created: latency-svc-wh5nd
Mar 26 17:41:30.535: INFO: Got endpoints: latency-svc-mqvlb [747.619571ms]
Mar 26 17:41:30.550: INFO: Created: latency-svc-7jp9q
Mar 26 17:41:30.585: INFO: Created: latency-svc-kpr92
Mar 26 17:41:30.587: INFO: Got endpoints: latency-svc-5qqwc [747.967731ms]
Mar 26 17:41:30.641: INFO: Got endpoints: latency-svc-w5b95 [741.29127ms]
Mar 26 17:41:30.644: INFO: Created: latency-svc-vn4mv
Mar 26 17:41:30.687: INFO: Got endpoints: latency-svc-7zhsh [725.745087ms]
Mar 26 17:41:30.696: INFO: Created: latency-svc-jd4qz
Mar 26 17:41:30.739: INFO: Got endpoints: latency-svc-25xk6 [725.85989ms]
Mar 26 17:41:30.743: INFO: Created: latency-svc-6qtmw
Mar 26 17:41:30.797: INFO: Got endpoints: latency-svc-jk22s [756.980383ms]
Mar 26 17:41:30.799: INFO: Created: latency-svc-rgbzl
Mar 26 17:41:30.833: INFO: Got endpoints: latency-svc-gsjhn [739.29766ms]
Mar 26 17:41:30.869: INFO: Created: latency-svc-tvkvl
Mar 26 17:41:30.891: INFO: Got endpoints: latency-svc-kqlz4 [737.056437ms]
Mar 26 17:41:30.900: INFO: Created: latency-svc-b69vc
Mar 26 17:41:30.939: INFO: Got endpoints: latency-svc-8z74x [742.29818ms]
Mar 26 17:41:30.950: INFO: Created: latency-svc-vg8jn
Mar 26 17:41:30.994: INFO: Got endpoints: latency-svc-wpgpd [756.587738ms]
Mar 26 17:41:31.006: INFO: Created: latency-svc-tm24d
Mar 26 17:41:31.034: INFO: Got endpoints: latency-svc-w7586 [742.474047ms]
Mar 26 17:41:31.066: INFO: Created: latency-svc-82qz9
Mar 26 17:41:31.097: INFO: Got endpoints: latency-svc-fhnkq [761.053862ms]
Mar 26 17:41:31.110: INFO: Created: latency-svc-92bch
Mar 26 17:41:31.134: INFO: Got endpoints: latency-svc-xgzpj [744.312183ms]
Mar 26 17:41:31.147: INFO: Created: latency-svc-gvnbh
Mar 26 17:41:31.185: INFO: Created: latency-svc-jzvxv
Mar 26 17:41:31.191: INFO: Got endpoints: latency-svc-wh5nd [753.265062ms]
Mar 26 17:41:31.236: INFO: Got endpoints: latency-svc-7jp9q [745.711515ms]
Mar 26 17:41:31.251: INFO: Created: latency-svc-r859n
Mar 26 17:41:31.292: INFO: Got endpoints: latency-svc-kpr92 [757.440007ms]
Mar 26 17:41:31.297: INFO: Created: latency-svc-fflpw
Mar 26 17:41:31.337: INFO: Got endpoints: latency-svc-vn4mv [750.266545ms]
Mar 26 17:41:31.346: INFO: Created: latency-svc-4m6lm
Mar 26 17:41:31.389: INFO: Got endpoints: latency-svc-jd4qz [748.033801ms]
Mar 26 17:41:31.391: INFO: Created: latency-svc-h7lnm
Mar 26 17:41:31.454: INFO: Got endpoints: latency-svc-6qtmw [767.885484ms]
Mar 26 17:41:31.465: INFO: Created: latency-svc-wdvql
Mar 26 17:41:31.486: INFO: Got endpoints: latency-svc-rgbzl [746.913352ms]
Mar 26 17:41:31.514: INFO: Created: latency-svc-25dfr
Mar 26 17:41:31.533: INFO: Got endpoints: latency-svc-tvkvl [736.632674ms]
Mar 26 17:41:31.544: INFO: Created: latency-svc-s9khm
Mar 26 17:41:31.585: INFO: Got endpoints: latency-svc-b69vc [751.579997ms]
Mar 26 17:41:31.594: INFO: Created: latency-svc-qvbk9
Mar 26 17:41:31.635: INFO: Created: latency-svc-9kvgd
Mar 26 17:41:31.640: INFO: Got endpoints: latency-svc-vg8jn [748.948633ms]
Mar 26 17:41:31.693: INFO: Got endpoints: latency-svc-tm24d [753.002343ms]
Mar 26 17:41:31.701: INFO: Created: latency-svc-f6fsm
Mar 26 17:41:31.732: INFO: Got endpoints: latency-svc-82qz9 [737.951382ms]
Mar 26 17:41:31.750: INFO: Created: latency-svc-mxqb5
Mar 26 17:41:31.789: INFO: Got endpoints: latency-svc-92bch [754.544923ms]
Mar 26 17:41:31.792: INFO: Created: latency-svc-w8wbj
Mar 26 17:41:31.841: INFO: Got endpoints: latency-svc-gvnbh [743.741154ms]
Mar 26 17:41:31.847: INFO: Created: latency-svc-csdq7
Mar 26 17:41:31.888: INFO: Got endpoints: latency-svc-jzvxv [754.292453ms]
Mar 26 17:41:31.897: INFO: Created: latency-svc-bt5wx
Mar 26 17:41:31.938: INFO: Got endpoints: latency-svc-r859n [746.87409ms]
Mar 26 17:41:31.946: INFO: Created: latency-svc-774sc
Mar 26 17:41:31.994: INFO: Got endpoints: latency-svc-fflpw [757.264638ms]
Mar 26 17:41:32.004: INFO: Created: latency-svc-9pcxl
Mar 26 17:41:32.041: INFO: Got endpoints: latency-svc-4m6lm [748.924776ms]
Mar 26 17:41:32.049: INFO: Created: latency-svc-ms2gr
Mar 26 17:41:32.088: INFO: Got endpoints: latency-svc-h7lnm [750.6749ms]
Mar 26 17:41:32.114: INFO: Created: latency-svc-qwwnz
Mar 26 17:41:32.155: INFO: Got endpoints: latency-svc-wdvql [765.81934ms]
Mar 26 17:41:32.164: INFO: Created: latency-svc-mh4n6
Mar 26 17:41:32.188: INFO: Got endpoints: latency-svc-25dfr [733.256564ms]
Mar 26 17:41:32.204: INFO: Created: latency-svc-t9m7x
Mar 26 17:41:32.241: INFO: Got endpoints: latency-svc-s9khm [755.146475ms]
Mar 26 17:41:32.247: INFO: Created: latency-svc-v529j
Mar 26 17:41:32.298: INFO: Got endpoints: latency-svc-qvbk9 [764.198549ms]
Mar 26 17:41:32.309: INFO: Created: latency-svc-sm4d7
Mar 26 17:41:32.333: INFO: Got endpoints: latency-svc-9kvgd [748.567181ms]
Mar 26 17:41:32.350: INFO: Created: latency-svc-vwx47
Mar 26 17:41:32.388: INFO: Created: latency-svc-x5kbq
Mar 26 17:41:32.390: INFO: Got endpoints: latency-svc-f6fsm [749.529889ms]
Mar 26 17:41:32.435: INFO: Got endpoints: latency-svc-mxqb5 [742.653761ms]
Mar 26 17:41:32.449: INFO: Created: latency-svc-98qmh
Mar 26 17:41:32.485: INFO: Created: latency-svc-c6t65
Mar 26 17:41:32.491: INFO: Got endpoints: latency-svc-w8wbj [759.496702ms]
Mar 26 17:41:32.534: INFO: Got endpoints: latency-svc-csdq7 [745.566677ms]
Mar 26 17:41:32.549: INFO: Created: latency-svc-v577m
Mar 26 17:41:32.587: INFO: Created: latency-svc-76gdd
Mar 26 17:41:32.591: INFO: Got endpoints: latency-svc-bt5wx [749.442945ms]
Mar 26 17:41:32.638: INFO: Got endpoints: latency-svc-774sc [750.047154ms]
Mar 26 17:41:32.646: INFO: Created: latency-svc-dnv4b
Mar 26 17:41:32.691: INFO: Got endpoints: latency-svc-9pcxl [753.589007ms]
Mar 26 17:41:32.696: INFO: Created: latency-svc-j2zr6
Mar 26 17:41:32.736: INFO: Got endpoints: latency-svc-ms2gr [742.816482ms]
Mar 26 17:41:32.749: INFO: Created: latency-svc-85gsh
Mar 26 17:41:32.788: INFO: Got endpoints: latency-svc-qwwnz [747.297227ms]
Mar 26 17:41:32.795: INFO: Created: latency-svc-psmkt
Mar 26 17:41:32.850: INFO: Got endpoints: latency-svc-mh4n6 [761.663658ms]
Mar 26 17:41:32.857: INFO: Created: latency-svc-p6rv8
Mar 26 17:41:32.887: INFO: Got endpoints: latency-svc-t9m7x [732.497417ms]
Mar 26 17:41:32.916: INFO: Created: latency-svc-bgt5g
Mar 26 17:41:32.938: INFO: Got endpoints: latency-svc-v529j [749.743448ms]
Mar 26 17:41:32.941: INFO: Created: latency-svc-hlzxq
Mar 26 17:41:32.996: INFO: Got endpoints: latency-svc-sm4d7 [754.500214ms]
Mar 26 17:41:33.001: INFO: Created: latency-svc-q2wx8
Mar 26 17:41:33.038: INFO: Got endpoints: latency-svc-vwx47 [740.219441ms]
Mar 26 17:41:33.048: INFO: Created: latency-svc-xz9wz
Mar 26 17:41:33.089: INFO: Got endpoints: latency-svc-x5kbq [755.125373ms]
Mar 26 17:41:33.097: INFO: Created: latency-svc-lfzqw
Mar 26 17:41:33.149: INFO: Got endpoints: latency-svc-98qmh [759.491935ms]
Mar 26 17:41:33.150: INFO: Created: latency-svc-rpvbq
Mar 26 17:41:33.182: INFO: Got endpoints: latency-svc-c6t65 [746.672108ms]
Mar 26 17:41:33.202: INFO: Created: latency-svc-vlvrl
Mar 26 17:41:33.238: INFO: Created: latency-svc-jx92h
Mar 26 17:41:33.255: INFO: Got endpoints: latency-svc-v577m [764.18654ms]
Mar 26 17:41:33.287: INFO: Got endpoints: latency-svc-76gdd [752.405503ms]
Mar 26 17:41:33.313: INFO: Created: latency-svc-jv82c
Mar 26 17:41:33.336: INFO: Got endpoints: latency-svc-dnv4b [745.649157ms]
Mar 26 17:41:33.337: INFO: Created: latency-svc-c9dzm
Mar 26 17:41:33.388: INFO: Created: latency-svc-zjff6
Mar 26 17:41:33.390: INFO: Got endpoints: latency-svc-j2zr6 [751.339404ms]
Mar 26 17:41:33.436: INFO: Got endpoints: latency-svc-85gsh [745.055857ms]
Mar 26 17:41:33.450: INFO: Created: latency-svc-grmtn
Mar 26 17:41:33.488: INFO: Got endpoints: latency-svc-psmkt [751.374021ms]
Mar 26 17:41:33.494: INFO: Created: latency-svc-p7rnw
Mar 26 17:41:33.540: INFO: Got endpoints: latency-svc-p6rv8 [751.894025ms]
Mar 26 17:41:33.545: INFO: Created: latency-svc-qrpbm
Mar 26 17:41:33.595: INFO: Got endpoints: latency-svc-bgt5g [745.458535ms]
Mar 26 17:41:33.708: INFO: Created: latency-svc-7d22f
Mar 26 17:41:33.763: INFO: Got endpoints: latency-svc-hlzxq [876.149886ms]
Mar 26 17:41:33.892: INFO: Got endpoints: latency-svc-xz9wz [896.090253ms]
Mar 26 17:41:33.892: INFO: Got endpoints: latency-svc-q2wx8 [954.067593ms]
Mar 26 17:41:33.901: INFO: Got endpoints: latency-svc-lfzqw [862.561629ms]
Mar 26 17:41:33.917: INFO: Created: latency-svc-g46ck
Mar 26 17:41:33.975: INFO: Created: latency-svc-ts2z2
Mar 26 17:41:33.978: INFO: Got endpoints: latency-svc-vlvrl [829.117017ms]
Mar 26 17:41:33.978: INFO: Got endpoints: latency-svc-rpvbq [889.680317ms]
Mar 26 17:41:34.028: INFO: Got endpoints: latency-svc-jx92h [845.885095ms]
Mar 26 17:41:34.110: INFO: Got endpoints: latency-svc-jv82c [854.931245ms]
Mar 26 17:41:34.124: INFO: Created: latency-svc-2mt5f
Mar 26 17:41:34.187: INFO: Created: latency-svc-rnbpx
Mar 26 17:41:34.195: INFO: Got endpoints: latency-svc-c9dzm [907.675546ms]
Mar 26 17:41:34.195: INFO: Got endpoints: latency-svc-zjff6 [858.393429ms]
Mar 26 17:41:34.229: INFO: Got endpoints: latency-svc-grmtn [839.21686ms]
Mar 26 17:41:34.370: INFO: Got endpoints: latency-svc-p7rnw [933.2723ms]
Mar 26 17:41:34.373: INFO: Created: latency-svc-xhs62
Mar 26 17:41:34.397: INFO: Got endpoints: latency-svc-qrpbm [909.531331ms]
Mar 26 17:41:34.428: INFO: Got endpoints: latency-svc-g46ck [832.492851ms]
Mar 26 17:41:34.428: INFO: Got endpoints: latency-svc-7d22f [887.422756ms]
Mar 26 17:41:34.441: INFO: Got endpoints: latency-svc-ts2z2 [678.012229ms]
Mar 26 17:41:34.445: INFO: Created: latency-svc-swb8x
Mar 26 17:41:34.468: INFO: Got endpoints: latency-svc-2mt5f [576.483186ms]
Mar 26 17:41:34.479: INFO: Created: latency-svc-56dbs
Mar 26 17:41:34.489: INFO: Created: latency-svc-vbxx7
Mar 26 17:41:34.499: INFO: Got endpoints: latency-svc-rnbpx [597.928592ms]
Mar 26 17:41:34.539: INFO: Created: latency-svc-bdtxv
Mar 26 17:41:34.567: INFO: Got endpoints: latency-svc-xhs62 [675.529772ms]
Mar 26 17:41:34.577: INFO: Created: latency-svc-jqczj
Mar 26 17:41:34.590: INFO: Got endpoints: latency-svc-swb8x [395.693502ms]
Mar 26 17:41:34.600: INFO: Created: latency-svc-8tbv6
Mar 26 17:41:34.615: INFO: Created: latency-svc-9nh9m
Mar 26 17:41:34.621: INFO: Created: latency-svc-z7k66
Mar 26 17:41:34.638: INFO: Created: latency-svc-7c74g
Mar 26 17:41:34.642: INFO: Got endpoints: latency-svc-56dbs [663.57457ms]
Mar 26 17:41:34.648: INFO: Created: latency-svc-rdhw6
Mar 26 17:41:34.657: INFO: Created: latency-svc-s8x2x
Mar 26 17:41:34.667: INFO: Created: latency-svc-7n98g
Mar 26 17:41:34.678: INFO: Created: latency-svc-db8ls
Mar 26 17:41:34.694: INFO: Got endpoints: latency-svc-vbxx7 [715.039516ms]
Mar 26 17:41:34.697: INFO: Created: latency-svc-9rhbf
Mar 26 17:41:34.710: INFO: Created: latency-svc-hp6g9
Mar 26 17:41:34.718: INFO: Created: latency-svc-qf8pr
Mar 26 17:41:34.726: INFO: Created: latency-svc-qtlpj
Mar 26 17:41:34.740: INFO: Got endpoints: latency-svc-bdtxv [711.811605ms]
Mar 26 17:41:34.750: INFO: Created: latency-svc-56fm9
Mar 26 17:41:34.788: INFO: Got endpoints: latency-svc-jqczj [677.612025ms]
Mar 26 17:41:34.835: INFO: Got endpoints: latency-svc-8tbv6 [639.939112ms]
Mar 26 17:41:34.886: INFO: Got endpoints: latency-svc-9nh9m [657.093938ms]
Mar 26 17:41:34.937: INFO: Got endpoints: latency-svc-z7k66 [566.847721ms]
Mar 26 17:41:34.996: INFO: Got endpoints: latency-svc-7c74g [598.565039ms]
Mar 26 17:41:35.038: INFO: Got endpoints: latency-svc-rdhw6 [609.926653ms]
Mar 26 17:41:35.086: INFO: Got endpoints: latency-svc-s8x2x [657.788832ms]
Mar 26 17:41:35.135: INFO: Got endpoints: latency-svc-7n98g [693.199871ms]
Mar 26 17:41:35.188: INFO: Got endpoints: latency-svc-db8ls [719.327507ms]
Mar 26 17:41:35.237: INFO: Got endpoints: latency-svc-9rhbf [738.222177ms]
Mar 26 17:41:35.287: INFO: Got endpoints: latency-svc-hp6g9 [719.804027ms]
Mar 26 17:41:35.373: INFO: Got endpoints: latency-svc-qf8pr [782.977263ms]
Mar 26 17:41:35.393: INFO: Got endpoints: latency-svc-qtlpj [750.655839ms]
Mar 26 17:41:35.438: INFO: Got endpoints: latency-svc-56fm9 [744.66238ms]
Mar 26 17:41:35.438: INFO: Latencies: [59.491774ms 75.128383ms 81.987379ms 129.497243ms 188.696641ms 194.318578ms 229.444485ms 231.359143ms 264.630722ms 264.665214ms 317.733317ms 332.038051ms 336.091716ms 338.88024ms 340.46956ms 343.071811ms 344.063365ms 344.555708ms 344.635313ms 345.818986ms 347.014184ms 351.897507ms 352.347193ms 352.776617ms 353.201543ms 353.465087ms 353.74431ms 355.451318ms 356.814504ms 358.892275ms 361.044304ms 361.711561ms 362.227776ms 363.841479ms 364.284525ms 367.061307ms 367.493447ms 367.803756ms 368.295871ms 368.617849ms 369.644537ms 369.680727ms 369.962586ms 371.731421ms 373.470071ms 378.464009ms 379.472796ms 390.476298ms 392.197772ms 393.198368ms 395.693502ms 395.938163ms 397.917261ms 400.959703ms 408.750986ms 414.81483ms 419.633206ms 420.582076ms 437.52719ms 443.139092ms 444.520482ms 452.467157ms 459.851403ms 463.694475ms 494.461885ms 531.013922ms 562.727427ms 566.847721ms 576.483186ms 591.499431ms 597.928592ms 598.565039ms 609.926653ms 628.694007ms 636.459884ms 639.939112ms 657.093938ms 657.788832ms 663.57457ms 665.040685ms 675.529772ms 677.612025ms 678.012229ms 693.199871ms 703.639371ms 711.811605ms 715.039516ms 719.327507ms 719.804027ms 719.925913ms 721.126632ms 722.99387ms 725.745087ms 725.85989ms 732.497417ms 733.256564ms 736.632674ms 737.056437ms 737.951382ms 738.222177ms 739.29766ms 740.219441ms 741.139724ms 741.29127ms 742.29818ms 742.474047ms 742.653761ms 742.816482ms 743.741154ms 743.754113ms 744.077177ms 744.312183ms 744.66238ms 745.055857ms 745.458535ms 745.499294ms 745.566677ms 745.649157ms 745.711515ms 745.975053ms 746.090845ms 746.672108ms 746.681135ms 746.87409ms 746.913352ms 747.222149ms 747.297227ms 747.619571ms 747.967731ms 748.033801ms 748.415075ms 748.415835ms 748.567181ms 748.632793ms 748.924776ms 748.948633ms 749.442945ms 749.529889ms 749.743448ms 749.765499ms 749.790859ms 750.047154ms 750.145033ms 750.266545ms 750.619791ms 750.655839ms 750.6749ms 751.339404ms 751.374021ms 751.579997ms 751.894025ms 751.936034ms 752.405503ms 753.002343ms 753.265062ms 753.589007ms 753.722469ms 754.044546ms 754.292453ms 754.500214ms 754.544923ms 755.125373ms 755.146475ms 755.731642ms 756.587738ms 756.980383ms 757.076929ms 757.264638ms 757.401207ms 757.440007ms 759.491935ms 759.496702ms 759.634956ms 761.053862ms 761.663658ms 764.18654ms 764.198549ms 765.81934ms 767.16585ms 767.76369ms 767.885484ms 774.64901ms 775.484646ms 775.716539ms 782.977263ms 829.117017ms 832.492851ms 839.21686ms 845.885095ms 854.931245ms 858.393429ms 862.561629ms 876.149886ms 887.422756ms 889.680317ms 896.090253ms 907.675546ms 909.531331ms 933.2723ms 954.067593ms]
Mar 26 17:41:35.439: INFO: 50 %ile: 739.29766ms
Mar 26 17:41:35.439: INFO: 90 %ile: 767.885484ms
Mar 26 17:41:35.439: INFO: 99 %ile: 933.2723ms
Mar 26 17:41:35.439: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:41:35.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8211" for this suite.
•{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":277,"completed":9,"skipped":144,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:41:35.535: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-388d0846-0ec4-447d-8e10-f355c89ab4e2
STEP: Creating configMap with name cm-test-opt-upd-edbff72e-476d-4731-af16-12623179b053
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-388d0846-0ec4-447d-8e10-f355c89ab4e2
STEP: Updating configmap cm-test-opt-upd-edbff72e-476d-4731-af16-12623179b053
STEP: Creating configMap with name cm-test-opt-create-9e97e596-7ca0-469b-8256-fa28818ef2df
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:42:58.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5830" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":10,"skipped":152,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:42:58.259: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-72506be5-e013-4447-b8ac-0d62c82f48c1 in namespace container-probe-2688
Mar 26 17:43:00.581: INFO: Started pod busybox-72506be5-e013-4447-b8ac-0d62c82f48c1 in namespace container-probe-2688
STEP: checking the pod's current state and verifying that restartCount is present
Mar 26 17:43:00.620: INFO: Initial restart count of pod busybox-72506be5-e013-4447-b8ac-0d62c82f48c1 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:47:01.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2688" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":11,"skipped":156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:47:01.398: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:47:03.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5382" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":12,"skipped":191,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:47:03.894: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1418
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 26 17:47:04.093: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1802'
Mar 26 17:47:04.337: INFO: stderr: ""
Mar 26 17:47:04.337: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1423
Mar 26 17:47:04.375: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config delete pods e2e-test-httpd-pod --namespace=kubectl-1802'
Mar 26 17:47:10.504: INFO: stderr: ""
Mar 26 17:47:10.504: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:47:10.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1802" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":277,"completed":13,"skipped":201,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:47:10.586: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar 26 17:47:10.824: INFO: Waiting up to 5m0s for pod "downward-api-05ef4f07-de02-4b10-9943-1264f51f0789" in namespace "downward-api-7993" to be "Succeeded or Failed"
Mar 26 17:47:10.876: INFO: Pod "downward-api-05ef4f07-de02-4b10-9943-1264f51f0789": Phase="Pending", Reason="", readiness=false. Elapsed: 51.540519ms
Mar 26 17:47:12.914: INFO: Pod "downward-api-05ef4f07-de02-4b10-9943-1264f51f0789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.090018168s
STEP: Saw pod success
Mar 26 17:47:12.915: INFO: Pod "downward-api-05ef4f07-de02-4b10-9943-1264f51f0789" satisfied condition "Succeeded or Failed"
Mar 26 17:47:12.953: INFO: Trying to get logs from node bootstrap-e2e-minion-group-d5w7 pod downward-api-05ef4f07-de02-4b10-9943-1264f51f0789 container dapi-container: <nil>
STEP: delete the pod
Mar 26 17:47:13.061: INFO: Waiting for pod downward-api-05ef4f07-de02-4b10-9943-1264f51f0789 to disappear
Mar 26 17:47:13.099: INFO: Pod downward-api-05ef4f07-de02-4b10-9943-1264f51f0789 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:47:13.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7993" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":277,"completed":14,"skipped":223,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:47:13.187: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-9936
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating stateful set ss in namespace statefulset-9936
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9936
Mar 26 17:47:13.528: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar 26 17:47:23.569: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 26 17:47:23.609: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-9936 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 17:47:24.155: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 17:47:24.155: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 17:47:24.155: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 17:47:24.194: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 26 17:47:34.235: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 17:47:34.235: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 17:47:34.425: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999633s
Mar 26 17:47:35.465: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.929567942s
Mar 26 17:47:36.505: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.8894447s
Mar 26 17:47:37.546: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.849243381s
Mar 26 17:47:38.586: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.808953872s
Mar 26 17:47:39.628: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.768900242s
Mar 26 17:47:40.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.726692287s
Mar 26 17:47:41.707: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.687068361s
Mar 26 17:47:42.748: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.64717181s
Mar 26 17:47:43.789: INFO: Verifying statefulset ss doesn't scale past 3 for another 606.330725ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9936
Mar 26 17:47:44.829: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-9936 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 17:47:45.367: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 26 17:47:45.367: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 17:47:45.367: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 17:47:45.367: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-9936 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 17:47:45.902: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 26 17:47:45.902: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 17:47:45.902: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 17:47:45.902: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-9936 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 17:47:46.475: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 26 17:47:46.475: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 17:47:46.475: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 17:47:46.514: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 17:47:46.514: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 17:47:46.514: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 26 17:47:46.554: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-9936 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 17:47:47.093: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 17:47:47.093: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 17:47:47.093: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 17:47:47.093: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-9936 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 17:47:47.605: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 17:47:47.605: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 17:47:47.605: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 17:47:47.605: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-9936 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 17:47:48.137: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 17:47:48.137: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 17:47:48.137: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 17:47:48.137: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 17:47:48.175: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Mar 26 17:47:58.254: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 17:47:58.254: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 17:47:58.254: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 17:47:58.392: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Mar 26 17:47:58.392: INFO: ss-0  bootstrap-e2e-minion-group-d5w7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:13 +0000 UTC  }]
Mar 26 17:47:58.393: INFO: ss-1  bootstrap-e2e-minion-group-1r9w  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  }]
Mar 26 17:47:58.393: INFO: ss-2  bootstrap-e2e-minion-group-gs7c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  }]
Mar 26 17:47:58.393: INFO: 
Mar 26 17:47:58.393: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 26 17:47:59.433: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Mar 26 17:47:59.433: INFO: ss-0  bootstrap-e2e-minion-group-d5w7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:13 +0000 UTC  }]
Mar 26 17:47:59.433: INFO: ss-1  bootstrap-e2e-minion-group-1r9w  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  }]
Mar 26 17:47:59.433: INFO: ss-2  bootstrap-e2e-minion-group-gs7c  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  }]
Mar 26 17:47:59.433: INFO: 
Mar 26 17:47:59.433: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 26 17:48:00.473: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Mar 26 17:48:00.474: INFO: ss-0  bootstrap-e2e-minion-group-d5w7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:13 +0000 UTC  }]
Mar 26 17:48:00.474: INFO: ss-1  bootstrap-e2e-minion-group-1r9w  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  }]
Mar 26 17:48:00.474: INFO: ss-2  bootstrap-e2e-minion-group-gs7c  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  }]
Mar 26 17:48:00.474: INFO: 
Mar 26 17:48:00.474: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 26 17:48:01.514: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Mar 26 17:48:01.514: INFO: ss-0  bootstrap-e2e-minion-group-d5w7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:13 +0000 UTC  }]
Mar 26 17:48:01.514: INFO: ss-1  bootstrap-e2e-minion-group-1r9w  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  }]
Mar 26 17:48:01.514: INFO: ss-2  bootstrap-e2e-minion-group-gs7c  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  }]
Mar 26 17:48:01.514: INFO: 
Mar 26 17:48:01.514: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 26 17:48:02.555: INFO: POD   NODE                             PHASE    GRACE  CONDITIONS
Mar 26 17:48:02.555: INFO: ss-1  bootstrap-e2e-minion-group-1r9w  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  }]
Mar 26 17:48:02.555: INFO: ss-2  bootstrap-e2e-minion-group-gs7c  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-26 17:47:34 +0000 UTC  }]
Mar 26 17:48:02.555: INFO: 
Mar 26 17:48:02.555: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 26 17:48:03.595: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.787137812s
Mar 26 17:48:04.633: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.746997825s
Mar 26 17:48:05.672: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.708359357s
Mar 26 17:48:06.710: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.669800221s
Mar 26 17:48:07.749: INFO: Verifying statefulset ss doesn't scale past 0 for another 631.482716ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9936
Mar 26 17:48:08.787: INFO: Scaling statefulset ss to 0
Mar 26 17:48:08.904: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 26 17:48:08.942: INFO: Deleting all statefulset in ns statefulset-9936
Mar 26 17:48:08.985: INFO: Scaling statefulset ss to 0
Mar 26 17:48:09.101: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 17:48:09.142: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:09.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9936" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":277,"completed":15,"skipped":227,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:09.347: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:48:09.618: INFO: Waiting up to 5m0s for pod "busybox-user-65534-9ee4e3bb-70cc-4e40-8210-5bad583961a9" in namespace "security-context-test-4123" to be "Succeeded or Failed"
Mar 26 17:48:09.656: INFO: Pod "busybox-user-65534-9ee4e3bb-70cc-4e40-8210-5bad583961a9": Phase="Pending", Reason="", readiness=false. Elapsed: 38.071226ms
Mar 26 17:48:11.694: INFO: Pod "busybox-user-65534-9ee4e3bb-70cc-4e40-8210-5bad583961a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076425104s
Mar 26 17:48:11.694: INFO: Pod "busybox-user-65534-9ee4e3bb-70cc-4e40-8210-5bad583961a9" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:11.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4123" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":16,"skipped":270,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:11.779: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-96003ef4-e4b7-4fd6-8106-7454a808fa93
STEP: Creating a pod to test consume secrets
Mar 26 17:48:12.054: INFO: Waiting up to 5m0s for pod "pod-secrets-c98626e5-6dc0-43d2-8045-499d2d640da9" in namespace "secrets-1964" to be "Succeeded or Failed"
Mar 26 17:48:12.092: INFO: Pod "pod-secrets-c98626e5-6dc0-43d2-8045-499d2d640da9": Phase="Pending", Reason="", readiness=false. Elapsed: 38.838716ms
Mar 26 17:48:14.131: INFO: Pod "pod-secrets-c98626e5-6dc0-43d2-8045-499d2d640da9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077479981s
STEP: Saw pod success
Mar 26 17:48:14.131: INFO: Pod "pod-secrets-c98626e5-6dc0-43d2-8045-499d2d640da9" satisfied condition "Succeeded or Failed"
Mar 26 17:48:14.172: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-secrets-c98626e5-6dc0-43d2-8045-499d2d640da9 container secret-env-test: <nil>
STEP: delete the pod
Mar 26 17:48:14.264: INFO: Waiting for pod pod-secrets-c98626e5-6dc0-43d2-8045-499d2d640da9 to disappear
Mar 26 17:48:14.303: INFO: Pod pod-secrets-c98626e5-6dc0-43d2-8045-499d2d640da9 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:14.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1964" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":277,"completed":17,"skipped":309,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Lease
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:14.392: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Lease
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:15.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1880" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":277,"completed":18,"skipped":366,"failed":0}
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:15.212: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override arguments
Mar 26 17:48:15.414: INFO: Waiting up to 5m0s for pod "client-containers-3e028f97-32e0-40d4-a579-031ba6fcd781" in namespace "containers-4419" to be "Succeeded or Failed"
Mar 26 17:48:15.452: INFO: Pod "client-containers-3e028f97-32e0-40d4-a579-031ba6fcd781": Phase="Pending", Reason="", readiness=false. Elapsed: 38.131018ms
Mar 26 17:48:17.491: INFO: Pod "client-containers-3e028f97-32e0-40d4-a579-031ba6fcd781": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077173672s
STEP: Saw pod success
Mar 26 17:48:17.491: INFO: Pod "client-containers-3e028f97-32e0-40d4-a579-031ba6fcd781" satisfied condition "Succeeded or Failed"
Mar 26 17:48:17.529: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod client-containers-3e028f97-32e0-40d4-a579-031ba6fcd781 container test-container: <nil>
STEP: delete the pod
Mar 26 17:48:17.648: INFO: Waiting for pod client-containers-3e028f97-32e0-40d4-a579-031ba6fcd781 to disappear
Mar 26 17:48:17.686: INFO: Pod client-containers-3e028f97-32e0-40d4-a579-031ba6fcd781 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:17.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4419" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":277,"completed":19,"skipped":371,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:17.774: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-3444
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 26 17:48:17.937: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 26 17:48:18.199: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 26 17:48:20.252: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:48:22.238: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:48:24.283: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:48:26.238: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:48:28.238: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:48:30.238: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:48:32.238: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:48:34.238: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:48:36.238: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:48:38.238: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 26 17:48:38.315: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 26 17:48:38.391: INFO: The status of Pod netserver-2 is Running (Ready = false)
Mar 26 17:48:40.430: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 26 17:48:42.669: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.1.14:8080/dial?request=hostname&protocol=udp&host=10.64.1.13&port=8081&tries=1'] Namespace:pod-network-test-3444 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:48:42.669: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 17:48:43.000: INFO: Waiting for responses: map[]
Mar 26 17:48:43.038: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.1.14:8080/dial?request=hostname&protocol=udp&host=10.64.0.12&port=8081&tries=1'] Namespace:pod-network-test-3444 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:48:43.038: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 17:48:43.382: INFO: Waiting for responses: map[]
Mar 26 17:48:43.420: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.1.14:8080/dial?request=hostname&protocol=udp&host=10.64.2.9&port=8081&tries=1'] Namespace:pod-network-test-3444 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:48:43.420: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 17:48:43.727: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:43.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3444" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":277,"completed":20,"skipped":375,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:43.811: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:48:44.836: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841724, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841724, loc:(*time.Location)(0x7b271c0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-779fdc84d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841724, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841724, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:48:46.876: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841724, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841724, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841724, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841724, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:48:49.931: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:50.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2510" for this suite.
STEP: Destroying namespace "webhook-2510-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":277,"completed":21,"skipped":383,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:50.845: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:48:51.049: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7a82af7-1821-4df2-96cc-312fc3ada5cf" in namespace "projected-876" to be "Succeeded or Failed"
Mar 26 17:48:51.098: INFO: Pod "downwardapi-volume-a7a82af7-1821-4df2-96cc-312fc3ada5cf": Phase="Pending", Reason="", readiness=false. Elapsed: 48.843204ms
Mar 26 17:48:53.140: INFO: Pod "downwardapi-volume-a7a82af7-1821-4df2-96cc-312fc3ada5cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.091149818s
STEP: Saw pod success
Mar 26 17:48:53.140: INFO: Pod "downwardapi-volume-a7a82af7-1821-4df2-96cc-312fc3ada5cf" satisfied condition "Succeeded or Failed"
Mar 26 17:48:53.178: INFO: Trying to get logs from node bootstrap-e2e-minion-group-d5w7 pod downwardapi-volume-a7a82af7-1821-4df2-96cc-312fc3ada5cf container client-container: <nil>
STEP: delete the pod
Mar 26 17:48:53.283: INFO: Waiting for pod downwardapi-volume-a7a82af7-1821-4df2-96cc-312fc3ada5cf to disappear
Mar 26 17:48:53.321: INFO: Pod downwardapi-volume-a7a82af7-1821-4df2-96cc-312fc3ada5cf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:53.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-876" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":277,"completed":22,"skipped":385,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:53.406: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:48:53.561: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 26 17:48:53.700: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 26 17:48:55.795: INFO: Creating deployment "test-rolling-update-deployment"
Mar 26 17:48:55.841: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 26 17:48:55.969: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Mar 26 17:48:58.046: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 26 17:48:58.084: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar 26 17:48:58.200: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3421 /apis/apps/v1/namespaces/deployment-3421/deployments/test-rolling-update-deployment a0177b67-c0ea-42b2-b1bd-049e606f3327 4711 1 2020-03-26 17:48:55 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-03-26 17:48:55 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-03-26 17:48:57 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002f07368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-26 17:48:55 +0000 UTC,LastTransitionTime:2020-03-26 17:48:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-59d5cb45c7" has successfully progressed.,LastUpdateTime:2020-03-26 17:48:57 +0000 UTC,LastTransitionTime:2020-03-26 17:48:55 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 26 17:48:58.240: INFO: New ReplicaSet "test-rolling-update-deployment-59d5cb45c7" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7  deployment-3421 /apis/apps/v1/namespaces/deployment-3421/replicasets/test-rolling-update-deployment-59d5cb45c7 6a7e259b-2d1d-4d14-985f-fba17669a9d8 4704 1 2020-03-26 17:48:55 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment a0177b67-c0ea-42b2-b1bd-049e606f3327 0xc005c008d7 0xc005c008d8}] []  [{kube-controller-manager Update apps/v1 2020-03-26 17:48:57 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 49 55 55 98 54 55 45 99 48 101 97 45 52 50 98 50 45 98 49 98 100 45 48 52 57 101 54 48 54 102 51 51 50 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 59d5cb45c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005c00968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 26 17:48:58.240: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 26 17:48:58.240: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3421 /apis/apps/v1/namespaces/deployment-3421/replicasets/test-rolling-update-controller 41b145c1-caf4-448e-8383-f544702e78e0 4710 2 2020-03-26 17:48:53 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment a0177b67-c0ea-42b2-b1bd-049e606f3327 0xc005c007cf 0xc005c007e0}] []  [{e2e.test Update apps/v1 2020-03-26 17:48:53 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-03-26 17:48:57 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 49 55 55 98 54 55 45 99 48 101 97 45 52 50 98 50 45 98 49 98 100 45 48 52 57 101 54 48 54 102 51 51 50 55 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005c00878 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 26 17:48:58.279: INFO: Pod "test-rolling-update-deployment-59d5cb45c7-7p4nj" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-59d5cb45c7-7p4nj test-rolling-update-deployment-59d5cb45c7- deployment-3421 /api/v1/namespaces/deployment-3421/pods/test-rolling-update-deployment-59d5cb45c7-7p4nj 7254e0dc-f646-46db-9277-221fce04dfc2 4703 0 2020-03-26 17:48:55 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:59d5cb45c7] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-59d5cb45c7 6a7e259b-2d1d-4d14-985f-fba17669a9d8 0xc005c00e67 0xc005c00e68}] []  [{kube-controller-manager Update v1 2020-03-26 17:48:55 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 54 97 55 101 50 53 57 98 45 50 100 49 100 45 52 100 49 52 45 57 56 53 102 45 102 98 97 49 55 54 54 57 97 57 100 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:48:57 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 54 52 46 50 46 49 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5jskz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5jskz,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5jskz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-gs7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:48:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:48:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:48:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.2.11,StartTime:2020-03-26 17:48:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 17:48:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://c5bc6260e994b4dfa042209e00753eddad2377027d355bb4f08677507d088081,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.2.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:48:58.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3421" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":23,"skipped":389,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:48:58.362: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 26 17:49:00.839: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:49:01.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-901" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":277,"completed":24,"skipped":442,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:49:01.094: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar 26 17:49:01.307: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:49:04.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2245" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":277,"completed":25,"skipped":450,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:49:04.692: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-b3e2c4bf-89bf-4531-a932-87eca55f7b79
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-b3e2c4bf-89bf-4531-a932-87eca55f7b79
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:49:11.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8099" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":26,"skipped":469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:49:11.534: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating api versions
Mar 26 17:49:11.700: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config api-versions'
Mar 26 17:49:11.936: INFO: stderr: ""
Mar 26 17:49:11.936: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncloud.google.com/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscalingpolicy.kope.io/v1alpha1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:49:11.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9365" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":277,"completed":27,"skipped":506,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:49:12.018: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar 26 17:49:15.042: INFO: Successfully updated pod "annotationupdate896f705b-fc21-4ff1-9a16-7e2a63119605"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:49:19.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9478" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":28,"skipped":513,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:49:19.273: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 26 17:49:19.552: INFO: Waiting up to 5m0s for pod "pod-1a3cf3a4-6e7a-4f7c-b3d3-ffe693bdfa94" in namespace "emptydir-1433" to be "Succeeded or Failed"
Mar 26 17:49:19.590: INFO: Pod "pod-1a3cf3a4-6e7a-4f7c-b3d3-ffe693bdfa94": Phase="Pending", Reason="", readiness=false. Elapsed: 38.285369ms
Mar 26 17:49:21.629: INFO: Pod "pod-1a3cf3a4-6e7a-4f7c-b3d3-ffe693bdfa94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077178742s
STEP: Saw pod success
Mar 26 17:49:21.629: INFO: Pod "pod-1a3cf3a4-6e7a-4f7c-b3d3-ffe693bdfa94" satisfied condition "Succeeded or Failed"
Mar 26 17:49:21.667: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-1a3cf3a4-6e7a-4f7c-b3d3-ffe693bdfa94 container test-container: <nil>
STEP: delete the pod
Mar 26 17:49:21.760: INFO: Waiting for pod pod-1a3cf3a4-6e7a-4f7c-b3d3-ffe693bdfa94 to disappear
Mar 26 17:49:21.798: INFO: Pod pod-1a3cf3a4-6e7a-4f7c-b3d3-ffe693bdfa94 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:49:21.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1433" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":29,"skipped":521,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:49:21.901: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:157
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:49:22.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5791" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":277,"completed":30,"skipped":547,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:49:22.268: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar 26 17:49:25.272: INFO: Successfully updated pod "labelsupdate257a81f8-5bc8-4e23-8604-e4007ab18724"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:49:27.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4816" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":31,"skipped":551,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:49:27.458: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-projected-all-test-volume-46fda4f8-f5ae-4076-8f33-2bf23508248d
STEP: Creating secret with name secret-projected-all-test-volume-1798d977-9901-473b-af0c-0a9c81021473
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 26 17:49:27.753: INFO: Waiting up to 5m0s for pod "projected-volume-f9c80017-109f-4dfb-b707-950238224711" in namespace "projected-4777" to be "Succeeded or Failed"
Mar 26 17:49:27.843: INFO: Pod "projected-volume-f9c80017-109f-4dfb-b707-950238224711": Phase="Pending", Reason="", readiness=false. Elapsed: 89.832665ms
Mar 26 17:49:29.882: INFO: Pod "projected-volume-f9c80017-109f-4dfb-b707-950238224711": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.128534196s
STEP: Saw pod success
Mar 26 17:49:29.882: INFO: Pod "projected-volume-f9c80017-109f-4dfb-b707-950238224711" satisfied condition "Succeeded or Failed"
Mar 26 17:49:29.926: INFO: Trying to get logs from node bootstrap-e2e-minion-group-d5w7 pod projected-volume-f9c80017-109f-4dfb-b707-950238224711 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 26 17:49:30.029: INFO: Waiting for pod projected-volume-f9c80017-109f-4dfb-b707-950238224711 to disappear
Mar 26 17:49:30.069: INFO: Pod projected-volume-f9c80017-109f-4dfb-b707-950238224711 no longer exists
[AfterEach] [sig-storage] Projected combined
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:49:30.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4777" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":277,"completed":32,"skipped":556,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:49:30.154: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:49:30.413: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b93a7779-9ea8-464b-be8f-3830b8c8cd4c" in namespace "security-context-test-27" to be "Succeeded or Failed"
Mar 26 17:49:30.454: INFO: Pod "alpine-nnp-false-b93a7779-9ea8-464b-be8f-3830b8c8cd4c": Phase="Pending", Reason="", readiness=false. Elapsed: 40.492569ms
Mar 26 17:49:32.510: INFO: Pod "alpine-nnp-false-b93a7779-9ea8-464b-be8f-3830b8c8cd4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096420539s
Mar 26 17:49:34.549: INFO: Pod "alpine-nnp-false-b93a7779-9ea8-464b-be8f-3830b8c8cd4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.135476962s
Mar 26 17:49:34.549: INFO: Pod "alpine-nnp-false-b93a7779-9ea8-464b-be8f-3830b8c8cd4c" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:49:34.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-27" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":33,"skipped":592,"failed":0}
S
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:49:34.672: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar 26 17:49:37.077: INFO: &Pod{ObjectMeta:{send-events-1c7e2cc6-2288-4fe0-bd7b-061a5537e2f4  events-7418 /api/v1/namespaces/events-7418/pods/send-events-1c7e2cc6-2288-4fe0-bd7b-061a5537e2f4 1370d902-b048-4ec6-b860-8e88fe0eab9c 5044 0 2020-03-26 17:49:34 +0000 UTC <nil> <nil> map[name:foo time:864681525] map[] [] []  [{e2e.test Update v1 2020-03-26 17:49:34 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 116 105 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 112 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 114 116 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 99 111 110 116 97 105 110 101 114 80 111 114 116 92 34 58 56 48 44 92 34 112 114 111 116 111 99 111 108 92 34 58 92 34 84 67 80 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 99 111 110 116 97 105 110 101 114 80 111 114 116 34 58 123 125 44 34 102 58 112 114 111 116 111 99 111 108 34 58 123 125 125 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:49:36 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 54 52 46 49 46 50 49 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9vfss,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9vfss,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9vfss,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-1r9w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:49:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:49:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:49:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:49:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:10.64.1.21,StartTime:2020-03-26 17:49:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 17:49:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://dfe09f762158d828aefd2430fb3c86e67ae87c3e3b288756fdee8b4ae6ea1445,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.1.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar 26 17:49:39.119: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar 26 17:49:41.157: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:49:41.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7418" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":277,"completed":34,"skipped":593,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:49:41.285: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 26 17:49:43.657: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:49:43.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3785" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":35,"skipped":610,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:49:43.824: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: validating cluster-info
Mar 26 17:49:44.022: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config cluster-info'
Mar 26 17:49:44.675: INFO: stderr: ""
Mar 26 17:49:44.675: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://34.83.25.82\x1b[0m\n\x1b[0;32mGLBCDefaultBackend\x1b[0m is running at \x1b[0;33mhttps://34.83.25.82/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://34.83.25.82/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://34.83.25.82/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://34.83.25.82/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:49:44.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-288" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":277,"completed":36,"skipped":636,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:49:44.758: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-6724
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 26 17:49:44.957: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 26 17:49:45.206: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 26 17:49:47.244: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:49:49.244: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:49:51.244: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:49:53.248: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:49:55.244: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:49:57.244: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:49:59.244: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:50:01.244: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:50:03.247: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 17:50:05.246: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 26 17:50:05.322: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 26 17:50:05.399: INFO: The status of Pod netserver-2 is Running (Ready = false)
Mar 26 17:50:07.439: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 26 17:50:09.819: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.64.1.22:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6724 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:50:09.819: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 17:50:10.127: INFO: Found all expected endpoints: [netserver-0]
Mar 26 17:50:10.165: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.64.0.18:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6724 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:50:10.165: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 17:50:10.506: INFO: Found all expected endpoints: [netserver-1]
Mar 26 17:50:10.544: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.64.2.14:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6724 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 17:50:10.544: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 17:50:10.897: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:50:10.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6724" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":37,"skipped":660,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:50:10.985: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
Mar 26 17:50:11.875: INFO: created pod pod-service-account-defaultsa
Mar 26 17:50:11.875: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 26 17:50:11.916: INFO: created pod pod-service-account-mountsa
Mar 26 17:50:11.917: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 26 17:50:11.959: INFO: created pod pod-service-account-nomountsa
Mar 26 17:50:11.959: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 26 17:50:12.004: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 26 17:50:12.004: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 26 17:50:12.048: INFO: created pod pod-service-account-mountsa-mountspec
Mar 26 17:50:12.048: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 26 17:50:12.093: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 26 17:50:12.093: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 26 17:50:12.133: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 26 17:50:12.133: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 26 17:50:12.172: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 26 17:50:12.172: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 26 17:50:12.212: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 26 17:50:12.212: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:50:12.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5119" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":277,"completed":38,"skipped":664,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:50:12.299: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:50:13.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841813, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841813, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841813, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841813, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:50:15.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841813, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841813, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841813, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841813, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:50:18.428: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar 26 17:50:20.694: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config attach --namespace=webhook-7155 to-be-attached-pod -i -c=container1'
Mar 26 17:50:21.024: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:50:21.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7155" for this suite.
STEP: Destroying namespace "webhook-7155-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":277,"completed":39,"skipped":672,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:50:21.421: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting the proxy server
Mar 26 17:50:21.573: INFO: Asynchronously running '/workspace/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:50:21.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4171" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":277,"completed":40,"skipped":685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:50:21.861: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:50:22.145: INFO: (0) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 46.346322ms)
Mar 26 17:50:22.186: INFO: (1) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 40.821032ms)
Mar 26 17:50:22.228: INFO: (2) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 42.125183ms)
Mar 26 17:50:22.270: INFO: (3) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 41.864459ms)
Mar 26 17:50:22.311: INFO: (4) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 40.629324ms)
Mar 26 17:50:22.351: INFO: (5) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 40.656278ms)
Mar 26 17:50:22.392: INFO: (6) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 40.218929ms)
Mar 26 17:50:22.432: INFO: (7) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 39.888398ms)
Mar 26 17:50:22.471: INFO: (8) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 39.494636ms)
Mar 26 17:50:22.512: INFO: (9) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 40.448319ms)
Mar 26 17:50:22.551: INFO: (10) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 39.344288ms)
Mar 26 17:50:22.591: INFO: (11) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 39.684254ms)
Mar 26 17:50:22.630: INFO: (12) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 39.284468ms)
Mar 26 17:50:22.670: INFO: (13) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 39.59112ms)
Mar 26 17:50:22.709: INFO: (14) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 39.558671ms)
Mar 26 17:50:22.751: INFO: (15) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 41.093693ms)
Mar 26 17:50:22.791: INFO: (16) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 39.948896ms)
Mar 26 17:50:22.830: INFO: (17) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 39.719366ms)
Mar 26 17:50:22.870: INFO: (18) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 39.55486ms)
Mar 26 17:50:22.910: INFO: (19) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 39.669655ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:50:22.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-204" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]","total":277,"completed":41,"skipped":708,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:50:22.993: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 26 17:50:23.228: INFO: Waiting up to 5m0s for pod "pod-fc6baac9-e2c0-4289-af76-e37b4005469e" in namespace "emptydir-2212" to be "Succeeded or Failed"
Mar 26 17:50:23.266: INFO: Pod "pod-fc6baac9-e2c0-4289-af76-e37b4005469e": Phase="Pending", Reason="", readiness=false. Elapsed: 38.123036ms
Mar 26 17:50:25.305: INFO: Pod "pod-fc6baac9-e2c0-4289-af76-e37b4005469e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077003286s
STEP: Saw pod success
Mar 26 17:50:25.305: INFO: Pod "pod-fc6baac9-e2c0-4289-af76-e37b4005469e" satisfied condition "Succeeded or Failed"
Mar 26 17:50:25.343: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-fc6baac9-e2c0-4289-af76-e37b4005469e container test-container: <nil>
STEP: delete the pod
Mar 26 17:50:25.453: INFO: Waiting for pod pod-fc6baac9-e2c0-4289-af76-e37b4005469e to disappear
Mar 26 17:50:25.491: INFO: Pod pod-fc6baac9-e2c0-4289-af76-e37b4005469e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:50:25.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2212" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":42,"skipped":719,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:50:25.573: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:51:25.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6533" for this suite.
•{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":277,"completed":43,"skipped":749,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:51:25.904: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap that has name configmap-test-emptyKey-2cbf10fa-caf8-4666-bf1b-0a1059458f93
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:51:26.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7369" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":277,"completed":44,"skipped":751,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:51:26.234: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:51:26.426: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:51:26.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1198" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":277,"completed":45,"skipped":783,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:51:26.862: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-0f317fe4-2508-433c-b5b2-853d88d96af1
STEP: Creating a pod to test consume secrets
Mar 26 17:51:27.318: INFO: Waiting up to 5m0s for pod "pod-secrets-48e3f389-22d3-429e-8dc8-5f81be38aec9" in namespace "secrets-897" to be "Succeeded or Failed"
Mar 26 17:51:27.357: INFO: Pod "pod-secrets-48e3f389-22d3-429e-8dc8-5f81be38aec9": Phase="Pending", Reason="", readiness=false. Elapsed: 38.995865ms
Mar 26 17:51:29.396: INFO: Pod "pod-secrets-48e3f389-22d3-429e-8dc8-5f81be38aec9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077739641s
STEP: Saw pod success
Mar 26 17:51:29.396: INFO: Pod "pod-secrets-48e3f389-22d3-429e-8dc8-5f81be38aec9" satisfied condition "Succeeded or Failed"
Mar 26 17:51:29.434: INFO: Trying to get logs from node bootstrap-e2e-minion-group-d5w7 pod pod-secrets-48e3f389-22d3-429e-8dc8-5f81be38aec9 container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 17:51:29.540: INFO: Waiting for pod pod-secrets-48e3f389-22d3-429e-8dc8-5f81be38aec9 to disappear
Mar 26 17:51:29.578: INFO: Pod pod-secrets-48e3f389-22d3-429e-8dc8-5f81be38aec9 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:51:29.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-897" for this suite.
STEP: Destroying namespace "secret-namespace-4377" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":277,"completed":46,"skipped":784,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:51:29.701: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:51:29.909: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar 26 17:51:31.459: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:51:31.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6283" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":277,"completed":47,"skipped":794,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:51:31.623: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:51:31.877: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-2a3023f2-4f4d-43fd-8a3e-71c83a31a4d8" in namespace "security-context-test-7470" to be "Succeeded or Failed"
Mar 26 17:51:31.930: INFO: Pod "busybox-readonly-false-2a3023f2-4f4d-43fd-8a3e-71c83a31a4d8": Phase="Pending", Reason="", readiness=false. Elapsed: 53.488842ms
Mar 26 17:51:33.969: INFO: Pod "busybox-readonly-false-2a3023f2-4f4d-43fd-8a3e-71c83a31a4d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.092287126s
Mar 26 17:51:33.969: INFO: Pod "busybox-readonly-false-2a3023f2-4f4d-43fd-8a3e-71c83a31a4d8" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:51:33.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7470" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":277,"completed":48,"skipped":823,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:51:34.051: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-ksxg
STEP: Creating a pod to test atomic-volume-subpath
Mar 26 17:51:34.334: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ksxg" in namespace "subpath-7197" to be "Succeeded or Failed"
Mar 26 17:51:34.373: INFO: Pod "pod-subpath-test-configmap-ksxg": Phase="Pending", Reason="", readiness=false. Elapsed: 38.504299ms
Mar 26 17:51:36.411: INFO: Pod "pod-subpath-test-configmap-ksxg": Phase="Running", Reason="", readiness=true. Elapsed: 2.076855733s
Mar 26 17:51:38.450: INFO: Pod "pod-subpath-test-configmap-ksxg": Phase="Running", Reason="", readiness=true. Elapsed: 4.115272001s
Mar 26 17:51:40.489: INFO: Pod "pod-subpath-test-configmap-ksxg": Phase="Running", Reason="", readiness=true. Elapsed: 6.15458469s
Mar 26 17:51:42.527: INFO: Pod "pod-subpath-test-configmap-ksxg": Phase="Running", Reason="", readiness=true. Elapsed: 8.192644616s
Mar 26 17:51:44.565: INFO: Pod "pod-subpath-test-configmap-ksxg": Phase="Running", Reason="", readiness=true. Elapsed: 10.230919193s
Mar 26 17:51:46.604: INFO: Pod "pod-subpath-test-configmap-ksxg": Phase="Running", Reason="", readiness=true. Elapsed: 12.269239227s
Mar 26 17:51:48.642: INFO: Pod "pod-subpath-test-configmap-ksxg": Phase="Running", Reason="", readiness=true. Elapsed: 14.307561031s
Mar 26 17:51:50.681: INFO: Pod "pod-subpath-test-configmap-ksxg": Phase="Running", Reason="", readiness=true. Elapsed: 16.346053429s
Mar 26 17:51:52.719: INFO: Pod "pod-subpath-test-configmap-ksxg": Phase="Running", Reason="", readiness=true. Elapsed: 18.384499931s
Mar 26 17:51:54.758: INFO: Pod "pod-subpath-test-configmap-ksxg": Phase="Running", Reason="", readiness=true. Elapsed: 20.422998925s
Mar 26 17:51:56.796: INFO: Pod "pod-subpath-test-configmap-ksxg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.461597809s
STEP: Saw pod success
Mar 26 17:51:56.796: INFO: Pod "pod-subpath-test-configmap-ksxg" satisfied condition "Succeeded or Failed"
Mar 26 17:51:56.837: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-subpath-test-configmap-ksxg container test-container-subpath-configmap-ksxg: <nil>
STEP: delete the pod
Mar 26 17:51:56.939: INFO: Waiting for pod pod-subpath-test-configmap-ksxg to disappear
Mar 26 17:51:56.977: INFO: Pod pod-subpath-test-configmap-ksxg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ksxg
Mar 26 17:51:56.977: INFO: Deleting pod "pod-subpath-test-configmap-ksxg" in namespace "subpath-7197"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:51:57.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7197" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":277,"completed":49,"skipped":849,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:51:57.100: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1288
STEP: creating an pod
Mar 26 17:51:57.256: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config run logs-generator --image=us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 --namespace=kubectl-528 -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 26 17:51:57.480: INFO: stderr: ""
Mar 26 17:51:57.480: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Waiting for log generator to start.
Mar 26 17:51:57.481: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 26 17:51:57.481: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-528" to be "running and ready, or succeeded"
Mar 26 17:51:57.519: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 38.350784ms
Mar 26 17:51:59.558: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.077807533s
Mar 26 17:51:59.558: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 26 17:51:59.558: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar 26 17:51:59.559: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config logs logs-generator logs-generator --namespace=kubectl-528'
Mar 26 17:51:59.808: INFO: stderr: ""
Mar 26 17:51:59.808: INFO: stdout: "I0326 17:51:58.363074       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/22r 592\nI0326 17:51:58.563289       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/x79 300\nI0326 17:51:58.763297       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/pxvw 533\nI0326 17:51:58.963281       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/lmm 569\nI0326 17:51:59.163321       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/65c 369\nI0326 17:51:59.363563       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/qhdj 538\nI0326 17:51:59.563512       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/4ps 309\nI0326 17:51:59.763476       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/hkv5 220\n"
STEP: limiting log lines
Mar 26 17:51:59.808: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config logs logs-generator logs-generator --namespace=kubectl-528 --tail=1'
Mar 26 17:52:00.069: INFO: stderr: ""
Mar 26 17:52:00.069: INFO: stdout: "I0326 17:51:59.963439       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/r5b 362\n"
Mar 26 17:52:00.069: INFO: got output "I0326 17:51:59.963439       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/r5b 362\n"
STEP: limiting log bytes
Mar 26 17:52:00.069: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config logs logs-generator logs-generator --namespace=kubectl-528 --limit-bytes=1'
Mar 26 17:52:00.314: INFO: stderr: ""
Mar 26 17:52:00.314: INFO: stdout: "I"
Mar 26 17:52:00.314: INFO: got output "I"
STEP: exposing timestamps
Mar 26 17:52:00.314: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config logs logs-generator logs-generator --namespace=kubectl-528 --tail=1 --timestamps'
Mar 26 17:52:00.570: INFO: stderr: ""
Mar 26 17:52:00.570: INFO: stdout: "2020-03-26T17:52:00.363560645Z I0326 17:52:00.363388       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/d4m 373\n"
Mar 26 17:52:00.570: INFO: got output "2020-03-26T17:52:00.363560645Z I0326 17:52:00.363388       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/d4m 373\n"
STEP: restricting to a time range
Mar 26 17:52:03.070: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config logs logs-generator logs-generator --namespace=kubectl-528 --since=1s'
Mar 26 17:52:03.326: INFO: stderr: ""
Mar 26 17:52:03.326: INFO: stdout: "I0326 17:52:02.363330       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/gmjd 275\nI0326 17:52:02.563319       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/dm8 433\nI0326 17:52:02.763422       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/ljvt 369\nI0326 17:52:02.963321       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/t4wd 412\nI0326 17:52:03.163324       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/rfp 367\n"
Mar 26 17:52:03.326: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config logs logs-generator logs-generator --namespace=kubectl-528 --since=24h'
Mar 26 17:52:03.568: INFO: stderr: ""
Mar 26 17:52:03.568: INFO: stdout: "I0326 17:51:58.363074       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/22r 592\nI0326 17:51:58.563289       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/x79 300\nI0326 17:51:58.763297       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/pxvw 533\nI0326 17:51:58.963281       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/lmm 569\nI0326 17:51:59.163321       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/65c 369\nI0326 17:51:59.363563       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/qhdj 538\nI0326 17:51:59.563512       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/4ps 309\nI0326 17:51:59.763476       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/hkv5 220\nI0326 17:51:59.963439       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/r5b 362\nI0326 17:52:00.163425       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/xnl 273\nI0326 17:52:00.363388       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/d4m 373\nI0326 17:52:00.563371       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/fk9d 392\nI0326 17:52:00.763374       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/cj65 376\nI0326 17:52:00.963367       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/jtp 497\nI0326 17:52:01.163323       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/5rc 370\nI0326 17:52:01.363277       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/29k 565\nI0326 17:52:01.563356       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/ppk 576\nI0326 17:52:01.763369       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/7rf9 591\nI0326 17:52:01.963344       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/9f4 238\nI0326 17:52:02.163304       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/b2z 331\nI0326 17:52:02.363330       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/gmjd 275\nI0326 17:52:02.563319       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/dm8 433\nI0326 17:52:02.763422       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/ljvt 369\nI0326 17:52:02.963321       1 logs_generator.go:76] 23 GET /api/v1/namespaces/default/pods/t4wd 412\nI0326 17:52:03.163324       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/rfp 367\nI0326 17:52:03.363283       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/z9pb 355\n"
[AfterEach] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1294
Mar 26 17:52:03.569: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config delete pod logs-generator --namespace=kubectl-528'
Mar 26 17:52:05.503: INFO: stderr: ""
Mar 26 17:52:05.503: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:52:05.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-528" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":277,"completed":50,"skipped":879,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:52:05.586: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 26 17:52:05.827: INFO: Waiting up to 5m0s for pod "pod-ce5a3965-8094-4a63-8cb3-d2e737fef2a8" in namespace "emptydir-1612" to be "Succeeded or Failed"
Mar 26 17:52:05.865: INFO: Pod "pod-ce5a3965-8094-4a63-8cb3-d2e737fef2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 37.795034ms
Mar 26 17:52:07.903: INFO: Pod "pod-ce5a3965-8094-4a63-8cb3-d2e737fef2a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076288599s
STEP: Saw pod success
Mar 26 17:52:07.903: INFO: Pod "pod-ce5a3965-8094-4a63-8cb3-d2e737fef2a8" satisfied condition "Succeeded or Failed"
Mar 26 17:52:07.941: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-ce5a3965-8094-4a63-8cb3-d2e737fef2a8 container test-container: <nil>
STEP: delete the pod
Mar 26 17:52:08.039: INFO: Waiting for pod pod-ce5a3965-8094-4a63-8cb3-d2e737fef2a8 to disappear
Mar 26 17:52:08.080: INFO: Pod pod-ce5a3965-8094-4a63-8cb3-d2e737fef2a8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:52:08.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1612" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":51,"skipped":891,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:52:08.164: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:52:08.448: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 26 17:52:10.525: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 26 17:52:12.563: INFO: Creating deployment "test-rollover-deployment"
Mar 26 17:52:12.681: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 26 17:52:14.768: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 26 17:52:14.845: INFO: Ensure that both replica sets have 1 created replica
Mar 26 17:52:14.921: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 26 17:52:15.002: INFO: Updating deployment test-rollover-deployment
Mar 26 17:52:15.002: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 26 17:52:17.086: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 26 17:52:17.165: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 26 17:52:17.245: INFO: all replica sets need to contain the pod-template-hash label
Mar 26 17:52:17.245: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841932, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841932, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841936, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841932, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:52:19.323: INFO: all replica sets need to contain the pod-template-hash label
Mar 26 17:52:19.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841932, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841932, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841936, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841932, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:52:21.324: INFO: all replica sets need to contain the pod-template-hash label
Mar 26 17:52:21.324: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841932, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841932, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841936, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841932, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:52:23.323: INFO: all replica sets need to contain the pod-template-hash label
Mar 26 17:52:23.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841932, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841932, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841936, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841932, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:52:25.323: INFO: all replica sets need to contain the pod-template-hash label
Mar 26 17:52:25.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841932, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841932, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841936, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720841932, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-84f7f6f64b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 17:52:27.323: INFO: 
Mar 26 17:52:27.323: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar 26 17:52:27.438: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8744 /apis/apps/v1/namespaces/deployment-8744/deployments/test-rollover-deployment fdd07fd2-7315-404b-89fc-e836f5ced0db 6055 2 2020-03-26 17:52:12 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-03-26 17:52:14 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-03-26 17:52:26 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005b4c9b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-26 17:52:12 +0000 UTC,LastTransitionTime:2020-03-26 17:52:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-84f7f6f64b" has successfully progressed.,LastUpdateTime:2020-03-26 17:52:26 +0000 UTC,LastTransitionTime:2020-03-26 17:52:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 26 17:52:27.478: INFO: New ReplicaSet "test-rollover-deployment-84f7f6f64b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-84f7f6f64b  deployment-8744 /apis/apps/v1/namespaces/deployment-8744/replicasets/test-rollover-deployment-84f7f6f64b e722e805-6886-4b2c-b66b-bd377e30d86e 6048 2 2020-03-26 17:52:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment fdd07fd2-7315-404b-89fc-e836f5ced0db 0xc005b4cfe7 0xc005b4cfe8}] []  [{kube-controller-manager Update apps/v1 2020-03-26 17:52:26 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 100 100 48 55 102 100 50 45 55 51 49 53 45 52 48 52 98 45 56 57 102 99 45 101 56 51 54 102 53 99 101 100 48 100 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 84f7f6f64b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005b4d078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 26 17:52:27.478: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 26 17:52:27.478: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8744 /apis/apps/v1/namespaces/deployment-8744/replicasets/test-rollover-controller 93bc4173-26a8-4797-9dd6-7d65963f9ee6 6054 2 2020-03-26 17:52:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment fdd07fd2-7315-404b-89fc-e836f5ced0db 0xc005b4cdef 0xc005b4ce00}] []  [{e2e.test Update apps/v1 2020-03-26 17:52:08 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-03-26 17:52:26 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 100 100 48 55 102 100 50 45 55 51 49 53 45 52 48 52 98 45 56 57 102 99 45 101 56 51 54 102 53 99 101 100 48 100 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005b4ce98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 26 17:52:27.478: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-5686c4cfd5  deployment-8744 /apis/apps/v1/namespaces/deployment-8744/replicasets/test-rollover-deployment-5686c4cfd5 1f21ee68-50d1-4836-ba72-e19142efb546 6003 2 2020-03-26 17:52:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment fdd07fd2-7315-404b-89fc-e836f5ced0db 0xc005b4cef7 0xc005b4cef8}] []  [{kube-controller-manager Update apps/v1 2020-03-26 17:52:15 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 102 100 100 48 55 102 100 50 45 55 51 49 53 45 52 48 52 98 45 56 57 102 99 45 101 56 51 54 102 53 99 101 100 48 100 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 109 105 110 82 101 97 100 121 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 114 101 100 105 115 45 115 108 97 118 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5686c4cfd5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5686c4cfd5] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005b4cf88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 26 17:52:27.517: INFO: Pod "test-rollover-deployment-84f7f6f64b-6gkqt" is available:
&Pod{ObjectMeta:{test-rollover-deployment-84f7f6f64b-6gkqt test-rollover-deployment-84f7f6f64b- deployment-8744 /api/v1/namespaces/deployment-8744/pods/test-rollover-deployment-84f7f6f64b-6gkqt 9ac85c0b-b1f6-40f2-b7b6-9981d55932a6 6012 0 2020-03-26 17:52:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:84f7f6f64b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-84f7f6f64b e722e805-6886-4b2c-b66b-bd377e30d86e 0xc005b4d687 0xc005b4d688}] []  [{kube-controller-manager Update v1 2020-03-26 17:52:15 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 55 50 50 101 56 48 53 45 54 56 56 54 45 52 98 50 99 45 98 54 54 98 45 98 100 51 55 55 101 51 48 100 56 54 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:52:16 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 54 52 46 49 46 51 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bmjx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bmjx6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bmjx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-1r9w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:52:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:52:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:52:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:52:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:10.64.1.33,StartTime:2020-03-26 17:52:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 17:52:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://8da781554edcb7d39ac8170b924d60604fc886e6ee7122e736238fb4b7233c56,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.1.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:52:27.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8744" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":277,"completed":52,"skipped":903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:52:27.601: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:52:27.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8425" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":277,"completed":53,"skipped":947,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:52:27.929: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:52:28.225: INFO: The status of Pod test-webserver-1a3dc471-546c-4058-b4a1-e1fab702833e is Pending, waiting for it to be Running (with Ready = true)
Mar 26 17:52:30.264: INFO: The status of Pod test-webserver-1a3dc471-546c-4058-b4a1-e1fab702833e is Running (Ready = false)
Mar 26 17:52:32.264: INFO: The status of Pod test-webserver-1a3dc471-546c-4058-b4a1-e1fab702833e is Running (Ready = false)
Mar 26 17:52:34.263: INFO: The status of Pod test-webserver-1a3dc471-546c-4058-b4a1-e1fab702833e is Running (Ready = false)
Mar 26 17:52:36.263: INFO: The status of Pod test-webserver-1a3dc471-546c-4058-b4a1-e1fab702833e is Running (Ready = false)
Mar 26 17:52:38.263: INFO: The status of Pod test-webserver-1a3dc471-546c-4058-b4a1-e1fab702833e is Running (Ready = false)
Mar 26 17:52:40.263: INFO: The status of Pod test-webserver-1a3dc471-546c-4058-b4a1-e1fab702833e is Running (Ready = false)
Mar 26 17:52:42.263: INFO: The status of Pod test-webserver-1a3dc471-546c-4058-b4a1-e1fab702833e is Running (Ready = false)
Mar 26 17:52:44.264: INFO: The status of Pod test-webserver-1a3dc471-546c-4058-b4a1-e1fab702833e is Running (Ready = false)
Mar 26 17:52:46.264: INFO: The status of Pod test-webserver-1a3dc471-546c-4058-b4a1-e1fab702833e is Running (Ready = false)
Mar 26 17:52:48.264: INFO: The status of Pod test-webserver-1a3dc471-546c-4058-b4a1-e1fab702833e is Running (Ready = false)
Mar 26 17:52:50.263: INFO: The status of Pod test-webserver-1a3dc471-546c-4058-b4a1-e1fab702833e is Running (Ready = true)
Mar 26 17:52:50.301: INFO: Container started at 2020-03-26 17:52:29 +0000 UTC, pod became ready at 2020-03-26 17:52:49 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:52:50.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6218" for this suite.
•{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":277,"completed":54,"skipped":956,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:52:50.384: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:53:04.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-262" for this suite.
STEP: Destroying namespace "nsdeletetest-1455" for this suite.
Mar 26 17:53:04.211: INFO: Namespace nsdeletetest-1455 was already deleted
STEP: Destroying namespace "nsdeletetest-8183" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":277,"completed":55,"skipped":970,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:53:04.250: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-2283
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Mar 26 17:53:04.598: INFO: Found 1 stateful pods, waiting for 3
Mar 26 17:53:14.641: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 17:53:14.641: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 17:53:14.641: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 26 17:53:14.860: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 26 17:53:15.034: INFO: Updating stateful set ss2
Mar 26 17:53:15.117: INFO: Waiting for Pod statefulset-2283/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar 26 17:53:25.416: INFO: Found 2 stateful pods, waiting for 3
Mar 26 17:53:35.456: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 17:53:35.456: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 17:53:35.456: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 26 17:53:35.627: INFO: Updating stateful set ss2
Mar 26 17:53:35.724: INFO: Waiting for Pod statefulset-2283/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 26 17:53:45.891: INFO: Updating stateful set ss2
Mar 26 17:53:45.970: INFO: Waiting for StatefulSet statefulset-2283/ss2 to complete update
Mar 26 17:53:45.970: INFO: Waiting for Pod statefulset-2283/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 26 17:53:56.057: INFO: Waiting for StatefulSet statefulset-2283/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 26 17:54:06.058: INFO: Deleting all statefulset in ns statefulset-2283
Mar 26 17:54:06.096: INFO: Scaling statefulset ss2 to 0
Mar 26 17:54:26.260: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 17:54:26.298: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:54:26.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2283" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":277,"completed":56,"skipped":977,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:54:26.498: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:54:26.739: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d69a84e8-527e-45bb-be3b-cdfee89e6b92" in namespace "projected-6411" to be "Succeeded or Failed"
Mar 26 17:54:26.777: INFO: Pod "downwardapi-volume-d69a84e8-527e-45bb-be3b-cdfee89e6b92": Phase="Pending", Reason="", readiness=false. Elapsed: 37.982522ms
Mar 26 17:54:28.815: INFO: Pod "downwardapi-volume-d69a84e8-527e-45bb-be3b-cdfee89e6b92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076449016s
STEP: Saw pod success
Mar 26 17:54:28.816: INFO: Pod "downwardapi-volume-d69a84e8-527e-45bb-be3b-cdfee89e6b92" satisfied condition "Succeeded or Failed"
Mar 26 17:54:28.854: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod downwardapi-volume-d69a84e8-527e-45bb-be3b-cdfee89e6b92 container client-container: <nil>
STEP: delete the pod
Mar 26 17:54:28.953: INFO: Waiting for pod downwardapi-volume-d69a84e8-527e-45bb-be3b-cdfee89e6b92 to disappear
Mar 26 17:54:28.990: INFO: Pod downwardapi-volume-d69a84e8-527e-45bb-be3b-cdfee89e6b92 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:54:28.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6411" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":57,"skipped":977,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:54:29.072: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:54:46.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6017" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":277,"completed":58,"skipped":999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:54:46.643: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 26 17:54:46.880: INFO: Waiting up to 5m0s for pod "pod-029287af-e4fb-4156-b90a-cdd974a39c7e" in namespace "emptydir-7166" to be "Succeeded or Failed"
Mar 26 17:54:46.918: INFO: Pod "pod-029287af-e4fb-4156-b90a-cdd974a39c7e": Phase="Pending", Reason="", readiness=false. Elapsed: 37.836909ms
Mar 26 17:54:48.958: INFO: Pod "pod-029287af-e4fb-4156-b90a-cdd974a39c7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077321179s
Mar 26 17:54:50.996: INFO: Pod "pod-029287af-e4fb-4156-b90a-cdd974a39c7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.115715174s
STEP: Saw pod success
Mar 26 17:54:50.996: INFO: Pod "pod-029287af-e4fb-4156-b90a-cdd974a39c7e" satisfied condition "Succeeded or Failed"
Mar 26 17:54:51.035: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-029287af-e4fb-4156-b90a-cdd974a39c7e container test-container: <nil>
STEP: delete the pod
Mar 26 17:54:51.129: INFO: Waiting for pod pod-029287af-e4fb-4156-b90a-cdd974a39c7e to disappear
Mar 26 17:54:51.172: INFO: Pod pod-029287af-e4fb-4156-b90a-cdd974a39c7e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:54:51.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7166" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":59,"skipped":1021,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:54:51.255: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 26 17:54:51.496: INFO: Waiting up to 5m0s for pod "pod-8e60fee2-0955-44f9-874f-076b8633495c" in namespace "emptydir-7987" to be "Succeeded or Failed"
Mar 26 17:54:51.536: INFO: Pod "pod-8e60fee2-0955-44f9-874f-076b8633495c": Phase="Pending", Reason="", readiness=false. Elapsed: 40.159737ms
Mar 26 17:54:53.575: INFO: Pod "pod-8e60fee2-0955-44f9-874f-076b8633495c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.078784623s
STEP: Saw pod success
Mar 26 17:54:53.575: INFO: Pod "pod-8e60fee2-0955-44f9-874f-076b8633495c" satisfied condition "Succeeded or Failed"
Mar 26 17:54:53.614: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-8e60fee2-0955-44f9-874f-076b8633495c container test-container: <nil>
STEP: delete the pod
Mar 26 17:54:53.708: INFO: Waiting for pod pod-8e60fee2-0955-44f9-874f-076b8633495c to disappear
Mar 26 17:54:53.746: INFO: Pod pod-8e60fee2-0955-44f9-874f-076b8633495c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:54:53.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7987" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":60,"skipped":1021,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:54:53.829: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:54:54.289: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8832f113-52f9-4d64-a247-d62b65ca17ac", Controller:(*bool)(0xc0012155de), BlockOwnerDeletion:(*bool)(0xc0012155df)}}
Mar 26 17:54:54.331: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"74826b9b-83f6-4f27-ab97-f423725c0934", Controller:(*bool)(0xc001215d76), BlockOwnerDeletion:(*bool)(0xc001215d77)}}
Mar 26 17:54:54.372: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"aa32c592-f97c-486c-a63c-b85de0e7688f", Controller:(*bool)(0xc000434e96), BlockOwnerDeletion:(*bool)(0xc000434e97)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:54:59.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2225" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":277,"completed":61,"skipped":1036,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:54:59.534: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:55:11.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3201" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":277,"completed":62,"skipped":1040,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:55:11.114: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override all
Mar 26 17:55:11.373: INFO: Waiting up to 5m0s for pod "client-containers-6972d8f8-a71e-4ab1-83ef-47899655056a" in namespace "containers-2237" to be "Succeeded or Failed"
Mar 26 17:55:11.412: INFO: Pod "client-containers-6972d8f8-a71e-4ab1-83ef-47899655056a": Phase="Pending", Reason="", readiness=false. Elapsed: 39.387064ms
Mar 26 17:55:13.450: INFO: Pod "client-containers-6972d8f8-a71e-4ab1-83ef-47899655056a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077906051s
STEP: Saw pod success
Mar 26 17:55:13.450: INFO: Pod "client-containers-6972d8f8-a71e-4ab1-83ef-47899655056a" satisfied condition "Succeeded or Failed"
Mar 26 17:55:13.489: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod client-containers-6972d8f8-a71e-4ab1-83ef-47899655056a container test-container: <nil>
STEP: delete the pod
Mar 26 17:55:13.579: INFO: Waiting for pod client-containers-6972d8f8-a71e-4ab1-83ef-47899655056a to disappear
Mar 26 17:55:13.617: INFO: Pod client-containers-6972d8f8-a71e-4ab1-83ef-47899655056a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:55:13.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2237" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":277,"completed":63,"skipped":1072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:55:13.700: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:55:15.589: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842115, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842115, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842115, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842115, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:55:18.674: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:55:18.738: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:55:20.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8942" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":277,"completed":64,"skipped":1117,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:55:21.849: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 26 17:55:25.082: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e7b4ffae-a066-4beb-a205-c338378399ae"
Mar 26 17:55:25.082: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e7b4ffae-a066-4beb-a205-c338378399ae" in namespace "pods-1344" to be "terminated due to deadline exceeded"
Mar 26 17:55:25.120: INFO: Pod "pod-update-activedeadlineseconds-e7b4ffae-a066-4beb-a205-c338378399ae": Phase="Running", Reason="", readiness=true. Elapsed: 37.94741ms
Mar 26 17:55:27.158: INFO: Pod "pod-update-activedeadlineseconds-e7b4ffae-a066-4beb-a205-c338378399ae": Phase="Running", Reason="", readiness=true. Elapsed: 2.076046031s
Mar 26 17:55:29.196: INFO: Pod "pod-update-activedeadlineseconds-e7b4ffae-a066-4beb-a205-c338378399ae": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.114471684s
Mar 26 17:55:29.196: INFO: Pod "pod-update-activedeadlineseconds-e7b4ffae-a066-4beb-a205-c338378399ae" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:55:29.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1344" for this suite.
•{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":277,"completed":65,"skipped":1125,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:55:29.280: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:55:29.436: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar 26 17:55:33.001: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-1519 create -f -'
Mar 26 17:55:33.796: INFO: stderr: ""
Mar 26 17:55:33.796: INFO: stdout: "e2e-test-crd-publish-openapi-1749-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 26 17:55:33.797: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-1519 delete e2e-test-crd-publish-openapi-1749-crds test-foo'
Mar 26 17:55:34.052: INFO: stderr: ""
Mar 26 17:55:34.052: INFO: stdout: "e2e-test-crd-publish-openapi-1749-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 26 17:55:34.052: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-1519 apply -f -'
Mar 26 17:55:34.511: INFO: stderr: ""
Mar 26 17:55:34.511: INFO: stdout: "e2e-test-crd-publish-openapi-1749-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 26 17:55:34.511: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-1519 delete e2e-test-crd-publish-openapi-1749-crds test-foo'
Mar 26 17:55:34.758: INFO: stderr: ""
Mar 26 17:55:34.758: INFO: stdout: "e2e-test-crd-publish-openapi-1749-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar 26 17:55:34.758: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-1519 create -f -'
Mar 26 17:55:35.079: INFO: rc: 1
Mar 26 17:55:35.079: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-1519 apply -f -'
Mar 26 17:55:35.419: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar 26 17:55:35.419: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-1519 create -f -'
Mar 26 17:55:35.721: INFO: rc: 1
Mar 26 17:55:35.721: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-1519 apply -f -'
Mar 26 17:55:36.045: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar 26 17:55:36.045: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config explain e2e-test-crd-publish-openapi-1749-crds'
Mar 26 17:55:36.394: INFO: stderr: ""
Mar 26 17:55:36.394: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1749-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar 26 17:55:36.394: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config explain e2e-test-crd-publish-openapi-1749-crds.metadata'
Mar 26 17:55:36.714: INFO: stderr: ""
Mar 26 17:55:36.715: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1749-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 26 17:55:36.715: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config explain e2e-test-crd-publish-openapi-1749-crds.spec'
Mar 26 17:55:37.092: INFO: stderr: ""
Mar 26 17:55:37.092: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1749-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 26 17:55:37.092: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config explain e2e-test-crd-publish-openapi-1749-crds.spec.bars'
Mar 26 17:55:37.465: INFO: stderr: ""
Mar 26 17:55:37.465: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1749-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar 26 17:55:37.465: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config explain e2e-test-crd-publish-openapi-1749-crds.spec.bars2'
Mar 26 17:55:37.812: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:55:41.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1519" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":277,"completed":66,"skipped":1149,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:55:41.563: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-13eae8c1-34e4-49f6-ae0f-2c50655381c1
STEP: Creating a pod to test consume configMaps
Mar 26 17:55:41.838: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b3e884ac-e6c9-4d2e-84fe-253aae2d9f20" in namespace "projected-9235" to be "Succeeded or Failed"
Mar 26 17:55:41.877: INFO: Pod "pod-projected-configmaps-b3e884ac-e6c9-4d2e-84fe-253aae2d9f20": Phase="Pending", Reason="", readiness=false. Elapsed: 38.903144ms
Mar 26 17:55:43.916: INFO: Pod "pod-projected-configmaps-b3e884ac-e6c9-4d2e-84fe-253aae2d9f20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077948512s
STEP: Saw pod success
Mar 26 17:55:43.916: INFO: Pod "pod-projected-configmaps-b3e884ac-e6c9-4d2e-84fe-253aae2d9f20" satisfied condition "Succeeded or Failed"
Mar 26 17:55:43.955: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-projected-configmaps-b3e884ac-e6c9-4d2e-84fe-253aae2d9f20 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 17:55:44.047: INFO: Waiting for pod pod-projected-configmaps-b3e884ac-e6c9-4d2e-84fe-253aae2d9f20 to disappear
Mar 26 17:55:44.085: INFO: Pod pod-projected-configmaps-b3e884ac-e6c9-4d2e-84fe-253aae2d9f20 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:55:44.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9235" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":67,"skipped":1174,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:55:44.170: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-downwardapi-jdxj
STEP: Creating a pod to test atomic-volume-subpath
Mar 26 17:55:44.448: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-jdxj" in namespace "subpath-1632" to be "Succeeded or Failed"
Mar 26 17:55:44.486: INFO: Pod "pod-subpath-test-downwardapi-jdxj": Phase="Pending", Reason="", readiness=false. Elapsed: 38.040166ms
Mar 26 17:55:46.525: INFO: Pod "pod-subpath-test-downwardapi-jdxj": Phase="Running", Reason="", readiness=true. Elapsed: 2.076849247s
Mar 26 17:55:48.563: INFO: Pod "pod-subpath-test-downwardapi-jdxj": Phase="Running", Reason="", readiness=true. Elapsed: 4.114837875s
Mar 26 17:55:50.602: INFO: Pod "pod-subpath-test-downwardapi-jdxj": Phase="Running", Reason="", readiness=true. Elapsed: 6.154200592s
Mar 26 17:55:52.641: INFO: Pod "pod-subpath-test-downwardapi-jdxj": Phase="Running", Reason="", readiness=true. Elapsed: 8.193443821s
Mar 26 17:55:54.679: INFO: Pod "pod-subpath-test-downwardapi-jdxj": Phase="Running", Reason="", readiness=true. Elapsed: 10.231451107s
Mar 26 17:55:56.717: INFO: Pod "pod-subpath-test-downwardapi-jdxj": Phase="Running", Reason="", readiness=true. Elapsed: 12.269577272s
Mar 26 17:55:58.755: INFO: Pod "pod-subpath-test-downwardapi-jdxj": Phase="Running", Reason="", readiness=true. Elapsed: 14.307661674s
Mar 26 17:56:00.794: INFO: Pod "pod-subpath-test-downwardapi-jdxj": Phase="Running", Reason="", readiness=true. Elapsed: 16.345869826s
Mar 26 17:56:02.831: INFO: Pod "pod-subpath-test-downwardapi-jdxj": Phase="Running", Reason="", readiness=true. Elapsed: 18.383674938s
Mar 26 17:56:04.870: INFO: Pod "pod-subpath-test-downwardapi-jdxj": Phase="Running", Reason="", readiness=true. Elapsed: 20.421841579s
Mar 26 17:56:06.911: INFO: Pod "pod-subpath-test-downwardapi-jdxj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.463402808s
STEP: Saw pod success
Mar 26 17:56:06.911: INFO: Pod "pod-subpath-test-downwardapi-jdxj" satisfied condition "Succeeded or Failed"
Mar 26 17:56:06.949: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-subpath-test-downwardapi-jdxj container test-container-subpath-downwardapi-jdxj: <nil>
STEP: delete the pod
Mar 26 17:56:07.046: INFO: Waiting for pod pod-subpath-test-downwardapi-jdxj to disappear
Mar 26 17:56:07.084: INFO: Pod pod-subpath-test-downwardapi-jdxj no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-jdxj
Mar 26 17:56:07.084: INFO: Deleting pod "pod-subpath-test-downwardapi-jdxj" in namespace "subpath-1632"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:56:07.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1632" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":277,"completed":68,"skipped":1193,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:56:07.209: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:56:07.450: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 26 17:56:09.526: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar 26 17:56:11.855: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4371 /apis/apps/v1/namespaces/deployment-4371/deployments/test-cleanup-deployment b1f584f4-e6f3-47f6-ae5e-2515a8c1d19f 7275 1 2020-03-26 17:56:09 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2020-03-26 17:56:09 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-03-26 17:56:10 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005793338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-26 17:56:09 +0000 UTC,LastTransitionTime:2020-03-26 17:56:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-b4867b47f" has successfully progressed.,LastUpdateTime:2020-03-26 17:56:10 +0000 UTC,LastTransitionTime:2020-03-26 17:56:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 26 17:56:11.893: INFO: New ReplicaSet "test-cleanup-deployment-b4867b47f" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-b4867b47f  deployment-4371 /apis/apps/v1/namespaces/deployment-4371/replicasets/test-cleanup-deployment-b4867b47f cc7f94f9-34c0-4e30-abad-93e97e83972b 7268 1 2020-03-26 17:56:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment b1f584f4-e6f3-47f6-ae5e-2515a8c1d19f 0xc0057938b0 0xc0057938b1}] []  [{kube-controller-manager Update apps/v1 2020-03-26 17:56:10 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 98 49 102 53 56 52 102 52 45 101 54 102 51 45 52 55 102 54 45 97 101 53 101 45 50 53 49 53 97 56 99 49 100 49 57 102 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: b4867b47f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005793938 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 26 17:56:11.932: INFO: Pod "test-cleanup-deployment-b4867b47f-f2mh6" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-b4867b47f-f2mh6 test-cleanup-deployment-b4867b47f- deployment-4371 /api/v1/namespaces/deployment-4371/pods/test-cleanup-deployment-b4867b47f-f2mh6 118fbc54-796c-4439-95a6-61710c013d61 7267 0 2020-03-26 17:56:09 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:b4867b47f] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-b4867b47f cc7f94f9-34c0-4e30-abad-93e97e83972b 0xc00573f440 0xc00573f441}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:09 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 99 99 55 102 57 52 102 57 45 51 52 99 48 45 52 101 51 48 45 97 98 97 100 45 57 51 101 57 55 101 56 51 57 55 50 98 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:10 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 54 52 46 48 46 51 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f92r5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f92r5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f92r5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-d5w7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:10.64.0.32,StartTime:2020-03-26 17:56:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 17:56:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,ImageID:docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c,ContainerID:docker://e642394f31a5751df083a113096afd7d5558d955b971ab483ac59ee0af284a2f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.0.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:56:11.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4371" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":277,"completed":69,"skipped":1208,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:56:12.017: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:56:12.174: INFO: Creating deployment "webserver-deployment"
Mar 26 17:56:12.219: INFO: Waiting for observed generation 1
Mar 26 17:56:14.312: INFO: Waiting for all required pods to come up
Mar 26 17:56:14.354: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 26 17:56:16.462: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 26 17:56:16.538: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 26 17:56:16.618: INFO: Updating deployment webserver-deployment
Mar 26 17:56:16.618: INFO: Waiting for observed generation 2
Mar 26 17:56:18.725: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 26 17:56:18.763: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 26 17:56:18.800: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 26 17:56:18.916: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 26 17:56:18.916: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 26 17:56:18.954: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 26 17:56:19.030: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 26 17:56:19.030: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 26 17:56:19.111: INFO: Updating deployment webserver-deployment
Mar 26 17:56:19.111: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 26 17:56:19.308: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 26 17:56:19.409: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar 26 17:56:19.570: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3580 /apis/apps/v1/namespaces/deployment-3580/deployments/webserver-deployment 9535896d-c2b7-4b50-bec0-ab93edaaf38e 7479 3 2020-03-26 17:56:12 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 114 111 108 108 105 110 103 85 112 100 97 116 101 34 58 123 34 46 34 58 123 125 44 34 102 58 109 97 120 83 117 114 103 101 34 58 123 125 44 34 102 58 109 97 120 85 110 97 118 97 105 108 97 98 108 101 34 58 123 125 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005907788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-26 17:56:19 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-6676bcd6d4" is progressing.,LastUpdateTime:2020-03-26 17:56:19 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 26 17:56:19.617: INFO: New ReplicaSet "webserver-deployment-6676bcd6d4" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-6676bcd6d4  deployment-3580 /apis/apps/v1/namespaces/deployment-3580/replicasets/webserver-deployment-6676bcd6d4 e32bed2f-4265-4a2b-8252-5751d795cfa6 7475 3 2020-03-26 17:56:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 9535896d-c2b7-4b50-bec0-ab93edaaf38e 0xc005932717 0xc005932718}] []  [{kube-controller-manager Update apps/v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 53 51 53 56 57 54 100 45 99 50 98 55 45 52 98 53 48 45 98 101 99 48 45 97 98 57 51 101 100 97 97 102 51 56 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 6676bcd6d4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0059327a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 26 17:56:19.617: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 26 17:56:19.617: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-84855cf797  deployment-3580 /apis/apps/v1/namespaces/deployment-3580/replicasets/webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 7470 3 2020-03-26 17:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 9535896d-c2b7-4b50-bec0-ab93edaaf38e 0xc005932807 0xc005932808}] []  [{kube-controller-manager Update apps/v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 57 53 51 53 56 57 54 100 45 99 50 98 55 45 52 98 53 48 45 98 101 99 48 45 97 98 57 51 101 100 97 97 102 51 56 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 97 100 121 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 84855cf797,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005932888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 26 17:56:19.723: INFO: Pod "webserver-deployment-6676bcd6d4-8ls8w" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-8ls8w webserver-deployment-6676bcd6d4- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-6676bcd6d4-8ls8w affd59ff-2e08-4353-8c0b-714878693091 7466 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e32bed2f-4265-4a2b-8252-5751d795cfa6 0xc0058cd5c0 0xc0058cd5c1}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 50 98 101 100 50 102 45 52 50 54 53 45 52 97 50 98 45 56 50 53 50 45 53 55 53 49 100 55 57 53 99 102 97 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-1r9w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.723: INFO: Pod "webserver-deployment-6676bcd6d4-gj7ln" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-gj7ln webserver-deployment-6676bcd6d4- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-6676bcd6d4-gj7ln 7bb7ea59-166a-4c11-9029-1f769da113fd 7458 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e32bed2f-4265-4a2b-8252-5751d795cfa6 0xc0058cd6f0 0xc0058cd6f1}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 50 98 101 100 50 102 45 52 50 54 53 45 52 97 50 98 45 56 50 53 50 45 53 55 53 49 100 55 57 53 99 102 97 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-1r9w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.723: INFO: Pod "webserver-deployment-6676bcd6d4-kdr5z" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-kdr5z webserver-deployment-6676bcd6d4- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-6676bcd6d4-kdr5z e78ecc30-3e84-4446-b92b-63e983ba5717 7481 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e32bed2f-4265-4a2b-8252-5751d795cfa6 0xc0058cd820 0xc0058cd821}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 50 98 101 100 50 102 45 52 50 54 53 45 52 97 50 98 45 56 50 53 50 45 53 55 53 49 100 55 57 53 99 102 97 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-gs7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2020-03-26 17:56:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.723: INFO: Pod "webserver-deployment-6676bcd6d4-kwxk4" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-kwxk4 webserver-deployment-6676bcd6d4- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-6676bcd6d4-kwxk4 e4452362-3ed4-4c0d-a8a4-7473acc333e3 7392 0 2020-03-26 17:56:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e32bed2f-4265-4a2b-8252-5751d795cfa6 0xc0058cda00 0xc0058cda01}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:16 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 50 98 101 100 50 102 45 52 50 54 53 45 52 97 50 98 45 56 50 53 50 45 53 55 53 49 100 55 57 53 99 102 97 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:16 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-d5w7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2020-03-26 17:56:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.723: INFO: Pod "webserver-deployment-6676bcd6d4-lhm48" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-lhm48 webserver-deployment-6676bcd6d4- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-6676bcd6d4-lhm48 88a6b799-cc86-4fec-9b48-81ffae5abf27 7485 0 2020-03-26 17:56:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e32bed2f-4265-4a2b-8252-5751d795cfa6 0xc0058cdbc0 0xc0058cdbc1}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:16 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 50 98 101 100 50 102 45 52 50 54 53 45 52 97 50 98 45 56 50 53 50 45 53 55 53 49 100 55 57 53 99 102 97 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 54 52 46 50 46 50 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-gs7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.2.26,StartTime:2020-03-26 17:56:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.2.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.724: INFO: Pod "webserver-deployment-6676bcd6d4-lv85m" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-lv85m webserver-deployment-6676bcd6d4- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-6676bcd6d4-lv85m 38e77c06-53ab-4582-9ee7-ab53071e039e 7405 0 2020-03-26 17:56:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e32bed2f-4265-4a2b-8252-5751d795cfa6 0xc0058cddd0 0xc0058cddd1}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:16 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 50 98 101 100 50 102 45 52 50 54 53 45 52 97 50 98 45 56 50 53 50 45 53 55 53 49 100 55 57 53 99 102 97 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:16 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-1r9w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2020-03-26 17:56:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.724: INFO: Pod "webserver-deployment-6676bcd6d4-m4pbv" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-m4pbv webserver-deployment-6676bcd6d4- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-6676bcd6d4-m4pbv 85d3ee67-541c-4969-82c4-1e28281da921 7468 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e32bed2f-4265-4a2b-8252-5751d795cfa6 0xc0058cdfb0 0xc0058cdfb1}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 50 98 101 100 50 102 45 52 50 54 53 45 52 97 50 98 45 56 50 53 50 45 53 55 53 49 100 55 57 53 99 102 97 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-d5w7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2020-03-26 17:56:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.724: INFO: Pod "webserver-deployment-6676bcd6d4-mx8fj" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-mx8fj webserver-deployment-6676bcd6d4- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-6676bcd6d4-mx8fj 26a29a42-08ea-4a51-b26e-942e2c4ba921 7473 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e32bed2f-4265-4a2b-8252-5751d795cfa6 0xc00595a180 0xc00595a181}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 50 98 101 100 50 102 45 52 50 54 53 45 52 97 50 98 45 56 50 53 50 45 53 55 53 49 100 55 57 53 99 102 97 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-1r9w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.727: INFO: Pod "webserver-deployment-6676bcd6d4-pqkd4" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-pqkd4 webserver-deployment-6676bcd6d4- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-6676bcd6d4-pqkd4 d7d81971-7bfc-49e5-9de7-c8abaa7bf3dd 7471 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e32bed2f-4265-4a2b-8252-5751d795cfa6 0xc00595a2e0 0xc00595a2e1}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 50 98 101 100 50 102 45 52 50 54 53 45 52 97 50 98 45 56 50 53 50 45 53 55 53 49 100 55 57 53 99 102 97 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-gs7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2020-03-26 17:56:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.737: INFO: Pod "webserver-deployment-6676bcd6d4-qjnmk" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-qjnmk webserver-deployment-6676bcd6d4- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-6676bcd6d4-qjnmk abe65135-4041-490c-ae2e-08b806e26c3b 7467 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e32bed2f-4265-4a2b-8252-5751d795cfa6 0xc00595a4b0 0xc00595a4b1}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 50 98 101 100 50 102 45 52 50 54 53 45 52 97 50 98 45 56 50 53 50 45 53 55 53 49 100 55 57 53 99 102 97 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-d5w7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.740: INFO: Pod "webserver-deployment-6676bcd6d4-qqrhg" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-qqrhg webserver-deployment-6676bcd6d4- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-6676bcd6d4-qqrhg 0882a2a1-cac3-432d-98e4-8ef69a87b36d 7406 0 2020-03-26 17:56:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e32bed2f-4265-4a2b-8252-5751d795cfa6 0xc00595a620 0xc00595a621}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:16 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 50 98 101 100 50 102 45 52 50 54 53 45 52 97 50 98 45 56 50 53 50 45 53 55 53 49 100 55 57 53 99 102 97 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:16 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-1r9w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2020-03-26 17:56:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.747: INFO: Pod "webserver-deployment-6676bcd6d4-wf4vs" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-wf4vs webserver-deployment-6676bcd6d4- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-6676bcd6d4-wf4vs af3c5210-220c-42a6-86aa-e7757dd024d9 7476 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e32bed2f-4265-4a2b-8252-5751d795cfa6 0xc00595a800 0xc00595a801}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 50 98 101 100 50 102 45 52 50 54 53 45 52 97 50 98 45 56 50 53 50 45 53 55 53 49 100 55 57 53 99 102 97 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-1r9w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.750: INFO: Pod "webserver-deployment-6676bcd6d4-zzbj7" is not available:
&Pod{ObjectMeta:{webserver-deployment-6676bcd6d4-zzbj7 webserver-deployment-6676bcd6d4- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-6676bcd6d4-zzbj7 630ab314-e315-4cd4-9ee4-a9ccc390c981 7478 0 2020-03-26 17:56:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:6676bcd6d4] map[] [{apps/v1 ReplicaSet webserver-deployment-6676bcd6d4 e32bed2f-4265-4a2b-8252-5751d795cfa6 0xc00595a950 0xc00595a951}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:16 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 101 51 50 98 101 100 50 102 45 52 50 54 53 45 52 97 50 98 45 56 50 53 50 45 53 55 53 49 100 55 57 53 99 102 97 54 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 54 52 46 49 46 53 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-1r9w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:10.64.1.53,StartTime:2020-03-26 17:56:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.1.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.759: INFO: Pod "webserver-deployment-84855cf797-2gdkv" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-2gdkv webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-2gdkv 4150b778-3867-455d-891c-b0f1091edae4 7480 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc00595ab10 0xc00595ab11}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-d5w7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2020-03-26 17:56:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.766: INFO: Pod "webserver-deployment-84855cf797-2tdn6" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-2tdn6 webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-2tdn6 468a7d6c-0696-4973-bf2e-03e73ac73c7f 7453 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc00595ac80 0xc00595ac81}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-gs7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2020-03-26 17:56:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.775: INFO: Pod "webserver-deployment-84855cf797-692g5" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-692g5 webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-692g5 1922e601-cd42-4718-9f01-277ec252c59b 7357 0 2020-03-26 17:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc00595adf0 0xc00595adf1}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 54 52 46 48 46 51 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-d5w7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:10.64.0.35,StartTime:2020-03-26 17:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 17:56:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://62643140762c5866603dc76a7cc13fff58298edb992f3f9762269449cda8869b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.0.35,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.781: INFO: Pod "webserver-deployment-84855cf797-88c52" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-88c52 webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-88c52 4f709c26-ec31-45f5-9f72-e358c1ee6812 7483 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc00595af80 0xc00595af81}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-gs7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:,StartTime:2020-03-26 17:56:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.790: INFO: Pod "webserver-deployment-84855cf797-cfx9g" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-cfx9g webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-cfx9g de4b9982-9d14-48cd-98e8-9db2ef35a043 7348 0 2020-03-26 17:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc00595b0f0 0xc00595b0f1}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 54 52 46 49 46 53 48 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-1r9w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:10.64.1.50,StartTime:2020-03-26 17:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 17:56:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://c50875be92347a1b1d069240fbd2414eef50b6424870a804eef50cab6872128e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.1.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.796: INFO: Pod "webserver-deployment-84855cf797-fd25x" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-fd25x webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-fd25x dcc4bb59-0fb6-4761-946c-d69f875b8f00 7339 0 2020-03-26 17:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc00595b280 0xc00595b281}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 54 52 46 50 46 50 53 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-gs7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.2.25,StartTime:2020-03-26 17:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 17:56:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://fa9eeca084f7e2c99c1cd1345ce37cedde50cd962cb153b86b5d1877ed60ebe5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.2.25,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.805: INFO: Pod "webserver-deployment-84855cf797-gkx4v" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-gkx4v webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-gkx4v a63de183-ff9c-4275-a756-bd8c7e046671 7472 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc00595b410 0xc00595b411}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-d5w7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.807: INFO: Pod "webserver-deployment-84855cf797-h7n77" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-h7n77 webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-h7n77 45d4aa1a-7ef3-43e7-a7c8-9de5950eba9e 7484 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc00595b530 0xc00595b531}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-1r9w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2020-03-26 17:56:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.817: INFO: Pod "webserver-deployment-84855cf797-j6jln" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-j6jln webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-j6jln e0662bc9-7d99-4133-babc-02a200d771ab 7463 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc00595b6a0 0xc00595b6a1}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-d5w7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.820: INFO: Pod "webserver-deployment-84855cf797-jck8d" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-jck8d webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-jck8d c971758b-e850-4b6f-9b7e-0ba19c2e2390 7342 0 2020-03-26 17:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc00595b7c0 0xc00595b7c1}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 54 52 46 50 46 50 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-gs7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.2.24,StartTime:2020-03-26 17:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 17:56:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://84a82f207b9d6bda92743837f4c3c9c1efa3412bfafc8c896e89bc4a7e6319a4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.2.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.828: INFO: Pod "webserver-deployment-84855cf797-jnrth" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-jnrth webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-jnrth d5ef6c9a-67cc-4e7b-996f-4e740809ca13 7360 0 2020-03-26 17:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc00595b990 0xc00595b991}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:15 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 54 52 46 48 46 51 52 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-d5w7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:10.64.0.34,StartTime:2020-03-26 17:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 17:56:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://9e4761b9f1c8423e24e977e43deff002ae2b4ee622eb08fb0acd4da69e1bcf34,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.0.34,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.835: INFO: Pod "webserver-deployment-84855cf797-jxnk7" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-jxnk7 webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-jxnk7 60058016-3765-4f42-92e4-826d7e18c198 7465 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc00595bb60 0xc00595bb61}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-gs7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.840: INFO: Pod "webserver-deployment-84855cf797-kqqtn" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-kqqtn webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-kqqtn 3143f64e-f511-40e6-bde1-5376a36ee2cc 7345 0 2020-03-26 17:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc00595bc90 0xc00595bc91}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 54 52 46 50 46 50 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-gs7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.5,PodIP:10.64.2.23,StartTime:2020-03-26 17:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 17:56:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://b81d241bc64742682920810bac82a52626d685ba1824243730672b369812c184,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.2.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.848: INFO: Pod "webserver-deployment-84855cf797-m6hsr" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-m6hsr webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-m6hsr aa0872a6-1c51-4c72-9031-e4729dd8a41d 7486 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc00595be60 0xc00595be61}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-d5w7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2020-03-26 17:56:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.859: INFO: Pod "webserver-deployment-84855cf797-mcdsv" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-mcdsv webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-mcdsv 8fc2429d-86b6-43e7-affe-3c3c7ad401a3 7336 0 2020-03-26 17:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc005982000 0xc005982001}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 54 52 46 48 46 51 51 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-d5w7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:10.64.0.33,StartTime:2020-03-26 17:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 17:56:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://dcc7889418e68cb50717b90cefb61f8a8909f6c6808dd79c17b2940d8b5ef131,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.0.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.868: INFO: Pod "webserver-deployment-84855cf797-mhns8" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-mhns8 webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-mhns8 2b091eb0-a4ae-4af8-8d06-3349c60bbacb 7482 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc0059821e0 0xc0059821e1}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-1r9w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2020-03-26 17:56:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.877: INFO: Pod "webserver-deployment-84855cf797-rqbrh" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-rqbrh webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-rqbrh 845117ff-89fb-4bbd-8144-bbdc1c99f3a3 7461 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc005982370 0xc005982371}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-1r9w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2020-03-26 17:56:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.882: INFO: Pod "webserver-deployment-84855cf797-rxlkl" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-rxlkl webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-rxlkl d00bf09b-ec9b-407f-b362-225c201ee3bc 7474 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc0059824f0 0xc0059824f1}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-gs7c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.888: INFO: Pod "webserver-deployment-84855cf797-sgqjs" is not available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-sgqjs webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-sgqjs 6a16f8a3-e955-4ad4-92e8-7fb776e7cd30 7444 0 2020-03-26 17:56:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc005982610 0xc005982611}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:19 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-d5w7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.3,PodIP:,StartTime:2020-03-26 17:56:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 17:56:19.894: INFO: Pod "webserver-deployment-84855cf797-wcd8h" is available:
&Pod{ObjectMeta:{webserver-deployment-84855cf797-wcd8h webserver-deployment-84855cf797- deployment-3580 /api/v1/namespaces/deployment-3580/pods/webserver-deployment-84855cf797-wcd8h 37e5b080-fa01-4ea7-9ae5-41c30a3f7f28 7351 0 2020-03-26 17:56:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:84855cf797] map[] [{apps/v1 ReplicaSet webserver-deployment-84855cf797 a0e73527-e03e-4f81-8eb7-db730637d7f9 0xc0059827d0 0xc0059827d1}] []  [{kube-controller-manager Update v1 2020-03-26 17:56:12 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 97 48 101 55 51 53 50 55 45 101 48 51 101 45 52 102 56 49 45 56 101 98 55 45 100 98 55 51 48 54 51 55 100 55 102 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 17:56:14 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 112 104 97 115 101 34 58 123 125 44 34 102 58 112 111 100 73 80 34 58 123 125 44 34 102 58 112 111 100 73 80 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 105 112 92 34 58 92 34 49 48 46 54 52 46 49 46 52 57 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 112 34 58 123 125 125 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6lwjg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6lwjg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6lwjg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-1r9w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 17:56:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:10.64.1.49,StartTime:2020-03-26 17:56:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-26 17:56:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:docker://4263da5ae10912315103df97eb80aba8ffb24db0f6086d8df6cc874507e7ed92,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.64.1.49,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:56:19.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3580" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":277,"completed":70,"skipped":1226,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:56:19.992: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-projected-bmz8
STEP: Creating a pod to test atomic-volume-subpath
Mar 26 17:56:20.290: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bmz8" in namespace "subpath-2728" to be "Succeeded or Failed"
Mar 26 17:56:20.332: INFO: Pod "pod-subpath-test-projected-bmz8": Phase="Pending", Reason="", readiness=false. Elapsed: 41.890454ms
Mar 26 17:56:22.373: INFO: Pod "pod-subpath-test-projected-bmz8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082495605s
Mar 26 17:56:24.426: INFO: Pod "pod-subpath-test-projected-bmz8": Phase="Running", Reason="", readiness=true. Elapsed: 4.13609874s
Mar 26 17:56:26.485: INFO: Pod "pod-subpath-test-projected-bmz8": Phase="Running", Reason="", readiness=true. Elapsed: 6.195041768s
Mar 26 17:56:28.523: INFO: Pod "pod-subpath-test-projected-bmz8": Phase="Running", Reason="", readiness=true. Elapsed: 8.23322415s
Mar 26 17:56:30.562: INFO: Pod "pod-subpath-test-projected-bmz8": Phase="Running", Reason="", readiness=true. Elapsed: 10.271393903s
Mar 26 17:56:32.600: INFO: Pod "pod-subpath-test-projected-bmz8": Phase="Running", Reason="", readiness=true. Elapsed: 12.309629849s
Mar 26 17:56:34.638: INFO: Pod "pod-subpath-test-projected-bmz8": Phase="Running", Reason="", readiness=true. Elapsed: 14.348083057s
Mar 26 17:56:36.705: INFO: Pod "pod-subpath-test-projected-bmz8": Phase="Running", Reason="", readiness=true. Elapsed: 16.415039968s
Mar 26 17:56:38.743: INFO: Pod "pod-subpath-test-projected-bmz8": Phase="Running", Reason="", readiness=true. Elapsed: 18.453229171s
Mar 26 17:56:40.783: INFO: Pod "pod-subpath-test-projected-bmz8": Phase="Running", Reason="", readiness=true. Elapsed: 20.492420224s
Mar 26 17:56:42.821: INFO: Pod "pod-subpath-test-projected-bmz8": Phase="Running", Reason="", readiness=true. Elapsed: 22.53069677s
Mar 26 17:56:44.859: INFO: Pod "pod-subpath-test-projected-bmz8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.568899695s
STEP: Saw pod success
Mar 26 17:56:44.859: INFO: Pod "pod-subpath-test-projected-bmz8" satisfied condition "Succeeded or Failed"
Mar 26 17:56:44.897: INFO: Trying to get logs from node bootstrap-e2e-minion-group-gs7c pod pod-subpath-test-projected-bmz8 container test-container-subpath-projected-bmz8: <nil>
STEP: delete the pod
Mar 26 17:56:45.010: INFO: Waiting for pod pod-subpath-test-projected-bmz8 to disappear
Mar 26 17:56:45.051: INFO: Pod pod-subpath-test-projected-bmz8 no longer exists
STEP: Deleting pod pod-subpath-test-projected-bmz8
Mar 26 17:56:45.051: INFO: Deleting pod "pod-subpath-test-projected-bmz8" in namespace "subpath-2728"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:56:45.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2728" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":277,"completed":71,"skipped":1250,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:56:45.171: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8809.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8809.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 17:56:55.653: INFO: DNS probes using dns-test-1ba8d0c4-dd21-4ae6-b0b9-2880c9044d59 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8809.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8809.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 17:56:58.005: INFO: File wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-4d73dce1-cada-4593-90a7-056ddbbdd292 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 26 17:56:58.044: INFO: File jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-4d73dce1-cada-4593-90a7-056ddbbdd292 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 26 17:56:58.044: INFO: Lookups using dns-8809/dns-test-4d73dce1-cada-4593-90a7-056ddbbdd292 failed for: [wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local]

Mar 26 17:57:03.085: INFO: File wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-4d73dce1-cada-4593-90a7-056ddbbdd292 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 26 17:57:03.125: INFO: File jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-4d73dce1-cada-4593-90a7-056ddbbdd292 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 26 17:57:03.125: INFO: Lookups using dns-8809/dns-test-4d73dce1-cada-4593-90a7-056ddbbdd292 failed for: [wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local]

Mar 26 17:57:08.094: INFO: File wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-4d73dce1-cada-4593-90a7-056ddbbdd292 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 26 17:57:08.134: INFO: File jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-4d73dce1-cada-4593-90a7-056ddbbdd292 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 26 17:57:08.134: INFO: Lookups using dns-8809/dns-test-4d73dce1-cada-4593-90a7-056ddbbdd292 failed for: [wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local]

Mar 26 17:57:13.085: INFO: File wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-4d73dce1-cada-4593-90a7-056ddbbdd292 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 26 17:57:13.125: INFO: File jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local from pod  dns-8809/dns-test-4d73dce1-cada-4593-90a7-056ddbbdd292 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 26 17:57:13.125: INFO: Lookups using dns-8809/dns-test-4d73dce1-cada-4593-90a7-056ddbbdd292 failed for: [wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local]

Mar 26 17:57:18.125: INFO: DNS probes using dns-test-4d73dce1-cada-4593-90a7-056ddbbdd292 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8809.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8809.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8809.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8809.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 17:57:20.555: INFO: DNS probes using dns-test-7526953c-dbae-45d5-8d97-8cbc68ab9db4 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:57:20.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8809" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":277,"completed":72,"skipped":1259,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:57:20.733: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test env composition
Mar 26 17:57:20.959: INFO: Waiting up to 5m0s for pod "var-expansion-41fa7483-5bee-4c50-b0bd-cce7b9042021" in namespace "var-expansion-1504" to be "Succeeded or Failed"
Mar 26 17:57:20.998: INFO: Pod "var-expansion-41fa7483-5bee-4c50-b0bd-cce7b9042021": Phase="Pending", Reason="", readiness=false. Elapsed: 39.483599ms
Mar 26 17:57:23.036: INFO: Pod "var-expansion-41fa7483-5bee-4c50-b0bd-cce7b9042021": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077764108s
STEP: Saw pod success
Mar 26 17:57:23.036: INFO: Pod "var-expansion-41fa7483-5bee-4c50-b0bd-cce7b9042021" satisfied condition "Succeeded or Failed"
Mar 26 17:57:23.074: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod var-expansion-41fa7483-5bee-4c50-b0bd-cce7b9042021 container dapi-container: <nil>
STEP: delete the pod
Mar 26 17:57:23.168: INFO: Waiting for pod var-expansion-41fa7483-5bee-4c50-b0bd-cce7b9042021 to disappear
Mar 26 17:57:23.206: INFO: Pod var-expansion-41fa7483-5bee-4c50-b0bd-cce7b9042021 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:57:23.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1504" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":277,"completed":73,"skipped":1266,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:57:23.288: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 17:57:25.722: INFO: Waiting up to 5m0s for pod "client-envvars-562eaa2c-7512-4f24-9da0-610431430bbd" in namespace "pods-9204" to be "Succeeded or Failed"
Mar 26 17:57:25.785: INFO: Pod "client-envvars-562eaa2c-7512-4f24-9da0-610431430bbd": Phase="Pending", Reason="", readiness=false. Elapsed: 63.510307ms
Mar 26 17:57:27.825: INFO: Pod "client-envvars-562eaa2c-7512-4f24-9da0-610431430bbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103772499s
STEP: Saw pod success
Mar 26 17:57:27.826: INFO: Pod "client-envvars-562eaa2c-7512-4f24-9da0-610431430bbd" satisfied condition "Succeeded or Failed"
Mar 26 17:57:27.863: INFO: Trying to get logs from node bootstrap-e2e-minion-group-d5w7 pod client-envvars-562eaa2c-7512-4f24-9da0-610431430bbd container env3cont: <nil>
STEP: delete the pod
Mar 26 17:57:27.978: INFO: Waiting for pod client-envvars-562eaa2c-7512-4f24-9da0-610431430bbd to disappear
Mar 26 17:57:28.049: INFO: Pod client-envvars-562eaa2c-7512-4f24-9da0-610431430bbd no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:57:28.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9204" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":277,"completed":74,"skipped":1287,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:57:28.161: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-b1c3ee74-b47c-4108-985f-e990c8b12c38
STEP: Creating a pod to test consume secrets
Mar 26 17:57:28.484: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e119d0e2-37df-4b95-804d-65eec94c7bef" in namespace "projected-6176" to be "Succeeded or Failed"
Mar 26 17:57:28.535: INFO: Pod "pod-projected-secrets-e119d0e2-37df-4b95-804d-65eec94c7bef": Phase="Pending", Reason="", readiness=false. Elapsed: 50.595944ms
Mar 26 17:57:30.573: INFO: Pod "pod-projected-secrets-e119d0e2-37df-4b95-804d-65eec94c7bef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.088793734s
STEP: Saw pod success
Mar 26 17:57:30.573: INFO: Pod "pod-projected-secrets-e119d0e2-37df-4b95-804d-65eec94c7bef" satisfied condition "Succeeded or Failed"
Mar 26 17:57:30.611: INFO: Trying to get logs from node bootstrap-e2e-minion-group-d5w7 pod pod-projected-secrets-e119d0e2-37df-4b95-804d-65eec94c7bef container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 26 17:57:30.705: INFO: Waiting for pod pod-projected-secrets-e119d0e2-37df-4b95-804d-65eec94c7bef to disappear
Mar 26 17:57:30.743: INFO: Pod pod-projected-secrets-e119d0e2-37df-4b95-804d-65eec94c7bef no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:57:30.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6176" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":75,"skipped":1288,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:57:30.825: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-4410
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4410
STEP: Creating statefulset with conflicting port in namespace statefulset-4410
STEP: Waiting until pod test-pod will start running in namespace statefulset-4410
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4410
Mar 26 17:57:35.323: INFO: Observed stateful pod in namespace: statefulset-4410, name: ss-0, uid: 6dc52bed-69ed-4c0a-9c57-ac4c186b94ab, status phase: Pending. Waiting for statefulset controller to delete.
Mar 26 17:57:35.763: INFO: Observed stateful pod in namespace: statefulset-4410, name: ss-0, uid: 6dc52bed-69ed-4c0a-9c57-ac4c186b94ab, status phase: Failed. Waiting for statefulset controller to delete.
Mar 26 17:57:35.775: INFO: Observed stateful pod in namespace: statefulset-4410, name: ss-0, uid: 6dc52bed-69ed-4c0a-9c57-ac4c186b94ab, status phase: Failed. Waiting for statefulset controller to delete.
Mar 26 17:57:35.788: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4410
STEP: Removing pod with conflicting port in namespace statefulset-4410
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4410 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 26 17:57:39.967: INFO: Deleting all statefulset in ns statefulset-4410
Mar 26 17:57:40.004: INFO: Scaling statefulset ss to 0
Mar 26 17:58:00.167: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 17:58:00.208: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:58:00.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4410" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":277,"completed":76,"skipped":1290,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:58:00.407: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Starting the proxy
Mar 26 17:58:00.561: INFO: Asynchronously running '/workspace/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config proxy --unix-socket=/tmp/kubectl-proxy-unix469402061/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:58:00.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3682" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":277,"completed":77,"skipped":1324,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:58:00.724: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 17:58:00.926: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ec55101-dd9e-478e-a291-4f0fb741613b" in namespace "downward-api-1191" to be "Succeeded or Failed"
Mar 26 17:58:00.967: INFO: Pod "downwardapi-volume-9ec55101-dd9e-478e-a291-4f0fb741613b": Phase="Pending", Reason="", readiness=false. Elapsed: 40.29951ms
Mar 26 17:58:03.004: INFO: Pod "downwardapi-volume-9ec55101-dd9e-478e-a291-4f0fb741613b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.078075453s
STEP: Saw pod success
Mar 26 17:58:03.004: INFO: Pod "downwardapi-volume-9ec55101-dd9e-478e-a291-4f0fb741613b" satisfied condition "Succeeded or Failed"
Mar 26 17:58:03.042: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod downwardapi-volume-9ec55101-dd9e-478e-a291-4f0fb741613b container client-container: <nil>
STEP: delete the pod
Mar 26 17:58:03.135: INFO: Waiting for pod downwardapi-volume-9ec55101-dd9e-478e-a291-4f0fb741613b to disappear
Mar 26 17:58:03.174: INFO: Pod downwardapi-volume-9ec55101-dd9e-478e-a291-4f0fb741613b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:58:03.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1191" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":277,"completed":78,"skipped":1336,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:58:03.255: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 17:58:04.799: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842284, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842284, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842284, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842284, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 17:58:07.884: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:58:08.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6648" for this suite.
STEP: Destroying namespace "webhook-6648-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":277,"completed":79,"skipped":1337,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:58:08.397: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 26 17:58:10.762: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:58:10.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9188" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":80,"skipped":1345,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:58:10.937: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-88ce0c6f-ec7e-4e71-a61b-b50e9a151908
STEP: Creating a pod to test consume secrets
Mar 26 17:58:11.175: INFO: Waiting up to 5m0s for pod "pod-secrets-2b5cc1ab-fd7e-42dd-b125-1a58589bdb7a" in namespace "secrets-3264" to be "Succeeded or Failed"
Mar 26 17:58:11.223: INFO: Pod "pod-secrets-2b5cc1ab-fd7e-42dd-b125-1a58589bdb7a": Phase="Pending", Reason="", readiness=false. Elapsed: 48.559018ms
Mar 26 17:58:13.270: INFO: Pod "pod-secrets-2b5cc1ab-fd7e-42dd-b125-1a58589bdb7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.095316798s
STEP: Saw pod success
Mar 26 17:58:13.270: INFO: Pod "pod-secrets-2b5cc1ab-fd7e-42dd-b125-1a58589bdb7a" satisfied condition "Succeeded or Failed"
Mar 26 17:58:13.310: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-secrets-2b5cc1ab-fd7e-42dd-b125-1a58589bdb7a container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 17:58:13.412: INFO: Waiting for pod pod-secrets-2b5cc1ab-fd7e-42dd-b125-1a58589bdb7a to disappear
Mar 26 17:58:13.459: INFO: Pod pod-secrets-2b5cc1ab-fd7e-42dd-b125-1a58589bdb7a no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:58:13.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3264" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":81,"skipped":1345,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:58:13.549: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 17:58:30.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4981" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":277,"completed":82,"skipped":1360,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 17:58:30.399: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-9882f688-e180-47fa-b17a-4fe4a0a29714 in namespace container-probe-2593
Mar 26 17:58:32.695: INFO: Started pod liveness-9882f688-e180-47fa-b17a-4fe4a0a29714 in namespace container-probe-2593
STEP: checking the pod's current state and verifying that restartCount is present
Mar 26 17:58:32.733: INFO: Initial restart count of pod liveness-9882f688-e180-47fa-b17a-4fe4a0a29714 is 0
Mar 26 17:58:49.084: INFO: Restart count of pod container-probe-2593/liveness-9882f688-e180-47fa-b17a-4fe4a0a29714 is now 1 (16.350943798s elapsed)
Mar 26 17:59:07.431: INFO: Restart count of pod container-probe-2593/liveness-9882f688-e180-47fa-b17a-4fe4a0a29714 is now 2 (34.697616493s elapsed)
Mar 26 17:59:27.814: INFO: Restart count of pod container-probe-2593/liveness-9882f688-e180-47fa-b17a-4fe4a0a29714 is now 3 (55.081056262s elapsed)
Mar 26 17:59:48.198: INFO: Restart count of pod container-probe-2593/liveness-9882f688-e180-47fa-b17a-4fe4a0a29714 is now 4 (1m15.464350034s elapsed)
Mar 26 18:00:53.429: INFO: Restart count of pod container-probe-2593/liveness-9882f688-e180-47fa-b17a-4fe4a0a29714 is now 5 (2m20.695961798s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:00:53.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2593" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":277,"completed":83,"skipped":1362,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:00:53.561: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:00:53.756: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:00:54.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8646" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":277,"completed":84,"skipped":1365,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:00:54.889: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 26 18:00:55.277: INFO: Waiting up to 5m0s for pod "pod-17712f94-687e-48e2-8e33-340dfd504310" in namespace "emptydir-8881" to be "Succeeded or Failed"
Mar 26 18:00:55.315: INFO: Pod "pod-17712f94-687e-48e2-8e33-340dfd504310": Phase="Pending", Reason="", readiness=false. Elapsed: 37.382206ms
Mar 26 18:00:57.353: INFO: Pod "pod-17712f94-687e-48e2-8e33-340dfd504310": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075595022s
STEP: Saw pod success
Mar 26 18:00:57.353: INFO: Pod "pod-17712f94-687e-48e2-8e33-340dfd504310" satisfied condition "Succeeded or Failed"
Mar 26 18:00:57.390: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-17712f94-687e-48e2-8e33-340dfd504310 container test-container: <nil>
STEP: delete the pod
Mar 26 18:00:57.491: INFO: Waiting for pod pod-17712f94-687e-48e2-8e33-340dfd504310 to disappear
Mar 26 18:00:57.529: INFO: Pod pod-17712f94-687e-48e2-8e33-340dfd504310 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:00:57.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8881" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":85,"skipped":1377,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:00:57.611: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:01:03.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7907" for this suite.
•{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":277,"completed":86,"skipped":1422,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:01:03.927: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-f75c99fb-a87a-4c3a-94fb-0a528ee32934
STEP: Creating a pod to test consume configMaps
Mar 26 18:01:04.203: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3810474d-b463-437c-90c8-4fecb7af774b" in namespace "projected-4187" to be "Succeeded or Failed"
Mar 26 18:01:04.240: INFO: Pod "pod-projected-configmaps-3810474d-b463-437c-90c8-4fecb7af774b": Phase="Pending", Reason="", readiness=false. Elapsed: 37.379179ms
Mar 26 18:01:06.281: INFO: Pod "pod-projected-configmaps-3810474d-b463-437c-90c8-4fecb7af774b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.078390585s
STEP: Saw pod success
Mar 26 18:01:06.281: INFO: Pod "pod-projected-configmaps-3810474d-b463-437c-90c8-4fecb7af774b" satisfied condition "Succeeded or Failed"
Mar 26 18:01:06.319: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-projected-configmaps-3810474d-b463-437c-90c8-4fecb7af774b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 18:01:06.412: INFO: Waiting for pod pod-projected-configmaps-3810474d-b463-437c-90c8-4fecb7af774b to disappear
Mar 26 18:01:06.450: INFO: Pod pod-projected-configmaps-3810474d-b463-437c-90c8-4fecb7af774b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:01:06.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4187" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":87,"skipped":1441,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:01:06.531: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 26 18:01:06.733: INFO: Waiting up to 5m0s for pod "pod-bda61851-f74b-494d-88c2-5afee0367df0" in namespace "emptydir-3813" to be "Succeeded or Failed"
Mar 26 18:01:06.783: INFO: Pod "pod-bda61851-f74b-494d-88c2-5afee0367df0": Phase="Pending", Reason="", readiness=false. Elapsed: 50.460708ms
Mar 26 18:01:08.821: INFO: Pod "pod-bda61851-f74b-494d-88c2-5afee0367df0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.088712368s
STEP: Saw pod success
Mar 26 18:01:08.821: INFO: Pod "pod-bda61851-f74b-494d-88c2-5afee0367df0" satisfied condition "Succeeded or Failed"
Mar 26 18:01:08.859: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-bda61851-f74b-494d-88c2-5afee0367df0 container test-container: <nil>
STEP: delete the pod
Mar 26 18:01:08.981: INFO: Waiting for pod pod-bda61851-f74b-494d-88c2-5afee0367df0 to disappear
Mar 26 18:01:09.019: INFO: Pod pod-bda61851-f74b-494d-88c2-5afee0367df0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:01:09.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3813" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":88,"skipped":1463,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:01:09.105: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:01:36.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9958" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":277,"completed":89,"skipped":1478,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:01:36.381: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:01:43.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6039" for this suite.
STEP: Destroying namespace "nsdeletetest-3059" for this suite.
Mar 26 18:01:43.150: INFO: Namespace nsdeletetest-3059 was already deleted
STEP: Destroying namespace "nsdeletetest-2548" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":277,"completed":90,"skipped":1490,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:01:43.191: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:01:59.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9314" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":277,"completed":91,"skipped":1498,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:01:59.791: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 18:02:00.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39f80a7d-e9bf-40b8-99ef-d2f7748589dc" in namespace "downward-api-3962" to be "Succeeded or Failed"
Mar 26 18:02:00.068: INFO: Pod "downwardapi-volume-39f80a7d-e9bf-40b8-99ef-d2f7748589dc": Phase="Pending", Reason="", readiness=false. Elapsed: 37.668103ms
Mar 26 18:02:02.107: INFO: Pod "downwardapi-volume-39f80a7d-e9bf-40b8-99ef-d2f7748589dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076054218s
STEP: Saw pod success
Mar 26 18:02:02.107: INFO: Pod "downwardapi-volume-39f80a7d-e9bf-40b8-99ef-d2f7748589dc" satisfied condition "Succeeded or Failed"
Mar 26 18:02:02.145: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod downwardapi-volume-39f80a7d-e9bf-40b8-99ef-d2f7748589dc container client-container: <nil>
STEP: delete the pod
Mar 26 18:02:02.234: INFO: Waiting for pod downwardapi-volume-39f80a7d-e9bf-40b8-99ef-d2f7748589dc to disappear
Mar 26 18:02:02.273: INFO: Pod downwardapi-volume-39f80a7d-e9bf-40b8-99ef-d2f7748589dc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:02:02.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3962" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":92,"skipped":1504,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:02:02.361: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-181fcacd-8f2f-48f1-b232-625821b6f867
STEP: Creating a pod to test consume secrets
Mar 26 18:02:02.651: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-410f9080-5669-4cb6-ae7e-ed82841c9d01" in namespace "projected-1672" to be "Succeeded or Failed"
Mar 26 18:02:02.689: INFO: Pod "pod-projected-secrets-410f9080-5669-4cb6-ae7e-ed82841c9d01": Phase="Pending", Reason="", readiness=false. Elapsed: 38.236623ms
Mar 26 18:02:04.727: INFO: Pod "pod-projected-secrets-410f9080-5669-4cb6-ae7e-ed82841c9d01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076456306s
STEP: Saw pod success
Mar 26 18:02:04.727: INFO: Pod "pod-projected-secrets-410f9080-5669-4cb6-ae7e-ed82841c9d01" satisfied condition "Succeeded or Failed"
Mar 26 18:02:04.765: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-projected-secrets-410f9080-5669-4cb6-ae7e-ed82841c9d01 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 26 18:02:05.029: INFO: Waiting for pod pod-projected-secrets-410f9080-5669-4cb6-ae7e-ed82841c9d01 to disappear
Mar 26 18:02:05.078: INFO: Pod pod-projected-secrets-410f9080-5669-4cb6-ae7e-ed82841c9d01 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:02:05.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1672" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":93,"skipped":1510,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:02:05.161: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-3103
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3103
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3103
Mar 26 18:02:05.492: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar 26 18:02:15.533: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 26 18:02:15.572: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 18:02:16.116: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 18:02:16.116: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 18:02:16.116: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 18:02:16.154: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 26 18:02:26.195: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 18:02:26.195: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 18:02:26.349: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999268s
Mar 26 18:02:27.388: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.961627162s
Mar 26 18:02:28.427: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.923118918s
Mar 26 18:02:29.466: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.883978711s
Mar 26 18:02:30.504: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.845049583s
Mar 26 18:02:31.543: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.806601313s
Mar 26 18:02:32.582: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.767950787s
Mar 26 18:02:33.620: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.729365754s
Mar 26 18:02:34.658: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.691118909s
Mar 26 18:02:35.702: INFO: Verifying statefulset ss doesn't scale past 1 for another 652.753204ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3103
Mar 26 18:02:36.741: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:02:37.269: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 26 18:02:37.269: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 18:02:37.269: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 18:02:37.307: INFO: Found 1 stateful pods, waiting for 3
Mar 26 18:02:47.348: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 18:02:47.348: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 18:02:47.348: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 26 18:02:47.425: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 18:02:47.977: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 18:02:47.977: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 18:02:47.977: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 18:02:47.977: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 18:02:48.507: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 18:02:48.507: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 18:02:48.507: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 18:02:48.507: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 18:02:49.115: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 18:02:49.115: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 18:02:49.115: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 18:02:49.115: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 18:02:49.153: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Mar 26 18:02:59.232: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 18:02:59.232: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 18:02:59.232: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 18:02:59.357: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999395s
Mar 26 18:03:00.397: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.960418373s
Mar 26 18:03:01.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.920325571s
Mar 26 18:03:02.477: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.880096511s
Mar 26 18:03:03.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.840766777s
Mar 26 18:03:04.556: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.801153209s
Mar 26 18:03:05.596: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.761660551s
Mar 26 18:03:06.636: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.721970413s
Mar 26 18:03:07.676: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.6817047s
Mar 26 18:03:08.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 641.992201ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3103
Mar 26 18:03:09.756: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:03:10.282: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 26 18:03:10.282: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 18:03:10.282: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 18:03:10.282: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:03:10.798: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 26 18:03:10.798: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 18:03:10.798: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 18:03:10.798: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:03:11.158: INFO: rc: 1
Mar 26 18:03:11.158: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar 26 18:03:21.158: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:03:21.406: INFO: rc: 1
Mar 26 18:03:21.406: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:03:31.406: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:03:31.665: INFO: rc: 1
Mar 26 18:03:31.665: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:03:41.665: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:03:41.905: INFO: rc: 1
Mar 26 18:03:41.905: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:03:51.905: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:03:52.142: INFO: rc: 1
Mar 26 18:03:52.142: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:04:02.143: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:04:02.380: INFO: rc: 1
Mar 26 18:04:02.380: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:04:12.380: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:04:12.626: INFO: rc: 1
Mar 26 18:04:12.626: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:04:22.626: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:04:22.861: INFO: rc: 1
Mar 26 18:04:22.861: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:04:32.861: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:04:33.121: INFO: rc: 1
Mar 26 18:04:33.121: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:04:43.121: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:04:43.368: INFO: rc: 1
Mar 26 18:04:43.368: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:04:53.369: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:04:53.615: INFO: rc: 1
Mar 26 18:04:53.615: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:05:03.616: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:05:03.855: INFO: rc: 1
Mar 26 18:05:03.855: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:05:13.855: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:05:14.088: INFO: rc: 1
Mar 26 18:05:14.088: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:05:24.088: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:05:24.341: INFO: rc: 1
Mar 26 18:05:24.341: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:05:34.342: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:05:34.873: INFO: rc: 1
Mar 26 18:05:34.873: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:05:44.873: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:05:45.110: INFO: rc: 1
Mar 26 18:05:45.110: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:05:55.111: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:05:55.356: INFO: rc: 1
Mar 26 18:05:55.356: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:06:05.356: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:06:05.598: INFO: rc: 1
Mar 26 18:06:05.598: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:06:15.598: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:06:15.843: INFO: rc: 1
Mar 26 18:06:15.843: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:06:25.844: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:06:26.087: INFO: rc: 1
Mar 26 18:06:26.087: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:06:36.087: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:06:36.334: INFO: rc: 1
Mar 26 18:06:36.334: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:06:46.334: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:06:46.596: INFO: rc: 1
Mar 26 18:06:46.596: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:06:56.597: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:06:56.831: INFO: rc: 1
Mar 26 18:06:56.831: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:07:06.832: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:07:07.093: INFO: rc: 1
Mar 26 18:07:07.093: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:07:17.094: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:07:17.320: INFO: rc: 1
Mar 26 18:07:17.320: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:07:27.321: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:07:27.581: INFO: rc: 1
Mar 26 18:07:27.581: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:07:37.581: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:07:37.827: INFO: rc: 1
Mar 26 18:07:37.827: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:07:47.828: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:07:48.088: INFO: rc: 1
Mar 26 18:07:48.088: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:07:58.088: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:07:58.323: INFO: rc: 1
Mar 26 18:07:58.323: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:08:08.323: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:08:08.558: INFO: rc: 1
Mar 26 18:08:08.558: INFO: Waiting 10s to retry failed RunHostCmd: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 26 18:08:18.558: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-3103 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:08:18.798: INFO: rc: 1
Mar 26 18:08:18.798: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Mar 26 18:08:18.798: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 26 18:08:18.918: INFO: Deleting all statefulset in ns statefulset-3103
Mar 26 18:08:18.998: INFO: Scaling statefulset ss to 0
Mar 26 18:08:19.113: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 18:08:19.154: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:08:19.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3103" for this suite.

• [SLOW TEST:374.198 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":277,"completed":94,"skipped":1517,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:08:19.359: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar 26 18:08:24.396: INFO: Successfully updated pod "annotationupdatef38727d7-139f-4a01-bc69-66a2bcd68853"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:08:26.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7401" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":277,"completed":95,"skipped":1521,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:08:26.571: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's args
Mar 26 18:08:26.836: INFO: Waiting up to 5m0s for pod "var-expansion-9e7a4cc1-b050-47ce-a9aa-5bfb7ba2fc26" in namespace "var-expansion-9304" to be "Succeeded or Failed"
Mar 26 18:08:26.882: INFO: Pod "var-expansion-9e7a4cc1-b050-47ce-a9aa-5bfb7ba2fc26": Phase="Pending", Reason="", readiness=false. Elapsed: 45.892171ms
Mar 26 18:08:28.920: INFO: Pod "var-expansion-9e7a4cc1-b050-47ce-a9aa-5bfb7ba2fc26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.083916469s
STEP: Saw pod success
Mar 26 18:08:28.920: INFO: Pod "var-expansion-9e7a4cc1-b050-47ce-a9aa-5bfb7ba2fc26" satisfied condition "Succeeded or Failed"
Mar 26 18:08:28.958: INFO: Trying to get logs from node bootstrap-e2e-minion-group-d5w7 pod var-expansion-9e7a4cc1-b050-47ce-a9aa-5bfb7ba2fc26 container dapi-container: <nil>
STEP: delete the pod
Mar 26 18:08:29.068: INFO: Waiting for pod var-expansion-9e7a4cc1-b050-47ce-a9aa-5bfb7ba2fc26 to disappear
Mar 26 18:08:29.106: INFO: Pod var-expansion-9e7a4cc1-b050-47ce-a9aa-5bfb7ba2fc26 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:08:29.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9304" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":277,"completed":96,"skipped":1552,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:08:29.186: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
Mar 26 18:08:31.858: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:08:31.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5379" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":277,"completed":97,"skipped":1558,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:08:31.939: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:08:32.132: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config version'
Mar 26 18:08:32.320: INFO: stderr: ""
Mar 26 18:08:32.321: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"18+\", GitVersion:\"v1.18.1-beta.0.5+d5dfb5cb416fcc\", GitCommit:\"d5dfb5cb416fcc32ad556b0d253307c9267b1d30\", GitTreeState:\"clean\", BuildDate:\"2020-03-25T22:16:25Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"18+\", GitVersion:\"v1.18.1-beta.0.5+d5dfb5cb416fcc\", GitCommit:\"d5dfb5cb416fcc32ad556b0d253307c9267b1d30\", GitTreeState:\"clean\", BuildDate:\"2020-03-25T22:16:25Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:08:32.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9653" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":277,"completed":98,"skipped":1601,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:08:32.400: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Mar 26 18:09:14.985: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:09:14.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1731" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":277,"completed":99,"skipped":1622,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:09:15.072: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar 26 18:09:17.614: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6912 PodName:pod-sharedvolume-b47c9781-2a50-4f96-a5c4-3f2c61e9a933 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:09:17.614: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:09:17.939: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:09:17.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6912" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":277,"completed":100,"skipped":1639,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:09:18.021: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:09:18.212: INFO: Creating ReplicaSet my-hostname-basic-a5519d09-efbc-4351-bf87-97e263042caa
Mar 26 18:09:18.298: INFO: Pod name my-hostname-basic-a5519d09-efbc-4351-bf87-97e263042caa: Found 1 pods out of 1
Mar 26 18:09:18.298: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a5519d09-efbc-4351-bf87-97e263042caa" is running
Mar 26 18:09:20.377: INFO: Pod "my-hostname-basic-a5519d09-efbc-4351-bf87-97e263042caa-sdp6j" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-26 18:09:18 +0000 UTC Reason: Message:}])
Mar 26 18:09:20.377: INFO: Trying to dial the pod
Mar 26 18:09:25.558: INFO: Controller my-hostname-basic-a5519d09-efbc-4351-bf87-97e263042caa: Got expected result from replica 1 [my-hostname-basic-a5519d09-efbc-4351-bf87-97e263042caa-sdp6j]: "my-hostname-basic-a5519d09-efbc-4351-bf87-97e263042caa-sdp6j", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:09:25.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9315" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":101,"skipped":1656,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:09:25.639: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 18:09:27.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842967, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842967, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842967, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842967, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 18:09:30.438: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:09:30.477: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6732-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:09:31.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2248" for this suite.
STEP: Destroying namespace "webhook-2248-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":277,"completed":102,"skipped":1660,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] LimitRange
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:09:32.080: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Mar 26 18:09:32.316: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Mar 26 18:09:32.392: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 26 18:09:32.392: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Mar 26 18:09:32.470: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 26 18:09:32.470: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Mar 26 18:09:32.549: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 26 18:09:32.549: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Mar 26 18:09:39.869: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:09:39.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-135" for this suite.
•{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":277,"completed":103,"skipped":1694,"failed":0}
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:09:39.999: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 26 18:09:44.551: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 26 18:09:44.589: INFO: Pod pod-with-poststart-http-hook still exists
Mar 26 18:09:46.590: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 26 18:09:46.629: INFO: Pod pod-with-poststart-http-hook still exists
Mar 26 18:09:48.590: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 26 18:09:48.629: INFO: Pod pod-with-poststart-http-hook still exists
Mar 26 18:09:50.590: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 26 18:09:50.640: INFO: Pod pod-with-poststart-http-hook still exists
Mar 26 18:09:52.590: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 26 18:09:52.628: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:09:52.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5345" for this suite.
•{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":277,"completed":104,"skipped":1699,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:09:52.709: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 18:09:52.932: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc57de00-0012-4298-9931-8699d08ead11" in namespace "downward-api-8170" to be "Succeeded or Failed"
Mar 26 18:09:52.969: INFO: Pod "downwardapi-volume-fc57de00-0012-4298-9931-8699d08ead11": Phase="Pending", Reason="", readiness=false. Elapsed: 37.617127ms
Mar 26 18:09:55.008: INFO: Pod "downwardapi-volume-fc57de00-0012-4298-9931-8699d08ead11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0758315s
STEP: Saw pod success
Mar 26 18:09:55.008: INFO: Pod "downwardapi-volume-fc57de00-0012-4298-9931-8699d08ead11" satisfied condition "Succeeded or Failed"
Mar 26 18:09:55.045: INFO: Trying to get logs from node bootstrap-e2e-minion-group-d5w7 pod downwardapi-volume-fc57de00-0012-4298-9931-8699d08ead11 container client-container: <nil>
STEP: delete the pod
Mar 26 18:09:55.137: INFO: Waiting for pod downwardapi-volume-fc57de00-0012-4298-9931-8699d08ead11 to disappear
Mar 26 18:09:55.175: INFO: Pod downwardapi-volume-fc57de00-0012-4298-9931-8699d08ead11 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:09:55.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8170" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":105,"skipped":1719,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:09:55.256: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 18:09:56.583: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842996, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842996, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842996, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842996, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:09:58.622: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842996, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842996, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842996, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720842996, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 18:10:01.672: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:10:01.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-75" for this suite.
STEP: Destroying namespace "webhook-75-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":277,"completed":106,"skipped":1732,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:10:02.232: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 18:10:03.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843003, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843003, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843003, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843003, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:10:05.471: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843003, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843003, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843003, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843003, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 18:10:08.518: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:10:08.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7375" for this suite.
STEP: Destroying namespace "webhook-7375-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":277,"completed":107,"skipped":1774,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:10:09.289: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-26bc6c0c-58f7-4d0b-99ec-f5fca0220e9d
STEP: Creating a pod to test consume configMaps
Mar 26 18:10:09.526: INFO: Waiting up to 5m0s for pod "pod-configmaps-21c2cd4f-1922-4cf9-8fa7-edc8fb0070d5" in namespace "configmap-9067" to be "Succeeded or Failed"
Mar 26 18:10:09.572: INFO: Pod "pod-configmaps-21c2cd4f-1922-4cf9-8fa7-edc8fb0070d5": Phase="Pending", Reason="", readiness=false. Elapsed: 45.779273ms
Mar 26 18:10:11.611: INFO: Pod "pod-configmaps-21c2cd4f-1922-4cf9-8fa7-edc8fb0070d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.084148773s
STEP: Saw pod success
Mar 26 18:10:11.611: INFO: Pod "pod-configmaps-21c2cd4f-1922-4cf9-8fa7-edc8fb0070d5" satisfied condition "Succeeded or Failed"
Mar 26 18:10:11.649: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-configmaps-21c2cd4f-1922-4cf9-8fa7-edc8fb0070d5 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 18:10:11.741: INFO: Waiting for pod pod-configmaps-21c2cd4f-1922-4cf9-8fa7-edc8fb0070d5 to disappear
Mar 26 18:10:11.779: INFO: Pod pod-configmaps-21c2cd4f-1922-4cf9-8fa7-edc8fb0070d5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:10:11.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9067" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":108,"skipped":1782,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:10:11.860: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar 26 18:10:12.013: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar 26 18:10:26.034: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:10:29.650: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:10:45.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1512" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":277,"completed":109,"skipped":1819,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:10:45.964: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar 26 18:10:46.224: INFO: Waiting up to 5m0s for pod "downward-api-664f2c2e-4b79-4ec6-956c-99a952db675f" in namespace "downward-api-2184" to be "Succeeded or Failed"
Mar 26 18:10:46.262: INFO: Pod "downward-api-664f2c2e-4b79-4ec6-956c-99a952db675f": Phase="Pending", Reason="", readiness=false. Elapsed: 37.775315ms
Mar 26 18:10:48.301: INFO: Pod "downward-api-664f2c2e-4b79-4ec6-956c-99a952db675f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.07708034s
STEP: Saw pod success
Mar 26 18:10:48.301: INFO: Pod "downward-api-664f2c2e-4b79-4ec6-956c-99a952db675f" satisfied condition "Succeeded or Failed"
Mar 26 18:10:48.344: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod downward-api-664f2c2e-4b79-4ec6-956c-99a952db675f container dapi-container: <nil>
STEP: delete the pod
Mar 26 18:10:48.487: INFO: Waiting for pod downward-api-664f2c2e-4b79-4ec6-956c-99a952db675f to disappear
Mar 26 18:10:48.545: INFO: Pod downward-api-664f2c2e-4b79-4ec6-956c-99a952db675f no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:10:48.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2184" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":277,"completed":110,"skipped":1833,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:10:48.635: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2947.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2947.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2947.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2947.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2947.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2947.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 18:10:51.422: INFO: DNS probes using dns-2947/dns-test-25fb757f-95b5-4682-8829-b60527c1d200 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:10:51.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2947" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":277,"completed":111,"skipped":1838,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:10:51.621: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 18:10:52.864: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843052, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843052, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843052, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843052, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:10:54.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843052, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843052, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843052, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843052, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 18:10:57.953: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:10:58.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7274" for this suite.
STEP: Destroying namespace "webhook-7274-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":277,"completed":112,"skipped":1840,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:10:59.044: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:11:05.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1343" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":277,"completed":113,"skipped":1859,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:11:05.821: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:11:06.017: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:11:06.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4031" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":277,"completed":114,"skipped":1882,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:11:06.668: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-ce97b138-4b49-4e57-b459-efbb7aa4a50a in namespace container-probe-710
Mar 26 18:11:08.987: INFO: Started pod liveness-ce97b138-4b49-4e57-b459-efbb7aa4a50a in namespace container-probe-710
STEP: checking the pod's current state and verifying that restartCount is present
Mar 26 18:11:09.037: INFO: Initial restart count of pod liveness-ce97b138-4b49-4e57-b459-efbb7aa4a50a is 0
Mar 26 18:11:33.597: INFO: Restart count of pod container-probe-710/liveness-ce97b138-4b49-4e57-b459-efbb7aa4a50a is now 1 (24.560044074s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:11:33.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-710" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":115,"skipped":1887,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:11:33.731: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 26 18:11:33.968: INFO: Waiting up to 5m0s for pod "pod-c5894c2b-56f6-4026-a330-0239b3ee8789" in namespace "emptydir-1878" to be "Succeeded or Failed"
Mar 26 18:11:34.009: INFO: Pod "pod-c5894c2b-56f6-4026-a330-0239b3ee8789": Phase="Pending", Reason="", readiness=false. Elapsed: 40.777239ms
Mar 26 18:11:36.047: INFO: Pod "pod-c5894c2b-56f6-4026-a330-0239b3ee8789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.07924275s
STEP: Saw pod success
Mar 26 18:11:36.047: INFO: Pod "pod-c5894c2b-56f6-4026-a330-0239b3ee8789" satisfied condition "Succeeded or Failed"
Mar 26 18:11:36.085: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-c5894c2b-56f6-4026-a330-0239b3ee8789 container test-container: <nil>
STEP: delete the pod
Mar 26 18:11:36.177: INFO: Waiting for pod pod-c5894c2b-56f6-4026-a330-0239b3ee8789 to disappear
Mar 26 18:11:36.215: INFO: Pod pod-c5894c2b-56f6-4026-a330-0239b3ee8789 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:11:36.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1878" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":116,"skipped":1910,"failed":0}

------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:11:36.297: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating pod
Mar 26 18:11:38.692: INFO: Pod pod-hostip-fe0df827-338a-42cb-90e3-9af47ecf7883 has hostIP: 10.138.0.4
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:11:38.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-927" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":277,"completed":117,"skipped":1910,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:11:38.782: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 26 18:11:39.349: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:11:39.392: INFO: Number of nodes with available pods: 0
Mar 26 18:11:39.392: INFO: Node bootstrap-e2e-minion-group-1r9w is running more than one daemon pod
Mar 26 18:11:40.437: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:11:40.480: INFO: Number of nodes with available pods: 1
Mar 26 18:11:40.481: INFO: Node bootstrap-e2e-minion-group-d5w7 is running more than one daemon pod
Mar 26 18:11:41.436: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:11:41.488: INFO: Number of nodes with available pods: 3
Mar 26 18:11:41.488: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 26 18:11:41.668: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:11:41.707: INFO: Number of nodes with available pods: 2
Mar 26 18:11:41.707: INFO: Node bootstrap-e2e-minion-group-1r9w is running more than one daemon pod
Mar 26 18:11:42.750: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:11:42.789: INFO: Number of nodes with available pods: 2
Mar 26 18:11:42.789: INFO: Node bootstrap-e2e-minion-group-1r9w is running more than one daemon pod
Mar 26 18:11:43.750: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:11:43.790: INFO: Number of nodes with available pods: 3
Mar 26 18:11:43.790: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9928, will wait for the garbage collector to delete the pods
Mar 26 18:11:44.021: INFO: Deleting DaemonSet.extensions daemon-set took: 65.586679ms
Mar 26 18:11:44.721: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.290357ms
Mar 26 18:11:53.161: INFO: Number of nodes with available pods: 0
Mar 26 18:11:53.161: INFO: Number of running nodes: 0, number of available pods: 0
Mar 26 18:11:53.199: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9928/daemonsets","resourceVersion":"12104"},"items":null}

Mar 26 18:11:53.238: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9928/pods","resourceVersion":"12104"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:11:53.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9928" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":277,"completed":118,"skipped":1913,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:11:53.478: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-64e531fc-5fe5-4dbe-bbd3-d89f3424f74f
STEP: Creating a pod to test consume secrets
Mar 26 18:11:53.750: INFO: Waiting up to 5m0s for pod "pod-secrets-30e15801-05a1-4717-9bed-e4c5ac3a848d" in namespace "secrets-5240" to be "Succeeded or Failed"
Mar 26 18:11:53.789: INFO: Pod "pod-secrets-30e15801-05a1-4717-9bed-e4c5ac3a848d": Phase="Pending", Reason="", readiness=false. Elapsed: 39.192981ms
Mar 26 18:11:55.828: INFO: Pod "pod-secrets-30e15801-05a1-4717-9bed-e4c5ac3a848d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077678518s
STEP: Saw pod success
Mar 26 18:11:55.828: INFO: Pod "pod-secrets-30e15801-05a1-4717-9bed-e4c5ac3a848d" satisfied condition "Succeeded or Failed"
Mar 26 18:11:55.868: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-secrets-30e15801-05a1-4717-9bed-e4c5ac3a848d container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 18:11:55.971: INFO: Waiting for pod pod-secrets-30e15801-05a1-4717-9bed-e4c5ac3a848d to disappear
Mar 26 18:11:56.009: INFO: Pod pod-secrets-30e15801-05a1-4717-9bed-e4c5ac3a848d no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:11:56.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5240" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":119,"skipped":1920,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:11:56.092: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:11:56.246: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:11:58.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1423" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":277,"completed":120,"skipped":1933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:11:58.605: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar 26 18:12:01.505: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-5775 pod-service-account-c3ff8a2f-0086-4aa8-b370-e9c08a503e36 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar 26 18:12:02.038: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-5775 pod-service-account-c3ff8a2f-0086-4aa8-b370-e9c08a503e36 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar 26 18:12:02.546: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-5775 pod-service-account-c3ff8a2f-0086-4aa8-b370-e9c08a503e36 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:12:03.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5775" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":277,"completed":121,"skipped":1991,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:12:03.177: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-r8gdk in namespace proxy-3425
I0326 18:12:03.423712    9212 runners.go:190] Created replication controller with name: proxy-service-r8gdk, namespace: proxy-3425, replica count: 1
I0326 18:12:04.474311    9212 runners.go:190] proxy-service-r8gdk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0326 18:12:05.474529    9212 runners.go:190] proxy-service-r8gdk Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0326 18:12:06.474776    9212 runners.go:190] proxy-service-r8gdk Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0326 18:12:07.475016    9212 runners.go:190] proxy-service-r8gdk Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0326 18:12:08.475243    9212 runners.go:190] proxy-service-r8gdk Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0326 18:12:09.475551    9212 runners.go:190] proxy-service-r8gdk Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0326 18:12:10.475949    9212 runners.go:190] proxy-service-r8gdk Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 26 18:12:10.515: INFO: setup took 7.183135256s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 26 18:12:10.569: INFO: (0) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 54.358142ms)
Mar 26 18:12:10.569: INFO: (0) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 54.529738ms)
Mar 26 18:12:10.569: INFO: (0) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 54.501706ms)
Mar 26 18:12:10.569: INFO: (0) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 54.330437ms)
Mar 26 18:12:10.569: INFO: (0) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 54.350996ms)
Mar 26 18:12:10.570: INFO: (0) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 55.372315ms)
Mar 26 18:12:10.571: INFO: (0) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 56.247892ms)
Mar 26 18:12:10.572: INFO: (0) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 57.167123ms)
Mar 26 18:12:10.572: INFO: (0) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 57.125246ms)
Mar 26 18:12:10.576: INFO: (0) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 61.192467ms)
Mar 26 18:12:10.592: INFO: (0) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 76.806177ms)
Mar 26 18:12:10.592: INFO: (0) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 77.209959ms)
Mar 26 18:12:10.592: INFO: (0) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 77.243317ms)
Mar 26 18:12:10.595: INFO: (0) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 79.878597ms)
Mar 26 18:12:10.598: INFO: (0) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 83.263366ms)
Mar 26 18:12:10.599: INFO: (0) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 84.47044ms)
Mar 26 18:12:10.649: INFO: (1) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 49.719871ms)
Mar 26 18:12:10.653: INFO: (1) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 53.424294ms)
Mar 26 18:12:10.653: INFO: (1) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 53.488867ms)
Mar 26 18:12:10.653: INFO: (1) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 53.476994ms)
Mar 26 18:12:10.653: INFO: (1) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 53.508382ms)
Mar 26 18:12:10.654: INFO: (1) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 54.504418ms)
Mar 26 18:12:10.654: INFO: (1) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 54.516415ms)
Mar 26 18:12:10.654: INFO: (1) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 54.547815ms)
Mar 26 18:12:10.659: INFO: (1) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 59.108282ms)
Mar 26 18:12:10.659: INFO: (1) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 58.903884ms)
Mar 26 18:12:10.661: INFO: (1) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 61.271116ms)
Mar 26 18:12:10.662: INFO: (1) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 62.072135ms)
Mar 26 18:12:10.663: INFO: (1) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 63.349186ms)
Mar 26 18:12:10.664: INFO: (1) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 63.853945ms)
Mar 26 18:12:10.664: INFO: (1) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 63.942038ms)
Mar 26 18:12:10.665: INFO: (1) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 65.129491ms)
Mar 26 18:12:10.714: INFO: (2) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 49.15757ms)
Mar 26 18:12:10.716: INFO: (2) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 50.924739ms)
Mar 26 18:12:10.716: INFO: (2) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 50.834507ms)
Mar 26 18:12:10.717: INFO: (2) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 52.256619ms)
Mar 26 18:12:10.719: INFO: (2) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 54.118661ms)
Mar 26 18:12:10.719: INFO: (2) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 54.114686ms)
Mar 26 18:12:10.720: INFO: (2) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 55.309782ms)
Mar 26 18:12:10.722: INFO: (2) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 56.694783ms)
Mar 26 18:12:10.724: INFO: (2) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 58.741206ms)
Mar 26 18:12:10.724: INFO: (2) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 59.317198ms)
Mar 26 18:12:10.724: INFO: (2) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 59.474375ms)
Mar 26 18:12:10.725: INFO: (2) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 59.635622ms)
Mar 26 18:12:10.726: INFO: (2) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 60.937473ms)
Mar 26 18:12:10.727: INFO: (2) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 61.497462ms)
Mar 26 18:12:10.728: INFO: (2) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 62.555237ms)
Mar 26 18:12:10.728: INFO: (2) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 62.571064ms)
Mar 26 18:12:10.779: INFO: (3) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 51.665956ms)
Mar 26 18:12:10.783: INFO: (3) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 55.405193ms)
Mar 26 18:12:10.783: INFO: (3) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 55.391022ms)
Mar 26 18:12:10.783: INFO: (3) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 55.552862ms)
Mar 26 18:12:10.784: INFO: (3) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 56.245071ms)
Mar 26 18:12:10.784: INFO: (3) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 56.454216ms)
Mar 26 18:12:10.784: INFO: (3) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 56.214982ms)
Mar 26 18:12:10.787: INFO: (3) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 58.960521ms)
Mar 26 18:12:10.787: INFO: (3) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 58.881735ms)
Mar 26 18:12:10.787: INFO: (3) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 58.916491ms)
Mar 26 18:12:10.789: INFO: (3) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 60.817983ms)
Mar 26 18:12:10.789: INFO: (3) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 60.915582ms)
Mar 26 18:12:10.789: INFO: (3) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 61.020672ms)
Mar 26 18:12:10.789: INFO: (3) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 61.151999ms)
Mar 26 18:12:10.789: INFO: (3) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 61.279329ms)
Mar 26 18:12:10.790: INFO: (3) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 62.361615ms)
Mar 26 18:12:10.840: INFO: (4) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 49.895683ms)
Mar 26 18:12:10.840: INFO: (4) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 50.030373ms)
Mar 26 18:12:10.840: INFO: (4) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 49.914914ms)
Mar 26 18:12:10.845: INFO: (4) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 53.915103ms)
Mar 26 18:12:10.850: INFO: (4) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 59.591969ms)
Mar 26 18:12:10.850: INFO: (4) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 59.655822ms)
Mar 26 18:12:10.850: INFO: (4) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 59.599069ms)
Mar 26 18:12:10.851: INFO: (4) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 60.631602ms)
Mar 26 18:12:10.851: INFO: (4) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 60.691931ms)
Mar 26 18:12:10.851: INFO: (4) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 60.660451ms)
Mar 26 18:12:10.851: INFO: (4) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 60.717923ms)
Mar 26 18:12:10.851: INFO: (4) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 60.951804ms)
Mar 26 18:12:10.852: INFO: (4) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 61.390299ms)
Mar 26 18:12:10.854: INFO: (4) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 63.430654ms)
Mar 26 18:12:10.854: INFO: (4) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 63.453679ms)
Mar 26 18:12:10.854: INFO: (4) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 63.502693ms)
Mar 26 18:12:10.902: INFO: (5) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 47.732334ms)
Mar 26 18:12:10.905: INFO: (5) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 50.722301ms)
Mar 26 18:12:10.908: INFO: (5) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 53.185003ms)
Mar 26 18:12:10.908: INFO: (5) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 53.492702ms)
Mar 26 18:12:10.908: INFO: (5) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 53.400683ms)
Mar 26 18:12:10.908: INFO: (5) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 53.33364ms)
Mar 26 18:12:10.908: INFO: (5) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 53.393211ms)
Mar 26 18:12:10.910: INFO: (5) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 55.418267ms)
Mar 26 18:12:10.910: INFO: (5) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 55.48991ms)
Mar 26 18:12:10.910: INFO: (5) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 55.581242ms)
Mar 26 18:12:10.914: INFO: (5) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 59.44236ms)
Mar 26 18:12:10.914: INFO: (5) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 59.882398ms)
Mar 26 18:12:10.915: INFO: (5) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 60.970271ms)
Mar 26 18:12:10.916: INFO: (5) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 61.357321ms)
Mar 26 18:12:10.916: INFO: (5) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 61.445303ms)
Mar 26 18:12:10.918: INFO: (5) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 63.371743ms)
Mar 26 18:12:10.965: INFO: (6) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 47.083783ms)
Mar 26 18:12:10.967: INFO: (6) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 49.41344ms)
Mar 26 18:12:10.968: INFO: (6) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 50.267796ms)
Mar 26 18:12:10.969: INFO: (6) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 51.311475ms)
Mar 26 18:12:10.972: INFO: (6) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 53.720871ms)
Mar 26 18:12:10.973: INFO: (6) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 55.179811ms)
Mar 26 18:12:10.973: INFO: (6) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 55.106812ms)
Mar 26 18:12:10.973: INFO: (6) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 55.277823ms)
Mar 26 18:12:10.974: INFO: (6) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 56.4319ms)
Mar 26 18:12:10.976: INFO: (6) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 58.13955ms)
Mar 26 18:12:10.979: INFO: (6) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 61.492376ms)
Mar 26 18:12:10.980: INFO: (6) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 62.581708ms)
Mar 26 18:12:10.980: INFO: (6) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 62.643746ms)
Mar 26 18:12:10.981: INFO: (6) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 63.132022ms)
Mar 26 18:12:10.981: INFO: (6) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 63.546514ms)
Mar 26 18:12:10.982: INFO: (6) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 64.536301ms)
Mar 26 18:12:11.057: INFO: (7) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 74.543418ms)
Mar 26 18:12:11.058: INFO: (7) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 75.561602ms)
Mar 26 18:12:11.060: INFO: (7) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 77.963925ms)
Mar 26 18:12:11.060: INFO: (7) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 78.137212ms)
Mar 26 18:12:11.061: INFO: (7) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 78.078629ms)
Mar 26 18:12:11.061: INFO: (7) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 78.195555ms)
Mar 26 18:12:11.063: INFO: (7) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 80.441656ms)
Mar 26 18:12:11.063: INFO: (7) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 80.572565ms)
Mar 26 18:12:11.064: INFO: (7) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 81.489491ms)
Mar 26 18:12:11.074: INFO: (7) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 91.401745ms)
Mar 26 18:12:11.076: INFO: (7) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 92.999244ms)
Mar 26 18:12:11.076: INFO: (7) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 93.029084ms)
Mar 26 18:12:11.076: INFO: (7) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 93.153936ms)
Mar 26 18:12:11.079: INFO: (7) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 95.966551ms)
Mar 26 18:12:11.079: INFO: (7) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 96.183305ms)
Mar 26 18:12:11.080: INFO: (7) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 97.183113ms)
Mar 26 18:12:11.130: INFO: (8) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 49.373038ms)
Mar 26 18:12:11.133: INFO: (8) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 52.590089ms)
Mar 26 18:12:11.133: INFO: (8) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 52.829807ms)
Mar 26 18:12:11.133: INFO: (8) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 52.73824ms)
Mar 26 18:12:11.135: INFO: (8) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 55.26297ms)
Mar 26 18:12:11.138: INFO: (8) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 57.368609ms)
Mar 26 18:12:11.138: INFO: (8) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 57.51415ms)
Mar 26 18:12:11.138: INFO: (8) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 57.533774ms)
Mar 26 18:12:11.139: INFO: (8) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 58.530253ms)
Mar 26 18:12:11.140: INFO: (8) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 59.779922ms)
Mar 26 18:12:11.140: INFO: (8) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 59.968867ms)
Mar 26 18:12:11.140: INFO: (8) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 60.151133ms)
Mar 26 18:12:11.140: INFO: (8) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 60.137775ms)
Mar 26 18:12:11.143: INFO: (8) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 63.283473ms)
Mar 26 18:12:11.143: INFO: (8) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 63.228273ms)
Mar 26 18:12:11.143: INFO: (8) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 63.546284ms)
Mar 26 18:12:11.196: INFO: (9) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 52.763449ms)
Mar 26 18:12:11.196: INFO: (9) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 52.674235ms)
Mar 26 18:12:11.196: INFO: (9) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 52.695669ms)
Mar 26 18:12:11.196: INFO: (9) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 52.881907ms)
Mar 26 18:12:11.196: INFO: (9) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 52.755933ms)
Mar 26 18:12:11.197: INFO: (9) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 53.134618ms)
Mar 26 18:12:11.200: INFO: (9) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 56.434842ms)
Mar 26 18:12:11.201: INFO: (9) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 57.070001ms)
Mar 26 18:12:11.202: INFO: (9) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 58.605051ms)
Mar 26 18:12:11.203: INFO: (9) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 59.038488ms)
Mar 26 18:12:11.205: INFO: (9) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 60.954036ms)
Mar 26 18:12:11.205: INFO: (9) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 60.81991ms)
Mar 26 18:12:11.205: INFO: (9) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 61.035043ms)
Mar 26 18:12:11.205: INFO: (9) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 60.991569ms)
Mar 26 18:12:11.205: INFO: (9) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 61.008302ms)
Mar 26 18:12:11.205: INFO: (9) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 61.336817ms)
Mar 26 18:12:11.257: INFO: (10) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 52.172649ms)
Mar 26 18:12:11.261: INFO: (10) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 55.500793ms)
Mar 26 18:12:11.261: INFO: (10) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 55.431903ms)
Mar 26 18:12:11.261: INFO: (10) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 55.647256ms)
Mar 26 18:12:11.262: INFO: (10) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 56.777792ms)
Mar 26 18:12:11.264: INFO: (10) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 59.075689ms)
Mar 26 18:12:11.264: INFO: (10) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 59.133912ms)
Mar 26 18:12:11.266: INFO: (10) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 60.518947ms)
Mar 26 18:12:11.267: INFO: (10) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 62.106753ms)
Mar 26 18:12:11.268: INFO: (10) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 62.347363ms)
Mar 26 18:12:11.268: INFO: (10) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 62.58676ms)
Mar 26 18:12:11.268: INFO: (10) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 62.999891ms)
Mar 26 18:12:11.270: INFO: (10) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 64.65397ms)
Mar 26 18:12:11.270: INFO: (10) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 64.590646ms)
Mar 26 18:12:11.270: INFO: (10) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 64.663351ms)
Mar 26 18:12:11.270: INFO: (10) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 64.720946ms)
Mar 26 18:12:11.317: INFO: (11) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 46.86844ms)
Mar 26 18:12:11.317: INFO: (11) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 47.013154ms)
Mar 26 18:12:11.317: INFO: (11) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 47.142265ms)
Mar 26 18:12:11.325: INFO: (11) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 55.292752ms)
Mar 26 18:12:11.329: INFO: (11) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 58.158354ms)
Mar 26 18:12:11.329: INFO: (11) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 58.235099ms)
Mar 26 18:12:11.329: INFO: (11) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 58.289244ms)
Mar 26 18:12:11.329: INFO: (11) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 58.246139ms)
Mar 26 18:12:11.329: INFO: (11) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 58.607067ms)
Mar 26 18:12:11.331: INFO: (11) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 61.115143ms)
Mar 26 18:12:11.332: INFO: (11) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 61.601298ms)
Mar 26 18:12:11.333: INFO: (11) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 62.417616ms)
Mar 26 18:12:11.333: INFO: (11) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 62.657325ms)
Mar 26 18:12:11.334: INFO: (11) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 63.834432ms)
Mar 26 18:12:11.334: INFO: (11) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 64.170859ms)
Mar 26 18:12:11.334: INFO: (11) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 64.179058ms)
Mar 26 18:12:11.384: INFO: (12) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 49.271911ms)
Mar 26 18:12:11.387: INFO: (12) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 51.967434ms)
Mar 26 18:12:11.387: INFO: (12) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 52.329659ms)
Mar 26 18:12:11.387: INFO: (12) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 52.572749ms)
Mar 26 18:12:11.391: INFO: (12) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 55.844023ms)
Mar 26 18:12:11.391: INFO: (12) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 56.458933ms)
Mar 26 18:12:11.392: INFO: (12) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 57.463722ms)
Mar 26 18:12:11.394: INFO: (12) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 59.68448ms)
Mar 26 18:12:11.394: INFO: (12) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 59.661871ms)
Mar 26 18:12:11.395: INFO: (12) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 60.152685ms)
Mar 26 18:12:11.395: INFO: (12) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 60.01027ms)
Mar 26 18:12:11.397: INFO: (12) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 61.927531ms)
Mar 26 18:12:11.398: INFO: (12) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 63.158525ms)
Mar 26 18:12:11.398: INFO: (12) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 63.074682ms)
Mar 26 18:12:11.398: INFO: (12) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 63.018559ms)
Mar 26 18:12:11.398: INFO: (12) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 63.103904ms)
Mar 26 18:12:11.448: INFO: (13) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 49.715283ms)
Mar 26 18:12:11.448: INFO: (13) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 49.609585ms)
Mar 26 18:12:11.452: INFO: (13) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 53.786657ms)
Mar 26 18:12:11.457: INFO: (13) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 58.459739ms)
Mar 26 18:12:11.457: INFO: (13) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 58.673235ms)
Mar 26 18:12:11.457: INFO: (13) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 58.676553ms)
Mar 26 18:12:11.457: INFO: (13) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 59.189827ms)
Mar 26 18:12:11.457: INFO: (13) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 59.233456ms)
Mar 26 18:12:11.457: INFO: (13) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 59.354018ms)
Mar 26 18:12:11.457: INFO: (13) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 59.255335ms)
Mar 26 18:12:11.458: INFO: (13) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 60.122611ms)
Mar 26 18:12:11.459: INFO: (13) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 60.616288ms)
Mar 26 18:12:11.460: INFO: (13) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 62.122573ms)
Mar 26 18:12:11.460: INFO: (13) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 62.105558ms)
Mar 26 18:12:11.461: INFO: (13) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 63.340834ms)
Mar 26 18:12:11.461: INFO: (13) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 63.488984ms)
Mar 26 18:12:11.513: INFO: (14) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 51.401104ms)
Mar 26 18:12:11.515: INFO: (14) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 53.242628ms)
Mar 26 18:12:11.515: INFO: (14) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 53.373381ms)
Mar 26 18:12:11.517: INFO: (14) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 55.076137ms)
Mar 26 18:12:11.519: INFO: (14) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 56.918495ms)
Mar 26 18:12:11.522: INFO: (14) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 59.887896ms)
Mar 26 18:12:11.522: INFO: (14) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 60.053333ms)
Mar 26 18:12:11.522: INFO: (14) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 60.091871ms)
Mar 26 18:12:11.523: INFO: (14) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 60.811903ms)
Mar 26 18:12:11.523: INFO: (14) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 61.554964ms)
Mar 26 18:12:11.523: INFO: (14) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 61.252188ms)
Mar 26 18:12:11.524: INFO: (14) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 62.406871ms)
Mar 26 18:12:11.524: INFO: (14) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 62.48862ms)
Mar 26 18:12:11.526: INFO: (14) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 63.937105ms)
Mar 26 18:12:11.526: INFO: (14) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 64.289011ms)
Mar 26 18:12:11.527: INFO: (14) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 65.356704ms)
Mar 26 18:12:11.574: INFO: (15) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 46.87324ms)
Mar 26 18:12:11.576: INFO: (15) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 49.255722ms)
Mar 26 18:12:11.577: INFO: (15) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 49.194299ms)
Mar 26 18:12:11.578: INFO: (15) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 50.757266ms)
Mar 26 18:12:11.583: INFO: (15) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 55.481005ms)
Mar 26 18:12:11.583: INFO: (15) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 55.498776ms)
Mar 26 18:12:11.583: INFO: (15) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 55.660013ms)
Mar 26 18:12:11.583: INFO: (15) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 55.955218ms)
Mar 26 18:12:11.584: INFO: (15) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 56.740345ms)
Mar 26 18:12:11.585: INFO: (15) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 57.862211ms)
Mar 26 18:12:11.588: INFO: (15) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 60.83986ms)
Mar 26 18:12:11.589: INFO: (15) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 62.019318ms)
Mar 26 18:12:11.589: INFO: (15) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 62.008852ms)
Mar 26 18:12:11.590: INFO: (15) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 62.392974ms)
Mar 26 18:12:11.590: INFO: (15) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 62.655137ms)
Mar 26 18:12:11.590: INFO: (15) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 62.621379ms)
Mar 26 18:12:11.638: INFO: (16) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 48.063299ms)
Mar 26 18:12:11.640: INFO: (16) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 49.558792ms)
Mar 26 18:12:11.640: INFO: (16) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 50.239102ms)
Mar 26 18:12:11.640: INFO: (16) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 50.309704ms)
Mar 26 18:12:11.640: INFO: (16) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 50.222043ms)
Mar 26 18:12:11.642: INFO: (16) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 51.779488ms)
Mar 26 18:12:11.650: INFO: (16) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 59.314925ms)
Mar 26 18:12:11.650: INFO: (16) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 59.435069ms)
Mar 26 18:12:11.650: INFO: (16) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 59.505845ms)
Mar 26 18:12:11.650: INFO: (16) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 59.394771ms)
Mar 26 18:12:11.650: INFO: (16) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 59.55608ms)
Mar 26 18:12:11.650: INFO: (16) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 59.615642ms)
Mar 26 18:12:11.651: INFO: (16) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 60.913703ms)
Mar 26 18:12:11.652: INFO: (16) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 61.744372ms)
Mar 26 18:12:11.652: INFO: (16) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 62.130644ms)
Mar 26 18:12:11.654: INFO: (16) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 63.96825ms)
Mar 26 18:12:11.707: INFO: (17) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 53.267776ms)
Mar 26 18:12:11.708: INFO: (17) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 53.782128ms)
Mar 26 18:12:11.708: INFO: (17) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 53.887125ms)
Mar 26 18:12:11.708: INFO: (17) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 53.877702ms)
Mar 26 18:12:11.708: INFO: (17) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 53.892293ms)
Mar 26 18:12:11.711: INFO: (17) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 57.019422ms)
Mar 26 18:12:11.712: INFO: (17) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 58.023742ms)
Mar 26 18:12:11.715: INFO: (17) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 60.912746ms)
Mar 26 18:12:11.716: INFO: (17) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 61.489268ms)
Mar 26 18:12:11.716: INFO: (17) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 61.651465ms)
Mar 26 18:12:11.717: INFO: (17) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 62.992185ms)
Mar 26 18:12:11.718: INFO: (17) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 63.498463ms)
Mar 26 18:12:11.719: INFO: (17) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 64.581123ms)
Mar 26 18:12:11.721: INFO: (17) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 66.15698ms)
Mar 26 18:12:11.721: INFO: (17) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 66.115149ms)
Mar 26 18:12:11.721: INFO: (17) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 66.113767ms)
Mar 26 18:12:11.771: INFO: (18) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 50.12633ms)
Mar 26 18:12:11.772: INFO: (18) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 51.249026ms)
Mar 26 18:12:11.772: INFO: (18) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 51.351452ms)
Mar 26 18:12:11.774: INFO: (18) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 53.067861ms)
Mar 26 18:12:11.775: INFO: (18) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 54.705907ms)
Mar 26 18:12:11.776: INFO: (18) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 54.69542ms)
Mar 26 18:12:11.776: INFO: (18) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 54.75798ms)
Mar 26 18:12:11.776: INFO: (18) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 55.332244ms)
Mar 26 18:12:11.777: INFO: (18) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 56.401897ms)
Mar 26 18:12:11.777: INFO: (18) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 56.471857ms)
Mar 26 18:12:11.781: INFO: (18) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 60.223186ms)
Mar 26 18:12:11.782: INFO: (18) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 61.038248ms)
Mar 26 18:12:11.782: INFO: (18) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 61.357487ms)
Mar 26 18:12:11.783: INFO: (18) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 61.717845ms)
Mar 26 18:12:11.783: INFO: (18) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 62.118369ms)
Mar 26 18:12:11.784: INFO: (18) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 63.266986ms)
Mar 26 18:12:11.836: INFO: (19) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">test<... (200; 51.616085ms)
Mar 26 18:12:11.836: INFO: (19) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:443/proxy/tlsrewritem... (200; 51.780992ms)
Mar 26 18:12:11.836: INFO: (19) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 51.70598ms)
Mar 26 18:12:11.836: INFO: (19) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:1080/proxy/rewriteme">... (200; 51.842385ms)
Mar 26 18:12:11.836: INFO: (19) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/: <a href="/api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn/proxy/rewriteme">test</a> (200; 51.670454ms)
Mar 26 18:12:11.857: INFO: (19) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 72.781027ms)
Mar 26 18:12:11.859: INFO: (19) /api/v1/namespaces/proxy-3425/pods/http:proxy-service-r8gdk-9h8vn:160/proxy/: foo (200; 74.671357ms)
Mar 26 18:12:11.859: INFO: (19) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:460/proxy/: tls baz (200; 74.535869ms)
Mar 26 18:12:11.859: INFO: (19) /api/v1/namespaces/proxy-3425/pods/proxy-service-r8gdk-9h8vn:162/proxy/: bar (200; 74.567958ms)
Mar 26 18:12:11.861: INFO: (19) /api/v1/namespaces/proxy-3425/pods/https:proxy-service-r8gdk-9h8vn:462/proxy/: tls qux (200; 76.969558ms)
Mar 26 18:12:11.868: INFO: (19) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname2/proxy/: bar (200; 83.579459ms)
Mar 26 18:12:11.868: INFO: (19) /api/v1/namespaces/proxy-3425/services/http:proxy-service-r8gdk:portname1/proxy/: foo (200; 84.045907ms)
Mar 26 18:12:11.868: INFO: (19) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname2/proxy/: bar (200; 84.084858ms)
Mar 26 18:12:11.870: INFO: (19) /api/v1/namespaces/proxy-3425/services/proxy-service-r8gdk:portname1/proxy/: foo (200; 85.245151ms)
Mar 26 18:12:11.873: INFO: (19) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname2/proxy/: tls qux (200; 88.348043ms)
Mar 26 18:12:11.874: INFO: (19) /api/v1/namespaces/proxy-3425/services/https:proxy-service-r8gdk:tlsportname1/proxy/: tls baz (200; 89.590996ms)
STEP: deleting ReplicationController proxy-service-r8gdk in namespace proxy-3425, will wait for the garbage collector to delete the pods
Mar 26 18:12:12.003: INFO: Deleting ReplicationController proxy-service-r8gdk took: 40.405136ms
Mar 26 18:12:12.703: INFO: Terminating ReplicationController proxy-service-r8gdk pods took: 700.3641ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:12:23.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3425" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":277,"completed":122,"skipped":1999,"failed":0}
S
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:12:23.189: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 26 18:12:27.665: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6560 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:12:27.665: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:12:27.986: INFO: Exec stderr: ""
Mar 26 18:12:27.986: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6560 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:12:27.987: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:12:28.333: INFO: Exec stderr: ""
Mar 26 18:12:28.333: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6560 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:12:28.333: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:12:28.685: INFO: Exec stderr: ""
Mar 26 18:12:28.685: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6560 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:12:28.685: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:12:29.025: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 26 18:12:29.025: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6560 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:12:29.025: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:12:29.325: INFO: Exec stderr: ""
Mar 26 18:12:29.325: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6560 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:12:29.325: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:12:29.668: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 26 18:12:29.668: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6560 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:12:29.668: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:12:29.991: INFO: Exec stderr: ""
Mar 26 18:12:29.991: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6560 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:12:29.992: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:12:30.348: INFO: Exec stderr: ""
Mar 26 18:12:30.349: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6560 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:12:30.349: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:12:30.697: INFO: Exec stderr: ""
Mar 26 18:12:30.697: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6560 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:12:30.697: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:12:31.040: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:12:31.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6560" for this suite.
•{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":123,"skipped":2000,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:12:31.122: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 18:12:32.240: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843152, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843152, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843152, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843152, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:12:34.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843152, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843152, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843152, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843152, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 18:12:37.370: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:12:37.410: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:12:38.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2362" for this suite.
STEP: Destroying namespace "webhook-2362-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":277,"completed":124,"skipped":2000,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:12:38.484: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 18:12:38.688: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6afd99d-9d7a-4197-9359-ce33fcdccebb" in namespace "projected-8028" to be "Succeeded or Failed"
Mar 26 18:12:38.726: INFO: Pod "downwardapi-volume-b6afd99d-9d7a-4197-9359-ce33fcdccebb": Phase="Pending", Reason="", readiness=false. Elapsed: 38.608249ms
Mar 26 18:12:40.765: INFO: Pod "downwardapi-volume-b6afd99d-9d7a-4197-9359-ce33fcdccebb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077236033s
STEP: Saw pod success
Mar 26 18:12:40.765: INFO: Pod "downwardapi-volume-b6afd99d-9d7a-4197-9359-ce33fcdccebb" satisfied condition "Succeeded or Failed"
Mar 26 18:12:40.803: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod downwardapi-volume-b6afd99d-9d7a-4197-9359-ce33fcdccebb container client-container: <nil>
STEP: delete the pod
Mar 26 18:12:40.896: INFO: Waiting for pod downwardapi-volume-b6afd99d-9d7a-4197-9359-ce33fcdccebb to disappear
Mar 26 18:12:40.934: INFO: Pod downwardapi-volume-b6afd99d-9d7a-4197-9359-ce33fcdccebb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:12:40.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8028" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":125,"skipped":2000,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:12:41.015: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8481 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8481;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8481 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8481;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8481.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8481.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8481.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8481.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8481.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8481.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8481.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8481.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8481.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8481.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8481.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8481.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8481.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 249.255.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.255.249_udp@PTR;check="$$(dig +tcp +noall +answer +search 249.255.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.255.249_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8481 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8481;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8481 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8481;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8481.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8481.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8481.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8481.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8481.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8481.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8481.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8481.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8481.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8481.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8481.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8481.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8481.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 249.255.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.255.249_udp@PTR;check="$$(dig +tcp +noall +answer +search 249.255.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.255.249_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 18:12:43.591: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:43.632: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:43.675: INFO: Unable to read wheezy_udp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:43.720: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:43.763: INFO: Unable to read wheezy_udp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:43.804: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:43.852: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:43.895: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:44.181: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:44.225: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:44.265: INFO: Unable to read jessie_udp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:44.305: INFO: Unable to read jessie_tcp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:44.344: INFO: Unable to read jessie_udp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:44.383: INFO: Unable to read jessie_tcp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:44.423: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:44.462: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:44.702: INFO: Lookups using dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8481 wheezy_tcp@dns-test-service.dns-8481 wheezy_udp@dns-test-service.dns-8481.svc wheezy_tcp@dns-test-service.dns-8481.svc wheezy_udp@_http._tcp.dns-test-service.dns-8481.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8481.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8481 jessie_tcp@dns-test-service.dns-8481 jessie_udp@dns-test-service.dns-8481.svc jessie_tcp@dns-test-service.dns-8481.svc jessie_udp@_http._tcp.dns-test-service.dns-8481.svc jessie_tcp@_http._tcp.dns-test-service.dns-8481.svc]

Mar 26 18:12:49.742: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:49.783: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:49.822: INFO: Unable to read wheezy_udp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:49.862: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:49.902: INFO: Unable to read wheezy_udp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:49.941: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:49.981: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:50.021: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:50.300: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:50.340: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:50.380: INFO: Unable to read jessie_udp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:50.419: INFO: Unable to read jessie_tcp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:50.459: INFO: Unable to read jessie_udp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:50.499: INFO: Unable to read jessie_tcp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:50.539: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:50.581: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:50.820: INFO: Lookups using dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8481 wheezy_tcp@dns-test-service.dns-8481 wheezy_udp@dns-test-service.dns-8481.svc wheezy_tcp@dns-test-service.dns-8481.svc wheezy_udp@_http._tcp.dns-test-service.dns-8481.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8481.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8481 jessie_tcp@dns-test-service.dns-8481 jessie_udp@dns-test-service.dns-8481.svc jessie_tcp@dns-test-service.dns-8481.svc jessie_udp@_http._tcp.dns-test-service.dns-8481.svc jessie_tcp@_http._tcp.dns-test-service.dns-8481.svc]

Mar 26 18:12:54.744: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:54.784: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:54.824: INFO: Unable to read wheezy_udp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:54.865: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:54.905: INFO: Unable to read wheezy_udp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:54.944: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:54.984: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:55.023: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:55.303: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:55.343: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:55.383: INFO: Unable to read jessie_udp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:55.422: INFO: Unable to read jessie_tcp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:55.462: INFO: Unable to read jessie_udp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:55.502: INFO: Unable to read jessie_tcp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:55.541: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:55.584: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:55.829: INFO: Lookups using dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8481 wheezy_tcp@dns-test-service.dns-8481 wheezy_udp@dns-test-service.dns-8481.svc wheezy_tcp@dns-test-service.dns-8481.svc wheezy_udp@_http._tcp.dns-test-service.dns-8481.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8481.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8481 jessie_tcp@dns-test-service.dns-8481 jessie_udp@dns-test-service.dns-8481.svc jessie_tcp@dns-test-service.dns-8481.svc jessie_udp@_http._tcp.dns-test-service.dns-8481.svc jessie_tcp@_http._tcp.dns-test-service.dns-8481.svc]

Mar 26 18:12:59.742: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:59.781: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:59.821: INFO: Unable to read wheezy_udp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:59.861: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:59.900: INFO: Unable to read wheezy_udp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:59.940: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:12:59.980: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:00.019: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:00.301: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:00.340: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:00.379: INFO: Unable to read jessie_udp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:00.419: INFO: Unable to read jessie_tcp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:00.458: INFO: Unable to read jessie_udp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:00.505: INFO: Unable to read jessie_tcp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:00.544: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:00.584: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:00.825: INFO: Lookups using dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8481 wheezy_tcp@dns-test-service.dns-8481 wheezy_udp@dns-test-service.dns-8481.svc wheezy_tcp@dns-test-service.dns-8481.svc wheezy_udp@_http._tcp.dns-test-service.dns-8481.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8481.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8481 jessie_tcp@dns-test-service.dns-8481 jessie_udp@dns-test-service.dns-8481.svc jessie_tcp@dns-test-service.dns-8481.svc jessie_udp@_http._tcp.dns-test-service.dns-8481.svc jessie_tcp@_http._tcp.dns-test-service.dns-8481.svc]

Mar 26 18:13:04.746: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:04.786: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:04.826: INFO: Unable to read wheezy_udp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:04.865: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:04.904: INFO: Unable to read wheezy_udp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:04.944: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:04.984: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:05.023: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:05.306: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:05.345: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:05.385: INFO: Unable to read jessie_udp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:05.425: INFO: Unable to read jessie_tcp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:05.464: INFO: Unable to read jessie_udp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:05.503: INFO: Unable to read jessie_tcp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:05.542: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:05.581: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:05.820: INFO: Lookups using dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8481 wheezy_tcp@dns-test-service.dns-8481 wheezy_udp@dns-test-service.dns-8481.svc wheezy_tcp@dns-test-service.dns-8481.svc wheezy_udp@_http._tcp.dns-test-service.dns-8481.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8481.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8481 jessie_tcp@dns-test-service.dns-8481 jessie_udp@dns-test-service.dns-8481.svc jessie_tcp@dns-test-service.dns-8481.svc jessie_udp@_http._tcp.dns-test-service.dns-8481.svc jessie_tcp@_http._tcp.dns-test-service.dns-8481.svc]

Mar 26 18:13:09.747: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:09.786: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:09.828: INFO: Unable to read wheezy_udp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:09.867: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:09.907: INFO: Unable to read wheezy_udp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:09.946: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:09.985: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:10.024: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:10.304: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:10.344: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:10.385: INFO: Unable to read jessie_udp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:10.425: INFO: Unable to read jessie_tcp@dns-test-service.dns-8481 from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:10.465: INFO: Unable to read jessie_udp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:10.508: INFO: Unable to read jessie_tcp@dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:10.548: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:10.588: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8481.svc from pod dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a: the server could not find the requested resource (get pods dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a)
Mar 26 18:13:10.826: INFO: Lookups using dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8481 wheezy_tcp@dns-test-service.dns-8481 wheezy_udp@dns-test-service.dns-8481.svc wheezy_tcp@dns-test-service.dns-8481.svc wheezy_udp@_http._tcp.dns-test-service.dns-8481.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8481.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8481 jessie_tcp@dns-test-service.dns-8481 jessie_udp@dns-test-service.dns-8481.svc jessie_tcp@dns-test-service.dns-8481.svc jessie_udp@_http._tcp.dns-test-service.dns-8481.svc jessie_tcp@_http._tcp.dns-test-service.dns-8481.svc]

Mar 26 18:13:15.862: INFO: DNS probes using dns-8481/dns-test-ae6ee700-a279-41e0-888d-22b3b2028d9a succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:13:16.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8481" for this suite.
•{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":277,"completed":126,"skipped":2012,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:13:16.365: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-secret-jmhc
STEP: Creating a pod to test atomic-volume-subpath
Mar 26 18:13:16.683: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jmhc" in namespace "subpath-3288" to be "Succeeded or Failed"
Mar 26 18:13:16.721: INFO: Pod "pod-subpath-test-secret-jmhc": Phase="Pending", Reason="", readiness=false. Elapsed: 37.846871ms
Mar 26 18:13:18.761: INFO: Pod "pod-subpath-test-secret-jmhc": Phase="Running", Reason="", readiness=true. Elapsed: 2.077293543s
Mar 26 18:13:20.799: INFO: Pod "pod-subpath-test-secret-jmhc": Phase="Running", Reason="", readiness=true. Elapsed: 4.115445986s
Mar 26 18:13:22.837: INFO: Pod "pod-subpath-test-secret-jmhc": Phase="Running", Reason="", readiness=true. Elapsed: 6.153680118s
Mar 26 18:13:24.876: INFO: Pod "pod-subpath-test-secret-jmhc": Phase="Running", Reason="", readiness=true. Elapsed: 8.192047885s
Mar 26 18:13:26.914: INFO: Pod "pod-subpath-test-secret-jmhc": Phase="Running", Reason="", readiness=true. Elapsed: 10.230061783s
Mar 26 18:13:28.952: INFO: Pod "pod-subpath-test-secret-jmhc": Phase="Running", Reason="", readiness=true. Elapsed: 12.268239426s
Mar 26 18:13:31.002: INFO: Pod "pod-subpath-test-secret-jmhc": Phase="Running", Reason="", readiness=true. Elapsed: 14.318655382s
Mar 26 18:13:33.040: INFO: Pod "pod-subpath-test-secret-jmhc": Phase="Running", Reason="", readiness=true. Elapsed: 16.356795582s
Mar 26 18:13:35.079: INFO: Pod "pod-subpath-test-secret-jmhc": Phase="Running", Reason="", readiness=true. Elapsed: 18.395232132s
Mar 26 18:13:37.117: INFO: Pod "pod-subpath-test-secret-jmhc": Phase="Running", Reason="", readiness=true. Elapsed: 20.433797233s
Mar 26 18:13:39.158: INFO: Pod "pod-subpath-test-secret-jmhc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.474173201s
STEP: Saw pod success
Mar 26 18:13:39.158: INFO: Pod "pod-subpath-test-secret-jmhc" satisfied condition "Succeeded or Failed"
Mar 26 18:13:39.196: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-subpath-test-secret-jmhc container test-container-subpath-secret-jmhc: <nil>
STEP: delete the pod
Mar 26 18:13:39.293: INFO: Waiting for pod pod-subpath-test-secret-jmhc to disappear
Mar 26 18:13:39.331: INFO: Pod pod-subpath-test-secret-jmhc no longer exists
STEP: Deleting pod pod-subpath-test-secret-jmhc
Mar 26 18:13:39.331: INFO: Deleting pod "pod-subpath-test-secret-jmhc" in namespace "subpath-3288"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:13:39.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3288" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":277,"completed":127,"skipped":2031,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:13:39.450: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar 26 18:13:52.405: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:13:52.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5100" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":277,"completed":128,"skipped":2034,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:13:52.487: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-238469bb-1b4b-47cc-8a73-766b9584b66d
STEP: Creating a pod to test consume secrets
Mar 26 18:13:52.725: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-055d43ce-45cb-402e-ac8a-94c3e68dd19c" in namespace "projected-6774" to be "Succeeded or Failed"
Mar 26 18:13:52.774: INFO: Pod "pod-projected-secrets-055d43ce-45cb-402e-ac8a-94c3e68dd19c": Phase="Pending", Reason="", readiness=false. Elapsed: 49.256253ms
Mar 26 18:13:54.817: INFO: Pod "pod-projected-secrets-055d43ce-45cb-402e-ac8a-94c3e68dd19c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.092187327s
STEP: Saw pod success
Mar 26 18:13:54.817: INFO: Pod "pod-projected-secrets-055d43ce-45cb-402e-ac8a-94c3e68dd19c" satisfied condition "Succeeded or Failed"
Mar 26 18:13:54.861: INFO: Trying to get logs from node bootstrap-e2e-minion-group-gs7c pod pod-projected-secrets-055d43ce-45cb-402e-ac8a-94c3e68dd19c container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 26 18:13:54.960: INFO: Waiting for pod pod-projected-secrets-055d43ce-45cb-402e-ac8a-94c3e68dd19c to disappear
Mar 26 18:13:54.998: INFO: Pod pod-projected-secrets-055d43ce-45cb-402e-ac8a-94c3e68dd19c no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:13:54.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6774" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":129,"skipped":2085,"failed":0}
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:13:55.081: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating secret secrets-8813/secret-test-0f78057f-7b41-4c65-b84e-6ffa1db8f8b7
STEP: Creating a pod to test consume secrets
Mar 26 18:13:55.328: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ef7f10d-bd6a-450d-9803-f77fd38f7228" in namespace "secrets-8813" to be "Succeeded or Failed"
Mar 26 18:13:55.372: INFO: Pod "pod-configmaps-0ef7f10d-bd6a-450d-9803-f77fd38f7228": Phase="Pending", Reason="", readiness=false. Elapsed: 43.744007ms
Mar 26 18:13:57.411: INFO: Pod "pod-configmaps-0ef7f10d-bd6a-450d-9803-f77fd38f7228": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.082761521s
STEP: Saw pod success
Mar 26 18:13:57.411: INFO: Pod "pod-configmaps-0ef7f10d-bd6a-450d-9803-f77fd38f7228" satisfied condition "Succeeded or Failed"
Mar 26 18:13:57.449: INFO: Trying to get logs from node bootstrap-e2e-minion-group-gs7c pod pod-configmaps-0ef7f10d-bd6a-450d-9803-f77fd38f7228 container env-test: <nil>
STEP: delete the pod
Mar 26 18:13:57.548: INFO: Waiting for pod pod-configmaps-0ef7f10d-bd6a-450d-9803-f77fd38f7228 to disappear
Mar 26 18:13:57.607: INFO: Pod pod-configmaps-0ef7f10d-bd6a-450d-9803-f77fd38f7228 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:13:57.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8813" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":130,"skipped":2088,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:13:57.689: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test substitution in container's command
Mar 26 18:13:57.920: INFO: Waiting up to 5m0s for pod "var-expansion-fac36a93-d5bb-4f39-8df8-8c4519d4c998" in namespace "var-expansion-4893" to be "Succeeded or Failed"
Mar 26 18:13:57.958: INFO: Pod "var-expansion-fac36a93-d5bb-4f39-8df8-8c4519d4c998": Phase="Pending", Reason="", readiness=false. Elapsed: 38.599325ms
Mar 26 18:13:59.998: INFO: Pod "var-expansion-fac36a93-d5bb-4f39-8df8-8c4519d4c998": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.078481394s
STEP: Saw pod success
Mar 26 18:13:59.998: INFO: Pod "var-expansion-fac36a93-d5bb-4f39-8df8-8c4519d4c998" satisfied condition "Succeeded or Failed"
Mar 26 18:14:00.037: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod var-expansion-fac36a93-d5bb-4f39-8df8-8c4519d4c998 container dapi-container: <nil>
STEP: delete the pod
Mar 26 18:14:00.132: INFO: Waiting for pod var-expansion-fac36a93-d5bb-4f39-8df8-8c4519d4c998 to disappear
Mar 26 18:14:00.172: INFO: Pod var-expansion-fac36a93-d5bb-4f39-8df8-8c4519d4c998 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:14:00.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4893" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":277,"completed":131,"skipped":2093,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:14:00.261: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-937d7415-fef6-425c-b573-b6cec811b975
STEP: Creating a pod to test consume configMaps
Mar 26 18:14:00.555: INFO: Waiting up to 5m0s for pod "pod-configmaps-2d42d4db-9858-4768-8ff2-b739f0d41ebf" in namespace "configmap-2867" to be "Succeeded or Failed"
Mar 26 18:14:00.592: INFO: Pod "pod-configmaps-2d42d4db-9858-4768-8ff2-b739f0d41ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 37.230196ms
Mar 26 18:14:02.631: INFO: Pod "pod-configmaps-2d42d4db-9858-4768-8ff2-b739f0d41ebf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075752493s
STEP: Saw pod success
Mar 26 18:14:02.631: INFO: Pod "pod-configmaps-2d42d4db-9858-4768-8ff2-b739f0d41ebf" satisfied condition "Succeeded or Failed"
Mar 26 18:14:02.668: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-configmaps-2d42d4db-9858-4768-8ff2-b739f0d41ebf container configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 18:14:02.784: INFO: Waiting for pod pod-configmaps-2d42d4db-9858-4768-8ff2-b739f0d41ebf to disappear
Mar 26 18:14:02.822: INFO: Pod pod-configmaps-2d42d4db-9858-4768-8ff2-b739f0d41ebf no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:14:02.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2867" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":277,"completed":132,"skipped":2130,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:14:02.935: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating the pod
Mar 26 18:14:06.044: INFO: Successfully updated pod "labelsupdateab42f01f-b7f4-4866-9c58-599ac8d9ecea"
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:14:08.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9459" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":277,"completed":133,"skipped":2160,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:14:08.218: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-bdf788e2-8bbd-4e93-ba65-dbdc3c52e58b
STEP: Creating a pod to test consume configMaps
Mar 26 18:14:08.463: INFO: Waiting up to 5m0s for pod "pod-configmaps-bd59b67d-b120-4c0b-9802-8cc20d9e5d98" in namespace "configmap-9181" to be "Succeeded or Failed"
Mar 26 18:14:08.513: INFO: Pod "pod-configmaps-bd59b67d-b120-4c0b-9802-8cc20d9e5d98": Phase="Pending", Reason="", readiness=false. Elapsed: 50.242471ms
Mar 26 18:14:10.555: INFO: Pod "pod-configmaps-bd59b67d-b120-4c0b-9802-8cc20d9e5d98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.092715478s
STEP: Saw pod success
Mar 26 18:14:10.555: INFO: Pod "pod-configmaps-bd59b67d-b120-4c0b-9802-8cc20d9e5d98" satisfied condition "Succeeded or Failed"
Mar 26 18:14:10.593: INFO: Trying to get logs from node bootstrap-e2e-minion-group-d5w7 pod pod-configmaps-bd59b67d-b120-4c0b-9802-8cc20d9e5d98 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 18:14:10.698: INFO: Waiting for pod pod-configmaps-bd59b67d-b120-4c0b-9802-8cc20d9e5d98 to disappear
Mar 26 18:14:10.735: INFO: Pod pod-configmaps-bd59b67d-b120-4c0b-9802-8cc20d9e5d98 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:14:10.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9181" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":134,"skipped":2170,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:14:10.817: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 18:14:11.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-16f4ceb6-007b-4cc5-bef6-5cd91c02aa1e" in namespace "downward-api-8638" to be "Succeeded or Failed"
Mar 26 18:14:11.067: INFO: Pod "downwardapi-volume-16f4ceb6-007b-4cc5-bef6-5cd91c02aa1e": Phase="Pending", Reason="", readiness=false. Elapsed: 47.509502ms
Mar 26 18:14:13.105: INFO: Pod "downwardapi-volume-16f4ceb6-007b-4cc5-bef6-5cd91c02aa1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.08601657s
STEP: Saw pod success
Mar 26 18:14:13.106: INFO: Pod "downwardapi-volume-16f4ceb6-007b-4cc5-bef6-5cd91c02aa1e" satisfied condition "Succeeded or Failed"
Mar 26 18:14:13.144: INFO: Trying to get logs from node bootstrap-e2e-minion-group-d5w7 pod downwardapi-volume-16f4ceb6-007b-4cc5-bef6-5cd91c02aa1e container client-container: <nil>
STEP: delete the pod
Mar 26 18:14:13.268: INFO: Waiting for pod downwardapi-volume-16f4ceb6-007b-4cc5-bef6-5cd91c02aa1e to disappear
Mar 26 18:14:13.312: INFO: Pod downwardapi-volume-16f4ceb6-007b-4cc5-bef6-5cd91c02aa1e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:14:13.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8638" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":135,"skipped":2177,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:14:13.412: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-map-71ea4cc9-4ed0-4d9a-b0b7-94b6489976e5
STEP: Creating a pod to test consume secrets
Mar 26 18:14:13.826: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d44d766d-ab9a-496d-8014-d0f9e0a42a81" in namespace "projected-9654" to be "Succeeded or Failed"
Mar 26 18:14:13.865: INFO: Pod "pod-projected-secrets-d44d766d-ab9a-496d-8014-d0f9e0a42a81": Phase="Pending", Reason="", readiness=false. Elapsed: 38.779201ms
Mar 26 18:14:15.905: INFO: Pod "pod-projected-secrets-d44d766d-ab9a-496d-8014-d0f9e0a42a81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.079097774s
STEP: Saw pod success
Mar 26 18:14:15.906: INFO: Pod "pod-projected-secrets-d44d766d-ab9a-496d-8014-d0f9e0a42a81" satisfied condition "Succeeded or Failed"
Mar 26 18:14:15.943: INFO: Trying to get logs from node bootstrap-e2e-minion-group-d5w7 pod pod-projected-secrets-d44d766d-ab9a-496d-8014-d0f9e0a42a81 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 26 18:14:16.066: INFO: Waiting for pod pod-projected-secrets-d44d766d-ab9a-496d-8014-d0f9e0a42a81 to disappear
Mar 26 18:14:16.106: INFO: Pod pod-projected-secrets-d44d766d-ab9a-496d-8014-d0f9e0a42a81 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:14:16.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9654" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":136,"skipped":2185,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:14:16.188: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 26 18:14:18.558: INFO: Pod name wrapped-volume-race-fd545c50-f246-44e6-9b57-ff28c5e1ef7a: Found 2 pods out of 5
Mar 26 18:14:23.612: INFO: Pod name wrapped-volume-race-fd545c50-f246-44e6-9b57-ff28c5e1ef7a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-fd545c50-f246-44e6-9b57-ff28c5e1ef7a in namespace emptydir-wrapper-413, will wait for the garbage collector to delete the pods
Mar 26 18:14:38.006: INFO: Deleting ReplicationController wrapped-volume-race-fd545c50-f246-44e6-9b57-ff28c5e1ef7a took: 67.288746ms
Mar 26 18:14:38.706: INFO: Terminating ReplicationController wrapped-volume-race-fd545c50-f246-44e6-9b57-ff28c5e1ef7a pods took: 700.30401ms
STEP: Creating RC which spawns configmap-volume pods
Mar 26 18:14:53.416: INFO: Pod name wrapped-volume-race-102d2fbf-40c4-41f3-8965-8f10390fe98c: Found 3 pods out of 5
Mar 26 18:14:58.462: INFO: Pod name wrapped-volume-race-102d2fbf-40c4-41f3-8965-8f10390fe98c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-102d2fbf-40c4-41f3-8965-8f10390fe98c in namespace emptydir-wrapper-413, will wait for the garbage collector to delete the pods
Mar 26 18:15:10.872: INFO: Deleting ReplicationController wrapped-volume-race-102d2fbf-40c4-41f3-8965-8f10390fe98c took: 80.710629ms
Mar 26 18:15:11.572: INFO: Terminating ReplicationController wrapped-volume-race-102d2fbf-40c4-41f3-8965-8f10390fe98c pods took: 700.514852ms
STEP: Creating RC which spawns configmap-volume pods
Mar 26 18:15:23.169: INFO: Pod name wrapped-volume-race-44e6d323-6ba6-43d5-ab5b-74d5bc2d556b: Found 3 pods out of 5
Mar 26 18:15:28.214: INFO: Pod name wrapped-volume-race-44e6d323-6ba6-43d5-ab5b-74d5bc2d556b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-44e6d323-6ba6-43d5-ab5b-74d5bc2d556b in namespace emptydir-wrapper-413, will wait for the garbage collector to delete the pods
Mar 26 18:15:42.605: INFO: Deleting ReplicationController wrapped-volume-race-44e6d323-6ba6-43d5-ab5b-74d5bc2d556b took: 63.874082ms
Mar 26 18:15:43.305: INFO: Terminating ReplicationController wrapped-volume-race-44e6d323-6ba6-43d5-ab5b-74d5bc2d556b pods took: 700.303007ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:15:55.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-413" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":277,"completed":137,"skipped":2188,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:15:55.321: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5982
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5982
STEP: creating replication controller externalsvc in namespace services-5982
I0326 18:15:55.616742    9212 runners.go:190] Created replication controller with name: externalsvc, namespace: services-5982, replica count: 2
I0326 18:15:58.717399    9212 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar 26 18:15:58.846: INFO: Creating new exec pod
Mar 26 18:16:00.971: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-5982 execpodrwgpf -- /bin/sh -x -c nslookup nodeport-service'
Mar 26 18:16:01.809: INFO: stderr: "+ nslookup nodeport-service\n"
Mar 26 18:16:01.809: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nnodeport-service.services-5982.svc.cluster.local\tcanonical name = externalsvc.services-5982.svc.cluster.local.\nName:\texternalsvc.services-5982.svc.cluster.local\nAddress: 10.0.159.239\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5982, will wait for the garbage collector to delete the pods
Mar 26 18:16:01.937: INFO: Deleting ReplicationController externalsvc took: 40.019152ms
Mar 26 18:16:02.637: INFO: Terminating ReplicationController externalsvc pods took: 700.297258ms
Mar 26 18:16:12.898: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:16:12.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5982" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":277,"completed":138,"skipped":2200,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:16:13.039: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-ad2c3838-4ec0-4e57-a1b8-9a1a01e6cf02
STEP: Creating a pod to test consume configMaps
Mar 26 18:16:13.321: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-15dda06d-450e-420b-a34c-745bb5440417" in namespace "projected-5899" to be "Succeeded or Failed"
Mar 26 18:16:13.359: INFO: Pod "pod-projected-configmaps-15dda06d-450e-420b-a34c-745bb5440417": Phase="Pending", Reason="", readiness=false. Elapsed: 37.558944ms
Mar 26 18:16:15.398: INFO: Pod "pod-projected-configmaps-15dda06d-450e-420b-a34c-745bb5440417": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076278763s
STEP: Saw pod success
Mar 26 18:16:15.398: INFO: Pod "pod-projected-configmaps-15dda06d-450e-420b-a34c-745bb5440417" satisfied condition "Succeeded or Failed"
Mar 26 18:16:15.436: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-projected-configmaps-15dda06d-450e-420b-a34c-745bb5440417 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 18:16:15.545: INFO: Waiting for pod pod-projected-configmaps-15dda06d-450e-420b-a34c-745bb5440417 to disappear
Mar 26 18:16:15.584: INFO: Pod pod-projected-configmaps-15dda06d-450e-420b-a34c-745bb5440417 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:16:15.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5899" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":139,"skipped":2207,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:16:15.664: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-4274
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 26 18:16:15.862: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 26 18:16:16.121: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 26 18:16:18.160: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 18:16:20.159: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 18:16:22.160: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 18:16:24.160: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 18:16:26.161: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 18:16:28.160: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 18:16:30.159: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 18:16:32.160: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 26 18:16:32.236: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 26 18:16:32.312: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 26 18:16:34.542: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.1.126:8080/dial?request=hostname&protocol=http&host=10.64.1.125&port=8080&tries=1'] Namespace:pod-network-test-4274 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:16:34.542: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:16:34.853: INFO: Waiting for responses: map[]
Mar 26 18:16:34.891: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.1.126:8080/dial?request=hostname&protocol=http&host=10.64.0.68&port=8080&tries=1'] Namespace:pod-network-test-4274 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:16:34.891: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:16:35.233: INFO: Waiting for responses: map[]
Mar 26 18:16:35.271: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.64.1.126:8080/dial?request=hostname&protocol=http&host=10.64.2.60&port=8080&tries=1'] Namespace:pod-network-test-4274 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:16:35.271: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:16:35.596: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:16:35.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4274" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":277,"completed":140,"skipped":2233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:16:35.679: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 18:16:36.783: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843396, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843396, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843396, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843396, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:16:38.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843396, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843396, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843396, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843396, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 18:16:41.879: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:16:42.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-705" for this suite.
STEP: Destroying namespace "webhook-705-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":277,"completed":141,"skipped":2262,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:16:42.470: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to create a functioning NodePort service [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service nodeport-test with type=NodePort in namespace services-3706
STEP: creating replication controller nodeport-test in namespace services-3706
I0326 18:16:42.750201    9212 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-3706, replica count: 2
Mar 26 18:16:45.800: INFO: Creating new exec pod
I0326 18:16:45.800879    9212 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 26 18:16:48.957: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-3706 execpod6zwmh -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar 26 18:16:51.512: INFO: rc: 1
Mar 26 18:16:51.512: INFO: Service reachability failing with error: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-3706 execpod6zwmh -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 nodeport-test 80
nc: connect to nodeport-test port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Mar 26 18:16:52.512: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-3706 execpod6zwmh -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar 26 18:16:53.031: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 26 18:16:53.031: INFO: stdout: ""
Mar 26 18:16:53.031: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-3706 execpod6zwmh -- /bin/sh -x -c nc -zv -t -w 2 10.0.159.52 80'
Mar 26 18:16:53.549: INFO: stderr: "+ nc -zv -t -w 2 10.0.159.52 80\nConnection to 10.0.159.52 80 port [tcp/http] succeeded!\n"
Mar 26 18:16:53.549: INFO: stdout: ""
Mar 26 18:16:53.549: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-3706 execpod6zwmh -- /bin/sh -x -c nc -zv -t -w 2 10.138.0.3 31683'
Mar 26 18:16:54.093: INFO: stderr: "+ nc -zv -t -w 2 10.138.0.3 31683\nConnection to 10.138.0.3 31683 port [tcp/31683] succeeded!\n"
Mar 26 18:16:54.093: INFO: stdout: ""
Mar 26 18:16:54.093: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-3706 execpod6zwmh -- /bin/sh -x -c nc -zv -t -w 2 10.138.0.4 31683'
Mar 26 18:16:54.618: INFO: stderr: "+ nc -zv -t -w 2 10.138.0.4 31683\nConnection to 10.138.0.4 31683 port [tcp/31683] succeeded!\n"
Mar 26 18:16:54.618: INFO: stdout: ""
Mar 26 18:16:54.618: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-3706 execpod6zwmh -- /bin/sh -x -c nc -zv -t -w 2 35.247.65.27 31683'
Mar 26 18:16:55.155: INFO: stderr: "+ nc -zv -t -w 2 35.247.65.27 31683\nConnection to 35.247.65.27 31683 port [tcp/31683] succeeded!\n"
Mar 26 18:16:55.155: INFO: stdout: ""
Mar 26 18:16:55.155: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-3706 execpod6zwmh -- /bin/sh -x -c nc -zv -t -w 2 34.82.43.160 31683'
Mar 26 18:16:55.682: INFO: stderr: "+ nc -zv -t -w 2 34.82.43.160 31683\nConnection to 34.82.43.160 31683 port [tcp/31683] succeeded!\n"
Mar 26 18:16:55.682: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:16:55.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3706" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":277,"completed":142,"skipped":2285,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:16:55.763: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar 26 18:16:55.962: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 26 18:16:56.081: INFO: Waiting for terminating namespaces to be deleted...
Mar 26 18:16:56.119: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-1r9w before test
Mar 26 18:16:56.163: INFO: nodeport-test-tmn4n from services-3706 started at 2020-03-26 18:16:42 +0000 UTC (1 container statuses recorded)
Mar 26 18:16:56.163: INFO: 	Container nodeport-test ready: true, restart count 0
Mar 26 18:16:56.163: INFO: metadata-proxy-v0.1-gh7hj from kube-system started at 2020-03-26 17:39:23 +0000 UTC (2 container statuses recorded)
Mar 26 18:16:56.163: INFO: 	Container metadata-proxy ready: true, restart count 0
Mar 26 18:16:56.163: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:16:56.163: INFO: event-exporter-v0.3.1-d877d48cb-vwtrx from kube-system started at 2020-03-26 17:39:34 +0000 UTC (2 container statuses recorded)
Mar 26 18:16:56.163: INFO: 	Container event-exporter ready: true, restart count 0
Mar 26 18:16:56.163: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:16:56.163: INFO: kube-proxy-bootstrap-e2e-minion-group-1r9w from kube-system started at 2020-03-26 17:39:23 +0000 UTC (1 container statuses recorded)
Mar 26 18:16:56.163: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 18:16:56.163: INFO: fluentd-gcp-v3.2.0-jzmnt from kube-system started at 2020-03-26 17:40:02 +0000 UTC (2 container statuses recorded)
Mar 26 18:16:56.163: INFO: 	Container fluentd-gcp ready: true, restart count 0
Mar 26 18:16:56.163: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:16:56.163: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-d5w7 before test
Mar 26 18:16:56.214: INFO: metadata-proxy-v0.1-v2jtw from kube-system started at 2020-03-26 17:39:23 +0000 UTC (2 container statuses recorded)
Mar 26 18:16:56.214: INFO: 	Container metadata-proxy ready: true, restart count 0
Mar 26 18:16:56.214: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:16:56.214: INFO: volume-snapshot-controller-0 from kube-system started at 2020-03-26 17:39:35 +0000 UTC (1 container statuses recorded)
Mar 26 18:16:56.214: INFO: 	Container volume-snapshot-controller ready: true, restart count 0
Mar 26 18:16:56.214: INFO: execpod6zwmh from services-3706 started at 2020-03-26 18:16:45 +0000 UTC (1 container statuses recorded)
Mar 26 18:16:56.214: INFO: 	Container agnhost-pause ready: true, restart count 0
Mar 26 18:16:56.214: INFO: fluentd-gcp-v3.2.0-tgnxg from kube-system started at 2020-03-26 17:40:18 +0000 UTC (2 container statuses recorded)
Mar 26 18:16:56.214: INFO: 	Container fluentd-gcp ready: true, restart count 0
Mar 26 18:16:56.214: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:16:56.214: INFO: kube-proxy-bootstrap-e2e-minion-group-d5w7 from kube-system started at 2020-03-26 17:39:22 +0000 UTC (1 container statuses recorded)
Mar 26 18:16:56.214: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 18:16:56.214: INFO: coredns-7876554b79-sxhp6 from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:16:56.214: INFO: 	Container coredns ready: true, restart count 0
Mar 26 18:16:56.214: INFO: metrics-server-v0.3.6-7d85574868-j5259 from kube-system started at 2020-03-26 17:39:44 +0000 UTC (2 container statuses recorded)
Mar 26 18:16:56.214: INFO: 	Container metrics-server ready: true, restart count 0
Mar 26 18:16:56.214: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 26 18:16:56.214: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-gs7c before test
Mar 26 18:16:56.267: INFO: kube-proxy-bootstrap-e2e-minion-group-gs7c from kube-system started at 2020-03-26 17:39:23 +0000 UTC (1 container statuses recorded)
Mar 26 18:16:56.267: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 18:16:56.267: INFO: nodeport-test-d7gpd from services-3706 started at 2020-03-26 18:16:42 +0000 UTC (1 container statuses recorded)
Mar 26 18:16:56.267: INFO: 	Container nodeport-test ready: true, restart count 0
Mar 26 18:16:56.267: INFO: fluentd-gcp-scaler-54b85fcc78-x2g9h from kube-system started at 2020-03-26 17:39:35 +0000 UTC (1 container statuses recorded)
Mar 26 18:16:56.267: INFO: 	Container fluentd-gcp-scaler ready: true, restart count 0
Mar 26 18:16:56.267: INFO: coredns-7876554b79-vp7xl from kube-system started at 2020-03-26 17:39:43 +0000 UTC (1 container statuses recorded)
Mar 26 18:16:56.267: INFO: 	Container coredns ready: true, restart count 0
Mar 26 18:16:56.267: INFO: metadata-proxy-v0.1-ljlsd from kube-system started at 2020-03-26 17:39:23 +0000 UTC (2 container statuses recorded)
Mar 26 18:16:56.267: INFO: 	Container metadata-proxy ready: true, restart count 0
Mar 26 18:16:56.267: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:16:56.267: INFO: kube-dns-autoscaler-579dbcdc47-brlvc from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:16:56.267: INFO: 	Container autoscaler ready: true, restart count 0
Mar 26 18:16:56.267: INFO: l7-default-backend-f947d4dd5-zfbcj from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:16:56.267: INFO: 	Container default-http-backend ready: true, restart count 0
Mar 26 18:16:56.267: INFO: fluentd-gcp-v3.2.0-bbgzp from kube-system started at 2020-03-26 17:40:13 +0000 UTC (2 container statuses recorded)
Mar 26 18:16:56.267: INFO: 	Container fluentd-gcp ready: true, restart count 0
Mar 26 18:16:56.267: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:16:56.267: INFO: kubernetes-dashboard-864d864f44-2knxp from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:16:56.267: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: verifying the node has the label node bootstrap-e2e-minion-group-1r9w
STEP: verifying the node has the label node bootstrap-e2e-minion-group-d5w7
STEP: verifying the node has the label node bootstrap-e2e-minion-group-gs7c
Mar 26 18:16:56.624: INFO: Pod coredns-7876554b79-sxhp6 requesting resource cpu=100m on Node bootstrap-e2e-minion-group-d5w7
Mar 26 18:16:56.624: INFO: Pod coredns-7876554b79-vp7xl requesting resource cpu=100m on Node bootstrap-e2e-minion-group-gs7c
Mar 26 18:16:56.624: INFO: Pod event-exporter-v0.3.1-d877d48cb-vwtrx requesting resource cpu=0m on Node bootstrap-e2e-minion-group-1r9w
Mar 26 18:16:56.624: INFO: Pod fluentd-gcp-scaler-54b85fcc78-x2g9h requesting resource cpu=0m on Node bootstrap-e2e-minion-group-gs7c
Mar 26 18:16:56.624: INFO: Pod fluentd-gcp-v3.2.0-bbgzp requesting resource cpu=100m on Node bootstrap-e2e-minion-group-gs7c
Mar 26 18:16:56.624: INFO: Pod fluentd-gcp-v3.2.0-jzmnt requesting resource cpu=100m on Node bootstrap-e2e-minion-group-1r9w
Mar 26 18:16:56.624: INFO: Pod fluentd-gcp-v3.2.0-tgnxg requesting resource cpu=100m on Node bootstrap-e2e-minion-group-d5w7
Mar 26 18:16:56.624: INFO: Pod kube-dns-autoscaler-579dbcdc47-brlvc requesting resource cpu=20m on Node bootstrap-e2e-minion-group-gs7c
Mar 26 18:16:56.624: INFO: Pod kube-proxy-bootstrap-e2e-minion-group-1r9w requesting resource cpu=100m on Node bootstrap-e2e-minion-group-1r9w
Mar 26 18:16:56.624: INFO: Pod kube-proxy-bootstrap-e2e-minion-group-d5w7 requesting resource cpu=100m on Node bootstrap-e2e-minion-group-d5w7
Mar 26 18:16:56.624: INFO: Pod kube-proxy-bootstrap-e2e-minion-group-gs7c requesting resource cpu=100m on Node bootstrap-e2e-minion-group-gs7c
Mar 26 18:16:56.624: INFO: Pod kubernetes-dashboard-864d864f44-2knxp requesting resource cpu=50m on Node bootstrap-e2e-minion-group-gs7c
Mar 26 18:16:56.624: INFO: Pod l7-default-backend-f947d4dd5-zfbcj requesting resource cpu=10m on Node bootstrap-e2e-minion-group-gs7c
Mar 26 18:16:56.624: INFO: Pod metadata-proxy-v0.1-gh7hj requesting resource cpu=32m on Node bootstrap-e2e-minion-group-1r9w
Mar 26 18:16:56.624: INFO: Pod metadata-proxy-v0.1-ljlsd requesting resource cpu=32m on Node bootstrap-e2e-minion-group-gs7c
Mar 26 18:16:56.624: INFO: Pod metadata-proxy-v0.1-v2jtw requesting resource cpu=32m on Node bootstrap-e2e-minion-group-d5w7
Mar 26 18:16:56.624: INFO: Pod metrics-server-v0.3.6-7d85574868-j5259 requesting resource cpu=53m on Node bootstrap-e2e-minion-group-d5w7
Mar 26 18:16:56.624: INFO: Pod volume-snapshot-controller-0 requesting resource cpu=0m on Node bootstrap-e2e-minion-group-d5w7
Mar 26 18:16:56.624: INFO: Pod execpod6zwmh requesting resource cpu=0m on Node bootstrap-e2e-minion-group-d5w7
Mar 26 18:16:56.624: INFO: Pod nodeport-test-d7gpd requesting resource cpu=0m on Node bootstrap-e2e-minion-group-gs7c
Mar 26 18:16:56.624: INFO: Pod nodeport-test-tmn4n requesting resource cpu=0m on Node bootstrap-e2e-minion-group-1r9w
STEP: Starting Pods to consume most of the cluster CPU.
Mar 26 18:16:56.624: INFO: Creating a pod which consumes cpu=1237m on Node bootstrap-e2e-minion-group-1r9w
Mar 26 18:16:56.667: INFO: Creating a pod which consumes cpu=1130m on Node bootstrap-e2e-minion-group-d5w7
Mar 26 18:16:56.711: INFO: Creating a pod which consumes cpu=1111m on Node bootstrap-e2e-minion-group-gs7c
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f870644-20a0-450e-b1f7-c280afaf2828.15ffed661dbbb313], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9203/filler-pod-0f870644-20a0-450e-b1f7-c280afaf2828 to bootstrap-e2e-minion-group-d5w7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f870644-20a0-450e-b1f7-c280afaf2828.15ffed6649584ca8], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f870644-20a0-450e-b1f7-c280afaf2828.15ffed664e8045bd], Reason = [Created], Message = [Created container filler-pod-0f870644-20a0-450e-b1f7-c280afaf2828]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0f870644-20a0-450e-b1f7-c280afaf2828.15ffed665582ef7e], Reason = [Started], Message = [Started container filler-pod-0f870644-20a0-450e-b1f7-c280afaf2828]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94ac9c5f-faf1-4e6a-b70e-a40251d464ff.15ffed661a5e4db5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9203/filler-pod-94ac9c5f-faf1-4e6a-b70e-a40251d464ff to bootstrap-e2e-minion-group-1r9w]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94ac9c5f-faf1-4e6a-b70e-a40251d464ff.15ffed66479d53cb], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94ac9c5f-faf1-4e6a-b70e-a40251d464ff.15ffed664a9c3bc4], Reason = [Created], Message = [Created container filler-pod-94ac9c5f-faf1-4e6a-b70e-a40251d464ff]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94ac9c5f-faf1-4e6a-b70e-a40251d464ff.15ffed66515634c5], Reason = [Started], Message = [Started container filler-pod-94ac9c5f-faf1-4e6a-b70e-a40251d464ff]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f40b96b9-b42c-4786-b676-9857ce3feda1.15ffed6623222a54], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9203/filler-pod-f40b96b9-b42c-4786-b676-9857ce3feda1 to bootstrap-e2e-minion-group-gs7c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f40b96b9-b42c-4786-b676-9857ce3feda1.15ffed664f4e7b51], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f40b96b9-b42c-4786-b676-9857ce3feda1.15ffed66531d3dd2], Reason = [Created], Message = [Created container filler-pod-f40b96b9-b42c-4786-b676-9857ce3feda1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f40b96b9-b42c-4786-b676-9857ce3feda1.15ffed665b0565f1], Reason = [Started], Message = [Started container filler-pod-f40b96b9-b42c-4786-b676-9857ce3feda1]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15ffed66a8564e5d], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) were unschedulable, 3 Insufficient cpu.]
STEP: removing the label node off the node bootstrap-e2e-minion-group-1r9w
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node bootstrap-e2e-minion-group-d5w7
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node bootstrap-e2e-minion-group-gs7c
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:17:00.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9203" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":277,"completed":143,"skipped":2302,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:17:00.534: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar 26 18:17:00.733: INFO: Waiting up to 5m0s for pod "downward-api-589edf65-f411-4b87-a4da-6eef5bf901af" in namespace "downward-api-8582" to be "Succeeded or Failed"
Mar 26 18:17:00.772: INFO: Pod "downward-api-589edf65-f411-4b87-a4da-6eef5bf901af": Phase="Pending", Reason="", readiness=false. Elapsed: 39.723588ms
Mar 26 18:17:02.811: INFO: Pod "downward-api-589edf65-f411-4b87-a4da-6eef5bf901af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077965326s
STEP: Saw pod success
Mar 26 18:17:02.811: INFO: Pod "downward-api-589edf65-f411-4b87-a4da-6eef5bf901af" satisfied condition "Succeeded or Failed"
Mar 26 18:17:02.860: INFO: Trying to get logs from node bootstrap-e2e-minion-group-d5w7 pod downward-api-589edf65-f411-4b87-a4da-6eef5bf901af container dapi-container: <nil>
STEP: delete the pod
Mar 26 18:17:02.956: INFO: Waiting for pod downward-api-589edf65-f411-4b87-a4da-6eef5bf901af to disappear
Mar 26 18:17:02.997: INFO: Pod downward-api-589edf65-f411-4b87-a4da-6eef5bf901af no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:17:02.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8582" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":277,"completed":144,"skipped":2326,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:17:03.076: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 18:17:03.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843423, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843423, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843423, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843423, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:17:05.951: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843423, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843423, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843423, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843423, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 18:17:09.030: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:17:21.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9160" for this suite.
STEP: Destroying namespace "webhook-9160-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":277,"completed":145,"skipped":2346,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:17:22.171: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 26 18:17:23.605: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843443, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843443, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843443, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843443, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:17:25.643: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843443, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843443, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843443, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843443, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-65c6cd5fdf\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 18:17:28.696: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:17:28.734: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:17:30.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2857" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":277,"completed":146,"skipped":2350,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:17:30.910: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Mar 26 18:17:31.077: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-8286'
Mar 26 18:17:31.570: INFO: stderr: ""
Mar 26 18:17:31.570: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 26 18:17:31.570: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8286'
Mar 26 18:17:31.892: INFO: stderr: ""
Mar 26 18:17:31.892: INFO: stdout: "update-demo-nautilus-pczpl update-demo-nautilus-qdpnm "
Mar 26 18:17:31.892: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-pczpl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8286'
Mar 26 18:17:32.284: INFO: stderr: ""
Mar 26 18:17:32.284: INFO: stdout: ""
Mar 26 18:17:32.284: INFO: update-demo-nautilus-pczpl is created but not running
Mar 26 18:17:37.284: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8286'
Mar 26 18:17:37.501: INFO: stderr: ""
Mar 26 18:17:37.501: INFO: stdout: "update-demo-nautilus-pczpl update-demo-nautilus-qdpnm "
Mar 26 18:17:37.501: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-pczpl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8286'
Mar 26 18:17:37.707: INFO: stderr: ""
Mar 26 18:17:37.707: INFO: stdout: "true"
Mar 26 18:17:37.707: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-pczpl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8286'
Mar 26 18:17:37.905: INFO: stderr: ""
Mar 26 18:17:37.905: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 18:17:37.905: INFO: validating pod update-demo-nautilus-pczpl
Mar 26 18:17:37.946: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 18:17:37.946: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 18:17:37.946: INFO: update-demo-nautilus-pczpl is verified up and running
Mar 26 18:17:37.947: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-qdpnm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8286'
Mar 26 18:17:38.152: INFO: stderr: ""
Mar 26 18:17:38.152: INFO: stdout: "true"
Mar 26 18:17:38.153: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-qdpnm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8286'
Mar 26 18:17:38.348: INFO: stderr: ""
Mar 26 18:17:38.348: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 18:17:38.348: INFO: validating pod update-demo-nautilus-qdpnm
Mar 26 18:17:38.390: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 18:17:38.391: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 18:17:38.391: INFO: update-demo-nautilus-qdpnm is verified up and running
STEP: using delete to clean up resources
Mar 26 18:17:38.391: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-8286'
Mar 26 18:17:38.681: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 18:17:38.681: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 26 18:17:38.681: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8286'
Mar 26 18:17:38.930: INFO: stderr: "No resources found in kubectl-8286 namespace.\n"
Mar 26 18:17:38.930: INFO: stdout: ""
Mar 26 18:17:38.930: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods -l name=update-demo --namespace=kubectl-8286 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 26 18:17:39.164: INFO: stderr: ""
Mar 26 18:17:39.164: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:17:39.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8286" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":277,"completed":147,"skipped":2351,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:17:39.243: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 18:17:40.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843460, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843460, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843460, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843460, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:17:42.742: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843460, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843460, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843460, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843460, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 18:17:45.792: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar 26 18:17:45.924: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:17:46.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1431" for this suite.
STEP: Destroying namespace "webhook-1431-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":277,"completed":148,"skipped":2352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:17:46.314: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 18:17:46.551: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a56d7296-2fdc-4265-ab6b-a964c011c01e" in namespace "projected-6839" to be "Succeeded or Failed"
Mar 26 18:17:46.589: INFO: Pod "downwardapi-volume-a56d7296-2fdc-4265-ab6b-a964c011c01e": Phase="Pending", Reason="", readiness=false. Elapsed: 38.16661ms
Mar 26 18:17:48.628: INFO: Pod "downwardapi-volume-a56d7296-2fdc-4265-ab6b-a964c011c01e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.07682993s
STEP: Saw pod success
Mar 26 18:17:48.628: INFO: Pod "downwardapi-volume-a56d7296-2fdc-4265-ab6b-a964c011c01e" satisfied condition "Succeeded or Failed"
Mar 26 18:17:48.666: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod downwardapi-volume-a56d7296-2fdc-4265-ab6b-a964c011c01e container client-container: <nil>
STEP: delete the pod
Mar 26 18:17:48.758: INFO: Waiting for pod downwardapi-volume-a56d7296-2fdc-4265-ab6b-a964c011c01e to disappear
Mar 26 18:17:48.796: INFO: Pod downwardapi-volume-a56d7296-2fdc-4265-ab6b-a964c011c01e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:17:48.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6839" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":277,"completed":149,"skipped":2378,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:17:48.880: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar 26 18:17:49.210: INFO: Created pod &Pod{ObjectMeta:{dns-1320  dns-1320 /api/v1/namespaces/dns-1320/pods/dns-1320 c6be2fab-4721-4582-94e4-9dea3a32d279 14463 0 2020-03-26 18:17:49 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2020-03-26 18:17:49 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 114 103 115 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 67 111 110 102 105 103 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 115 101 114 118 101 114 115 34 58 123 125 44 34 102 58 115 101 97 114 99 104 101 115 34 58 123 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6z6dv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6z6dv,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6z6dv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 26 18:17:49.258: INFO: The status of Pod dns-1320 is Pending, waiting for it to be Running (with Ready = true)
Mar 26 18:17:51.299: INFO: The status of Pod dns-1320 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Mar 26 18:17:51.299: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1320 PodName:dns-1320 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:17:51.300: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Verifying customized DNS server is configured on pod...
Mar 26 18:17:51.639: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1320 PodName:dns-1320 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:17:51.639: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:17:51.973: INFO: Deleting pod dns-1320...
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:17:52.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1320" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":277,"completed":150,"skipped":2384,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:17:52.103: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating server pod server in namespace prestop-2628
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2628
STEP: Deleting pre-stop pod
Mar 26 18:18:01.678: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:18:01.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2628" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":277,"completed":151,"skipped":2412,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:18:01.799: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod liveness-ec8ca8ac-154d-4fc3-ab44-1a1fdf010908 in namespace container-probe-5548
Mar 26 18:18:04.123: INFO: Started pod liveness-ec8ca8ac-154d-4fc3-ab44-1a1fdf010908 in namespace container-probe-5548
STEP: checking the pod's current state and verifying that restartCount is present
Mar 26 18:18:04.161: INFO: Initial restart count of pod liveness-ec8ca8ac-154d-4fc3-ab44-1a1fdf010908 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:22:04.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5548" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":277,"completed":152,"skipped":2418,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:22:05.048: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Mar 26 18:22:05.322: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 26 18:23:05.568: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:23:05.606: INFO: Starting informer...
STEP: Starting pods...
Mar 26 18:23:05.730: INFO: Pod1 is running on bootstrap-e2e-minion-group-1r9w. Tainting Node
Mar 26 18:23:07.923: INFO: Pod2 is running on bootstrap-e2e-minion-group-1r9w. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar 26 18:23:22.773: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 26 18:23:34.890: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:23:35.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-1091" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":277,"completed":153,"skipped":2459,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:23:35.095: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-7c209479-92b9-4ada-8cfb-161df7ba09fd
STEP: Creating a pod to test consume secrets
Mar 26 18:23:35.330: INFO: Waiting up to 5m0s for pod "pod-secrets-bca21ec2-2806-4de8-b610-0c392f46b321" in namespace "secrets-2494" to be "Succeeded or Failed"
Mar 26 18:23:35.367: INFO: Pod "pod-secrets-bca21ec2-2806-4de8-b610-0c392f46b321": Phase="Pending", Reason="", readiness=false. Elapsed: 37.472155ms
Mar 26 18:23:37.457: INFO: Pod "pod-secrets-bca21ec2-2806-4de8-b610-0c392f46b321": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.127188931s
STEP: Saw pod success
Mar 26 18:23:37.457: INFO: Pod "pod-secrets-bca21ec2-2806-4de8-b610-0c392f46b321" satisfied condition "Succeeded or Failed"
Mar 26 18:23:37.495: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-secrets-bca21ec2-2806-4de8-b610-0c392f46b321 container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 18:23:37.609: INFO: Waiting for pod pod-secrets-bca21ec2-2806-4de8-b610-0c392f46b321 to disappear
Mar 26 18:23:37.646: INFO: Pod pod-secrets-bca21ec2-2806-4de8-b610-0c392f46b321 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:23:37.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2494" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":154,"skipped":2465,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:23:37.731: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:23:38.160: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 26 18:23:38.269: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:38.320: INFO: Number of nodes with available pods: 0
Mar 26 18:23:38.320: INFO: Node bootstrap-e2e-minion-group-1r9w is running more than one daemon pod
Mar 26 18:23:39.361: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:39.400: INFO: Number of nodes with available pods: 0
Mar 26 18:23:39.400: INFO: Node bootstrap-e2e-minion-group-1r9w is running more than one daemon pod
Mar 26 18:23:40.362: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:40.404: INFO: Number of nodes with available pods: 3
Mar 26 18:23:40.404: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 26 18:23:40.684: INFO: Wrong image for pod: daemon-set-8x8d5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:40.684: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:40.684: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:40.724: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:41.764: INFO: Wrong image for pod: daemon-set-8x8d5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:41.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:41.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:41.804: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:42.788: INFO: Wrong image for pod: daemon-set-8x8d5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:42.788: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:42.788: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:42.859: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:43.764: INFO: Wrong image for pod: daemon-set-8x8d5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:43.764: INFO: Pod daemon-set-8x8d5 is not available
Mar 26 18:23:43.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:43.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:43.803: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:44.764: INFO: Wrong image for pod: daemon-set-8x8d5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:44.764: INFO: Pod daemon-set-8x8d5 is not available
Mar 26 18:23:44.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:44.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:44.803: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:45.764: INFO: Wrong image for pod: daemon-set-8x8d5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:45.764: INFO: Pod daemon-set-8x8d5 is not available
Mar 26 18:23:45.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:45.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:45.804: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:46.764: INFO: Wrong image for pod: daemon-set-8x8d5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:46.764: INFO: Pod daemon-set-8x8d5 is not available
Mar 26 18:23:46.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:46.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:46.803: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:47.764: INFO: Wrong image for pod: daemon-set-8x8d5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:47.764: INFO: Pod daemon-set-8x8d5 is not available
Mar 26 18:23:47.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:47.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:47.806: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:48.764: INFO: Wrong image for pod: daemon-set-8x8d5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:48.764: INFO: Pod daemon-set-8x8d5 is not available
Mar 26 18:23:48.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:48.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:48.804: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:49.764: INFO: Wrong image for pod: daemon-set-8x8d5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:49.764: INFO: Pod daemon-set-8x8d5 is not available
Mar 26 18:23:49.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:49.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:49.804: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:50.764: INFO: Wrong image for pod: daemon-set-8x8d5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:50.764: INFO: Pod daemon-set-8x8d5 is not available
Mar 26 18:23:50.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:50.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:50.804: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:51.764: INFO: Wrong image for pod: daemon-set-8x8d5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:51.764: INFO: Pod daemon-set-8x8d5 is not available
Mar 26 18:23:51.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:51.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:51.804: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:52.764: INFO: Wrong image for pod: daemon-set-8x8d5. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:52.764: INFO: Pod daemon-set-8x8d5 is not available
Mar 26 18:23:52.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:52.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:52.804: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:53.764: INFO: Pod daemon-set-dcrcc is not available
Mar 26 18:23:53.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:53.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:53.804: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:54.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:54.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:54.803: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:55.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:55.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:55.764: INFO: Pod daemon-set-wx68j is not available
Mar 26 18:23:55.803: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:56.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:56.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:56.764: INFO: Pod daemon-set-wx68j is not available
Mar 26 18:23:56.803: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:57.767: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:57.767: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:57.767: INFO: Pod daemon-set-wx68j is not available
Mar 26 18:23:57.834: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:58.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:58.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:58.764: INFO: Pod daemon-set-wx68j is not available
Mar 26 18:23:58.803: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:23:59.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:59.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:23:59.764: INFO: Pod daemon-set-wx68j is not available
Mar 26 18:23:59.804: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:24:00.767: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:24:00.767: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:24:00.767: INFO: Pod daemon-set-wx68j is not available
Mar 26 18:24:00.806: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:24:01.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:24:01.764: INFO: Wrong image for pod: daemon-set-wx68j. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:24:01.764: INFO: Pod daemon-set-wx68j is not available
Mar 26 18:24:01.804: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:24:02.768: INFO: Pod daemon-set-qx5qn is not available
Mar 26 18:24:02.768: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:24:02.810: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:24:03.764: INFO: Pod daemon-set-qx5qn is not available
Mar 26 18:24:03.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:24:03.803: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:24:04.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:24:04.804: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:24:05.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:24:05.803: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:24:06.764: INFO: Wrong image for pod: daemon-set-v46p7. Expected: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12, got: docker.io/library/httpd:2.4.38-alpine.
Mar 26 18:24:06.764: INFO: Pod daemon-set-v46p7 is not available
Mar 26 18:24:06.805: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:24:07.764: INFO: Pod daemon-set-92tnr is not available
Mar 26 18:24:07.805: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 26 18:24:07.845: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:24:07.885: INFO: Number of nodes with available pods: 3
Mar 26 18:24:07.885: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6640, will wait for the garbage collector to delete the pods
Mar 26 18:24:08.262: INFO: Deleting DaemonSet.extensions daemon-set took: 42.139422ms
Mar 26 18:24:08.962: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.375878ms
Mar 26 18:24:22.501: INFO: Number of nodes with available pods: 0
Mar 26 18:24:22.501: INFO: Number of running nodes: 0, number of available pods: 0
Mar 26 18:24:22.539: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6640/daemonsets","resourceVersion":"15861"},"items":null}

Mar 26 18:24:22.577: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6640/pods","resourceVersion":"15861"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:24:22.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6640" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":277,"completed":155,"skipped":2478,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:24:22.813: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 26 18:24:27.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 18:24:27.409: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 26 18:24:29.410: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 18:24:29.449: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 26 18:24:31.410: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 18:24:31.449: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:24:31.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2921" for this suite.
•{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":277,"completed":156,"skipped":2503,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:24:31.531: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-8004
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a new StatefulSet
Mar 26 18:24:31.913: INFO: Found 1 stateful pods, waiting for 3
Mar 26 18:24:41.955: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 18:24:41.955: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 18:24:41.955: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 18:24:42.101: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-8004 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 18:24:42.624: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 18:24:42.624: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 18:24:42.624: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 26 18:24:52.872: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 26 18:24:52.998: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-8004 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:24:53.508: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 26 18:24:53.508: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 18:24:53.508: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 18:25:03.743: INFO: Waiting for StatefulSet statefulset-8004/ss2 to complete update
Mar 26 18:25:03.743: INFO: Waiting for Pod statefulset-8004/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 26 18:25:03.743: INFO: Waiting for Pod statefulset-8004/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Mar 26 18:25:13.822: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-8004 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 26 18:25:14.364: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 26 18:25:14.364: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 26 18:25:14.364: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 26 18:25:24.609: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 26 18:25:24.725: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=statefulset-8004 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 26 18:25:25.264: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 26 18:25:25.264: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 26 18:25:25.264: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 26 18:25:45.499: INFO: Waiting for StatefulSet statefulset-8004/ss2 to complete update
Mar 26 18:25:45.499: INFO: Waiting for Pod statefulset-8004/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 26 18:25:55.577: INFO: Deleting all statefulset in ns statefulset-8004
Mar 26 18:25:55.616: INFO: Scaling statefulset ss2 to 0
Mar 26 18:26:25.776: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 18:26:25.814: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:26:25.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8004" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":277,"completed":157,"skipped":2521,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:26:26.011: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 18:26:27.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843986, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843986, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843986, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843986, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:26:29.134: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843986, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843986, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843986, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720843986, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 18:26:32.182: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:26:33.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6890" for this suite.
STEP: Destroying namespace "webhook-6890-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":277,"completed":158,"skipped":2533,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:26:33.320: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 26 18:26:33.559: INFO: Waiting up to 5m0s for pod "pod-8ef4aba4-7b63-41cb-9961-62e557ccd36c" in namespace "emptydir-6169" to be "Succeeded or Failed"
Mar 26 18:26:33.600: INFO: Pod "pod-8ef4aba4-7b63-41cb-9961-62e557ccd36c": Phase="Pending", Reason="", readiness=false. Elapsed: 40.949856ms
Mar 26 18:26:35.638: INFO: Pod "pod-8ef4aba4-7b63-41cb-9961-62e557ccd36c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.079115582s
STEP: Saw pod success
Mar 26 18:26:35.638: INFO: Pod "pod-8ef4aba4-7b63-41cb-9961-62e557ccd36c" satisfied condition "Succeeded or Failed"
Mar 26 18:26:35.676: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-8ef4aba4-7b63-41cb-9961-62e557ccd36c container test-container: <nil>
STEP: delete the pod
Mar 26 18:26:35.808: INFO: Waiting for pod pod-8ef4aba4-7b63-41cb-9961-62e557ccd36c to disappear
Mar 26 18:26:35.845: INFO: Pod pod-8ef4aba4-7b63-41cb-9961-62e557ccd36c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:26:35.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6169" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":159,"skipped":2562,"failed":0}
SSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:26:35.929: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:26:36.126: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-4440d5c5-37ca-4a01-9ad6-aa55dedd2df6" in namespace "security-context-test-3917" to be "Succeeded or Failed"
Mar 26 18:26:36.164: INFO: Pod "busybox-privileged-false-4440d5c5-37ca-4a01-9ad6-aa55dedd2df6": Phase="Pending", Reason="", readiness=false. Elapsed: 37.77504ms
Mar 26 18:26:38.205: INFO: Pod "busybox-privileged-false-4440d5c5-37ca-4a01-9ad6-aa55dedd2df6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.079036765s
Mar 26 18:26:38.205: INFO: Pod "busybox-privileged-false-4440d5c5-37ca-4a01-9ad6-aa55dedd2df6" satisfied condition "Succeeded or Failed"
Mar 26 18:26:38.259: INFO: Got logs for pod "busybox-privileged-false-4440d5c5-37ca-4a01-9ad6-aa55dedd2df6": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:26:38.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3917" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":160,"skipped":2565,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:26:38.354: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-29b3d12c-3f56-43e1-87aa-dfb2fb459eb2
STEP: Creating a pod to test consume configMaps
Mar 26 18:26:38.609: INFO: Waiting up to 5m0s for pod "pod-configmaps-79a63e5f-6a86-4546-9818-b0f9c035e5d0" in namespace "configmap-311" to be "Succeeded or Failed"
Mar 26 18:26:38.651: INFO: Pod "pod-configmaps-79a63e5f-6a86-4546-9818-b0f9c035e5d0": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012477ms
Mar 26 18:26:40.690: INFO: Pod "pod-configmaps-79a63e5f-6a86-4546-9818-b0f9c035e5d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.080406124s
STEP: Saw pod success
Mar 26 18:26:40.690: INFO: Pod "pod-configmaps-79a63e5f-6a86-4546-9818-b0f9c035e5d0" satisfied condition "Succeeded or Failed"
Mar 26 18:26:40.728: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-configmaps-79a63e5f-6a86-4546-9818-b0f9c035e5d0 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 18:26:40.818: INFO: Waiting for pod pod-configmaps-79a63e5f-6a86-4546-9818-b0f9c035e5d0 to disappear
Mar 26 18:26:40.861: INFO: Pod pod-configmaps-79a63e5f-6a86-4546-9818-b0f9c035e5d0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:26:40.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-311" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":161,"skipped":2590,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:26:40.972: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating replication controller my-hostname-basic-0135cb20-1f43-4409-8ce1-081a83236b0a
Mar 26 18:26:41.235: INFO: Pod name my-hostname-basic-0135cb20-1f43-4409-8ce1-081a83236b0a: Found 1 pods out of 1
Mar 26 18:26:41.235: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0135cb20-1f43-4409-8ce1-081a83236b0a" are running
Mar 26 18:26:43.313: INFO: Pod "my-hostname-basic-0135cb20-1f43-4409-8ce1-081a83236b0a-m7qfn" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-26 18:26:41 +0000 UTC Reason: Message:}])
Mar 26 18:26:43.313: INFO: Trying to dial the pod
Mar 26 18:26:48.431: INFO: Controller my-hostname-basic-0135cb20-1f43-4409-8ce1-081a83236b0a: Got expected result from replica 1 [my-hostname-basic-0135cb20-1f43-4409-8ce1-081a83236b0a-m7qfn]: "my-hostname-basic-0135cb20-1f43-4409-8ce1-081a83236b0a-m7qfn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:26:48.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7729" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":277,"completed":162,"skipped":2619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:26:48.511: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 26 18:26:48.944: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3871 /api/v1/namespaces/watch-3871/configmaps/e2e-watch-test-label-changed 2a7616d6-f91b-4bf7-b629-1fda20c79351 16711 0 2020-03-26 18:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-03-26 18:26:48 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 18:26:48.944: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3871 /api/v1/namespaces/watch-3871/configmaps/e2e-watch-test-label-changed 2a7616d6-f91b-4bf7-b629-1fda20c79351 16712 0 2020-03-26 18:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-03-26 18:26:48 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 18:26:48.944: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3871 /api/v1/namespaces/watch-3871/configmaps/e2e-watch-test-label-changed 2a7616d6-f91b-4bf7-b629-1fda20c79351 16713 0 2020-03-26 18:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-03-26 18:26:48 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 26 18:26:59.217: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3871 /api/v1/namespaces/watch-3871/configmaps/e2e-watch-test-label-changed 2a7616d6-f91b-4bf7-b629-1fda20c79351 16753 0 2020-03-26 18:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-03-26 18:26:59 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 18:26:59.217: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3871 /api/v1/namespaces/watch-3871/configmaps/e2e-watch-test-label-changed 2a7616d6-f91b-4bf7-b629-1fda20c79351 16754 0 2020-03-26 18:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-03-26 18:26:59 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 18:26:59.218: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3871 /api/v1/namespaces/watch-3871/configmaps/e2e-watch-test-label-changed 2a7616d6-f91b-4bf7-b629-1fda20c79351 16755 0 2020-03-26 18:26:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-03-26 18:26:59 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:26:59.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3871" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":277,"completed":163,"skipped":2665,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:26:59.297: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar 26 18:26:59.495: INFO: Waiting up to 5m0s for pod "downward-api-1d96681d-6913-4b24-a9b9-cc6d534c643e" in namespace "downward-api-3072" to be "Succeeded or Failed"
Mar 26 18:26:59.539: INFO: Pod "downward-api-1d96681d-6913-4b24-a9b9-cc6d534c643e": Phase="Pending", Reason="", readiness=false. Elapsed: 43.881905ms
Mar 26 18:27:01.577: INFO: Pod "downward-api-1d96681d-6913-4b24-a9b9-cc6d534c643e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.08213895s
STEP: Saw pod success
Mar 26 18:27:01.577: INFO: Pod "downward-api-1d96681d-6913-4b24-a9b9-cc6d534c643e" satisfied condition "Succeeded or Failed"
Mar 26 18:27:01.615: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod downward-api-1d96681d-6913-4b24-a9b9-cc6d534c643e container dapi-container: <nil>
STEP: delete the pod
Mar 26 18:27:01.710: INFO: Waiting for pod downward-api-1d96681d-6913-4b24-a9b9-cc6d534c643e to disappear
Mar 26 18:27:01.748: INFO: Pod downward-api-1d96681d-6913-4b24-a9b9-cc6d534c643e no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:27:01.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3072" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":277,"completed":164,"skipped":2681,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:27:01.828: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 26 18:27:04.221: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:27:04.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8449" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":277,"completed":165,"skipped":2717,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:27:04.406: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar 26 18:27:04.645: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 26 18:27:04.766: INFO: Waiting for terminating namespaces to be deleted...
Mar 26 18:27:04.804: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-1r9w before test
Mar 26 18:27:04.846: INFO: metadata-proxy-v0.1-gh7hj from kube-system started at 2020-03-26 17:39:23 +0000 UTC (2 container statuses recorded)
Mar 26 18:27:04.846: INFO: 	Container metadata-proxy ready: true, restart count 0
Mar 26 18:27:04.846: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:27:04.846: INFO: kube-proxy-bootstrap-e2e-minion-group-1r9w from kube-system started at 2020-03-26 17:39:23 +0000 UTC (1 container statuses recorded)
Mar 26 18:27:04.846: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 18:27:04.846: INFO: fluentd-gcp-v3.2.0-jzmnt from kube-system started at 2020-03-26 17:40:02 +0000 UTC (2 container statuses recorded)
Mar 26 18:27:04.846: INFO: 	Container fluentd-gcp ready: true, restart count 0
Mar 26 18:27:04.846: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:27:04.846: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-d5w7 before test
Mar 26 18:27:04.904: INFO: metrics-server-v0.3.6-7d85574868-j5259 from kube-system started at 2020-03-26 17:39:44 +0000 UTC (2 container statuses recorded)
Mar 26 18:27:04.904: INFO: 	Container metrics-server ready: true, restart count 0
Mar 26 18:27:04.904: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 26 18:27:04.904: INFO: metadata-proxy-v0.1-v2jtw from kube-system started at 2020-03-26 17:39:23 +0000 UTC (2 container statuses recorded)
Mar 26 18:27:04.904: INFO: 	Container metadata-proxy ready: true, restart count 0
Mar 26 18:27:04.904: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:27:04.904: INFO: volume-snapshot-controller-0 from kube-system started at 2020-03-26 17:39:35 +0000 UTC (1 container statuses recorded)
Mar 26 18:27:04.904: INFO: 	Container volume-snapshot-controller ready: true, restart count 0
Mar 26 18:27:04.904: INFO: fluentd-gcp-v3.2.0-tgnxg from kube-system started at 2020-03-26 17:40:18 +0000 UTC (2 container statuses recorded)
Mar 26 18:27:04.904: INFO: 	Container fluentd-gcp ready: true, restart count 0
Mar 26 18:27:04.904: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:27:04.904: INFO: event-exporter-v0.3.1-d877d48cb-lv2tb from kube-system started at 2020-03-26 18:23:08 +0000 UTC (2 container statuses recorded)
Mar 26 18:27:04.904: INFO: 	Container event-exporter ready: true, restart count 0
Mar 26 18:27:04.904: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:27:04.904: INFO: kube-proxy-bootstrap-e2e-minion-group-d5w7 from kube-system started at 2020-03-26 17:39:22 +0000 UTC (1 container statuses recorded)
Mar 26 18:27:04.904: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 18:27:04.904: INFO: coredns-7876554b79-sxhp6 from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:27:04.904: INFO: 	Container coredns ready: true, restart count 0
Mar 26 18:27:04.904: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-gs7c before test
Mar 26 18:27:04.965: INFO: kube-proxy-bootstrap-e2e-minion-group-gs7c from kube-system started at 2020-03-26 17:39:23 +0000 UTC (1 container statuses recorded)
Mar 26 18:27:04.965: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 18:27:04.965: INFO: coredns-7876554b79-vp7xl from kube-system started at 2020-03-26 17:39:43 +0000 UTC (1 container statuses recorded)
Mar 26 18:27:04.965: INFO: 	Container coredns ready: true, restart count 0
Mar 26 18:27:04.965: INFO: metadata-proxy-v0.1-ljlsd from kube-system started at 2020-03-26 17:39:23 +0000 UTC (2 container statuses recorded)
Mar 26 18:27:04.965: INFO: 	Container metadata-proxy ready: true, restart count 0
Mar 26 18:27:04.965: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:27:04.965: INFO: kube-dns-autoscaler-579dbcdc47-brlvc from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:27:04.965: INFO: 	Container autoscaler ready: true, restart count 0
Mar 26 18:27:04.965: INFO: l7-default-backend-f947d4dd5-zfbcj from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:27:04.965: INFO: 	Container default-http-backend ready: true, restart count 0
Mar 26 18:27:04.965: INFO: fluentd-gcp-scaler-54b85fcc78-x2g9h from kube-system started at 2020-03-26 17:39:35 +0000 UTC (1 container statuses recorded)
Mar 26 18:27:04.965: INFO: 	Container fluentd-gcp-scaler ready: true, restart count 0
Mar 26 18:27:04.965: INFO: fluentd-gcp-v3.2.0-bbgzp from kube-system started at 2020-03-26 17:40:13 +0000 UTC (2 container statuses recorded)
Mar 26 18:27:04.965: INFO: 	Container fluentd-gcp ready: true, restart count 0
Mar 26 18:27:04.965: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:27:04.965: INFO: kubernetes-dashboard-864d864f44-2knxp from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:27:04.965: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-01cee460-aa4e-4194-be32-fc37874e7ab3 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-01cee460-aa4e-4194-be32-fc37874e7ab3 off the node bootstrap-e2e-minion-group-1r9w
STEP: verifying the node doesn't have the label kubernetes.io/e2e-01cee460-aa4e-4194-be32-fc37874e7ab3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:32:09.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1330" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82

• [SLOW TEST:305.431 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":277,"completed":166,"skipped":2718,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:32:09.838: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:32:10.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5991" for this suite.
STEP: Destroying namespace "nspatchtest-b4f7c5fb-5166-4fe3-9a7e-dd6161fa41c4-1261" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":277,"completed":167,"skipped":2741,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:32:10.314: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 26 18:32:10.511: INFO: Waiting up to 5m0s for pod "pod-e1b3d356-f0b5-4c52-8530-9208a1f038f6" in namespace "emptydir-7915" to be "Succeeded or Failed"
Mar 26 18:32:10.550: INFO: Pod "pod-e1b3d356-f0b5-4c52-8530-9208a1f038f6": Phase="Pending", Reason="", readiness=false. Elapsed: 39.004888ms
Mar 26 18:32:12.589: INFO: Pod "pod-e1b3d356-f0b5-4c52-8530-9208a1f038f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077611478s
STEP: Saw pod success
Mar 26 18:32:12.589: INFO: Pod "pod-e1b3d356-f0b5-4c52-8530-9208a1f038f6" satisfied condition "Succeeded or Failed"
Mar 26 18:32:12.627: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-e1b3d356-f0b5-4c52-8530-9208a1f038f6 container test-container: <nil>
STEP: delete the pod
Mar 26 18:32:12.729: INFO: Waiting for pod pod-e1b3d356-f0b5-4c52-8530-9208a1f038f6 to disappear
Mar 26 18:32:12.767: INFO: Pod pod-e1b3d356-f0b5-4c52-8530-9208a1f038f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:32:12.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7915" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":168,"skipped":2751,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:32:12.847: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 18:32:13.109: INFO: Waiting up to 5m0s for pod "downwardapi-volume-140f249f-5966-49c1-9576-4196724a4066" in namespace "downward-api-9323" to be "Succeeded or Failed"
Mar 26 18:32:13.156: INFO: Pod "downwardapi-volume-140f249f-5966-49c1-9576-4196724a4066": Phase="Pending", Reason="", readiness=false. Elapsed: 47.186221ms
Mar 26 18:32:15.202: INFO: Pod "downwardapi-volume-140f249f-5966-49c1-9576-4196724a4066": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.092226947s
STEP: Saw pod success
Mar 26 18:32:15.202: INFO: Pod "downwardapi-volume-140f249f-5966-49c1-9576-4196724a4066" satisfied condition "Succeeded or Failed"
Mar 26 18:32:15.243: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod downwardapi-volume-140f249f-5966-49c1-9576-4196724a4066 container client-container: <nil>
STEP: delete the pod
Mar 26 18:32:15.395: INFO: Waiting for pod downwardapi-volume-140f249f-5966-49c1-9576-4196724a4066 to disappear
Mar 26 18:32:15.435: INFO: Pod downwardapi-volume-140f249f-5966-49c1-9576-4196724a4066 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:32:15.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9323" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":277,"completed":169,"skipped":2762,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:32:15.533: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 18:32:16.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844336, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844336, loc:(*time.Location)(0x7b271c0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-779fdc84d9\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844336, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844336, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:32:18.868: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844336, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844336, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844336, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844336, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 18:32:21.915: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:32:32.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3935" for this suite.
STEP: Destroying namespace "webhook-3935-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":277,"completed":170,"skipped":2828,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:32:32.884: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-upd-02be3c19-e8f9-4704-81cf-7486e690d1ec
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:32:35.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5624" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":171,"skipped":2829,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:32:35.470: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:32:42.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8718" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":277,"completed":172,"skipped":2908,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:32:42.874: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar 26 18:32:43.027: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 26 18:32:43.146: INFO: Waiting for terminating namespaces to be deleted...
Mar 26 18:32:43.184: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-1r9w before test
Mar 26 18:32:43.227: INFO: pod-configmaps-a85c908f-e901-42fb-b7ce-05cc2b263105 from configmap-5624 started at 2020-03-26 18:32:33 +0000 UTC (2 container statuses recorded)
Mar 26 18:32:43.227: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
Mar 26 18:32:43.227: INFO: 	Container configmap-volume-data-test ready: false, restart count 0
Mar 26 18:32:43.227: INFO: kube-proxy-bootstrap-e2e-minion-group-1r9w from kube-system started at 2020-03-26 17:39:23 +0000 UTC (1 container statuses recorded)
Mar 26 18:32:43.227: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 18:32:43.227: INFO: fluentd-gcp-v3.2.0-jzmnt from kube-system started at 2020-03-26 17:40:02 +0000 UTC (2 container statuses recorded)
Mar 26 18:32:43.227: INFO: 	Container fluentd-gcp ready: true, restart count 0
Mar 26 18:32:43.227: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:32:43.227: INFO: metadata-proxy-v0.1-gh7hj from kube-system started at 2020-03-26 17:39:23 +0000 UTC (2 container statuses recorded)
Mar 26 18:32:43.227: INFO: 	Container metadata-proxy ready: true, restart count 0
Mar 26 18:32:43.227: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:32:43.227: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-d5w7 before test
Mar 26 18:32:43.283: INFO: fluentd-gcp-v3.2.0-tgnxg from kube-system started at 2020-03-26 17:40:18 +0000 UTC (2 container statuses recorded)
Mar 26 18:32:43.283: INFO: 	Container fluentd-gcp ready: true, restart count 0
Mar 26 18:32:43.283: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:32:43.283: INFO: kube-proxy-bootstrap-e2e-minion-group-d5w7 from kube-system started at 2020-03-26 17:39:22 +0000 UTC (1 container statuses recorded)
Mar 26 18:32:43.283: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 18:32:43.283: INFO: coredns-7876554b79-sxhp6 from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:32:43.283: INFO: 	Container coredns ready: true, restart count 0
Mar 26 18:32:43.283: INFO: event-exporter-v0.3.1-d877d48cb-lv2tb from kube-system started at 2020-03-26 18:23:08 +0000 UTC (2 container statuses recorded)
Mar 26 18:32:43.283: INFO: 	Container event-exporter ready: true, restart count 0
Mar 26 18:32:43.283: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:32:43.283: INFO: metrics-server-v0.3.6-7d85574868-j5259 from kube-system started at 2020-03-26 17:39:44 +0000 UTC (2 container statuses recorded)
Mar 26 18:32:43.283: INFO: 	Container metrics-server ready: true, restart count 0
Mar 26 18:32:43.283: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 26 18:32:43.283: INFO: metadata-proxy-v0.1-v2jtw from kube-system started at 2020-03-26 17:39:23 +0000 UTC (2 container statuses recorded)
Mar 26 18:32:43.283: INFO: 	Container metadata-proxy ready: true, restart count 0
Mar 26 18:32:43.283: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:32:43.283: INFO: volume-snapshot-controller-0 from kube-system started at 2020-03-26 17:39:35 +0000 UTC (1 container statuses recorded)
Mar 26 18:32:43.283: INFO: 	Container volume-snapshot-controller ready: true, restart count 0
Mar 26 18:32:43.283: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-gs7c before test
Mar 26 18:32:43.340: INFO: kubernetes-dashboard-864d864f44-2knxp from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:32:43.340: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 26 18:32:43.340: INFO: fluentd-gcp-v3.2.0-bbgzp from kube-system started at 2020-03-26 17:40:13 +0000 UTC (2 container statuses recorded)
Mar 26 18:32:43.340: INFO: 	Container fluentd-gcp ready: true, restart count 0
Mar 26 18:32:43.340: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:32:43.340: INFO: kube-proxy-bootstrap-e2e-minion-group-gs7c from kube-system started at 2020-03-26 17:39:23 +0000 UTC (1 container statuses recorded)
Mar 26 18:32:43.340: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 18:32:43.340: INFO: kube-dns-autoscaler-579dbcdc47-brlvc from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:32:43.340: INFO: 	Container autoscaler ready: true, restart count 0
Mar 26 18:32:43.340: INFO: l7-default-backend-f947d4dd5-zfbcj from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:32:43.340: INFO: 	Container default-http-backend ready: true, restart count 0
Mar 26 18:32:43.340: INFO: fluentd-gcp-scaler-54b85fcc78-x2g9h from kube-system started at 2020-03-26 17:39:35 +0000 UTC (1 container statuses recorded)
Mar 26 18:32:43.340: INFO: 	Container fluentd-gcp-scaler ready: true, restart count 0
Mar 26 18:32:43.340: INFO: coredns-7876554b79-vp7xl from kube-system started at 2020-03-26 17:39:43 +0000 UTC (1 container statuses recorded)
Mar 26 18:32:43.340: INFO: 	Container coredns ready: true, restart count 0
Mar 26 18:32:43.340: INFO: metadata-proxy-v0.1-ljlsd from kube-system started at 2020-03-26 17:39:23 +0000 UTC (2 container statuses recorded)
Mar 26 18:32:43.340: INFO: 	Container metadata-proxy ready: true, restart count 0
Mar 26 18:32:43.340: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-0cbd4f3b-fe92-48d9-99a3-4a19e09aecf1 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-0cbd4f3b-fe92-48d9-99a3-4a19e09aecf1 off the node bootstrap-e2e-minion-group-1r9w
STEP: verifying the node doesn't have the label kubernetes.io/e2e-0cbd4f3b-fe92-48d9-99a3-4a19e09aecf1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:32:54.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5055" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":277,"completed":173,"skipped":2911,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:32:54.244: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:74
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:32:54.399: INFO: Creating deployment "test-recreate-deployment"
Mar 26 18:32:54.441: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 26 18:32:54.541: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 26 18:32:54.579: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844374, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844374, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844374, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844374, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-74d98b5f7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:32:56.617: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 26 18:32:56.698: INFO: Updating deployment test-recreate-deployment
Mar 26 18:32:56.698: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
Mar 26 18:32:56.936: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8571 /apis/apps/v1/namespaces/deployment-8571/deployments/test-recreate-deployment 4fa8f437-8f1f-45d8-9a8e-568fb2739462 18117 2 2020-03-26 18:32:54 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-03-26 18:32:56 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 112 114 111 103 114 101 115 115 68 101 97 100 108 105 110 101 83 101 99 111 110 100 115 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 114 101 118 105 115 105 111 110 72 105 115 116 111 114 121 76 105 109 105 116 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 116 114 97 116 101 103 121 34 58 123 34 102 58 116 121 112 101 34 58 123 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 125],}} {kube-controller-manager Update apps/v1 2020-03-26 18:32:56 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 65 118 97 105 108 97 98 108 101 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 80 114 111 103 114 101 115 115 105 110 103 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 85 112 100 97 116 101 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 110 97 118 97 105 108 97 98 108 101 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 117 112 100 97 116 101 100 82 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005b02458 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-26 18:32:56 +0000 UTC,LastTransitionTime:2020-03-26 18:32:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-d5667d9c7" is progressing.,LastUpdateTime:2020-03-26 18:32:56 +0000 UTC,LastTransitionTime:2020-03-26 18:32:54 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 26 18:32:56.975: INFO: New ReplicaSet "test-recreate-deployment-d5667d9c7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-d5667d9c7  deployment-8571 /apis/apps/v1/namespaces/deployment-8571/replicasets/test-recreate-deployment-d5667d9c7 6a1f1fb3-e951-47dc-bb75-c26210f705e8 18116 1 2020-03-26 18:32:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4fa8f437-8f1f-45d8-9a8e-568fb2739462 0xc005b02c10 0xc005b02c11}] []  [{kube-controller-manager Update apps/v1 2020-03-26 18:32:56 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 102 97 56 102 52 51 55 45 56 102 49 102 45 52 53 100 56 45 57 97 56 101 45 53 54 56 102 98 50 55 51 57 52 54 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 102 117 108 108 121 76 97 98 101 108 101 100 82 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: d5667d9c7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005b02d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 26 18:32:56.975: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 26 18:32:56.975: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-74d98b5f7c  deployment-8571 /apis/apps/v1/namespaces/deployment-8571/replicasets/test-recreate-deployment-74d98b5f7c f4c16117-7d1e-4a8e-8ed8-7438352fa02a 18108 2 2020-03-26 18:32:54 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4fa8f437-8f1f-45d8-9a8e-568fb2739462 0xc005b02927 0xc005b02928}] []  [{kube-controller-manager Update apps/v1 2020-03-26 18:32:56 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 97 110 110 111 116 97 116 105 111 110 115 34 58 123 34 46 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 100 101 115 105 114 101 100 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 109 97 120 45 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 100 101 112 108 111 121 109 101 110 116 46 107 117 98 101 114 110 101 116 101 115 46 105 111 47 114 101 118 105 115 105 111 110 34 58 123 125 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 52 102 97 56 102 52 51 55 45 56 102 49 102 45 52 53 100 56 45 57 97 56 101 45 53 54 56 102 98 50 55 51 57 52 54 50 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 44 34 102 58 115 101 108 101 99 116 111 114 34 58 123 34 102 58 109 97 116 99 104 76 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 116 101 109 112 108 97 116 101 34 58 123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 97 103 110 104 111 115 116 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125 125 44 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 111 98 115 101 114 118 101 100 71 101 110 101 114 97 116 105 111 110 34 58 123 125 44 34 102 58 114 101 112 108 105 99 97 115 34 58 123 125 125 125],}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 74d98b5f7c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:74d98b5f7c] map[] [] []  []} {[] [] [{agnhost us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005b02a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 26 18:32:57.013: INFO: Pod "test-recreate-deployment-d5667d9c7-pklxk" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-d5667d9c7-pklxk test-recreate-deployment-d5667d9c7- deployment-8571 /api/v1/namespaces/deployment-8571/pods/test-recreate-deployment-d5667d9c7-pklxk 4b3cf665-75ff-4c1f-b157-6f916816e550 18115 0 2020-03-26 18:32:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:d5667d9c7] map[] [{apps/v1 ReplicaSet test-recreate-deployment-d5667d9c7 6a1f1fb3-e951-47dc-bb75-c26210f705e8 0xc005b033a0 0xc005b033a1}] []  [{kube-controller-manager Update v1 2020-03-26 18:32:56 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 103 101 110 101 114 97 116 101 78 97 109 101 34 58 123 125 44 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 112 111 100 45 116 101 109 112 108 97 116 101 45 104 97 115 104 34 58 123 125 125 44 34 102 58 111 119 110 101 114 82 101 102 101 114 101 110 99 101 115 34 58 123 34 46 34 58 123 125 44 34 107 58 123 92 34 117 105 100 92 34 58 92 34 54 97 49 102 49 102 98 51 45 101 57 53 49 45 52 55 100 99 45 98 98 55 53 45 99 50 54 50 49 48 102 55 48 53 101 56 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 97 112 105 86 101 114 115 105 111 110 34 58 123 125 44 34 102 58 98 108 111 99 107 79 119 110 101 114 68 101 108 101 116 105 111 110 34 58 123 125 44 34 102 58 99 111 110 116 114 111 108 108 101 114 34 58 123 125 44 34 102 58 107 105 110 100 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 117 105 100 34 58 123 125 125 125 125 44 34 102 58 115 112 101 99 34 58 123 34 102 58 99 111 110 116 97 105 110 101 114 115 34 58 123 34 107 58 123 92 34 110 97 109 101 92 34 58 92 34 104 116 116 112 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 105 109 97 103 101 34 58 123 125 44 34 102 58 105 109 97 103 101 80 117 108 108 80 111 108 105 99 121 34 58 123 125 44 34 102 58 110 97 109 101 34 58 123 125 44 34 102 58 114 101 115 111 117 114 99 101 115 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 97 116 104 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 77 101 115 115 97 103 101 80 111 108 105 99 121 34 58 123 125 125 125 44 34 102 58 100 110 115 80 111 108 105 99 121 34 58 123 125 44 34 102 58 101 110 97 98 108 101 83 101 114 118 105 99 101 76 105 110 107 115 34 58 123 125 44 34 102 58 114 101 115 116 97 114 116 80 111 108 105 99 121 34 58 123 125 44 34 102 58 115 99 104 101 100 117 108 101 114 78 97 109 101 34 58 123 125 44 34 102 58 115 101 99 117 114 105 116 121 67 111 110 116 101 120 116 34 58 123 125 44 34 102 58 116 101 114 109 105 110 97 116 105 111 110 71 114 97 99 101 80 101 114 105 111 100 83 101 99 111 110 100 115 34 58 123 125 125 125],}} {kubelet Update v1 2020-03-26 18:32:56 +0000 UTC FieldsV1 &FieldsV1{Raw:*[123 34 102 58 115 116 97 116 117 115 34 58 123 34 102 58 99 111 110 100 105 116 105 111 110 115 34 58 123 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 67 111 110 116 97 105 110 101 114 115 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 73 110 105 116 105 97 108 105 122 101 100 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 44 34 107 58 123 92 34 116 121 112 101 92 34 58 92 34 82 101 97 100 121 92 34 125 34 58 123 34 46 34 58 123 125 44 34 102 58 108 97 115 116 80 114 111 98 101 84 105 109 101 34 58 123 125 44 34 102 58 108 97 115 116 84 114 97 110 115 105 116 105 111 110 84 105 109 101 34 58 123 125 44 34 102 58 109 101 115 115 97 103 101 34 58 123 125 44 34 102 58 114 101 97 115 111 110 34 58 123 125 44 34 102 58 115 116 97 116 117 115 34 58 123 125 44 34 102 58 116 121 112 101 34 58 123 125 125 125 44 34 102 58 99 111 110 116 97 105 110 101 114 83 116 97 116 117 115 101 115 34 58 123 125 44 34 102 58 104 111 115 116 73 80 34 58 123 125 44 34 102 58 115 116 97 114 116 84 105 109 101 34 58 123 125 125 125],}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-qxgsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-qxgsq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-qxgsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:bootstrap-e2e-minion-group-1r9w,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 18:32:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 18:32:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 18:32:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-26 18:32:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.138.0.4,PodIP:,StartTime:2020-03-26 18:32:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:32:57.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8571" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":277,"completed":174,"skipped":2933,"failed":0}
SSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:32:57.096: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:32:59.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1171" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":277,"completed":175,"skipped":2936,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:32:59.558: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:33:00.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2080" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":277,"completed":176,"skipped":2950,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:33:00.099: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 26 18:33:02.616: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:33:02.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3911" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":277,"completed":177,"skipped":2984,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:33:02.788: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Mar 26 18:33:06.198: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:33:06.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4086" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":277,"completed":178,"skipped":2998,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:33:06.302: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Mar 26 18:33:06.460: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:33:26.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7226" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":277,"completed":179,"skipped":3004,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:33:26.394: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:33:26.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6483" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":277,"completed":180,"skipped":3024,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:33:26.718: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:33:43.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5744" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":277,"completed":181,"skipped":3046,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:33:43.585: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:33:44.051: INFO: Create a RollingUpdate DaemonSet
Mar 26 18:33:44.094: INFO: Check that daemon pods launch on every node of the cluster
Mar 26 18:33:44.164: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:33:44.241: INFO: Number of nodes with available pods: 0
Mar 26 18:33:44.241: INFO: Node bootstrap-e2e-minion-group-1r9w is running more than one daemon pod
Mar 26 18:33:45.288: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:33:45.326: INFO: Number of nodes with available pods: 1
Mar 26 18:33:45.326: INFO: Node bootstrap-e2e-minion-group-1r9w is running more than one daemon pod
Mar 26 18:33:46.282: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:33:46.321: INFO: Number of nodes with available pods: 3
Mar 26 18:33:46.321: INFO: Number of running nodes: 3, number of available pods: 3
Mar 26 18:33:46.321: INFO: Update the DaemonSet to trigger a rollout
Mar 26 18:33:46.401: INFO: Updating DaemonSet daemon-set
Mar 26 18:33:49.563: INFO: Roll back the DaemonSet before rollout is complete
Mar 26 18:33:49.644: INFO: Updating DaemonSet daemon-set
Mar 26 18:33:49.644: INFO: Make sure DaemonSet rollback is complete
Mar 26 18:33:49.683: INFO: Wrong image for pod: daemon-set-pccfv. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 26 18:33:49.683: INFO: Pod daemon-set-pccfv is not available
Mar 26 18:33:49.724: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:33:50.764: INFO: Wrong image for pod: daemon-set-pccfv. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 26 18:33:50.764: INFO: Pod daemon-set-pccfv is not available
Mar 26 18:33:50.804: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:33:51.764: INFO: Pod daemon-set-qsvk6 is not available
Mar 26 18:33:51.804: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8545, will wait for the garbage collector to delete the pods
Mar 26 18:33:52.009: INFO: Deleting DaemonSet.extensions daemon-set took: 39.942056ms
Mar 26 18:33:52.710: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.386783ms
Mar 26 18:34:03.152: INFO: Number of nodes with available pods: 0
Mar 26 18:34:03.152: INFO: Number of running nodes: 0, number of available pods: 0
Mar 26 18:34:03.191: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8545/daemonsets","resourceVersion":"18565"},"items":null}

Mar 26 18:34:03.244: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8545/pods","resourceVersion":"18565"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:34:03.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8545" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":277,"completed":182,"skipped":3126,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:34:03.513: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 18:34:03.856: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a01b481-307e-4720-a2a5-5fc5679dd4ef" in namespace "projected-2376" to be "Succeeded or Failed"
Mar 26 18:34:03.894: INFO: Pod "downwardapi-volume-5a01b481-307e-4720-a2a5-5fc5679dd4ef": Phase="Pending", Reason="", readiness=false. Elapsed: 37.263774ms
Mar 26 18:34:05.932: INFO: Pod "downwardapi-volume-5a01b481-307e-4720-a2a5-5fc5679dd4ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075284517s
STEP: Saw pod success
Mar 26 18:34:05.932: INFO: Pod "downwardapi-volume-5a01b481-307e-4720-a2a5-5fc5679dd4ef" satisfied condition "Succeeded or Failed"
Mar 26 18:34:05.969: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod downwardapi-volume-5a01b481-307e-4720-a2a5-5fc5679dd4ef container client-container: <nil>
STEP: delete the pod
Mar 26 18:34:06.061: INFO: Waiting for pod downwardapi-volume-5a01b481-307e-4720-a2a5-5fc5679dd4ef to disappear
Mar 26 18:34:06.098: INFO: Pod downwardapi-volume-5a01b481-307e-4720-a2a5-5fc5679dd4ef no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:34:06.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2376" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":183,"skipped":3137,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:34:06.178: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5575
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5575
STEP: creating replication controller externalsvc in namespace services-5575
I0326 18:34:06.507418    9212 runners.go:190] Created replication controller with name: externalsvc, namespace: services-5575, replica count: 2
I0326 18:34:09.557983    9212 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar 26 18:34:09.680: INFO: Creating new exec pod
Mar 26 18:34:11.799: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-5575 execpodlsshc -- /bin/sh -x -c nslookup clusterip-service'
Mar 26 18:34:12.640: INFO: stderr: "+ nslookup clusterip-service\n"
Mar 26 18:34:12.640: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nclusterip-service.services-5575.svc.cluster.local\tcanonical name = externalsvc.services-5575.svc.cluster.local.\nName:\texternalsvc.services-5575.svc.cluster.local\nAddress: 10.0.3.251\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5575, will wait for the garbage collector to delete the pods
Mar 26 18:34:12.769: INFO: Deleting ReplicationController externalsvc took: 40.419188ms
Mar 26 18:34:13.469: INFO: Terminating ReplicationController externalsvc pods took: 700.340467ms
Mar 26 18:34:17.626: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:34:17.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5575" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":277,"completed":184,"skipped":3158,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:34:17.763: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name projected-secret-test-e4ae3dc4-1812-4b76-9d17-cf1fd4a150ff
STEP: Creating a pod to test consume secrets
Mar 26 18:34:18.037: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9e88d761-3613-4614-9c61-5046f27081a4" in namespace "projected-9550" to be "Succeeded or Failed"
Mar 26 18:34:18.078: INFO: Pod "pod-projected-secrets-9e88d761-3613-4614-9c61-5046f27081a4": Phase="Pending", Reason="", readiness=false. Elapsed: 40.679077ms
Mar 26 18:34:20.116: INFO: Pod "pod-projected-secrets-9e88d761-3613-4614-9c61-5046f27081a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.078972352s
STEP: Saw pod success
Mar 26 18:34:20.116: INFO: Pod "pod-projected-secrets-9e88d761-3613-4614-9c61-5046f27081a4" satisfied condition "Succeeded or Failed"
Mar 26 18:34:20.154: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-projected-secrets-9e88d761-3613-4614-9c61-5046f27081a4 container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 18:34:20.253: INFO: Waiting for pod pod-projected-secrets-9e88d761-3613-4614-9c61-5046f27081a4 to disappear
Mar 26 18:34:20.291: INFO: Pod pod-projected-secrets-9e88d761-3613-4614-9c61-5046f27081a4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:34:20.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9550" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":277,"completed":185,"skipped":3169,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:34:20.370: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 26 18:34:20.672: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8969 /api/v1/namespaces/watch-8969/configmaps/e2e-watch-test-configmap-a 810244bc-7f80-448d-a4af-fb006b45b85d 18722 0 2020-03-26 18:34:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 18:34:20 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 18:34:20.672: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8969 /api/v1/namespaces/watch-8969/configmaps/e2e-watch-test-configmap-a 810244bc-7f80-448d-a4af-fb006b45b85d 18722 0 2020-03-26 18:34:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 18:34:20 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 26 18:34:30.753: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8969 /api/v1/namespaces/watch-8969/configmaps/e2e-watch-test-configmap-a 810244bc-7f80-448d-a4af-fb006b45b85d 18765 0 2020-03-26 18:34:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 18:34:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 18:34:30.754: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8969 /api/v1/namespaces/watch-8969/configmaps/e2e-watch-test-configmap-a 810244bc-7f80-448d-a4af-fb006b45b85d 18765 0 2020-03-26 18:34:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 18:34:30 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 26 18:34:40.834: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8969 /api/v1/namespaces/watch-8969/configmaps/e2e-watch-test-configmap-a 810244bc-7f80-448d-a4af-fb006b45b85d 18799 0 2020-03-26 18:34:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 18:34:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 18:34:40.834: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8969 /api/v1/namespaces/watch-8969/configmaps/e2e-watch-test-configmap-a 810244bc-7f80-448d-a4af-fb006b45b85d 18799 0 2020-03-26 18:34:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 18:34:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 26 18:34:50.880: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8969 /api/v1/namespaces/watch-8969/configmaps/e2e-watch-test-configmap-a 810244bc-7f80-448d-a4af-fb006b45b85d 18830 0 2020-03-26 18:34:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 18:34:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 18:34:50.880: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8969 /api/v1/namespaces/watch-8969/configmaps/e2e-watch-test-configmap-a 810244bc-7f80-448d-a4af-fb006b45b85d 18830 0 2020-03-26 18:34:20 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-03-26 18:34:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 26 18:35:00.924: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8969 /api/v1/namespaces/watch-8969/configmaps/e2e-watch-test-configmap-b 4a7ccaac-4e15-4e4d-a6ff-531215c86a61 18861 0 2020-03-26 18:35:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-03-26 18:35:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 18:35:00.926: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8969 /api/v1/namespaces/watch-8969/configmaps/e2e-watch-test-configmap-b 4a7ccaac-4e15-4e4d-a6ff-531215c86a61 18861 0 2020-03-26 18:35:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-03-26 18:35:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 26 18:35:10.967: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8969 /api/v1/namespaces/watch-8969/configmaps/e2e-watch-test-configmap-b 4a7ccaac-4e15-4e4d-a6ff-531215c86a61 18892 0 2020-03-26 18:35:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-03-26 18:35:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 18:35:10.968: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8969 /api/v1/namespaces/watch-8969/configmaps/e2e-watch-test-configmap-b 4a7ccaac-4e15-4e4d-a6ff-531215c86a61 18892 0 2020-03-26 18:35:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-03-26 18:35:00 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:35:20.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8969" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":277,"completed":186,"skipped":3189,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:35:21.053: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod test-webserver-fd5c102d-97f5-4e86-aff0-fb6be9036556 in namespace container-probe-3502
Mar 26 18:35:23.338: INFO: Started pod test-webserver-fd5c102d-97f5-4e86-aff0-fb6be9036556 in namespace container-probe-3502
STEP: checking the pod's current state and verifying that restartCount is present
Mar 26 18:35:23.376: INFO: Initial restart count of pod test-webserver-fd5c102d-97f5-4e86-aff0-fb6be9036556 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:39:24.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3502" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":277,"completed":187,"skipped":3231,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:39:24.103: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:39:24.256: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 26 18:39:27.639: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-9727 create -f -'
Mar 26 18:39:28.517: INFO: stderr: ""
Mar 26 18:39:28.517: INFO: stdout: "e2e-test-crd-publish-openapi-5343-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 26 18:39:28.517: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-9727 delete e2e-test-crd-publish-openapi-5343-crds test-cr'
Mar 26 18:39:28.748: INFO: stderr: ""
Mar 26 18:39:28.748: INFO: stdout: "e2e-test-crd-publish-openapi-5343-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 26 18:39:28.748: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-9727 apply -f -'
Mar 26 18:39:29.260: INFO: stderr: ""
Mar 26 18:39:29.260: INFO: stdout: "e2e-test-crd-publish-openapi-5343-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 26 18:39:29.260: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-9727 delete e2e-test-crd-publish-openapi-5343-crds test-cr'
Mar 26 18:39:29.497: INFO: stderr: ""
Mar 26 18:39:29.497: INFO: stdout: "e2e-test-crd-publish-openapi-5343-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 26 18:39:29.497: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config explain e2e-test-crd-publish-openapi-5343-crds'
Mar 26 18:39:29.822: INFO: stderr: ""
Mar 26 18:39:29.822: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5343-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:39:33.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9727" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":277,"completed":188,"skipped":3254,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:39:33.807: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 18:39:34.874: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844774, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844774, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844774, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720844774, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 18:39:37.955: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:39:38.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-452" for this suite.
STEP: Destroying namespace "webhook-452-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":277,"completed":189,"skipped":3255,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:39:38.728: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Mar 26 18:39:38.907: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-6997'
Mar 26 18:39:39.412: INFO: stderr: ""
Mar 26 18:39:39.412: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar 26 18:39:40.453: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 18:39:40.453: INFO: Found 0 / 1
Mar 26 18:39:41.456: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 18:39:41.456: INFO: Found 1 / 1
Mar 26 18:39:41.456: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 26 18:39:41.495: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 18:39:41.495: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 26 18:39:41.495: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config patch pod agnhost-master-c56cp --namespace=kubectl-6997 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 26 18:39:41.728: INFO: stderr: ""
Mar 26 18:39:41.728: INFO: stdout: "pod/agnhost-master-c56cp patched\n"
STEP: checking annotations
Mar 26 18:39:41.767: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 18:39:41.767: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:39:41.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6997" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":277,"completed":190,"skipped":3261,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:39:41.847: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar 26 18:39:42.046: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:39:45.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-568" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":277,"completed":191,"skipped":3262,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:39:46.047: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 26 18:39:46.245: INFO: Waiting up to 5m0s for pod "pod-efc7ffb7-8fdc-48ff-b6d3-0985028be27e" in namespace "emptydir-2288" to be "Succeeded or Failed"
Mar 26 18:39:46.283: INFO: Pod "pod-efc7ffb7-8fdc-48ff-b6d3-0985028be27e": Phase="Pending", Reason="", readiness=false. Elapsed: 37.725385ms
Mar 26 18:39:48.321: INFO: Pod "pod-efc7ffb7-8fdc-48ff-b6d3-0985028be27e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075958086s
STEP: Saw pod success
Mar 26 18:39:48.321: INFO: Pod "pod-efc7ffb7-8fdc-48ff-b6d3-0985028be27e" satisfied condition "Succeeded or Failed"
Mar 26 18:39:48.359: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-efc7ffb7-8fdc-48ff-b6d3-0985028be27e container test-container: <nil>
STEP: delete the pod
Mar 26 18:39:48.468: INFO: Waiting for pod pod-efc7ffb7-8fdc-48ff-b6d3-0985028be27e to disappear
Mar 26 18:39:48.505: INFO: Pod pod-efc7ffb7-8fdc-48ff-b6d3-0985028be27e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:39:48.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2288" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":192,"skipped":3263,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:39:48.595: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2657.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2657.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2657.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2657.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2657.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2657.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2657.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2657.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2657.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2657.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 18:39:53.013: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:53.052: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:53.092: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:53.131: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:53.250: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:53.290: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:53.330: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:53.369: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:53.449: INFO: Lookups using dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2657.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2657.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local jessie_udp@dns-test-service-2.dns-2657.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2657.svc.cluster.local]

Mar 26 18:39:58.489: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:58.529: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:58.569: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:58.612: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:58.738: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:58.778: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:58.817: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:58.857: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:39:58.938: INFO: Lookups using dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2657.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2657.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local jessie_udp@dns-test-service-2.dns-2657.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2657.svc.cluster.local]

Mar 26 18:40:03.490: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:03.529: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:03.569: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:03.609: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:03.729: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:03.770: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:03.810: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:03.849: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:03.929: INFO: Lookups using dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2657.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2657.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local jessie_udp@dns-test-service-2.dns-2657.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2657.svc.cluster.local]

Mar 26 18:40:08.490: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:08.530: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:08.570: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:08.609: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:08.728: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:08.771: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:08.814: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:08.854: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:08.933: INFO: Lookups using dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2657.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2657.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local jessie_udp@dns-test-service-2.dns-2657.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2657.svc.cluster.local]

Mar 26 18:40:13.490: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:13.530: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:13.569: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:13.609: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:13.728: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:13.768: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:13.807: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:13.847: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:13.927: INFO: Lookups using dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2657.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2657.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local jessie_udp@dns-test-service-2.dns-2657.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2657.svc.cluster.local]

Mar 26 18:40:18.489: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:18.529: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:18.568: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:18.608: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:18.727: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:18.769: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:18.809: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:18.853: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2657.svc.cluster.local from pod dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5: the server could not find the requested resource (get pods dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5)
Mar 26 18:40:18.955: INFO: Lookups using dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2657.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2657.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2657.svc.cluster.local jessie_udp@dns-test-service-2.dns-2657.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2657.svc.cluster.local]

Mar 26 18:40:23.940: INFO: DNS probes using dns-2657/dns-test-2990ab7e-437e-4ef3-9276-2e3a5c792dd5 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:40:24.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2657" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":277,"completed":193,"skipped":3265,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:40:24.191: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7658.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7658.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7658.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7658.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7658.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7658.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 18:40:28.830: INFO: DNS probes using dns-7658/dns-test-191d75b6-2de5-427a-ab93-d361f159fee5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:40:28.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7658" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":277,"completed":194,"skipped":3269,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:40:28.975: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-8350de5b-4556-4ea2-98af-84e8f8235d50
STEP: Creating secret with name s-test-opt-upd-39dc049c-9d8f-4d90-b95c-401f75a9dca5
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8350de5b-4556-4ea2-98af-84e8f8235d50
STEP: Updating secret s-test-opt-upd-39dc049c-9d8f-4d90-b95c-401f75a9dca5
STEP: Creating secret with name s-test-opt-create-bfc921ed-f35f-4cdd-a831-9e0464826a56
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:42:03.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5966" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":195,"skipped":3275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:42:03.979: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:42:08.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5555" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":277,"completed":196,"skipped":3332,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:42:08.336: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar 26 18:42:08.610: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:42:22.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3700" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":277,"completed":197,"skipped":3340,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:42:22.889: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service multi-endpoint-test in namespace services-6649
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6649 to expose endpoints map[]
Mar 26 18:42:23.168: INFO: successfully validated that service multi-endpoint-test in namespace services-6649 exposes endpoints map[] (39.318471ms elapsed)
STEP: Creating pod pod1 in namespace services-6649
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6649 to expose endpoints map[pod1:[100]]
Mar 26 18:42:25.441: INFO: successfully validated that service multi-endpoint-test in namespace services-6649 exposes endpoints map[pod1:[100]] (2.229303772s elapsed)
STEP: Creating pod pod2 in namespace services-6649
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6649 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 26 18:42:27.834: INFO: successfully validated that service multi-endpoint-test in namespace services-6649 exposes endpoints map[pod1:[100] pod2:[101]] (2.352671319s elapsed)
STEP: Deleting pod pod1 in namespace services-6649
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6649 to expose endpoints map[pod2:[101]]
Mar 26 18:42:27.974: INFO: successfully validated that service multi-endpoint-test in namespace services-6649 exposes endpoints map[pod2:[101]] (98.793291ms elapsed)
STEP: Deleting pod pod2 in namespace services-6649
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6649 to expose endpoints map[]
Mar 26 18:42:28.065: INFO: successfully validated that service multi-endpoint-test in namespace services-6649 exposes endpoints map[] (39.981651ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:42:28.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6649" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":277,"completed":198,"skipped":3395,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:42:28.249: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name s-test-opt-del-7f88a10f-e76d-4b4e-8b1a-6adb68689cbc
STEP: Creating secret with name s-test-opt-upd-aaf8c21b-3c3b-494b-8b36-448eca2cbda5
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-7f88a10f-e76d-4b4e-8b1a-6adb68689cbc
STEP: Updating secret s-test-opt-upd-aaf8c21b-3c3b-494b-8b36-448eca2cbda5
STEP: Creating secret with name s-test-opt-create-0fab463b-829b-4bf3-b2ee-d117530ac9a3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:44:03.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8612" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":199,"skipped":3397,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:44:03.132: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 18:44:03.329: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d6d40e48-655c-4ffc-bbec-5e9a55353b34" in namespace "downward-api-4693" to be "Succeeded or Failed"
Mar 26 18:44:03.367: INFO: Pod "downwardapi-volume-d6d40e48-655c-4ffc-bbec-5e9a55353b34": Phase="Pending", Reason="", readiness=false. Elapsed: 37.756748ms
Mar 26 18:44:05.406: INFO: Pod "downwardapi-volume-d6d40e48-655c-4ffc-bbec-5e9a55353b34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076370803s
STEP: Saw pod success
Mar 26 18:44:05.406: INFO: Pod "downwardapi-volume-d6d40e48-655c-4ffc-bbec-5e9a55353b34" satisfied condition "Succeeded or Failed"
Mar 26 18:44:05.445: INFO: Trying to get logs from node bootstrap-e2e-minion-group-gs7c pod downwardapi-volume-d6d40e48-655c-4ffc-bbec-5e9a55353b34 container client-container: <nil>
STEP: delete the pod
Mar 26 18:44:05.550: INFO: Waiting for pod downwardapi-volume-d6d40e48-655c-4ffc-bbec-5e9a55353b34 to disappear
Mar 26 18:44:05.587: INFO: Pod downwardapi-volume-d6d40e48-655c-4ffc-bbec-5e9a55353b34 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:44:05.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4693" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":200,"skipped":3397,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:44:05.677: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar 26 18:44:14.325: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:44:14.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-475" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":277,"completed":201,"skipped":3398,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:44:14.405: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar 26 18:44:14.563: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 26 18:44:14.683: INFO: Waiting for terminating namespaces to be deleted...
Mar 26 18:44:14.721: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-1r9w before test
Mar 26 18:44:14.764: INFO: kube-proxy-bootstrap-e2e-minion-group-1r9w from kube-system started at 2020-03-26 17:39:23 +0000 UTC (1 container statuses recorded)
Mar 26 18:44:14.764: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 18:44:14.764: INFO: fluentd-gcp-v3.2.0-jzmnt from kube-system started at 2020-03-26 17:40:02 +0000 UTC (2 container statuses recorded)
Mar 26 18:44:14.764: INFO: 	Container fluentd-gcp ready: true, restart count 0
Mar 26 18:44:14.764: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:44:14.764: INFO: metadata-proxy-v0.1-gh7hj from kube-system started at 2020-03-26 17:39:23 +0000 UTC (2 container statuses recorded)
Mar 26 18:44:14.764: INFO: 	Container metadata-proxy ready: true, restart count 0
Mar 26 18:44:14.764: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:44:14.764: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-d5w7 before test
Mar 26 18:44:14.819: INFO: metrics-server-v0.3.6-7d85574868-j5259 from kube-system started at 2020-03-26 17:39:44 +0000 UTC (2 container statuses recorded)
Mar 26 18:44:14.819: INFO: 	Container metrics-server ready: true, restart count 0
Mar 26 18:44:14.819: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 26 18:44:14.819: INFO: metadata-proxy-v0.1-v2jtw from kube-system started at 2020-03-26 17:39:23 +0000 UTC (2 container statuses recorded)
Mar 26 18:44:14.819: INFO: 	Container metadata-proxy ready: true, restart count 0
Mar 26 18:44:14.819: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:44:14.819: INFO: volume-snapshot-controller-0 from kube-system started at 2020-03-26 17:39:35 +0000 UTC (1 container statuses recorded)
Mar 26 18:44:14.819: INFO: 	Container volume-snapshot-controller ready: true, restart count 0
Mar 26 18:44:14.819: INFO: fluentd-gcp-v3.2.0-tgnxg from kube-system started at 2020-03-26 17:40:18 +0000 UTC (2 container statuses recorded)
Mar 26 18:44:14.819: INFO: 	Container fluentd-gcp ready: true, restart count 0
Mar 26 18:44:14.819: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:44:14.819: INFO: kube-proxy-bootstrap-e2e-minion-group-d5w7 from kube-system started at 2020-03-26 17:39:22 +0000 UTC (1 container statuses recorded)
Mar 26 18:44:14.819: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 18:44:14.819: INFO: coredns-7876554b79-sxhp6 from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:44:14.819: INFO: 	Container coredns ready: true, restart count 0
Mar 26 18:44:14.819: INFO: event-exporter-v0.3.1-d877d48cb-lv2tb from kube-system started at 2020-03-26 18:23:08 +0000 UTC (2 container statuses recorded)
Mar 26 18:44:14.819: INFO: 	Container event-exporter ready: true, restart count 0
Mar 26 18:44:14.819: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:44:14.820: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-gs7c before test
Mar 26 18:44:14.864: INFO: metadata-proxy-v0.1-ljlsd from kube-system started at 2020-03-26 17:39:23 +0000 UTC (2 container statuses recorded)
Mar 26 18:44:14.864: INFO: 	Container metadata-proxy ready: true, restart count 0
Mar 26 18:44:14.864: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:44:14.864: INFO: kube-dns-autoscaler-579dbcdc47-brlvc from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:44:14.864: INFO: 	Container autoscaler ready: true, restart count 0
Mar 26 18:44:14.864: INFO: l7-default-backend-f947d4dd5-zfbcj from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:44:14.864: INFO: 	Container default-http-backend ready: true, restart count 0
Mar 26 18:44:14.864: INFO: fluentd-gcp-scaler-54b85fcc78-x2g9h from kube-system started at 2020-03-26 17:39:35 +0000 UTC (1 container statuses recorded)
Mar 26 18:44:14.864: INFO: 	Container fluentd-gcp-scaler ready: true, restart count 0
Mar 26 18:44:14.864: INFO: coredns-7876554b79-vp7xl from kube-system started at 2020-03-26 17:39:43 +0000 UTC (1 container statuses recorded)
Mar 26 18:44:14.864: INFO: 	Container coredns ready: true, restart count 0
Mar 26 18:44:14.864: INFO: fluentd-gcp-v3.2.0-bbgzp from kube-system started at 2020-03-26 17:40:13 +0000 UTC (2 container statuses recorded)
Mar 26 18:44:14.864: INFO: 	Container fluentd-gcp ready: true, restart count 0
Mar 26 18:44:14.864: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:44:14.864: INFO: kubernetes-dashboard-864d864f44-2knxp from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:44:14.864: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 26 18:44:14.864: INFO: kube-proxy-bootstrap-e2e-minion-group-gs7c from kube-system started at 2020-03-26 17:39:23 +0000 UTC (1 container statuses recorded)
Mar 26 18:44:14.864: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15ffeee3901c3fa3], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) were unschedulable, 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:44:16.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8851" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":277,"completed":202,"skipped":3398,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:44:16.158: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name cm-test-opt-del-06a9a72d-54f4-4843-bccd-bb48185113ef
STEP: Creating configMap with name cm-test-opt-upd-8ba40f36-c9c4-49f1-b774-d09af2af17a9
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-06a9a72d-54f4-4843-bccd-bb48185113ef
STEP: Updating configmap cm-test-opt-upd-8ba40f36-c9c4-49f1-b774-d09af2af17a9
STEP: Creating configMap with name cm-test-opt-create-f84604af-b3e0-4ace-8f2d-5d3b91fea011
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:44:21.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9818" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":203,"skipped":3412,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:44:21.109: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward api env vars
Mar 26 18:44:21.327: INFO: Waiting up to 5m0s for pod "downward-api-11bef184-9fae-40df-853c-91b5971cfe58" in namespace "downward-api-7024" to be "Succeeded or Failed"
Mar 26 18:44:21.372: INFO: Pod "downward-api-11bef184-9fae-40df-853c-91b5971cfe58": Phase="Pending", Reason="", readiness=false. Elapsed: 45.031053ms
Mar 26 18:44:23.411: INFO: Pod "downward-api-11bef184-9fae-40df-853c-91b5971cfe58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.083810194s
STEP: Saw pod success
Mar 26 18:44:23.411: INFO: Pod "downward-api-11bef184-9fae-40df-853c-91b5971cfe58" satisfied condition "Succeeded or Failed"
Mar 26 18:44:23.449: INFO: Trying to get logs from node bootstrap-e2e-minion-group-gs7c pod downward-api-11bef184-9fae-40df-853c-91b5971cfe58 container dapi-container: <nil>
STEP: delete the pod
Mar 26 18:44:23.546: INFO: Waiting for pod downward-api-11bef184-9fae-40df-853c-91b5971cfe58 to disappear
Mar 26 18:44:23.586: INFO: Pod downward-api-11bef184-9fae-40df-853c-91b5971cfe58 no longer exists
[AfterEach] [sig-node] Downward API
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:44:23.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7024" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":277,"completed":204,"skipped":3415,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:44:23.666: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Mar 26 18:44:36.306: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:44:36.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4765" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":277,"completed":205,"skipped":3424,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:44:36.387: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:84
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
STEP: Creating service test in namespace statefulset-666
[It] should have a working scale subresource [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating statefulset ss in namespace statefulset-666
Mar 26 18:44:36.709: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Mar 26 18:44:46.746: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:110
Mar 26 18:44:46.950: INFO: Deleting all statefulset in ns statefulset-666
Mar 26 18:44:46.988: INFO: Scaling statefulset ss to 0
Mar 26 18:44:57.136: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 18:44:57.171: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:44:57.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-666" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":277,"completed":206,"skipped":3452,"failed":0}
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:44:57.358: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod busybox-ea46b1b6-3a5a-4bea-bec6-fd1ddd562bd9 in namespace container-probe-2634
Mar 26 18:44:59.652: INFO: Started pod busybox-ea46b1b6-3a5a-4bea-bec6-fd1ddd562bd9 in namespace container-probe-2634
STEP: checking the pod's current state and verifying that restartCount is present
Mar 26 18:44:59.688: INFO: Initial restart count of pod busybox-ea46b1b6-3a5a-4bea-bec6-fd1ddd562bd9 is 0
Mar 26 18:45:46.547: INFO: Restart count of pod container-probe-2634/busybox-ea46b1b6-3a5a-4bea-bec6-fd1ddd562bd9 is now 1 (46.85903139s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:45:46.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2634" for this suite.
•{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":277,"completed":207,"skipped":3457,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:45:46.671: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:45:46.855: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:45:54.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2711" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":277,"completed":208,"skipped":3468,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:45:54.550: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test override command
Mar 26 18:45:54.817: INFO: Waiting up to 5m0s for pod "client-containers-3c0dc694-da7c-4680-9fed-c7733acadc8e" in namespace "containers-5882" to be "Succeeded or Failed"
Mar 26 18:45:54.867: INFO: Pod "client-containers-3c0dc694-da7c-4680-9fed-c7733acadc8e": Phase="Pending", Reason="", readiness=false. Elapsed: 50.426393ms
Mar 26 18:45:56.903: INFO: Pod "client-containers-3c0dc694-da7c-4680-9fed-c7733acadc8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.085907052s
STEP: Saw pod success
Mar 26 18:45:56.903: INFO: Pod "client-containers-3c0dc694-da7c-4680-9fed-c7733acadc8e" satisfied condition "Succeeded or Failed"
Mar 26 18:45:56.938: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod client-containers-3c0dc694-da7c-4680-9fed-c7733acadc8e container test-container: <nil>
STEP: delete the pod
Mar 26 18:45:57.038: INFO: Waiting for pod client-containers-3c0dc694-da7c-4680-9fed-c7733acadc8e to disappear
Mar 26 18:45:57.072: INFO: Pod client-containers-3c0dc694-da7c-4680-9fed-c7733acadc8e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:45:57.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5882" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":277,"completed":209,"skipped":3470,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:45:57.161: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:45:57.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8476" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":277,"completed":210,"skipped":3483,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:45:57.494: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl label
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1206
STEP: creating the pod
Mar 26 18:45:57.684: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-2005'
Mar 26 18:45:58.176: INFO: stderr: ""
Mar 26 18:45:58.176: INFO: stdout: "pod/pause created\n"
Mar 26 18:45:58.176: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 26 18:45:58.176: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2005" to be "running and ready"
Mar 26 18:45:58.211: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 34.944543ms
Mar 26 18:46:00.246: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.070125383s
Mar 26 18:46:00.246: INFO: Pod "pause" satisfied condition "running and ready"
Mar 26 18:46:00.246: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 26 18:46:00.246: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config label pods pause testing-label=testing-label-value --namespace=kubectl-2005'
Mar 26 18:46:00.476: INFO: stderr: ""
Mar 26 18:46:00.476: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 26 18:46:00.476: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pod pause -L testing-label --namespace=kubectl-2005'
Mar 26 18:46:00.665: INFO: stderr: ""
Mar 26 18:46:00.665: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 26 18:46:00.665: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config label pods pause testing-label- --namespace=kubectl-2005'
Mar 26 18:46:00.894: INFO: stderr: ""
Mar 26 18:46:00.894: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 26 18:46:00.894: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pod pause -L testing-label --namespace=kubectl-2005'
Mar 26 18:46:01.082: INFO: stderr: ""
Mar 26 18:46:01.082: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1213
STEP: using delete to clean up resources
Mar 26 18:46:01.082: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-2005'
Mar 26 18:46:01.340: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 18:46:01.340: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 26 18:46:01.340: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get rc,svc -l name=pause --no-headers --namespace=kubectl-2005'
Mar 26 18:46:01.579: INFO: stderr: "No resources found in kubectl-2005 namespace.\n"
Mar 26 18:46:01.579: INFO: stdout: ""
Mar 26 18:46:01.579: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods -l name=pause --namespace=kubectl-2005 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 26 18:46:01.777: INFO: stderr: ""
Mar 26 18:46:01.777: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:46:01.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2005" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":277,"completed":211,"skipped":3509,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:46:01.856: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-615de5de-bbc0-4496-97f6-64f78ee80896
STEP: Creating a pod to test consume secrets
Mar 26 18:46:02.138: INFO: Waiting up to 5m0s for pod "pod-secrets-6839625d-b8e2-417d-8785-e7b870eb2ec2" in namespace "secrets-6088" to be "Succeeded or Failed"
Mar 26 18:46:02.204: INFO: Pod "pod-secrets-6839625d-b8e2-417d-8785-e7b870eb2ec2": Phase="Pending", Reason="", readiness=false. Elapsed: 65.69797ms
Mar 26 18:46:04.240: INFO: Pod "pod-secrets-6839625d-b8e2-417d-8785-e7b870eb2ec2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.101535165s
STEP: Saw pod success
Mar 26 18:46:04.240: INFO: Pod "pod-secrets-6839625d-b8e2-417d-8785-e7b870eb2ec2" satisfied condition "Succeeded or Failed"
Mar 26 18:46:04.275: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-secrets-6839625d-b8e2-417d-8785-e7b870eb2ec2 container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 18:46:04.363: INFO: Waiting for pod pod-secrets-6839625d-b8e2-417d-8785-e7b870eb2ec2 to disappear
Mar 26 18:46:04.398: INFO: Pod pod-secrets-6839625d-b8e2-417d-8785-e7b870eb2ec2 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:46:04.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6088" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":212,"skipped":3518,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:46:04.479: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:46:04.621: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-8662'
Mar 26 18:46:04.965: INFO: stderr: ""
Mar 26 18:46:04.965: INFO: stdout: "replicationcontroller/agnhost-master created\n"
Mar 26 18:46:04.965: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-8662'
Mar 26 18:46:05.328: INFO: stderr: ""
Mar 26 18:46:05.328: INFO: stdout: "service/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar 26 18:46:06.364: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 18:46:06.364: INFO: Found 1 / 1
Mar 26 18:46:06.364: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 26 18:46:06.400: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 18:46:06.400: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 26 18:46:06.400: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config describe pod agnhost-master-64h82 --namespace=kubectl-8662'
Mar 26 18:46:06.690: INFO: stderr: ""
Mar 26 18:46:06.690: INFO: stdout: "Name:         agnhost-master-64h82\nNamespace:    kubectl-8662\nPriority:     0\nNode:         bootstrap-e2e-minion-group-1r9w/10.138.0.4\nStart Time:   Thu, 26 Mar 2020 18:46:04 +0000\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nStatus:       Running\nIP:           10.64.1.198\nIPs:\n  IP:           10.64.1.198\nControlled By:  ReplicationController/agnhost-master\nContainers:\n  agnhost-master:\n    Container ID:   docker://6e5b6b234bc18441808a778e5bdbd292be6219e66752cecfff123bf4aae7e1fa\n    Image:          us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Image ID:       docker-pullable://us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost@sha256:1d7f0d77a6f07fd507f147a38d06a7c8269ebabd4f923bfe46d4fb8b396a520c\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 26 Mar 2020 18:46:05 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-xp45f (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-xp45f:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-xp45f\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                      Message\n  ----    ------     ----  ----                                      -------\n  Normal  Scheduled  2s    default-scheduler                         Successfully assigned kubectl-8662/agnhost-master-64h82 to bootstrap-e2e-minion-group-1r9w\n  Normal  Pulled     1s    kubelet, bootstrap-e2e-minion-group-1r9w  Container image \"us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\" already present on machine\n  Normal  Created    1s    kubelet, bootstrap-e2e-minion-group-1r9w  Created container agnhost-master\n  Normal  Started    1s    kubelet, bootstrap-e2e-minion-group-1r9w  Started container agnhost-master\n"
Mar 26 18:46:06.690: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config describe rc agnhost-master --namespace=kubectl-8662'
Mar 26 18:46:07.050: INFO: stderr: ""
Mar 26 18:46:07.050: INFO: stdout: "Name:         agnhost-master\nNamespace:    kubectl-8662\nSelector:     app=agnhost,role=master\nLabels:       app=agnhost\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=master\n  Containers:\n   agnhost-master:\n    Image:        us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-master-64h82\n"
Mar 26 18:46:07.051: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config describe service agnhost-master --namespace=kubectl-8662'
Mar 26 18:46:07.346: INFO: stderr: ""
Mar 26 18:46:07.346: INFO: stdout: "Name:              agnhost-master\nNamespace:         kubectl-8662\nLabels:            app=agnhost\n                   role=master\nAnnotations:       <none>\nSelector:          app=agnhost,role=master\nType:              ClusterIP\nIP:                10.0.50.32\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.64.1.198:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 26 18:46:07.387: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config describe node bootstrap-e2e-master'
Mar 26 18:46:07.799: INFO: stderr: ""
Mar 26 18:46:07.799: INFO: stdout: "Name:               bootstrap-e2e-master\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=n1-standard-1\n                    beta.kubernetes.io/os=linux\n                    cloud.google.com/metadata-proxy-ready=true\n                    failure-domain.beta.kubernetes.io/region=us-west1\n                    failure-domain.beta.kubernetes.io/zone=us-west1-b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=bootstrap-e2e-master\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=n1-standard-1\n                    topology.kubernetes.io/region=us-west1\n                    topology.kubernetes.io/zone=us-west1-b\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 26 Mar 2020 17:39:35 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\n                    node.kubernetes.io/unschedulable:NoSchedule\nUnschedulable:      true\nLease:\n  HolderIdentity:  bootstrap-e2e-master\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 26 Mar 2020 18:45:58 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 26 Mar 2020 17:39:50 +0000   Thu, 26 Mar 2020 17:39:50 +0000   RouteCreated                 RouteController created a route\n  MemoryPressure       False   Thu, 26 Mar 2020 18:45:25 +0000   Thu, 26 Mar 2020 17:39:35 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 26 Mar 2020 18:45:25 +0000   Thu, 26 Mar 2020 17:39:35 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 26 Mar 2020 18:45:25 +0000   Thu, 26 Mar 2020 17:39:35 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 26 Mar 2020 18:45:25 +0000   Thu, 26 Mar 2020 17:39:36 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.138.0.2\n  ExternalIP:   34.83.25.82\n  InternalDNS:  bootstrap-e2e-master.c.k8s-boskos-gce-project-10.internal\n  Hostname:     bootstrap-e2e-master.c.k8s-boskos-gce-project-10.internal\nCapacity:\n  attachable-volumes-gce-pd:  127\n  cpu:                        1\n  ephemeral-storage:          16293736Ki\n  hugepages-2Mi:              0\n  memory:                     3785940Ki\n  pods:                       110\nAllocatable:\n  attachable-volumes-gce-pd:  127\n  cpu:                        1\n  ephemeral-storage:          15016307073\n  hugepages-2Mi:              0\n  memory:                     3529940Ki\n  pods:                       110\nSystem Info:\n  Machine ID:                 a27504a9a8de9326ab25236db517b6d4\n  System UUID:                a27504a9-a8de-9326-ab25-236db517b6d4\n  Boot ID:                    28592544-4d25-4f91-81dd-f50d0366a1e6\n  Kernel Version:             4.19.102+\n  OS Image:                   Container-Optimized OS from Google\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.1\n  Kubelet Version:            v1.18.1-beta.0.5+d5dfb5cb416fcc\n  Kube-Proxy Version:         v1.18.1-beta.0.5+d5dfb5cb416fcc\nPodCIDR:                      10.64.3.0/24\nPodCIDRs:                     10.64.3.0/24\nProviderID:                   gce://k8s-boskos-gce-project-10/us-west1-b/bootstrap-e2e-master\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                            CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                            ------------  ----------  ---------------  -------------  ---\n  kube-system                 etcd-empty-dir-cleanup-bootstrap-e2e-master     0 (0%)        0 (0%)      0 (0%)           0 (0%)         66m\n  kube-system                 etcd-server-bootstrap-e2e-master                200m (20%)    0 (0%)      0 (0%)           0 (0%)         66m\n  kube-system                 etcd-server-events-bootstrap-e2e-master         100m (10%)    0 (0%)      0 (0%)           0 (0%)         66m\n  kube-system                 fluentd-gcp-v3.2.0-fk5bk                        100m (10%)    1 (100%)    200Mi (5%)       500Mi (14%)    66m\n  kube-system                 kube-addon-manager-bootstrap-e2e-master         5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         66m\n  kube-system                 kube-apiserver-bootstrap-e2e-master             250m (25%)    0 (0%)      0 (0%)           0 (0%)         66m\n  kube-system                 kube-controller-manager-bootstrap-e2e-master    200m (20%)    0 (0%)      0 (0%)           0 (0%)         66m\n  kube-system                 kube-scheduler-bootstrap-e2e-master             75m (7%)      0 (0%)      0 (0%)           0 (0%)         66m\n  kube-system                 l7-lb-controller-bootstrap-e2e-master           10m (1%)      0 (0%)      50Mi (1%)        0 (0%)         66m\n  kube-system                 metadata-proxy-v0.1-4p9g7                       32m (3%)      32m (3%)    45Mi (1%)        45Mi (1%)      66m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests     Limits\n  --------                   --------     ------\n  cpu                        972m (97%)   1032m (103%)\n  memory                     345Mi (10%)  545Mi (15%)\n  ephemeral-storage          0 (0%)       0 (0%)\n  hugepages-2Mi              0 (0%)       0 (0%)\n  attachable-volumes-gce-pd  0            0\nEvents:                      <none>\n"
Mar 26 18:46:07.800: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config describe namespace kubectl-8662'
Mar 26 18:46:08.102: INFO: stderr: ""
Mar 26 18:46:08.102: INFO: stdout: "Name:         kubectl-8662\nLabels:       e2e-framework=kubectl\n              e2e-run=a0745263-a32d-41d3-98dd-d629e3e59f62\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:46:08.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8662" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":277,"completed":213,"skipped":3549,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:46:08.184: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 26 18:46:08.366: INFO: Waiting up to 5m0s for pod "pod-b0132f48-8350-4218-bff2-cd9e15568729" in namespace "emptydir-7028" to be "Succeeded or Failed"
Mar 26 18:46:08.401: INFO: Pod "pod-b0132f48-8350-4218-bff2-cd9e15568729": Phase="Pending", Reason="", readiness=false. Elapsed: 34.817097ms
Mar 26 18:46:10.437: INFO: Pod "pod-b0132f48-8350-4218-bff2-cd9e15568729": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.070490918s
STEP: Saw pod success
Mar 26 18:46:10.437: INFO: Pod "pod-b0132f48-8350-4218-bff2-cd9e15568729" satisfied condition "Succeeded or Failed"
Mar 26 18:46:10.472: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-b0132f48-8350-4218-bff2-cd9e15568729 container test-container: <nil>
STEP: delete the pod
Mar 26 18:46:10.565: INFO: Waiting for pod pod-b0132f48-8350-4218-bff2-cd9e15568729 to disappear
Mar 26 18:46:10.600: INFO: Pod pod-b0132f48-8350-4218-bff2-cd9e15568729 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:46:10.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7028" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":214,"skipped":3580,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:46:10.680: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-2653/configmap-test-62a5d998-ddd6-4728-86f7-51b7c22c1670
STEP: Creating a pod to test consume configMaps
Mar 26 18:46:10.910: INFO: Waiting up to 5m0s for pod "pod-configmaps-d1fd383a-0473-4173-a0b2-6dc7e8e2a08d" in namespace "configmap-2653" to be "Succeeded or Failed"
Mar 26 18:46:10.944: INFO: Pod "pod-configmaps-d1fd383a-0473-4173-a0b2-6dc7e8e2a08d": Phase="Pending", Reason="", readiness=false. Elapsed: 34.767667ms
Mar 26 18:46:12.980: INFO: Pod "pod-configmaps-d1fd383a-0473-4173-a0b2-6dc7e8e2a08d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.070341079s
STEP: Saw pod success
Mar 26 18:46:12.980: INFO: Pod "pod-configmaps-d1fd383a-0473-4173-a0b2-6dc7e8e2a08d" satisfied condition "Succeeded or Failed"
Mar 26 18:46:13.015: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-configmaps-d1fd383a-0473-4173-a0b2-6dc7e8e2a08d container env-test: <nil>
STEP: delete the pod
Mar 26 18:46:13.100: INFO: Waiting for pod pod-configmaps-d1fd383a-0473-4173-a0b2-6dc7e8e2a08d to disappear
Mar 26 18:46:13.135: INFO: Pod pod-configmaps-d1fd383a-0473-4173-a0b2-6dc7e8e2a08d no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:46:13.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2653" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":277,"completed":215,"skipped":3611,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:46:13.226: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 26 18:46:16.213: INFO: Successfully updated pod "pod-update-3fd8b470-feff-4d48-a153-cc18766ef627"
STEP: verifying the updated pod is in kubernetes
Mar 26 18:46:16.290: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:46:16.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6102" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":277,"completed":216,"skipped":3681,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:46:16.367: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:46:16.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2726" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":277,"completed":217,"skipped":3696,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:46:16.734: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 18:46:16.948: INFO: Waiting up to 5m0s for pod "downwardapi-volume-629e8c57-1ecd-49eb-8a10-20bd4d630cbd" in namespace "projected-3765" to be "Succeeded or Failed"
Mar 26 18:46:16.987: INFO: Pod "downwardapi-volume-629e8c57-1ecd-49eb-8a10-20bd4d630cbd": Phase="Pending", Reason="", readiness=false. Elapsed: 39.50552ms
Mar 26 18:46:19.023: INFO: Pod "downwardapi-volume-629e8c57-1ecd-49eb-8a10-20bd4d630cbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.075403954s
STEP: Saw pod success
Mar 26 18:46:19.023: INFO: Pod "downwardapi-volume-629e8c57-1ecd-49eb-8a10-20bd4d630cbd" satisfied condition "Succeeded or Failed"
Mar 26 18:46:19.058: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod downwardapi-volume-629e8c57-1ecd-49eb-8a10-20bd4d630cbd container client-container: <nil>
STEP: delete the pod
Mar 26 18:46:19.145: INFO: Waiting for pod downwardapi-volume-629e8c57-1ecd-49eb-8a10-20bd4d630cbd to disappear
Mar 26 18:46:19.181: INFO: Pod downwardapi-volume-629e8c57-1ecd-49eb-8a10-20bd4d630cbd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:46:19.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3765" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":218,"skipped":3722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:46:19.262: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:46:19.627: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 26 18:46:19.701: INFO: Number of nodes with available pods: 0
Mar 26 18:46:19.701: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar 26 18:46:19.863: INFO: Number of nodes with available pods: 0
Mar 26 18:46:19.863: INFO: Node bootstrap-e2e-minion-group-d5w7 is running more than one daemon pod
Mar 26 18:46:20.899: INFO: Number of nodes with available pods: 0
Mar 26 18:46:20.899: INFO: Node bootstrap-e2e-minion-group-d5w7 is running more than one daemon pod
Mar 26 18:46:21.901: INFO: Number of nodes with available pods: 1
Mar 26 18:46:21.901: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 26 18:46:22.070: INFO: Number of nodes with available pods: 0
Mar 26 18:46:22.070: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 26 18:46:22.146: INFO: Number of nodes with available pods: 0
Mar 26 18:46:22.146: INFO: Node bootstrap-e2e-minion-group-d5w7 is running more than one daemon pod
Mar 26 18:46:23.188: INFO: Number of nodes with available pods: 0
Mar 26 18:46:23.188: INFO: Node bootstrap-e2e-minion-group-d5w7 is running more than one daemon pod
Mar 26 18:46:24.182: INFO: Number of nodes with available pods: 0
Mar 26 18:46:24.182: INFO: Node bootstrap-e2e-minion-group-d5w7 is running more than one daemon pod
Mar 26 18:46:25.182: INFO: Number of nodes with available pods: 0
Mar 26 18:46:25.182: INFO: Node bootstrap-e2e-minion-group-d5w7 is running more than one daemon pod
Mar 26 18:46:26.182: INFO: Number of nodes with available pods: 0
Mar 26 18:46:26.182: INFO: Node bootstrap-e2e-minion-group-d5w7 is running more than one daemon pod
Mar 26 18:46:27.182: INFO: Number of nodes with available pods: 0
Mar 26 18:46:27.182: INFO: Node bootstrap-e2e-minion-group-d5w7 is running more than one daemon pod
Mar 26 18:46:28.182: INFO: Number of nodes with available pods: 0
Mar 26 18:46:28.182: INFO: Node bootstrap-e2e-minion-group-d5w7 is running more than one daemon pod
Mar 26 18:46:29.181: INFO: Number of nodes with available pods: 0
Mar 26 18:46:29.181: INFO: Node bootstrap-e2e-minion-group-d5w7 is running more than one daemon pod
Mar 26 18:46:30.182: INFO: Number of nodes with available pods: 0
Mar 26 18:46:30.182: INFO: Node bootstrap-e2e-minion-group-d5w7 is running more than one daemon pod
Mar 26 18:46:31.182: INFO: Number of nodes with available pods: 0
Mar 26 18:46:31.182: INFO: Node bootstrap-e2e-minion-group-d5w7 is running more than one daemon pod
Mar 26 18:46:32.182: INFO: Number of nodes with available pods: 0
Mar 26 18:46:32.182: INFO: Node bootstrap-e2e-minion-group-d5w7 is running more than one daemon pod
Mar 26 18:46:33.182: INFO: Number of nodes with available pods: 0
Mar 26 18:46:33.182: INFO: Node bootstrap-e2e-minion-group-d5w7 is running more than one daemon pod
Mar 26 18:46:34.183: INFO: Number of nodes with available pods: 1
Mar 26 18:46:34.183: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5467, will wait for the garbage collector to delete the pods
Mar 26 18:46:34.376: INFO: Deleting DaemonSet.extensions daemon-set took: 37.612015ms
Mar 26 18:46:35.076: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.301266ms
Mar 26 18:46:42.513: INFO: Number of nodes with available pods: 0
Mar 26 18:46:42.513: INFO: Number of running nodes: 0, number of available pods: 0
Mar 26 18:46:42.548: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5467/daemonsets","resourceVersion":"21855"},"items":null}

Mar 26 18:46:42.583: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5467/pods","resourceVersion":"21855"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:46:42.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5467" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":277,"completed":219,"skipped":3748,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:46:42.852: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 18:46:43.086: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3002d31e-75e1-4c48-bcc6-06ce57b8363d" in namespace "projected-1417" to be "Succeeded or Failed"
Mar 26 18:46:43.121: INFO: Pod "downwardapi-volume-3002d31e-75e1-4c48-bcc6-06ce57b8363d": Phase="Pending", Reason="", readiness=false. Elapsed: 34.864015ms
Mar 26 18:46:45.157: INFO: Pod "downwardapi-volume-3002d31e-75e1-4c48-bcc6-06ce57b8363d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.07061002s
STEP: Saw pod success
Mar 26 18:46:45.157: INFO: Pod "downwardapi-volume-3002d31e-75e1-4c48-bcc6-06ce57b8363d" satisfied condition "Succeeded or Failed"
Mar 26 18:46:45.192: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod downwardapi-volume-3002d31e-75e1-4c48-bcc6-06ce57b8363d container client-container: <nil>
STEP: delete the pod
Mar 26 18:46:45.279: INFO: Waiting for pod downwardapi-volume-3002d31e-75e1-4c48-bcc6-06ce57b8363d to disappear
Mar 26 18:46:45.314: INFO: Pod downwardapi-volume-3002d31e-75e1-4c48-bcc6-06ce57b8363d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:46:45.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1417" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":277,"completed":220,"skipped":3758,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:46:45.394: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Update Demo
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:271
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a replication controller
Mar 26 18:46:45.536: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-1632'
Mar 26 18:46:45.898: INFO: stderr: ""
Mar 26 18:46:45.898: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 26 18:46:45.898: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1632'
Mar 26 18:46:46.088: INFO: stderr: ""
Mar 26 18:46:46.088: INFO: stdout: "update-demo-nautilus-b5zr7 update-demo-nautilus-f2gv9 "
Mar 26 18:46:46.088: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-b5zr7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1632'
Mar 26 18:46:46.281: INFO: stderr: ""
Mar 26 18:46:46.281: INFO: stdout: ""
Mar 26 18:46:46.281: INFO: update-demo-nautilus-b5zr7 is created but not running
Mar 26 18:46:51.281: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1632'
Mar 26 18:46:51.476: INFO: stderr: ""
Mar 26 18:46:51.476: INFO: stdout: "update-demo-nautilus-b5zr7 update-demo-nautilus-f2gv9 "
Mar 26 18:46:51.476: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-b5zr7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1632'
Mar 26 18:46:51.669: INFO: stderr: ""
Mar 26 18:46:51.669: INFO: stdout: "true"
Mar 26 18:46:51.669: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-b5zr7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1632'
Mar 26 18:46:51.855: INFO: stderr: ""
Mar 26 18:46:51.855: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 18:46:51.855: INFO: validating pod update-demo-nautilus-b5zr7
Mar 26 18:46:51.893: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 18:46:51.893: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 18:46:51.893: INFO: update-demo-nautilus-b5zr7 is verified up and running
Mar 26 18:46:51.893: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-f2gv9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1632'
Mar 26 18:46:52.097: INFO: stderr: ""
Mar 26 18:46:52.097: INFO: stdout: "true"
Mar 26 18:46:52.098: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-f2gv9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1632'
Mar 26 18:46:52.293: INFO: stderr: ""
Mar 26 18:46:52.293: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 18:46:52.293: INFO: validating pod update-demo-nautilus-f2gv9
Mar 26 18:46:52.331: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 18:46:52.331: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 18:46:52.331: INFO: update-demo-nautilus-f2gv9 is verified up and running
STEP: scaling down the replication controller
Mar 26 18:46:52.334: INFO: scanned /workspace for discovery docs: <nil>
Mar 26 18:46:52.334: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1632'
Mar 26 18:46:52.607: INFO: stderr: ""
Mar 26 18:46:52.607: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 26 18:46:52.607: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1632'
Mar 26 18:46:52.799: INFO: stderr: ""
Mar 26 18:46:52.799: INFO: stdout: "update-demo-nautilus-b5zr7 update-demo-nautilus-f2gv9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 26 18:46:57.799: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1632'
Mar 26 18:46:57.998: INFO: stderr: ""
Mar 26 18:46:57.998: INFO: stdout: "update-demo-nautilus-f2gv9 "
Mar 26 18:46:57.998: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-f2gv9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1632'
Mar 26 18:46:58.199: INFO: stderr: ""
Mar 26 18:46:58.199: INFO: stdout: "true"
Mar 26 18:46:58.199: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-f2gv9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1632'
Mar 26 18:46:58.391: INFO: stderr: ""
Mar 26 18:46:58.391: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 18:46:58.391: INFO: validating pod update-demo-nautilus-f2gv9
Mar 26 18:46:58.430: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 18:46:58.430: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 18:46:58.430: INFO: update-demo-nautilus-f2gv9 is verified up and running
STEP: scaling up the replication controller
Mar 26 18:46:58.432: INFO: scanned /workspace for discovery docs: <nil>
Mar 26 18:46:58.432: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1632'
Mar 26 18:46:58.716: INFO: stderr: ""
Mar 26 18:46:58.716: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 26 18:46:58.716: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1632'
Mar 26 18:46:58.923: INFO: stderr: ""
Mar 26 18:46:58.923: INFO: stdout: "update-demo-nautilus-f2gv9 update-demo-nautilus-nb79r "
Mar 26 18:46:58.923: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-f2gv9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1632'
Mar 26 18:46:59.109: INFO: stderr: ""
Mar 26 18:46:59.109: INFO: stdout: "true"
Mar 26 18:46:59.109: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-f2gv9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1632'
Mar 26 18:46:59.310: INFO: stderr: ""
Mar 26 18:46:59.310: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 18:46:59.310: INFO: validating pod update-demo-nautilus-f2gv9
Mar 26 18:46:59.357: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 18:46:59.357: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 18:46:59.357: INFO: update-demo-nautilus-f2gv9 is verified up and running
Mar 26 18:46:59.357: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-nb79r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1632'
Mar 26 18:46:59.567: INFO: stderr: ""
Mar 26 18:46:59.567: INFO: stdout: ""
Mar 26 18:46:59.567: INFO: update-demo-nautilus-nb79r is created but not running
Mar 26 18:47:04.567: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1632'
Mar 26 18:47:04.771: INFO: stderr: ""
Mar 26 18:47:04.771: INFO: stdout: "update-demo-nautilus-f2gv9 update-demo-nautilus-nb79r "
Mar 26 18:47:04.771: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-f2gv9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1632'
Mar 26 18:47:04.978: INFO: stderr: ""
Mar 26 18:47:04.978: INFO: stdout: "true"
Mar 26 18:47:04.978: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-f2gv9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1632'
Mar 26 18:47:05.184: INFO: stderr: ""
Mar 26 18:47:05.184: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 18:47:05.184: INFO: validating pod update-demo-nautilus-f2gv9
Mar 26 18:47:05.229: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 18:47:05.229: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 18:47:05.229: INFO: update-demo-nautilus-f2gv9 is verified up and running
Mar 26 18:47:05.229: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-nb79r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1632'
Mar 26 18:47:05.433: INFO: stderr: ""
Mar 26 18:47:05.433: INFO: stdout: "true"
Mar 26 18:47:05.433: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods update-demo-nautilus-nb79r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1632'
Mar 26 18:47:05.629: INFO: stderr: ""
Mar 26 18:47:05.629: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 18:47:05.629: INFO: validating pod update-demo-nautilus-nb79r
Mar 26 18:47:05.668: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 18:47:05.668: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 18:47:05.668: INFO: update-demo-nautilus-nb79r is verified up and running
STEP: using delete to clean up resources
Mar 26 18:47:05.668: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-1632'
Mar 26 18:47:05.916: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 18:47:05.916: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 26 18:47:05.916: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1632'
Mar 26 18:47:06.171: INFO: stderr: "No resources found in kubectl-1632 namespace.\n"
Mar 26 18:47:06.171: INFO: stdout: ""
Mar 26 18:47:06.171: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods -l name=update-demo --namespace=kubectl-1632 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 26 18:47:06.379: INFO: stderr: ""
Mar 26 18:47:06.379: INFO: stdout: "update-demo-nautilus-f2gv9\nupdate-demo-nautilus-nb79r\n"
Mar 26 18:47:06.880: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1632'
Mar 26 18:47:07.133: INFO: stderr: "No resources found in kubectl-1632 namespace.\n"
Mar 26 18:47:07.133: INFO: stdout: ""
Mar 26 18:47:07.133: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pods -l name=update-demo --namespace=kubectl-1632 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 26 18:47:07.365: INFO: stderr: ""
Mar 26 18:47:07.365: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:47:07.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1632" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":277,"completed":221,"skipped":3783,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:47:07.443: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8336.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8336.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 18:47:10.051: INFO: DNS probes using dns-8336/dns-test-447840eb-d51c-4b1e-8dde-e8660f12eebb succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:47:10.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8336" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":277,"completed":222,"skipped":3813,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:47:10.174: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 26 18:47:10.634: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:10.694: INFO: Number of nodes with available pods: 0
Mar 26 18:47:10.695: INFO: Node bootstrap-e2e-minion-group-1r9w is running more than one daemon pod
Mar 26 18:47:11.736: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:11.772: INFO: Number of nodes with available pods: 0
Mar 26 18:47:11.772: INFO: Node bootstrap-e2e-minion-group-1r9w is running more than one daemon pod
Mar 26 18:47:12.737: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:12.780: INFO: Number of nodes with available pods: 3
Mar 26 18:47:12.780: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 26 18:47:12.932: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:12.968: INFO: Number of nodes with available pods: 2
Mar 26 18:47:12.968: INFO: Node bootstrap-e2e-minion-group-gs7c is running more than one daemon pod
Mar 26 18:47:14.009: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:14.045: INFO: Number of nodes with available pods: 2
Mar 26 18:47:14.045: INFO: Node bootstrap-e2e-minion-group-gs7c is running more than one daemon pod
Mar 26 18:47:15.009: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:15.046: INFO: Number of nodes with available pods: 2
Mar 26 18:47:15.046: INFO: Node bootstrap-e2e-minion-group-gs7c is running more than one daemon pod
Mar 26 18:47:16.009: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:16.051: INFO: Number of nodes with available pods: 2
Mar 26 18:47:16.051: INFO: Node bootstrap-e2e-minion-group-gs7c is running more than one daemon pod
Mar 26 18:47:17.008: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:17.045: INFO: Number of nodes with available pods: 2
Mar 26 18:47:17.045: INFO: Node bootstrap-e2e-minion-group-gs7c is running more than one daemon pod
Mar 26 18:47:18.010: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:18.047: INFO: Number of nodes with available pods: 2
Mar 26 18:47:18.048: INFO: Node bootstrap-e2e-minion-group-gs7c is running more than one daemon pod
Mar 26 18:47:19.009: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:19.046: INFO: Number of nodes with available pods: 2
Mar 26 18:47:19.046: INFO: Node bootstrap-e2e-minion-group-gs7c is running more than one daemon pod
Mar 26 18:47:20.009: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:20.045: INFO: Number of nodes with available pods: 2
Mar 26 18:47:20.045: INFO: Node bootstrap-e2e-minion-group-gs7c is running more than one daemon pod
Mar 26 18:47:21.009: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:21.046: INFO: Number of nodes with available pods: 2
Mar 26 18:47:21.046: INFO: Node bootstrap-e2e-minion-group-gs7c is running more than one daemon pod
Mar 26 18:47:22.009: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:22.046: INFO: Number of nodes with available pods: 2
Mar 26 18:47:22.046: INFO: Node bootstrap-e2e-minion-group-gs7c is running more than one daemon pod
Mar 26 18:47:23.009: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:23.045: INFO: Number of nodes with available pods: 2
Mar 26 18:47:23.046: INFO: Node bootstrap-e2e-minion-group-gs7c is running more than one daemon pod
Mar 26 18:47:24.009: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:24.045: INFO: Number of nodes with available pods: 2
Mar 26 18:47:24.045: INFO: Node bootstrap-e2e-minion-group-gs7c is running more than one daemon pod
Mar 26 18:47:25.009: INFO: DaemonSet pods can't tolerate node bootstrap-e2e-master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.kubernetes.io/unschedulable Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:47:25.045: INFO: Number of nodes with available pods: 3
Mar 26 18:47:25.045: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5392, will wait for the garbage collector to delete the pods
Mar 26 18:47:25.204: INFO: Deleting DaemonSet.extensions daemon-set took: 38.285358ms
Mar 26 18:47:25.304: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.307668ms
Mar 26 18:47:33.142: INFO: Number of nodes with available pods: 0
Mar 26 18:47:33.142: INFO: Number of running nodes: 0, number of available pods: 0
Mar 26 18:47:33.180: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5392/daemonsets","resourceVersion":"22141"},"items":null}

Mar 26 18:47:33.215: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5392/pods","resourceVersion":"22141"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:47:33.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5392" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":277,"completed":223,"skipped":3825,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:47:33.440: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8236
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8236
I0326 18:47:33.745698    9212 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8236, replica count: 2
Mar 26 18:47:36.796: INFO: Creating new exec pod
I0326 18:47:36.796420    9212 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 26 18:47:39.920: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-8236 execpod2vw9t -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 26 18:47:41.481: INFO: rc: 1
Mar 26 18:47:41.481: INFO: Service reachability failing with error: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-8236 execpod2vw9t -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 externalname-service 80
nc: connect to externalname-service port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Mar 26 18:47:42.481: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-8236 execpod2vw9t -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 26 18:47:44.037: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 26 18:47:44.037: INFO: stdout: ""
Mar 26 18:47:44.037: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-8236 execpod2vw9t -- /bin/sh -x -c nc -zv -t -w 2 10.0.65.93 80'
Mar 26 18:47:44.575: INFO: stderr: "+ nc -zv -t -w 2 10.0.65.93 80\nConnection to 10.0.65.93 80 port [tcp/http] succeeded!\n"
Mar 26 18:47:44.575: INFO: stdout: ""
Mar 26 18:47:44.575: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:47:44.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8236" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":277,"completed":224,"skipped":3830,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:47:44.730: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap configmap-1577/configmap-test-b2cf65c6-8a1e-41db-9d92-9e08ffc4698c
STEP: Creating a pod to test consume configMaps
Mar 26 18:47:45.058: INFO: Waiting up to 5m0s for pod "pod-configmaps-fc6af0d3-7798-4372-8aaf-9d62c4644de2" in namespace "configmap-1577" to be "Succeeded or Failed"
Mar 26 18:47:45.126: INFO: Pod "pod-configmaps-fc6af0d3-7798-4372-8aaf-9d62c4644de2": Phase="Pending", Reason="", readiness=false. Elapsed: 68.275811ms
Mar 26 18:47:47.161: INFO: Pod "pod-configmaps-fc6af0d3-7798-4372-8aaf-9d62c4644de2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.103595582s
STEP: Saw pod success
Mar 26 18:47:47.161: INFO: Pod "pod-configmaps-fc6af0d3-7798-4372-8aaf-9d62c4644de2" satisfied condition "Succeeded or Failed"
Mar 26 18:47:47.197: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-configmaps-fc6af0d3-7798-4372-8aaf-9d62c4644de2 container env-test: <nil>
STEP: delete the pod
Mar 26 18:47:47.283: INFO: Waiting for pod pod-configmaps-fc6af0d3-7798-4372-8aaf-9d62c4644de2 to disappear
Mar 26 18:47:47.318: INFO: Pod pod-configmaps-fc6af0d3-7798-4372-8aaf-9d62c4644de2 no longer exists
[AfterEach] [sig-node] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:47:47.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1577" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":277,"completed":225,"skipped":3839,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:47:47.396: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:47:49.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4543" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":226,"skipped":3855,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:47:49.857: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating service endpoint-test2 in namespace services-3950
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3950 to expose endpoints map[]
Mar 26 18:47:50.134: INFO: successfully validated that service endpoint-test2 in namespace services-3950 exposes endpoints map[] (35.463944ms elapsed)
STEP: Creating pod pod1 in namespace services-3950
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3950 to expose endpoints map[pod1:[80]]
Mar 26 18:47:52.402: INFO: successfully validated that service endpoint-test2 in namespace services-3950 exposes endpoints map[pod1:[80]] (2.226418841s elapsed)
STEP: Creating pod pod2 in namespace services-3950
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3950 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 26 18:47:54.766: INFO: successfully validated that service endpoint-test2 in namespace services-3950 exposes endpoints map[pod1:[80] pod2:[80]] (2.323900971s elapsed)
STEP: Deleting pod pod1 in namespace services-3950
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3950 to expose endpoints map[pod2:[80]]
Mar 26 18:47:54.901: INFO: successfully validated that service endpoint-test2 in namespace services-3950 exposes endpoints map[pod2:[80]] (96.552461ms elapsed)
STEP: Deleting pod pod2 in namespace services-3950
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3950 to expose endpoints map[]
Mar 26 18:47:54.980: INFO: successfully validated that service endpoint-test2 in namespace services-3950 exposes endpoints map[] (35.841262ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:47:55.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3950" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":277,"completed":227,"skipped":3869,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:47:55.131: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:698
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2789
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-2789
I0326 18:47:55.580290    9212 runners.go:190] Created replication controller with name: externalname-service, namespace: services-2789, replica count: 2
Mar 26 18:47:58.630: INFO: Creating new exec pod
I0326 18:47:58.630922    9212 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 26 18:48:01.782: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-2789 execpod2n7nx -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 26 18:48:04.289: INFO: rc: 1
Mar 26 18:48:04.289: INFO: Service reachability failing with error: error running /workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-2789 execpod2n7nx -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80:
Command stdout:

stderr:
+ nc -zv -t -w 2 externalname-service 80
nc: connect to externalname-service port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Mar 26 18:48:05.290: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-2789 execpod2n7nx -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 26 18:48:05.896: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 26 18:48:05.896: INFO: stdout: ""
Mar 26 18:48:05.897: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-2789 execpod2n7nx -- /bin/sh -x -c nc -zv -t -w 2 10.0.11.188 80'
Mar 26 18:48:06.426: INFO: stderr: "+ nc -zv -t -w 2 10.0.11.188 80\nConnection to 10.0.11.188 80 port [tcp/http] succeeded!\n"
Mar 26 18:48:06.426: INFO: stdout: ""
Mar 26 18:48:06.426: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-2789 execpod2n7nx -- /bin/sh -x -c nc -zv -t -w 2 10.138.0.3 30129'
Mar 26 18:48:06.972: INFO: stderr: "+ nc -zv -t -w 2 10.138.0.3 30129\nConnection to 10.138.0.3 30129 port [tcp/30129] succeeded!\n"
Mar 26 18:48:06.972: INFO: stdout: ""
Mar 26 18:48:06.972: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-2789 execpod2n7nx -- /bin/sh -x -c nc -zv -t -w 2 10.138.0.5 30129'
Mar 26 18:48:07.484: INFO: stderr: "+ nc -zv -t -w 2 10.138.0.5 30129\nConnection to 10.138.0.5 30129 port [tcp/30129] succeeded!\n"
Mar 26 18:48:07.485: INFO: stdout: ""
Mar 26 18:48:07.485: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-2789 execpod2n7nx -- /bin/sh -x -c nc -zv -t -w 2 35.247.65.27 30129'
Mar 26 18:48:08.002: INFO: stderr: "+ nc -zv -t -w 2 35.247.65.27 30129\nConnection to 35.247.65.27 30129 port [tcp/30129] succeeded!\n"
Mar 26 18:48:08.002: INFO: stdout: ""
Mar 26 18:48:08.002: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config exec --namespace=services-2789 execpod2n7nx -- /bin/sh -x -c nc -zv -t -w 2 34.82.1.168 30129'
Mar 26 18:48:08.528: INFO: stderr: "+ nc -zv -t -w 2 34.82.1.168 30129\nConnection to 34.82.1.168 30129 port [tcp/30129] succeeded!\n"
Mar 26 18:48:08.528: INFO: stdout: ""
Mar 26 18:48:08.528: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:48:08.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2789" for this suite.
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:702
•{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":277,"completed":228,"skipped":3906,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:48:08.688: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:48:11.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-756" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":277,"completed":229,"skipped":3907,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:48:11.297: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[BeforeEach] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1454
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 26 18:48:11.440: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-8321'
Mar 26 18:48:11.639: INFO: stderr: ""
Mar 26 18:48:11.639: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar 26 18:48:16.690: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config get pod e2e-test-httpd-pod --namespace=kubectl-8321 -o json'
Mar 26 18:48:16.896: INFO: stderr: ""
Mar 26 18:48:16.896: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-03-26T18:48:11Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-03-26T18:48:11Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.64.1.218\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-03-26T18:48:13Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8321\",\n        \"resourceVersion\": \"22462\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8321/pods/e2e-test-httpd-pod\",\n        \"uid\": \"6f4bc46a-6db9-48a5-8f16-cd4c59958cc6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-rpwdb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"bootstrap-e2e-minion-group-1r9w\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-rpwdb\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-rpwdb\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-26T18:48:11Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-26T18:48:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-26T18:48:13Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-26T18:48:11Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://ebd3d3cffaf4df6718c84b72e04582572203d3dc03580b06ac38d34c52867d8e\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-03-26T18:48:12Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.138.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.64.1.218\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.64.1.218\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-03-26T18:48:11Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 26 18:48:16.896: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config replace -f - --namespace=kubectl-8321'
Mar 26 18:48:17.256: INFO: stderr: ""
Mar 26 18:48:17.257: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1459
Mar 26 18:48:17.293: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config delete pods e2e-test-httpd-pod --namespace=kubectl-8321'
Mar 26 18:48:22.767: INFO: stderr: ""
Mar 26 18:48:22.767: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:48:22.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8321" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":277,"completed":230,"skipped":3907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:48:22.844: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 18:48:23.037: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c42ba738-4b06-4e59-b107-d7e3a4e764ca" in namespace "projected-9049" to be "Succeeded or Failed"
Mar 26 18:48:23.074: INFO: Pod "downwardapi-volume-c42ba738-4b06-4e59-b107-d7e3a4e764ca": Phase="Pending", Reason="", readiness=false. Elapsed: 37.72815ms
Mar 26 18:48:25.110: INFO: Pod "downwardapi-volume-c42ba738-4b06-4e59-b107-d7e3a4e764ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.073482301s
STEP: Saw pod success
Mar 26 18:48:25.110: INFO: Pod "downwardapi-volume-c42ba738-4b06-4e59-b107-d7e3a4e764ca" satisfied condition "Succeeded or Failed"
Mar 26 18:48:25.145: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod downwardapi-volume-c42ba738-4b06-4e59-b107-d7e3a4e764ca container client-container: <nil>
STEP: delete the pod
Mar 26 18:48:25.232: INFO: Waiting for pod downwardapi-volume-c42ba738-4b06-4e59-b107-d7e3a4e764ca to disappear
Mar 26 18:48:25.267: INFO: Pod downwardapi-volume-c42ba738-4b06-4e59-b107-d7e3a4e764ca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:48:25.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9049" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":231,"skipped":3934,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:48:25.348: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar 26 18:48:30.218: INFO: Successfully updated pod "adopt-release-2f247"
STEP: Checking that the Job readopts the Pod
Mar 26 18:48:30.218: INFO: Waiting up to 15m0s for pod "adopt-release-2f247" in namespace "job-6098" to be "adopted"
Mar 26 18:48:30.256: INFO: Pod "adopt-release-2f247": Phase="Running", Reason="", readiness=true. Elapsed: 37.508376ms
Mar 26 18:48:30.256: INFO: Pod "adopt-release-2f247" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar 26 18:48:30.830: INFO: Successfully updated pod "adopt-release-2f247"
STEP: Checking that the Job releases the Pod
Mar 26 18:48:30.831: INFO: Waiting up to 15m0s for pod "adopt-release-2f247" in namespace "job-6098" to be "released"
Mar 26 18:48:30.914: INFO: Pod "adopt-release-2f247": Phase="Running", Reason="", readiness=true. Elapsed: 82.967462ms
Mar 26 18:48:30.914: INFO: Pod "adopt-release-2f247" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:48:30.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6098" for this suite.
•{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":277,"completed":232,"skipped":3939,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:48:31.002: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating Agnhost RC
Mar 26 18:48:31.145: INFO: namespace kubectl-1604
Mar 26 18:48:31.145: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-1604'
Mar 26 18:48:31.521: INFO: stderr: ""
Mar 26 18:48:31.521: INFO: stdout: "replicationcontroller/agnhost-master created\n"
STEP: Waiting for Agnhost master to start.
Mar 26 18:48:32.558: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 18:48:32.558: INFO: Found 0 / 1
Mar 26 18:48:33.558: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 18:48:33.558: INFO: Found 1 / 1
Mar 26 18:48:33.558: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 26 18:48:33.594: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 26 18:48:33.594: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 26 18:48:33.594: INFO: wait on agnhost-master startup in kubectl-1604 
Mar 26 18:48:33.594: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config logs agnhost-master-8fqpz agnhost-master --namespace=kubectl-1604'
Mar 26 18:48:33.847: INFO: stderr: ""
Mar 26 18:48:33.847: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar 26 18:48:33.847: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config expose rc agnhost-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1604'
Mar 26 18:48:34.128: INFO: stderr: ""
Mar 26 18:48:34.128: INFO: stdout: "service/rm2 exposed\n"
Mar 26 18:48:34.163: INFO: Service rm2 in namespace kubectl-1604 found.
STEP: exposing service
Mar 26 18:48:36.241: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1604'
Mar 26 18:48:36.511: INFO: stderr: ""
Mar 26 18:48:36.511: INFO: stdout: "service/rm3 exposed\n"
Mar 26 18:48:36.550: INFO: Service rm3 in namespace kubectl-1604 found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:48:38.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1604" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":277,"completed":233,"skipped":3958,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:48:38.697: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 18:48:38.973: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd7e5861-79d8-4256-8581-cf47b5e54aee" in namespace "downward-api-3280" to be "Succeeded or Failed"
Mar 26 18:48:39.013: INFO: Pod "downwardapi-volume-fd7e5861-79d8-4256-8581-cf47b5e54aee": Phase="Pending", Reason="", readiness=false. Elapsed: 40.205421ms
Mar 26 18:48:41.048: INFO: Pod "downwardapi-volume-fd7e5861-79d8-4256-8581-cf47b5e54aee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.07553485s
STEP: Saw pod success
Mar 26 18:48:41.049: INFO: Pod "downwardapi-volume-fd7e5861-79d8-4256-8581-cf47b5e54aee" satisfied condition "Succeeded or Failed"
Mar 26 18:48:41.084: INFO: Trying to get logs from node bootstrap-e2e-minion-group-gs7c pod downwardapi-volume-fd7e5861-79d8-4256-8581-cf47b5e54aee container client-container: <nil>
STEP: delete the pod
Mar 26 18:48:41.178: INFO: Waiting for pod downwardapi-volume-fd7e5861-79d8-4256-8581-cf47b5e54aee to disappear
Mar 26 18:48:41.213: INFO: Pod downwardapi-volume-fd7e5861-79d8-4256-8581-cf47b5e54aee no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:48:41.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3280" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":234,"skipped":3965,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:48:41.292: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-8e181944-7a1c-4705-9def-319d2c210f13
STEP: Creating a pod to test consume configMaps
Mar 26 18:48:41.598: INFO: Waiting up to 5m0s for pod "pod-configmaps-73ad0b35-1eb7-4ea7-ba63-d5f1a9178750" in namespace "configmap-9720" to be "Succeeded or Failed"
Mar 26 18:48:41.638: INFO: Pod "pod-configmaps-73ad0b35-1eb7-4ea7-ba63-d5f1a9178750": Phase="Pending", Reason="", readiness=false. Elapsed: 39.723622ms
Mar 26 18:48:43.675: INFO: Pod "pod-configmaps-73ad0b35-1eb7-4ea7-ba63-d5f1a9178750": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076815909s
STEP: Saw pod success
Mar 26 18:48:43.675: INFO: Pod "pod-configmaps-73ad0b35-1eb7-4ea7-ba63-d5f1a9178750" satisfied condition "Succeeded or Failed"
Mar 26 18:48:43.717: INFO: Trying to get logs from node bootstrap-e2e-minion-group-gs7c pod pod-configmaps-73ad0b35-1eb7-4ea7-ba63-d5f1a9178750 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 18:48:43.853: INFO: Waiting for pod pod-configmaps-73ad0b35-1eb7-4ea7-ba63-d5f1a9178750 to disappear
Mar 26 18:48:43.891: INFO: Pod pod-configmaps-73ad0b35-1eb7-4ea7-ba63-d5f1a9178750 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:48:43.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9720" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":235,"skipped":3976,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:48:43.983: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:91
Mar 26 18:48:44.143: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 26 18:48:44.264: INFO: Waiting for terminating namespaces to be deleted...
Mar 26 18:48:44.301: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-1r9w before test
Mar 26 18:48:44.346: INFO: adopt-release-rhn9c from job-6098 started at 2020-03-26 18:48:25 +0000 UTC (1 container statuses recorded)
Mar 26 18:48:44.346: INFO: 	Container c ready: true, restart count 0
Mar 26 18:48:44.346: INFO: metadata-proxy-v0.1-gh7hj from kube-system started at 2020-03-26 17:39:23 +0000 UTC (2 container statuses recorded)
Mar 26 18:48:44.346: INFO: 	Container metadata-proxy ready: true, restart count 0
Mar 26 18:48:44.346: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:48:44.346: INFO: agnhost-master-8fqpz from kubectl-1604 started at 2020-03-26 18:48:31 +0000 UTC (1 container statuses recorded)
Mar 26 18:48:44.346: INFO: 	Container agnhost-master ready: true, restart count 0
Mar 26 18:48:44.346: INFO: adopt-release-2f247 from job-6098 started at 2020-03-26 18:48:25 +0000 UTC (1 container statuses recorded)
Mar 26 18:48:44.346: INFO: 	Container c ready: true, restart count 0
Mar 26 18:48:44.346: INFO: adopt-release-zs5lh from job-6098 started at 2020-03-26 18:48:30 +0000 UTC (1 container statuses recorded)
Mar 26 18:48:44.346: INFO: 	Container c ready: true, restart count 0
Mar 26 18:48:44.346: INFO: kube-proxy-bootstrap-e2e-minion-group-1r9w from kube-system started at 2020-03-26 17:39:23 +0000 UTC (1 container statuses recorded)
Mar 26 18:48:44.346: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 18:48:44.346: INFO: fluentd-gcp-v3.2.0-jzmnt from kube-system started at 2020-03-26 17:40:02 +0000 UTC (2 container statuses recorded)
Mar 26 18:48:44.346: INFO: 	Container fluentd-gcp ready: true, restart count 0
Mar 26 18:48:44.346: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:48:44.346: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-d5w7 before test
Mar 26 18:48:44.400: INFO: fluentd-gcp-v3.2.0-tgnxg from kube-system started at 2020-03-26 17:40:18 +0000 UTC (2 container statuses recorded)
Mar 26 18:48:44.400: INFO: 	Container fluentd-gcp ready: true, restart count 0
Mar 26 18:48:44.400: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:48:44.400: INFO: kube-proxy-bootstrap-e2e-minion-group-d5w7 from kube-system started at 2020-03-26 17:39:22 +0000 UTC (1 container statuses recorded)
Mar 26 18:48:44.400: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 18:48:44.400: INFO: coredns-7876554b79-sxhp6 from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:48:44.400: INFO: 	Container coredns ready: true, restart count 0
Mar 26 18:48:44.400: INFO: event-exporter-v0.3.1-d877d48cb-lv2tb from kube-system started at 2020-03-26 18:23:08 +0000 UTC (2 container statuses recorded)
Mar 26 18:48:44.400: INFO: 	Container event-exporter ready: true, restart count 0
Mar 26 18:48:44.400: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:48:44.400: INFO: metrics-server-v0.3.6-7d85574868-j5259 from kube-system started at 2020-03-26 17:39:44 +0000 UTC (2 container statuses recorded)
Mar 26 18:48:44.400: INFO: 	Container metrics-server ready: true, restart count 0
Mar 26 18:48:44.400: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Mar 26 18:48:44.400: INFO: metadata-proxy-v0.1-v2jtw from kube-system started at 2020-03-26 17:39:23 +0000 UTC (2 container statuses recorded)
Mar 26 18:48:44.400: INFO: 	Container metadata-proxy ready: true, restart count 0
Mar 26 18:48:44.400: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:48:44.400: INFO: volume-snapshot-controller-0 from kube-system started at 2020-03-26 17:39:35 +0000 UTC (1 container statuses recorded)
Mar 26 18:48:44.400: INFO: 	Container volume-snapshot-controller ready: true, restart count 0
Mar 26 18:48:44.400: INFO: 
Logging pods the kubelet thinks is on node bootstrap-e2e-minion-group-gs7c before test
Mar 26 18:48:44.446: INFO: coredns-7876554b79-vp7xl from kube-system started at 2020-03-26 17:39:43 +0000 UTC (1 container statuses recorded)
Mar 26 18:48:44.446: INFO: 	Container coredns ready: true, restart count 0
Mar 26 18:48:44.446: INFO: metadata-proxy-v0.1-ljlsd from kube-system started at 2020-03-26 17:39:23 +0000 UTC (2 container statuses recorded)
Mar 26 18:48:44.446: INFO: 	Container metadata-proxy ready: true, restart count 0
Mar 26 18:48:44.446: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:48:44.446: INFO: kube-dns-autoscaler-579dbcdc47-brlvc from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:48:44.446: INFO: 	Container autoscaler ready: true, restart count 0
Mar 26 18:48:44.446: INFO: l7-default-backend-f947d4dd5-zfbcj from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:48:44.446: INFO: 	Container default-http-backend ready: true, restart count 0
Mar 26 18:48:44.446: INFO: fluentd-gcp-scaler-54b85fcc78-x2g9h from kube-system started at 2020-03-26 17:39:35 +0000 UTC (1 container statuses recorded)
Mar 26 18:48:44.446: INFO: 	Container fluentd-gcp-scaler ready: true, restart count 0
Mar 26 18:48:44.446: INFO: fluentd-gcp-v3.2.0-bbgzp from kube-system started at 2020-03-26 17:40:13 +0000 UTC (2 container statuses recorded)
Mar 26 18:48:44.446: INFO: 	Container fluentd-gcp ready: true, restart count 0
Mar 26 18:48:44.446: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Mar 26 18:48:44.446: INFO: kubernetes-dashboard-864d864f44-2knxp from kube-system started at 2020-03-26 17:39:34 +0000 UTC (1 container statuses recorded)
Mar 26 18:48:44.446: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Mar 26 18:48:44.446: INFO: kube-proxy-bootstrap-e2e-minion-group-gs7c from kube-system started at 2020-03-26 17:39:23 +0000 UTC (1 container statuses recorded)
Mar 26 18:48:44.446: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b48e49d5-770d-413e-b7d4-3cd8eb85328c 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-b48e49d5-770d-413e-b7d4-3cd8eb85328c off the node bootstrap-e2e-minion-group-gs7c
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b48e49d5-770d-413e-b7d4-3cd8eb85328c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:48:51.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4843" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:82
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":277,"completed":236,"skipped":4009,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:48:51.096: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:49:02.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9863" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":277,"completed":237,"skipped":4013,"failed":0}
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:49:02.601: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:49:02.871: INFO: (0) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 45.526513ms)
Mar 26 18:49:02.908: INFO: (1) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 36.651976ms)
Mar 26 18:49:02.945: INFO: (2) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 37.880379ms)
Mar 26 18:49:02.982: INFO: (3) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 36.356987ms)
Mar 26 18:49:03.018: INFO: (4) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 36.404454ms)
Mar 26 18:49:03.062: INFO: (5) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 43.404888ms)
Mar 26 18:49:03.099: INFO: (6) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 36.666919ms)
Mar 26 18:49:03.135: INFO: (7) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 36.636354ms)
Mar 26 18:49:03.172: INFO: (8) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 36.509384ms)
Mar 26 18:49:03.209: INFO: (9) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 36.747075ms)
Mar 26 18:49:03.245: INFO: (10) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 36.297965ms)
Mar 26 18:49:03.283: INFO: (11) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 37.385923ms)
Mar 26 18:49:03.320: INFO: (12) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 37.74257ms)
Mar 26 18:49:03.357: INFO: (13) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 36.601416ms)
Mar 26 18:49:03.394: INFO: (14) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 36.821128ms)
Mar 26 18:49:03.431: INFO: (15) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 36.8471ms)
Mar 26 18:49:03.469: INFO: (16) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 37.974128ms)
Mar 26 18:49:03.505: INFO: (17) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 36.43381ms)
Mar 26 18:49:03.542: INFO: (18) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 36.47084ms)
Mar 26 18:49:03.580: INFO: (19) /api/v1/nodes/bootstrap-e2e-minion-group-gs7c:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="cloud-init.log">cloud-init.log</a>... (200; 37.676673ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:49:03.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2792" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]","total":277,"completed":238,"skipped":4017,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:49:03.655: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:49:03.798: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 26 18:49:07.726: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-6355 create -f -'
Mar 26 18:49:08.509: INFO: stderr: ""
Mar 26 18:49:08.509: INFO: stdout: "e2e-test-crd-publish-openapi-649-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 26 18:49:08.509: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-6355 delete e2e-test-crd-publish-openapi-649-crds test-cr'
Mar 26 18:49:08.745: INFO: stderr: ""
Mar 26 18:49:08.745: INFO: stdout: "e2e-test-crd-publish-openapi-649-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 26 18:49:08.745: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-6355 apply -f -'
Mar 26 18:49:09.168: INFO: stderr: ""
Mar 26 18:49:09.168: INFO: stdout: "e2e-test-crd-publish-openapi-649-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 26 18:49:09.169: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-6355 delete e2e-test-crd-publish-openapi-649-crds test-cr'
Mar 26 18:49:09.395: INFO: stderr: ""
Mar 26 18:49:09.395: INFO: stdout: "e2e-test-crd-publish-openapi-649-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 26 18:49:09.395: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config explain e2e-test-crd-publish-openapi-649-crds'
Mar 26 18:49:09.707: INFO: stderr: ""
Mar 26 18:49:09.707: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-649-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:49:13.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6355" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":277,"completed":239,"skipped":4045,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:49:13.147: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test hostPath mode
Mar 26 18:49:13.334: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-216" to be "Succeeded or Failed"
Mar 26 18:49:13.385: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 50.439622ms
Mar 26 18:49:15.420: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.085762928s
STEP: Saw pod success
Mar 26 18:49:15.420: INFO: Pod "pod-host-path-test" satisfied condition "Succeeded or Failed"
Mar 26 18:49:15.455: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar 26 18:49:15.540: INFO: Waiting for pod pod-host-path-test to disappear
Mar 26 18:49:15.575: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:49:15.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-216" for this suite.
•{"msg":"PASSED [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":240,"skipped":4102,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:49:15.649: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name configmap-test-volume-map-db17bec1-41d4-467b-8f22-1a33ab883e90
STEP: Creating a pod to test consume configMaps
Mar 26 18:49:15.926: INFO: Waiting up to 5m0s for pod "pod-configmaps-86ef8e8e-d635-421a-89cc-c33e504e91b8" in namespace "configmap-1832" to be "Succeeded or Failed"
Mar 26 18:49:15.972: INFO: Pod "pod-configmaps-86ef8e8e-d635-421a-89cc-c33e504e91b8": Phase="Pending", Reason="", readiness=false. Elapsed: 45.934412ms
Mar 26 18:49:18.049: INFO: Pod "pod-configmaps-86ef8e8e-d635-421a-89cc-c33e504e91b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.123771653s
STEP: Saw pod success
Mar 26 18:49:18.050: INFO: Pod "pod-configmaps-86ef8e8e-d635-421a-89cc-c33e504e91b8" satisfied condition "Succeeded or Failed"
Mar 26 18:49:18.086: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-configmaps-86ef8e8e-d635-421a-89cc-c33e504e91b8 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 18:49:18.202: INFO: Waiting for pod pod-configmaps-86ef8e8e-d635-421a-89cc-c33e504e91b8 to disappear
Mar 26 18:49:18.257: INFO: Pod pod-configmaps-86ef8e8e-d635-421a-89cc-c33e504e91b8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:49:18.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1832" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":241,"skipped":4106,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:49:18.336: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name projected-secret-test-c76efc05-2c4b-41f0-9d86-997983b1675d
STEP: Creating a pod to test consume secrets
Mar 26 18:49:18.569: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-41851ff8-3922-4d17-9ed4-293356629816" in namespace "projected-4166" to be "Succeeded or Failed"
Mar 26 18:49:18.604: INFO: Pod "pod-projected-secrets-41851ff8-3922-4d17-9ed4-293356629816": Phase="Pending", Reason="", readiness=false. Elapsed: 34.990419ms
Mar 26 18:49:20.639: INFO: Pod "pod-projected-secrets-41851ff8-3922-4d17-9ed4-293356629816": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.070488177s
STEP: Saw pod success
Mar 26 18:49:20.640: INFO: Pod "pod-projected-secrets-41851ff8-3922-4d17-9ed4-293356629816" satisfied condition "Succeeded or Failed"
Mar 26 18:49:20.687: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-projected-secrets-41851ff8-3922-4d17-9ed4-293356629816 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 26 18:49:20.791: INFO: Waiting for pod pod-projected-secrets-41851ff8-3922-4d17-9ed4-293356629816 to disappear
Mar 26 18:49:20.829: INFO: Pod pod-projected-secrets-41851ff8-3922-4d17-9ed4-293356629816 no longer exists
[AfterEach] [sig-storage] Projected secret
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:49:20.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4166" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":242,"skipped":4111,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:49:20.926: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:178
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:49:21.130: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:49:23.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4132" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":277,"completed":243,"skipped":4134,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:49:23.580: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 26 18:49:23.809: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:49:23.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6639" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":277,"completed":244,"skipped":4136,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:49:24.040: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 18:49:24.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845364, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845364, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845364, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845364, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:49:26.930: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845364, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845364, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845364, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845364, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 18:49:29.973: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:49:30.008: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8488-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:49:31.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6226" for this suite.
STEP: Destroying namespace "webhook-6226-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":277,"completed":245,"skipped":4172,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:49:32.246: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Performing setup for networking test in namespace pod-network-test-4203
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 26 18:49:32.429: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 26 18:49:32.658: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 26 18:49:34.694: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 18:49:36.694: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 18:49:38.693: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 18:49:40.696: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 18:49:42.693: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 18:49:44.695: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 18:49:46.694: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 18:49:48.719: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 26 18:49:50.694: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 26 18:49:50.764: INFO: The status of Pod netserver-1 is Running (Ready = false)
Mar 26 18:49:52.800: INFO: The status of Pod netserver-1 is Running (Ready = true)
Mar 26 18:49:52.870: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Mar 26 18:49:55.196: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.64.1.230 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4203 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:49:55.196: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:49:56.543: INFO: Found all expected endpoints: [netserver-0]
Mar 26 18:49:56.578: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.64.0.91 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4203 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:49:56.578: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:49:57.883: INFO: Found all expected endpoints: [netserver-1]
Mar 26 18:49:57.918: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.64.2.92 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4203 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 18:49:57.918: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:49:59.221: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:49:59.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4203" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":246,"skipped":4206,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:49:59.337: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8763.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8763.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 145.29.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.29.145_udp@PTR;check="$$(dig +tcp +noall +answer +search 145.29.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.29.145_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8763.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8763.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 145.29.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.29.145_udp@PTR;check="$$(dig +tcp +noall +answer +search 145.29.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.29.145_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 26 18:50:01.835: INFO: Unable to read wheezy_udp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:01.874: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:01.913: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:01.953: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:02.239: INFO: Unable to read jessie_udp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:02.278: INFO: Unable to read jessie_tcp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:02.318: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:02.358: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:02.601: INFO: Lookups using dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3 failed for: [wheezy_udp@dns-test-service.dns-8763.svc.cluster.local wheezy_tcp@dns-test-service.dns-8763.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local jessie_udp@dns-test-service.dns-8763.svc.cluster.local jessie_tcp@dns-test-service.dns-8763.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local]

Mar 26 18:50:07.642: INFO: Unable to read wheezy_udp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:07.681: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:07.721: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:07.766: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:08.048: INFO: Unable to read jessie_udp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:08.089: INFO: Unable to read jessie_tcp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:08.128: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:08.168: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:08.414: INFO: Lookups using dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3 failed for: [wheezy_udp@dns-test-service.dns-8763.svc.cluster.local wheezy_tcp@dns-test-service.dns-8763.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local jessie_udp@dns-test-service.dns-8763.svc.cluster.local jessie_tcp@dns-test-service.dns-8763.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local]

Mar 26 18:50:12.641: INFO: Unable to read wheezy_udp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:12.680: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:12.721: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:12.763: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:13.044: INFO: Unable to read jessie_udp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:13.083: INFO: Unable to read jessie_tcp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:13.123: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:13.163: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:13.406: INFO: Lookups using dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3 failed for: [wheezy_udp@dns-test-service.dns-8763.svc.cluster.local wheezy_tcp@dns-test-service.dns-8763.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local jessie_udp@dns-test-service.dns-8763.svc.cluster.local jessie_tcp@dns-test-service.dns-8763.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local]

Mar 26 18:50:17.641: INFO: Unable to read wheezy_udp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:17.681: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:17.721: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:17.761: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:18.046: INFO: Unable to read jessie_udp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:18.085: INFO: Unable to read jessie_tcp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:18.125: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:18.165: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:18.407: INFO: Lookups using dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3 failed for: [wheezy_udp@dns-test-service.dns-8763.svc.cluster.local wheezy_tcp@dns-test-service.dns-8763.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local jessie_udp@dns-test-service.dns-8763.svc.cluster.local jessie_tcp@dns-test-service.dns-8763.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local]

Mar 26 18:50:22.644: INFO: Unable to read wheezy_udp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:22.685: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:22.725: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:22.767: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:23.045: INFO: Unable to read jessie_udp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:23.087: INFO: Unable to read jessie_tcp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:23.126: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:23.166: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:23.406: INFO: Lookups using dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3 failed for: [wheezy_udp@dns-test-service.dns-8763.svc.cluster.local wheezy_tcp@dns-test-service.dns-8763.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local jessie_udp@dns-test-service.dns-8763.svc.cluster.local jessie_tcp@dns-test-service.dns-8763.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local]

Mar 26 18:50:27.641: INFO: Unable to read wheezy_udp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:27.681: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:27.721: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:27.761: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:28.041: INFO: Unable to read jessie_udp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:28.081: INFO: Unable to read jessie_tcp@dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:28.121: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:28.161: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local from pod dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3: the server could not find the requested resource (get pods dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3)
Mar 26 18:50:28.401: INFO: Lookups using dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3 failed for: [wheezy_udp@dns-test-service.dns-8763.svc.cluster.local wheezy_tcp@dns-test-service.dns-8763.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local jessie_udp@dns-test-service.dns-8763.svc.cluster.local jessie_tcp@dns-test-service.dns-8763.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8763.svc.cluster.local]

Mar 26 18:50:33.407: INFO: DNS probes using dns-8763/dns-test-79a671e0-5872-48df-83c3-e5b78cc525e3 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:50:33.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8763" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":277,"completed":247,"skipped":4241,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:50:33.710: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar 26 18:50:33.906: INFO: >>> kubeConfig: /workspace/.kube/config
Mar 26 18:50:37.820: INFO: >>> kubeConfig: /workspace/.kube/config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:50:51.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9955" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":277,"completed":248,"skipped":4279,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:50:51.781: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-117e69e3-a435-4810-91a9-ef3670e3cfd6
STEP: Creating a pod to test consume configMaps
Mar 26 18:50:52.061: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a2087081-8c2d-42e4-bce6-9f4698da3a57" in namespace "projected-9128" to be "Succeeded or Failed"
Mar 26 18:50:52.099: INFO: Pod "pod-projected-configmaps-a2087081-8c2d-42e4-bce6-9f4698da3a57": Phase="Pending", Reason="", readiness=false. Elapsed: 38.147521ms
Mar 26 18:50:54.139: INFO: Pod "pod-projected-configmaps-a2087081-8c2d-42e4-bce6-9f4698da3a57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.078431391s
STEP: Saw pod success
Mar 26 18:50:54.139: INFO: Pod "pod-projected-configmaps-a2087081-8c2d-42e4-bce6-9f4698da3a57" satisfied condition "Succeeded or Failed"
Mar 26 18:50:54.178: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-projected-configmaps-a2087081-8c2d-42e4-bce6-9f4698da3a57 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 18:50:54.291: INFO: Waiting for pod pod-projected-configmaps-a2087081-8c2d-42e4-bce6-9f4698da3a57 to disappear
Mar 26 18:50:54.335: INFO: Pod pod-projected-configmaps-a2087081-8c2d-42e4-bce6-9f4698da3a57 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:50:54.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9128" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":277,"completed":249,"skipped":4280,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:50:54.417: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 18:50:54.619: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5856c462-4476-4d3d-a95d-50921b2ba0f2" in namespace "downward-api-464" to be "Succeeded or Failed"
Mar 26 18:50:54.657: INFO: Pod "downwardapi-volume-5856c462-4476-4d3d-a95d-50921b2ba0f2": Phase="Pending", Reason="", readiness=false. Elapsed: 38.127277ms
Mar 26 18:50:56.696: INFO: Pod "downwardapi-volume-5856c462-4476-4d3d-a95d-50921b2ba0f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076690161s
STEP: Saw pod success
Mar 26 18:50:56.696: INFO: Pod "downwardapi-volume-5856c462-4476-4d3d-a95d-50921b2ba0f2" satisfied condition "Succeeded or Failed"
Mar 26 18:50:56.734: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod downwardapi-volume-5856c462-4476-4d3d-a95d-50921b2ba0f2 container client-container: <nil>
STEP: delete the pod
Mar 26 18:50:56.834: INFO: Waiting for pod downwardapi-volume-5856c462-4476-4d3d-a95d-50921b2ba0f2 to disappear
Mar 26 18:50:56.879: INFO: Pod downwardapi-volume-5856c462-4476-4d3d-a95d-50921b2ba0f2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:50:56.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-464" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":277,"completed":250,"skipped":4312,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:50:56.965: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating pod pod-subpath-test-configmap-4xxq
STEP: Creating a pod to test atomic-volume-subpath
Mar 26 18:50:57.322: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4xxq" in namespace "subpath-7125" to be "Succeeded or Failed"
Mar 26 18:50:57.360: INFO: Pod "pod-subpath-test-configmap-4xxq": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007768ms
Mar 26 18:50:59.399: INFO: Pod "pod-subpath-test-configmap-4xxq": Phase="Running", Reason="", readiness=true. Elapsed: 2.076840036s
Mar 26 18:51:01.438: INFO: Pod "pod-subpath-test-configmap-4xxq": Phase="Running", Reason="", readiness=true. Elapsed: 4.115673089s
Mar 26 18:51:03.477: INFO: Pod "pod-subpath-test-configmap-4xxq": Phase="Running", Reason="", readiness=true. Elapsed: 6.155144579s
Mar 26 18:51:05.516: INFO: Pod "pod-subpath-test-configmap-4xxq": Phase="Running", Reason="", readiness=true. Elapsed: 8.193523082s
Mar 26 18:51:07.556: INFO: Pod "pod-subpath-test-configmap-4xxq": Phase="Running", Reason="", readiness=true. Elapsed: 10.234430896s
Mar 26 18:51:09.595: INFO: Pod "pod-subpath-test-configmap-4xxq": Phase="Running", Reason="", readiness=true. Elapsed: 12.273211412s
Mar 26 18:51:11.634: INFO: Pod "pod-subpath-test-configmap-4xxq": Phase="Running", Reason="", readiness=true. Elapsed: 14.311873873s
Mar 26 18:51:13.672: INFO: Pod "pod-subpath-test-configmap-4xxq": Phase="Running", Reason="", readiness=true. Elapsed: 16.350405785s
Mar 26 18:51:15.713: INFO: Pod "pod-subpath-test-configmap-4xxq": Phase="Running", Reason="", readiness=true. Elapsed: 18.391188042s
Mar 26 18:51:17.752: INFO: Pod "pod-subpath-test-configmap-4xxq": Phase="Running", Reason="", readiness=true. Elapsed: 20.429994407s
Mar 26 18:51:19.791: INFO: Pod "pod-subpath-test-configmap-4xxq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.469005405s
STEP: Saw pod success
Mar 26 18:51:19.791: INFO: Pod "pod-subpath-test-configmap-4xxq" satisfied condition "Succeeded or Failed"
Mar 26 18:51:19.829: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-subpath-test-configmap-4xxq container test-container-subpath-configmap-4xxq: <nil>
STEP: delete the pod
Mar 26 18:51:19.923: INFO: Waiting for pod pod-subpath-test-configmap-4xxq to disappear
Mar 26 18:51:19.961: INFO: Pod pod-subpath-test-configmap-4xxq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4xxq
Mar 26 18:51:19.961: INFO: Deleting pod "pod-subpath-test-configmap-4xxq" in namespace "subpath-7125"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:51:20.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7125" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":277,"completed":251,"skipped":4329,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:51:20.081: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 18:51:21.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845480, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845480, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845480, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845480, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:51:23.099: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845480, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845480, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845480, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845480, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 18:51:26.191: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:51:26.230: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9545-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:51:27.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1953" for this suite.
STEP: Destroying namespace "webhook-1953-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":277,"completed":252,"skipped":4331,"failed":0}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:51:28.200: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar 26 18:51:28.394: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:51:30.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2100" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":277,"completed":253,"skipped":4333,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:51:30.832: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:51:31.025: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 26 18:51:33.912: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-2859 create -f -'
Mar 26 18:51:34.704: INFO: stderr: ""
Mar 26 18:51:34.704: INFO: stdout: "e2e-test-crd-publish-openapi-907-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 26 18:51:34.704: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-2859 delete e2e-test-crd-publish-openapi-907-crds test-cr'
Mar 26 18:51:34.939: INFO: stderr: ""
Mar 26 18:51:34.939: INFO: stdout: "e2e-test-crd-publish-openapi-907-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 26 18:51:34.939: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-2859 apply -f -'
Mar 26 18:51:35.399: INFO: stderr: ""
Mar 26 18:51:35.399: INFO: stdout: "e2e-test-crd-publish-openapi-907-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 26 18:51:35.399: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config --namespace=crd-publish-openapi-2859 delete e2e-test-crd-publish-openapi-907-crds test-cr'
Mar 26 18:51:35.638: INFO: stderr: ""
Mar 26 18:51:35.638: INFO: stdout: "e2e-test-crd-publish-openapi-907-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar 26 18:51:35.638: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config explain e2e-test-crd-publish-openapi-907-crds'
Mar 26 18:51:35.950: INFO: stderr: ""
Mar 26 18:51:35.950: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-907-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:51:39.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2859" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":277,"completed":254,"skipped":4341,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:51:40.004: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating secret with name secret-test-map-817d2020-c65f-495c-9fb6-21ef1816cbd1
STEP: Creating a pod to test consume secrets
Mar 26 18:51:40.243: INFO: Waiting up to 5m0s for pod "pod-secrets-1f48d234-9784-476f-a1a3-5aac9ef7b867" in namespace "secrets-4418" to be "Succeeded or Failed"
Mar 26 18:51:40.282: INFO: Pod "pod-secrets-1f48d234-9784-476f-a1a3-5aac9ef7b867": Phase="Pending", Reason="", readiness=false. Elapsed: 38.339336ms
Mar 26 18:51:42.320: INFO: Pod "pod-secrets-1f48d234-9784-476f-a1a3-5aac9ef7b867": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077129379s
STEP: Saw pod success
Mar 26 18:51:42.320: INFO: Pod "pod-secrets-1f48d234-9784-476f-a1a3-5aac9ef7b867" satisfied condition "Succeeded or Failed"
Mar 26 18:51:42.359: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-secrets-1f48d234-9784-476f-a1a3-5aac9ef7b867 container secret-volume-test: <nil>
STEP: delete the pod
Mar 26 18:51:42.450: INFO: Waiting for pod pod-secrets-1f48d234-9784-476f-a1a3-5aac9ef7b867 to disappear
Mar 26 18:51:42.488: INFO: Pod pod-secrets-1f48d234-9784-476f-a1a3-5aac9ef7b867 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:51:42.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4418" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":277,"completed":255,"skipped":4349,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:51:42.570: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3272, will wait for the garbage collector to delete the pods
Mar 26 18:51:46.982: INFO: Deleting Job.batch foo took: 40.647264ms
Mar 26 18:51:47.682: INFO: Terminating Job.batch foo pods took: 700.269325ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:52:19.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3272" for this suite.
•{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":277,"completed":256,"skipped":4362,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:52:19.603: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Mar 26 18:52:19.759: INFO: >>> kubeConfig: /workspace/.kube/config
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Registering the sample API server.
Mar 26 18:52:20.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:52:22.684: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:52:24.684: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:52:26.684: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:52:28.684: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:52:30.684: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845540, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7996d54f97\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:52:34.866: INFO: Waited 2.140974742s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:52:37.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9562" for this suite.
•{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":277,"completed":257,"skipped":4371,"failed":0}
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:52:37.085: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[AfterEach] [k8s.io] Kubelet
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:52:39.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6839" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":277,"completed":258,"skipped":4377,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:52:39.569: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with configMap that has name projected-configmap-test-upd-3dbaf412-531f-484e-a06b-10dcd793d9d9
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-3dbaf412-531f-484e-a06b-10dcd793d9d9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:52:44.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9212" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":277,"completed":259,"skipped":4441,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:52:44.266: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:219
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating all guestbook components
Mar 26 18:52:44.464: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-slave
  labels:
    app: agnhost
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: slave
    tier: backend

Mar 26 18:52:44.464: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-6986'
Mar 26 18:52:44.966: INFO: stderr: ""
Mar 26 18:52:44.966: INFO: stdout: "service/agnhost-slave created\n"
Mar 26 18:52:44.966: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-master
  labels:
    app: agnhost
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: master
    tier: backend

Mar 26 18:52:44.966: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-6986'
Mar 26 18:52:45.335: INFO: stderr: ""
Mar 26 18:52:45.335: INFO: stdout: "service/agnhost-master created\n"
Mar 26 18:52:45.335: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 26 18:52:45.335: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-6986'
Mar 26 18:52:45.715: INFO: stderr: ""
Mar 26 18:52:45.715: INFO: stdout: "service/frontend created\n"
Mar 26 18:52:45.715: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 26 18:52:45.715: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-6986'
Mar 26 18:52:46.057: INFO: stderr: ""
Mar 26 18:52:46.057: INFO: stdout: "deployment.apps/frontend created\n"
Mar 26 18:52:46.057: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 26 18:52:46.057: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-6986'
Mar 26 18:52:46.465: INFO: stderr: ""
Mar 26 18:52:46.465: INFO: stdout: "deployment.apps/agnhost-master created\n"
Mar 26 18:52:46.465: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: us.gcr.io/k8s-artifacts-prod/e2e-test-images/agnhost:2.12
        args: [ "guestbook", "--slaveof", "agnhost-master", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 26 18:52:46.465: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config create -f - --namespace=kubectl-6986'
Mar 26 18:52:46.807: INFO: stderr: ""
Mar 26 18:52:46.807: INFO: stdout: "deployment.apps/agnhost-slave created\n"
STEP: validating guestbook app
Mar 26 18:52:46.807: INFO: Waiting for all frontend pods to be Running.
Mar 26 18:52:51.908: INFO: Waiting for frontend to serve content.
Mar 26 18:52:51.961: INFO: Trying to add a new entry to the guestbook.
Mar 26 18:52:52.015: INFO: Verifying that added entry can be retrieved.
Mar 26 18:52:52.068: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Mar 26 18:52:57.112: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-6986'
Mar 26 18:52:57.371: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 18:52:57.371: INFO: stdout: "service \"agnhost-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar 26 18:52:57.371: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-6986'
Mar 26 18:52:57.630: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 18:52:57.630: INFO: stdout: "service \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 26 18:52:57.630: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-6986'
Mar 26 18:52:57.879: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 18:52:57.879: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 26 18:52:57.879: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-6986'
Mar 26 18:52:58.126: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 18:52:58.126: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 26 18:52:58.127: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-6986'
Mar 26 18:52:58.351: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 18:52:58.351: INFO: stdout: "deployment.apps \"agnhost-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 26 18:52:58.352: INFO: Running '/workspace/kubernetes/platforms/linux/amd64/kubectl --server=https://34.83.25.82 --kubeconfig=/workspace/.kube/config delete --grace-period=0 --force -f - --namespace=kubectl-6986'
Mar 26 18:52:58.588: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 18:52:58.588: INFO: stdout: "deployment.apps \"agnhost-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:52:58.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6986" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":277,"completed":260,"skipped":4442,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:52:58.668: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-757febfb-98c9-41aa-a307-387bc7177cd7
STEP: Creating a pod to test consume configMaps
Mar 26 18:52:58.946: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-61ff0a84-836a-4e14-a36e-c84d96994779" in namespace "projected-1492" to be "Succeeded or Failed"
Mar 26 18:52:58.996: INFO: Pod "pod-projected-configmaps-61ff0a84-836a-4e14-a36e-c84d96994779": Phase="Pending", Reason="", readiness=false. Elapsed: 49.413641ms
Mar 26 18:53:01.035: INFO: Pod "pod-projected-configmaps-61ff0a84-836a-4e14-a36e-c84d96994779": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.088385563s
STEP: Saw pod success
Mar 26 18:53:01.035: INFO: Pod "pod-projected-configmaps-61ff0a84-836a-4e14-a36e-c84d96994779" satisfied condition "Succeeded or Failed"
Mar 26 18:53:01.073: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-projected-configmaps-61ff0a84-836a-4e14-a36e-c84d96994779 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 18:53:01.165: INFO: Waiting for pod pod-projected-configmaps-61ff0a84-836a-4e14-a36e-c84d96994779 to disappear
Mar 26 18:53:01.204: INFO: Pod pod-projected-configmaps-61ff0a84-836a-4e14-a36e-c84d96994779 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:53:01.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1492" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":261,"skipped":4453,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:53:01.289: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:53:12.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7308" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":277,"completed":262,"skipped":4497,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:53:12.878: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Mar 26 18:53:13.070: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 26 18:54:13.321: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:54:13.359: INFO: Starting informer...
STEP: Starting pod...
Mar 26 18:54:13.457: INFO: Pod is running on bootstrap-e2e-minion-group-1r9w. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar 26 18:54:13.582: INFO: Pod wasn't evicted. Proceeding
Mar 26 18:54:13.582: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar 26 18:55:28.707: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:55:28.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-6944" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":277,"completed":263,"skipped":4514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:55:28.789: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 26 18:55:29.030: INFO: Waiting up to 5m0s for pod "pod-852c9bd5-203b-410b-a6ee-2043a96c4a56" in namespace "emptydir-7455" to be "Succeeded or Failed"
Mar 26 18:55:29.069: INFO: Pod "pod-852c9bd5-203b-410b-a6ee-2043a96c4a56": Phase="Pending", Reason="", readiness=false. Elapsed: 38.014033ms
Mar 26 18:55:31.107: INFO: Pod "pod-852c9bd5-203b-410b-a6ee-2043a96c4a56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076638208s
STEP: Saw pod success
Mar 26 18:55:31.107: INFO: Pod "pod-852c9bd5-203b-410b-a6ee-2043a96c4a56" satisfied condition "Succeeded or Failed"
Mar 26 18:55:31.146: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-852c9bd5-203b-410b-a6ee-2043a96c4a56 container test-container: <nil>
STEP: delete the pod
Mar 26 18:55:31.250: INFO: Waiting for pod pod-852c9bd5-203b-410b-a6ee-2043a96c4a56 to disappear
Mar 26 18:55:31.288: INFO: Pod pod-852c9bd5-203b-410b-a6ee-2043a96c4a56 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:55:31.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7455" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":264,"skipped":4541,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:55:31.369: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 26 18:55:35.930: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 18:55:35.969: INFO: Pod pod-with-prestop-http-hook still exists
Mar 26 18:55:37.970: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 18:55:38.009: INFO: Pod pod-with-prestop-http-hook still exists
Mar 26 18:55:39.970: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 18:55:40.008: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:55:40.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9005" for this suite.
•{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":277,"completed":265,"skipped":4554,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:55:40.133: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:55:40.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9189" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":277,"completed":266,"skipped":4571,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:55:40.666: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 26 18:55:40.992: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3511 /api/v1/namespaces/watch-3511/configmaps/e2e-watch-test-watch-closed 632ba962-e17a-4d63-9a0f-133700f13f68 24866 0 2020-03-26 18:55:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-03-26 18:55:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 18:55:40.993: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3511 /api/v1/namespaces/watch-3511/configmaps/e2e-watch-test-watch-closed 632ba962-e17a-4d63-9a0f-133700f13f68 24867 0 2020-03-26 18:55:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-03-26 18:55:40 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 26 18:55:41.150: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3511 /api/v1/namespaces/watch-3511/configmaps/e2e-watch-test-watch-closed 632ba962-e17a-4d63-9a0f-133700f13f68 24868 0 2020-03-26 18:55:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-03-26 18:55:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 18:55:41.151: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3511 /api/v1/namespaces/watch-3511/configmaps/e2e-watch-test-watch-closed 632ba962-e17a-4d63-9a0f-133700f13f68 24869 0 2020-03-26 18:55:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-03-26 18:55:41 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:55:41.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3511" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":277,"completed":267,"skipped":4575,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:55:41.232: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-b54b80ac-6a5b-4957-84a6-d7a9e22e6f02
STEP: Creating a pod to test consume configMaps
Mar 26 18:55:41.515: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-baa1b829-eeba-4c99-a093-5e2288cda34e" in namespace "projected-5464" to be "Succeeded or Failed"
Mar 26 18:55:41.552: INFO: Pod "pod-projected-configmaps-baa1b829-eeba-4c99-a093-5e2288cda34e": Phase="Pending", Reason="", readiness=false. Elapsed: 37.735923ms
Mar 26 18:55:43.591: INFO: Pod "pod-projected-configmaps-baa1b829-eeba-4c99-a093-5e2288cda34e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076678043s
STEP: Saw pod success
Mar 26 18:55:43.591: INFO: Pod "pod-projected-configmaps-baa1b829-eeba-4c99-a093-5e2288cda34e" satisfied condition "Succeeded or Failed"
Mar 26 18:55:43.630: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-projected-configmaps-baa1b829-eeba-4c99-a093-5e2288cda34e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 18:55:43.722: INFO: Waiting for pod pod-projected-configmaps-baa1b829-eeba-4c99-a093-5e2288cda34e to disappear
Mar 26 18:55:43.760: INFO: Pod pod-projected-configmaps-baa1b829-eeba-4c99-a093-5e2288cda34e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:55:43.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5464" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":277,"completed":268,"skipped":4575,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:55:43.841: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
Mar 26 18:55:43.996: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Creating first CR 
Mar 26 18:55:44.381: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-26T18:55:44Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-03-26T18:55:44Z]] name:name1 resourceVersion:24898 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4c0cf263-4759-4b99-ada3-45c11a129ff9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar 26 18:55:54.424: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-26T18:55:54Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-03-26T18:55:54Z]] name:name2 resourceVersion:24952 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:08bbc207-fc5e-4df2-b732-ffa8c2f68c21] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar 26 18:56:04.467: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-26T18:55:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-03-26T18:56:04Z]] name:name1 resourceVersion:24983 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4c0cf263-4759-4b99-ada3-45c11a129ff9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar 26 18:56:14.511: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-26T18:55:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-03-26T18:56:14Z]] name:name2 resourceVersion:25016 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:08bbc207-fc5e-4df2-b732-ffa8c2f68c21] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar 26 18:56:24.555: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-26T18:55:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-03-26T18:56:04Z]] name:name1 resourceVersion:25050 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4c0cf263-4759-4b99-ada3-45c11a129ff9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar 26 18:56:34.599: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-26T18:55:54Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-03-26T18:56:14Z]] name:name2 resourceVersion:25081 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:08bbc207-fc5e-4df2-b732-ffa8c2f68c21] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:56:44.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-322" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":277,"completed":269,"skipped":4622,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:56:44.779: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 26 18:56:45.183: INFO: Waiting up to 5m0s for pod "pod-8946d7bd-e362-4593-9410-8a8b120e6fb7" in namespace "emptydir-79" to be "Succeeded or Failed"
Mar 26 18:56:45.306: INFO: Pod "pod-8946d7bd-e362-4593-9410-8a8b120e6fb7": Phase="Pending", Reason="", readiness=false. Elapsed: 123.306634ms
Mar 26 18:56:47.345: INFO: Pod "pod-8946d7bd-e362-4593-9410-8a8b120e6fb7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.161553925s
STEP: Saw pod success
Mar 26 18:56:47.345: INFO: Pod "pod-8946d7bd-e362-4593-9410-8a8b120e6fb7" satisfied condition "Succeeded or Failed"
Mar 26 18:56:47.383: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-8946d7bd-e362-4593-9410-8a8b120e6fb7 container test-container: <nil>
STEP: delete the pod
Mar 26 18:56:47.483: INFO: Waiting for pod pod-8946d7bd-e362-4593-9410-8a8b120e6fb7 to disappear
Mar 26 18:56:47.521: INFO: Pod pod-8946d7bd-e362-4593-9410-8a8b120e6fb7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:56:47.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-79" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":277,"completed":270,"skipped":4639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:56:47.602: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 26 18:56:48.033: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1140 /api/v1/namespaces/watch-1140/configmaps/e2e-watch-test-resource-version 0e4c285f-b926-41bf-ab03-d68c699f553f 25142 0 2020-03-26 18:56:47 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-03-26 18:56:47 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 26 18:56:48.034: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1140 /api/v1/namespaces/watch-1140/configmaps/e2e-watch-test-resource-version 0e4c285f-b926-41bf-ab03-d68c699f553f 25143 0 2020-03-26 18:56:47 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-03-26 18:56:47 +0000 UTC FieldsV1 FieldsV1{Raw:*[123 34 102 58 100 97 116 97 34 58 123 34 46 34 58 123 125 44 34 102 58 109 117 116 97 116 105 111 110 34 58 123 125 125 44 34 102 58 109 101 116 97 100 97 116 97 34 58 123 34 102 58 108 97 98 101 108 115 34 58 123 34 46 34 58 123 125 44 34 102 58 119 97 116 99 104 45 116 104 105 115 45 99 111 110 102 105 103 109 97 112 34 58 123 125 125 125 125],}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:56:48.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1140" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":277,"completed":271,"skipped":4670,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:56:48.113: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating a pod to test downward API volume plugin
Mar 26 18:56:48.378: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c92c103-2534-4af8-bef9-8e5bc1a062ba" in namespace "projected-8620" to be "Succeeded or Failed"
Mar 26 18:56:48.431: INFO: Pod "downwardapi-volume-2c92c103-2534-4af8-bef9-8e5bc1a062ba": Phase="Pending", Reason="", readiness=false. Elapsed: 52.561422ms
Mar 26 18:56:50.470: INFO: Pod "downwardapi-volume-2c92c103-2534-4af8-bef9-8e5bc1a062ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.092065156s
STEP: Saw pod success
Mar 26 18:56:50.470: INFO: Pod "downwardapi-volume-2c92c103-2534-4af8-bef9-8e5bc1a062ba" satisfied condition "Succeeded or Failed"
Mar 26 18:56:50.509: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod downwardapi-volume-2c92c103-2534-4af8-bef9-8e5bc1a062ba container client-container: <nil>
STEP: delete the pod
Mar 26 18:56:50.606: INFO: Waiting for pod downwardapi-volume-2c92c103-2534-4af8-bef9-8e5bc1a062ba to disappear
Mar 26 18:56:50.645: INFO: Pod downwardapi-volume-2c92c103-2534-4af8-bef9-8e5bc1a062ba no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:56:50.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8620" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":277,"completed":272,"skipped":4677,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:56:50.725: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating configMap with name projected-configmap-test-volume-map-f13e4e6d-e3c1-4bdd-95b7-4fa85657f675
STEP: Creating a pod to test consume configMaps
Mar 26 18:56:51.010: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-51839ab2-ace7-43da-86ba-70c5ea8304e5" in namespace "projected-4098" to be "Succeeded or Failed"
Mar 26 18:56:51.048: INFO: Pod "pod-projected-configmaps-51839ab2-ace7-43da-86ba-70c5ea8304e5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.017312ms
Mar 26 18:56:53.087: INFO: Pod "pod-projected-configmaps-51839ab2-ace7-43da-86ba-70c5ea8304e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.077567759s
STEP: Saw pod success
Mar 26 18:56:53.087: INFO: Pod "pod-projected-configmaps-51839ab2-ace7-43da-86ba-70c5ea8304e5" satisfied condition "Succeeded or Failed"
Mar 26 18:56:53.127: INFO: Trying to get logs from node bootstrap-e2e-minion-group-1r9w pod pod-projected-configmaps-51839ab2-ace7-43da-86ba-70c5ea8304e5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 26 18:56:53.233: INFO: Waiting for pod pod-projected-configmaps-51839ab2-ace7-43da-86ba-70c5ea8304e5 to disappear
Mar 26 18:56:53.273: INFO: Pod pod-projected-configmaps-51839ab2-ace7-43da-86ba-70c5ea8304e5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:56:53.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4098" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":277,"completed":273,"skipped":4684,"failed":0}

------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:56:53.358: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Creating projection with secret that has name secret-emptykey-test-e473fc6d-004a-455c-b8cf-f63a52132e76
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:56:53.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1299" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":277,"completed":274,"skipped":4684,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:56:53.633: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 26 18:56:54.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845814, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845814, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845814, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845814, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 18:56:56.665: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845814, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845814, loc:(*time.Location)(0x7b271c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845814, loc:(*time.Location)(0x7b271c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845814, loc:(*time.Location)(0x7b271c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-779fdc84d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 26 18:56:59.738: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:57:00.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1804" for this suite.
STEP: Destroying namespace "webhook-1804-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":277,"completed":275,"skipped":4687,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:57:00.881: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: set up a multi version CRD
Mar 26 18:57:01.036: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:57:22.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6636" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":277,"completed":276,"skipped":4696,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:178
STEP: Creating a kubernetes client
Mar 26 18:57:22.911: INFO: >>> kubeConfig: /workspace/.kube/config
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:153
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:703
STEP: creating the pod
Mar 26 18:57:23.070: INFO: PodSpec: initContainers in spec.initContainers
Mar 26 18:58:06.775: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-3dc23cdf-b15e-45f8-a5cf-e0bfa4928f31", GenerateName:"", Namespace:"init-container-5622", SelfLink:"/api/v1/namespaces/init-container-5622/pods/pod-init-3dc23cdf-b15e-45f8-a5cf-e0bfa4928f31", UID:"3c7ead11-d247-4c0e-a86b-907458a4e4c9", ResourceVersion:"25541", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63720845843, loc:(*time.Location)(0x7b271c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"70619161"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00312c7c0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00312c7e0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00312c800), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00312c820)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-xnchp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc005fe6cc0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xnchp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xnchp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xnchp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004e872f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"bootstrap-e2e-minion-group-1r9w", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00155a380), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004e873a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004e873c0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004e873c8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004e873cc), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845843, loc:(*time.Location)(0x7b271c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845843, loc:(*time.Location)(0x7b271c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845843, loc:(*time.Location)(0x7b271c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720845843, loc:(*time.Location)(0x7b271c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.138.0.4", PodIP:"10.64.1.4", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.64.1.4"}}, StartTime:(*v1.Time)(0xc00312c840), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00155a460)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00155a540)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:e004c2cc521c95383aebb1fb5893719aa7a8eae2e7a71f316a4410784edb00a9", ContainerID:"docker://4a6b821040c44022b1758794d45523dd537b4aa3423acca4b1aaa11890a3df31", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00312c8c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00312c880), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc004e8749f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:179
Mar 26 18:58:06.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5622" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":277,"completed":277,"skipped":4713,"failed":0}
SSMar 26 18:58:06.860: INFO: Running AfterSuite actions on all nodes
Mar 26 18:58:06.860: INFO: Running AfterSuite actions on node 1
Mar 26 18:58:06.860: INFO: Skipping dumping logs from cluster

JUnit report was created: /workspace/_artifacts/junit_01.xml
{"msg":"Test Suite completed","total":277,"completed":277,"skipped":4715,"failed":0}

Ran 277 of 4992 Specs in 4687.751 seconds
SUCCESS! -- 277 Passed | 0 Failed | 0 Pending | 4715 Skipped
